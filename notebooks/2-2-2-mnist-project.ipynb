{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Project: Your First Image Classifier\n",
    "\n",
    "**Module 2.2, Lesson 2 (Project)** | CourseAI\n",
    "\n",
    "This is a project notebook \u2014 the main deliverable of the lesson. You will build a complete image classifier from scratch:\n",
    "\n",
    "1. **Load MNIST** with torchvision \u2014 transforms, DataLoaders\n",
    "2. **Build the model** \u2014 MNISTClassifier with three linear layers\n",
    "3. **Write the training loop** \u2014 cross-entropy loss, Adam, accuracy tracking\n",
    "4. **Evaluate on the test set** \u2014 model.eval(), torch.no_grad()\n",
    "5. **Visualize predictions** \u2014 correct and incorrect with confidence scores\n",
    "6. **Build an improved model** \u2014 BatchNorm, Dropout, weight decay; compare\n",
    "\n",
    "Steps 1\u20134 are **guided** (mostly complete code with small blanks). Steps 5\u20136 are **supported** (template provided, more work required).\n",
    "\n",
    "**Estimated time:** 20\u201330 minutes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to import everything and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# For nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Load MNIST\n",
    "\n",
    "MNIST contains 70,000 grayscale images of handwritten digits (0\u20139), each 28\u00d728 pixels. We apply two transforms:\n",
    "\n",
    "- **ToTensor()** \u2014 converts PIL images to tensors and scales pixel values from [0, 255] to [0.0, 1.0]\n",
    "- **Normalize((0.1307,), (0.3081,))** \u2014 normalizes using the MNIST dataset mean and standard deviation\n",
    "\n",
    "Fill in the DataLoader creation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "# Download and load MNIST\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# TODO: Create DataLoaders for train and test sets\n",
    "# train_loader: batch_size=64, shuffle=True\n",
    "# test_loader:  batch_size=64, shuffle=False\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=False\n",
    ")\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples:     {len(test_dataset)}')\n",
    "print(f'Image shape:      {train_dataset[0][0].shape}')\n",
    "print(f'Train batches:    {len(train_loader)}')\n",
    "print(f'Test batches:     {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 10, figsize=(14, 3))\n",
    "\n",
    "for i in range(20):\n",
    "    img, label = train_dataset[i]\n",
    "    row, col = i // 10, i % 10\n",
    "    axes[row, col].imshow(img.squeeze(), cmap='gray')\n",
    "    axes[row, col].set_title(str(label), fontsize=10)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "fig.suptitle('MNIST Sample Images', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Build the Model\n",
    "\n",
    "Your `MNISTClassifier` is a simple dense (fully-connected) network:\n",
    "\n",
    "```\n",
    "Input (1x28x28)\n",
    "  -> Flatten            # 784\n",
    "  -> Linear(784, 256)   # first hidden layer\n",
    "  -> ReLU\n",
    "  -> Linear(256, 128)   # second hidden layer\n",
    "  -> ReLU\n",
    "  -> Linear(128, 10)    # output (one per digit class)\n",
    "```\n",
    "\n",
    "The `__init__` method is provided. Fill in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass\n",
    "        # 1. Flatten the input\n",
    "        # 2. fc1 -> ReLU\n",
    "        # 3. fc2 -> ReLU\n",
    "        # 4. fc3 (no activation \u2014 CrossEntropyLoss handles softmax)\n",
    "        #\n",
    "        # Hint: x = self.relu(self.fc1(x))\n",
    "        pass  # Replace this\n",
    "\n",
    "# Create model and verify\n",
    "model = MNISTClassifier().to(device)\n",
    "\n",
    "# Quick dimension check\n",
    "test_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f'Input shape:  {test_input.shape}')   # [1, 1, 28, 28]\n",
    "print(f'Output shape: {test_output.shape}')  # [1, 10]\n",
    "print(f'Parameters:   {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "if test_output is not None and test_output.shape == torch.Size([1, 10]):\n",
    "    print('\\nDimensions correct! Ready to train.')\n",
    "else:\n",
    "    print('\\nDimension mismatch \u2014 check your forward method.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Training Loop\n",
    "\n",
    "The training loop follows the standard PyTorch pattern:\n",
    "\n",
    "1. Forward pass \u2192 compute loss\n",
    "2. Backward pass \u2192 compute gradients\n",
    "3. Optimizer step \u2192 update weights\n",
    "4. Track metrics\n",
    "\n",
    "We use **CrossEntropyLoss** (combines LogSoftmax + NLLLoss) and **Adam** optimizer with lr=1e-3.\n",
    "\n",
    "Fill in the missing line in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 10\n",
    "\n",
    "# History tracking\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_acc': [],\n",
    "}\n",
    "\n",
    "print(f'Training for {num_epochs} epochs...')\n",
    "print('=' * 65)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training phase ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # TODO: Backward pass and optimizer step\n",
    "        # Hint: three lines \u2014 zero_grad, backward, step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track metrics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    history['train_loss'].append(epoch_loss)\n",
    "    history['train_acc'].append(epoch_acc)\n",
    "\n",
    "    # --- Evaluation phase (computed here for logging, full eval in Step 4) ---\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "    test_acc = test_correct / test_total\n",
    "    history['test_acc'].append(test_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1:2d}/{num_epochs}  '\n",
    "          f'Train Loss: {epoch_loss:.4f}  '\n",
    "          f'Train Acc: {epoch_acc:.1%}  '\n",
    "          f'Test Acc: {test_acc:.1%}')\n",
    "\n",
    "print('=' * 65)\n",
    "print(f'\\nFinal test accuracy: {history[\"test_acc\"][-1]:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Evaluate on the Test Set\n",
    "\n",
    "We already computed test accuracy inside the training loop for logging, but here we do a clean standalone evaluation to confirm the final numbers.\n",
    "\n",
    "Two important patterns:\n",
    "- **model.eval()** \u2014 switches BatchNorm/Dropout to inference mode (matters later in Step 6)\n",
    "- **torch.no_grad()** \u2014 disables gradient computation, saves memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Evaluate model on the test set. Returns accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "test_accuracy = evaluate_model(model, test_loader)\n",
    "print(f'Test accuracy: {test_accuracy:.2%}')\n",
    "print(f'Correctly classified: {int(test_accuracy * len(test_dataset)):,} / {len(test_dataset):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "epochs_range = range(1, num_epochs + 1)\n",
    "\n",
    "# Loss\n",
    "ax1.plot(epochs_range, history['train_loss'], 'o-', linewidth=2, label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(epochs_range, history['train_acc'], 'o-', linewidth=2, label='Train Acc')\n",
    "ax2.plot(epochs_range, history['test_acc'], 's-', linewidth=2, label='Test Acc')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_ylim(0.9, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Visualize Predictions\n",
    "\n",
    "Numbers only tell part of the story. Looking at actual predictions reveals **what** the model gets wrong and **how confident** it is.\n",
    "\n",
    "Your task: fill in the plotting code to show a grid of predictions. Correct predictions get a green title, incorrect get a red title. Show the softmax confidence for each.\n",
    "\n",
    "This is a **supported** step \u2014 the template is provided but you need to fill in more than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions on a batch of test images\n",
    "model.eval()\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    probabilities = torch.softmax(outputs, dim=1)\n",
    "    confidences, predicted = torch.max(probabilities, dim=1)\n",
    "\n",
    "# Move to CPU for plotting\n",
    "images_cpu = images.cpu()\n",
    "labels_cpu = labels.cpu()\n",
    "predicted_cpu = predicted.cpu()\n",
    "confidences_cpu = confidences.cpu()\n",
    "\n",
    "# TODO: Plot a 4x8 grid of predictions\n",
    "#\n",
    "# For each image:\n",
    "# - Show the grayscale image\n",
    "# - Title format: \"pred (conf%)\" e.g. \"7 (98.3%)\"\n",
    "# - If correct: green title\n",
    "# - If incorrect: red title, and include true label: \"pred (conf%) [true]\"\n",
    "#\n",
    "# Hint:\n",
    "#   ax.set_title(title, color='green')  or  color='red'\n",
    "#   Use images_cpu[i].squeeze() with cmap='gray'\n",
    "\n",
    "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "\n",
    "for i in range(32):\n",
    "    row, col = i // 8, i % 8\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    ax.imshow(images_cpu[i].squeeze(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "    pred = predicted_cpu[i].item()\n",
    "    true = labels_cpu[i].item()\n",
    "    conf = confidences_cpu[i].item() * 100\n",
    "\n",
    "    # Your code here: set the title with color based on correctness\n",
    "    pass\n",
    "\n",
    "fig.suptitle('Model Predictions (green=correct, red=incorrect)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show specifically the incorrect predictions (if any)\n",
    "# This helps you understand what the model finds hard\n",
    "\n",
    "model.eval()\n",
    "wrong_images = []\n",
    "wrong_labels = []\n",
    "wrong_preds = []\n",
    "wrong_confs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        confs, preds = torch.max(probs, dim=1)\n",
    "\n",
    "        mask = preds != labels\n",
    "        if mask.any():\n",
    "            wrong_images.append(images[mask].cpu())\n",
    "            wrong_labels.append(labels[mask].cpu())\n",
    "            wrong_preds.append(preds[mask].cpu())\n",
    "            wrong_confs.append(confs[mask].cpu())\n",
    "\n",
    "        if sum(len(w) for w in wrong_images) >= 16:\n",
    "            break\n",
    "\n",
    "wrong_images = torch.cat(wrong_images)[:16]\n",
    "wrong_labels = torch.cat(wrong_labels)[:16]\n",
    "wrong_preds = torch.cat(wrong_preds)[:16]\n",
    "wrong_confs = torch.cat(wrong_confs)[:16]\n",
    "\n",
    "n_wrong = len(wrong_images)\n",
    "cols = min(8, n_wrong)\n",
    "rows = (n_wrong + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(2 * cols, 2.5 * rows))\n",
    "if rows == 1:\n",
    "    axes = axes[np.newaxis, :] if cols > 1 else np.array([[axes]])\n",
    "\n",
    "for i in range(n_wrong):\n",
    "    row, col = i // cols, i % cols\n",
    "    ax = axes[row, col]\n",
    "    ax.imshow(wrong_images[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(\n",
    "        f'Pred: {wrong_preds[i].item()} ({wrong_confs[i].item()*100:.1f}%)\\nTrue: {wrong_labels[i].item()}',\n",
    "        color='red', fontsize=9\n",
    "    )\n",
    "    ax.axis('off')\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(n_wrong, rows * cols):\n",
    "    row, col = i // cols, i % cols\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "fig.suptitle('Incorrect Predictions', fontsize=14, color='red')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Showing {n_wrong} incorrect predictions from the test set.')\n",
    "print('Notice: many of these are genuinely ambiguous \u2014 even humans might disagree.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Build an Improved Model\n",
    "\n",
    "The simple model works, but we can do better with three standard techniques:\n",
    "\n",
    "- **BatchNorm1d** after each linear layer \u2014 normalizes activations, stabilizes training\n",
    "- **Dropout(0.3)** \u2014 randomly zeros 30% of activations during training, reduces overfitting\n",
    "- **weight_decay=1e-4** in the optimizer \u2014 L2 regularization on weights\n",
    "\n",
    "This is a **supported** step. The template gives you the structure, but you fill in the layers.\n",
    "\n",
    "```\n",
    "Input (1x28x28)\n",
    "  -> Flatten                      # 784\n",
    "  -> Linear(784, 256)\n",
    "  -> BatchNorm1d(256)\n",
    "  -> ReLU\n",
    "  -> Dropout(0.3)\n",
    "  -> Linear(256, 128)\n",
    "  -> BatchNorm1d(128)\n",
    "  -> ReLU\n",
    "  -> Dropout(0.3)\n",
    "  -> Linear(128, 10)              # output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: Define the layers\n",
    "        # Same linear layers as MNISTClassifier, but add:\n",
    "        #   - nn.BatchNorm1d(size) after each hidden linear layer\n",
    "        #   - nn.Dropout(0.3) after each ReLU\n",
    "        #\n",
    "        # Layers to define:\n",
    "        #   self.flatten\n",
    "        #   self.fc1, self.bn1\n",
    "        #   self.fc2, self.bn2\n",
    "        #   self.fc3\n",
    "        #   self.relu, self.dropout\n",
    "        pass  # Replace with your layer definitions\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass\n",
    "        # Order for each hidden layer: linear -> batchnorm -> relu -> dropout\n",
    "        # Final layer: just linear (no batchnorm, no relu, no dropout)\n",
    "        pass  # Replace this\n",
    "\n",
    "improved_model = ImprovedMNIST().to(device)\n",
    "\n",
    "# Verify dimensions\n",
    "test_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "test_output = improved_model(test_input)\n",
    "print(f'Input shape:  {test_input.shape}')\n",
    "print(f'Output shape: {test_output.shape}')\n",
    "print(f'Parameters:   {sum(p.numel() for p in improved_model.parameters()):,}')\n",
    "\n",
    "if test_output is not None and test_output.shape == torch.Size([1, 10]):\n",
    "    print('\\nDimensions correct! Ready to train.')\n",
    "else:\n",
    "    print('\\nDimension mismatch \u2014 check your forward method.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the improved model\n",
    "# Note: weight_decay=1e-4 adds L2 regularization\n",
    "improved_criterion = nn.CrossEntropyLoss()\n",
    "improved_optimizer = optim.Adam(improved_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "improved_history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_acc': [],\n",
    "}\n",
    "\n",
    "print(f'Training improved model for {num_epochs} epochs...')\n",
    "print('=' * 65)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    improved_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        improved_optimizer.zero_grad()\n",
    "        outputs = improved_model(images)\n",
    "        loss = improved_criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        improved_optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    improved_history['train_loss'].append(epoch_loss)\n",
    "    improved_history['train_acc'].append(epoch_acc)\n",
    "\n",
    "    # Evaluation\n",
    "    test_acc = evaluate_model(improved_model, test_loader)\n",
    "    improved_history['test_acc'].append(test_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1:2d}/{num_epochs}  '\n",
    "          f'Train Loss: {epoch_loss:.4f}  '\n",
    "          f'Train Acc: {epoch_acc:.1%}  '\n",
    "          f'Test Acc: {test_acc:.1%}')\n",
    "\n",
    "print('=' * 65)\n",
    "print(f'\\nFinal test accuracy: {improved_history[\"test_acc\"][-1]:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side training curves: simple vs improved\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, num_epochs + 1)\n",
    "\n",
    "# Loss comparison\n",
    "ax1.plot(epochs_range, history['train_loss'], 'o-', linewidth=2, label='Simple')\n",
    "ax1.plot(epochs_range, improved_history['train_loss'], 's-', linewidth=2, label='Improved')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Training Loss')\n",
    "ax1.set_title('Training Loss: Simple vs Improved')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Accuracy comparison\n",
    "ax2.plot(epochs_range, history['test_acc'], 'o-', linewidth=2, label='Simple')\n",
    "ax2.plot(epochs_range, improved_history['test_acc'], 's-', linewidth=2, label='Improved')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Test Accuracy')\n",
    "ax2.set_title('Test Accuracy: Simple vs Improved')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_ylim(0.95, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Final comparison of the two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison table\n",
    "simple_acc = history['test_acc'][-1]\n",
    "improved_acc = improved_history['test_acc'][-1]\n",
    "simple_params = sum(p.numel() for p in model.parameters())\n",
    "improved_params = sum(p.numel() for p in improved_model.parameters())\n",
    "\n",
    "print('=' * 55)\n",
    "print(f'{\"Model\":<20} {\"Test Accuracy\":>15} {\"Parameters\":>15}')\n",
    "print('-' * 55)\n",
    "print(f'{\"Simple\":<20} {simple_acc:>14.2%} {simple_params:>15,}')\n",
    "print(f'{\"Improved\":<20} {improved_acc:>14.2%} {improved_params:>15,}')\n",
    "print('-' * 55)\n",
    "\n",
    "diff = improved_acc - simple_acc\n",
    "direction = 'improvement' if diff > 0 else 'decrease'\n",
    "print(f'Difference: {abs(diff):.2%} {direction}')\n",
    "print(f'Extra parameters from BatchNorm: {improved_params - simple_params:,}')\n",
    "print('=' * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What You Built\n",
    "\n",
    "| Step | What Happened |\n",
    "|------|---------------|\n",
    "| **1. Data Loading** | MNIST with transforms, DataLoaders for batching |\n",
    "| **2. Simple Model** | Three linear layers, ReLU activations |\n",
    "| **3. Training Loop** | CrossEntropyLoss, Adam optimizer, epoch tracking |\n",
    "| **4. Evaluation** | model.eval(), torch.no_grad(), test accuracy |\n",
    "| **5. Visualization** | Correct/incorrect predictions with confidence scores |\n",
    "| **6. Improved Model** | BatchNorm + Dropout + weight decay, compared training curves |\n",
    "\n",
    "**Key takeaways:**\n",
    "- The full training pipeline is: data \u2192 model \u2192 loss \u2192 optimizer \u2192 loop \u2192 evaluate\n",
    "- `model.eval()` and `torch.no_grad()` are essential for correct evaluation\n",
    "- BatchNorm and Dropout are nearly free improvements that regularize training\n",
    "- Looking at actual predictions (not just accuracy numbers) reveals what the model struggles with"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}