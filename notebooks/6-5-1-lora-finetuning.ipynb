{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LoRA Fine-Tuning for Diffusion Models\n\n**Module 6.5, Lesson 1** | CourseAI\n\nYou know LoRA from Module 4.4 (highway-and-detour bypass, B=0 initialization, merge at inference). You know the Stable Diffusion pipeline from Module 6.4 (text â†’ CLIP â†’ U-Net denoising loop â†’ VAE decode). This notebook connects them: applying LoRA to the diffusion U-Net for style and subject customization.\n\n**What you will do:**\n- Inspect a real SD U-Net to find cross-attention projection layers and compute LoRA parameter counts for different ranks\n- Execute one LoRA training step by hand: VAE encode, noise, U-Net forward, MSE loss, verify gradient flow\n- Train a style LoRA end-to-end using diffusers + PEFT on real anime-style images from Hugging Face\n- Train a second LoRA and compose both adapters, experimenting with alpha scaling to control the blend\n\n**For each exercise, PREDICT the output before running the cell.**\n\n**Estimated time:** 45â€“60 minutes (Exercises 3â€“4 involve training and download time).\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies and import everything. This notebook requires a GPU for reasonable training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate peft datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float16 if device.type == 'cuda' else torch.float32\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB')\n",
    "\n",
    "print('\\nSetup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Shared Helpers\n\nDefine display helpers and the model ID. Each exercise loads only the components it needs and cleans up afterward to stay within free-tier Colab VRAM (~16 GB on a T4).\n\n> **VRAM tip:** If you encounter an out-of-memory error, go to Runtime â†’ Restart runtime and rerun from Setup. Exercise 3 uses the most VRAM."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model_id = 'stable-diffusion-v1-5/stable-diffusion-v1-5'\n\n\ndef show_images(images, titles, figsize=None):\n    \"\"\"Display a list of PIL images side by side.\"\"\"\n    n = len(images)\n    if figsize is None:\n        figsize = (5 * n, 5)\n    fig, axes = plt.subplots(1, n, figsize=figsize)\n    if n == 1:\n        axes = [axes]\n    for ax, img, title in zip(axes, images, titles):\n        ax.imshow(np.array(img))\n        ax.set_title(title, fontsize=10)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n\ndef show_image_grid(images, titles, nrows, ncols, figsize=None, suptitle=None):\n    \"\"\"Display images in a grid with the given number of rows and columns.\"\"\"\n    if figsize is None:\n        figsize = (4 * ncols, 4 * nrows)\n    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n    axes_flat = axes.flat if nrows > 1 or ncols > 1 else [axes]\n    for ax, img, title in zip(axes_flat, images, titles):\n        ax.imshow(np.array(img))\n        ax.set_title(title, fontsize=10)\n        ax.axis('off')\n    for ax in list(axes_flat)[len(images):]:\n        ax.axis('off')\n    if suptitle:\n        plt.suptitle(suptitle, fontsize=13)\n    plt.tight_layout()\n    plt.show()\n\n\nprint('Helpers defined.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Inspect LoRA Target Layers [Guided]\n",
    "\n",
    "The lesson explained that LoRA for diffusion targets the **cross-attention projection matrices** (W_Q, W_K, W_V, W_out) because that is where text meaning meets spatial features. But which layers are those, concretely? How many parameters would a rank-4 LoRA add versus rank-16?\n",
    "\n",
    "This exercise inspects the real U-Net to ground the architectural understanding in actual layer names and parameter counts.\n",
    "\n",
    "**Before running, predict:**\n",
    "- How many cross-attention blocks does the SD v1.5 U-Net have? (Hint: cross-attention occurs at attention resolutions 16Ã—16 and 32Ã—32, in both the downsampling and upsampling paths.)\n",
    "- For a projection matrix of shape [320, 768], how many LoRA parameters does rank-4 add? (Think: B is [320, 4], A is [4, 768]. Total = ?)\n",
    "- What fraction of total U-Net parameters will a rank-4 LoRA represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from diffusers import UNet2DConditionModel\n\n# Load only the U-Net for parameter inspection--no need for the full pipeline.\n# This saves ~4 GB of VRAM (no VAE, no CLIP, no safety checker).\nprint('Loading U-Net for inspection...')\nunet = UNet2DConditionModel.from_pretrained(model_id, subfolder='unet', torch_dtype=dtype).to(device)\nprint(f'U-Net loaded on {device}.')\n\n# Step 1: Find all cross-attention projection layers.\n# In the SD U-Net, cross-attention layers have names containing 'attn2'\n# (self-attention is 'attn1'). The projection matrices are:\n#   to_q, to_k, to_v  (input projections)\n#   to_out.0           (output projection)\n\ncross_attn_layers = {}\nfor name, param in unet.named_parameters():\n    if 'attn2' in name and any(proj in name for proj in ['to_q', 'to_k', 'to_v', 'to_out.0.weight']):\n        cross_attn_layers[name] = param.shape\n\nprint(f'Cross-attention projection layers found: {len(cross_attn_layers)}')\nprint(f'\\n{\"Layer Name\":<75} {\"Shape\":>15}')\nprint('-' * 92)\nfor name, shape in sorted(cross_attn_layers.items()):\n    print(f'{name:<75} {str(list(shape)):>15}')\n\n# Step 2: Count LoRA parameters for different ranks.\n# For each weight matrix W of shape [out_features, in_features],\n# LoRA adds B [out_features, r] and A [r, in_features].\n# Total LoRA params per layer = r * (out_features + in_features)\n\nprint('\\n' + '=' * 60)\nprint('LoRA Parameter Counts by Rank')\nprint('=' * 60)\n\nfor rank in [4, 8, 16]:\n    total_lora_params = 0\n    for name, shape in cross_attn_layers.items():\n        out_features, in_features = shape[0], shape[1]\n        lora_params = rank * (out_features + in_features)\n        total_lora_params += lora_params\n    print(f'\\nRank {rank}:')\n    print(f'  Total LoRA params:  {total_lora_params:>12,}')\n    print(f'  In MB (fp16):       {total_lora_params * 2 / 1024**2:>12.2f} MB')\n\n# Step 3: Compare to total U-Net parameters.\ntotal_unet_params = sum(p.numel() for p in unet.parameters())\ncross_attn_params = sum(shape[0] * shape[1] for shape in cross_attn_layers.values())\n\nprint(f'\\n{\"=\" * 60}')\nprint('U-Net Parameter Breakdown')\nprint(f'{\"=\" * 60}')\nprint(f'  Total U-Net params:         {total_unet_params:>12,}')\nprint(f'  Cross-attn projection params:{cross_attn_params:>12,} ({100*cross_attn_params/total_unet_params:.1f}%)')\nprint(f'  Everything else (frozen):    {total_unet_params - cross_attn_params:>12,} ({100*(total_unet_params - cross_attn_params)/total_unet_params:.1f}%)')\n\n# LoRA as fraction of total U-Net\nfor rank in [4, 8, 16]:\n    total_lora = sum(\n        rank * (s[0] + s[1]) for s in cross_attn_layers.values()\n    )\n    pct = 100 * total_lora / total_unet_params\n    print(f'  Rank-{rank} LoRA params:       {total_lora:>12,} ({pct:.3f}% of U-Net)')\n\n# Clean up--free VRAM before Exercise 2.\ndel unet\ntorch.cuda.empty_cache() if device.type == 'cuda' else None\nprint('\\nU-Net freed from VRAM.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You inspected the real SD v1.5 U-Net and found its cross-attention projection layers by name. Key observations:\n",
    "\n",
    "1. **Cross-attention layers are named `attn2`** in the diffusers implementation (self-attention is `attn1`). The projections are `to_q`, `to_k`, `to_v`, and `to_out.0`. This maps directly to the W_Q, W_K, W_V, and W_out from the lesson.\n",
    "\n",
    "2. **The projection shapes vary by resolution.** At lower resolutions (deeper in the U-Net), the channel dimension is larger (640, 1280), so the projection matrices are bigger. At higher resolutions (32Ã—32), the channel dimension is smaller (320).\n",
    "\n",
    "3. **LoRA adds a tiny fraction of the total parameters.** A rank-4 LoRA on all cross-attention projections adds roughly 0.1â€“0.3% of the U-Net's total parameters. That is the \"surgical\" nature of LoRAâ€”a small detour on a massive highway. This is why LoRA files are typically just 2â€“50 MB.\n",
    "\n",
    "4. **Rank scales linearly.** Rank-16 has exactly 4Ã— the parameters of rank-4 (four times as many columns in B and rows in A). The lesson noted that rank 4â€“8 is the sweet spot for diffusionâ€”enough capacity for style adaptation without overfitting on small datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: One LoRA Training Step by Hand [Guided]\n",
    "\n",
    "The lesson traced one training step in pseudocode: load image â†’ VAE encode â†’ sample timestep â†’ add noise â†’ U-Net predicts noise (only LoRA params have gradients) â†’ MSE loss â†’ backprop into LoRA params only.\n",
    "\n",
    "Now you will do it for real. This exercise verifies the training loop from the lesson with actual tensors and real gradient flow.\n",
    "\n",
    "**Before running, predict:**\n",
    "- After VAE encoding, what shape will the latent z_0 have for a 512Ã—512 image? (Think: 512/8 = 64, and the VAE's latent space has 4 channels.)\n",
    "- After the forward pass, should the U-Net's **base** weight parameters have gradients? Should the LoRA adapter parameters have gradients?\n",
    "- What is the loss function? What are its two arguments? (The model predicts ÎµÌ‚, the target is the actual noise Îµ.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from peft import LoraConfig, get_peft_model\nfrom diffusers import UNet2DConditionModel, AutoencoderKL, DDPMScheduler\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n# We work in float32 for this exercise to get clean gradients.\nprint('Loading components in float32 for training step...')\nunet_f32 = UNet2DConditionModel.from_pretrained(model_id, subfolder='unet', torch_dtype=torch.float32).to(device)\nvae = AutoencoderKL.from_pretrained(model_id, subfolder='vae', torch_dtype=torch.float32).to(device)\ntokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder='tokenizer')\ntext_encoder = CLIPTextModel.from_pretrained(model_id, subfolder='text_encoder', torch_dtype=torch.float32).to(device)\nnoise_scheduler = DDPMScheduler.from_pretrained(model_id, subfolder='scheduler')\n\n# Freeze everything: VAE, text encoder, and U-Net base weights.\nvae.requires_grad_(False)\ntext_encoder.requires_grad_(False)\n\nprint('\\nComponents loaded.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply LoRA to the U-Net.\n",
    "# We target the cross-attention projections (attn2) with rank 4.\n",
    "# PEFT uses target_modules to specify which layers get LoRA adapters.\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                           # Rank 4 â€” the small detour\n",
    "    lora_alpha=4,                  # Alpha = rank, so scaling is 1.0\n",
    "    target_modules=[\n",
    "        'to_q', 'to_k', 'to_v', 'to_out.0',  # Cross-attention projections\n",
    "    ],\n",
    "    lora_dropout=0.0,\n",
    ")\n",
    "\n",
    "unet_lora = get_peft_model(unet_f32, lora_config)\n",
    "\n",
    "# Print trainable vs total parameters.\n",
    "trainable_params = sum(p.numel() for p in unet_lora.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in unet_lora.parameters())\n",
    "print(f'Trainable LoRA params: {trainable_params:,}')\n",
    "print(f'Total U-Net params:    {total_params:,}')\n",
    "print(f'Trainable fraction:    {100 * trainable_params / total_params:.3f}%')\n",
    "print(f'\\nThis matches Exercise 1\\'s rank-4 count. PEFT applied LoRA to exactly')\n",
    "print(f'the cross-attention projections we identified.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a synthetic training sample.\n",
    "# In a real training run, this would be a (image, caption) pair from your dataset.\n",
    "# Here we use a random image to focus on the mechanics of the training step.\n",
    "\n",
    "# \"Training image\" â€” a random 512x512 image (standing in for a real one)\n",
    "train_image = torch.randn(1, 3, 512, 512, device=device)\n",
    "caption = \"a watercolor painting of a village\"\n",
    "\n",
    "# Step 3: VAE encode â€” compress the image to latent space.\n",
    "# The VAE is frozen. We detach and scale by the VAE's scaling factor.\n",
    "with torch.no_grad():\n",
    "    latent_dist = vae.encode(train_image)\n",
    "    z_0 = latent_dist.latent_dist.sample() * vae.config.scaling_factor\n",
    "\n",
    "print(f'Input image shape:  {list(train_image.shape)}')\n",
    "print(f'Latent z_0 shape:   {list(z_0.shape)}')\n",
    "print(f'VAE scaling factor: {vae.config.scaling_factor}')\n",
    "print(f'\\n8x spatial compression: 512/8 = 64. Four latent channels.')\n",
    "print(f'This is the same VAE encoding from \"From Pixels to Latents.\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Sample a random timestep and add noise.\n",
    "# This is the forward process from \"The Forward Process\":\n",
    "#   z_t = sqrt(alpha_bar_t) * z_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
    "\n",
    "timestep = torch.tensor([500], device=device)  # Middle of the schedule\n",
    "\n",
    "# Sample the noise that we will try to predict\n",
    "epsilon = torch.randn_like(z_0)\n",
    "\n",
    "# Add noise using the scheduler's closed-form formula\n",
    "z_t = noise_scheduler.add_noise(z_0, epsilon, timestep)\n",
    "\n",
    "print(f'Timestep:       t = {timestep.item()}')\n",
    "print(f'Noise shape:    {list(epsilon.shape)}')\n",
    "print(f'Noised latent:  {list(z_t.shape)}')\n",
    "print(f'\\nThe noise epsilon is the TARGET. The U-Net will try to predict it.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Encode the caption with frozen CLIP.\n",
    "\n",
    "tokens = tokenizer(\n",
    "    caption,\n",
    "    padding='max_length',\n",
    "    max_length=tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(tokens.input_ids.to(device))[0]\n",
    "\n",
    "print(f'Caption: \"{caption}\"')\n",
    "print(f'Token IDs shape:      {list(tokens.input_ids.shape)}')\n",
    "print(f'Text embeddings shape: {list(text_embeddings.shape)}')\n",
    "print(f'\\n77 tokens (padded), 768-dim CLIP embeddings.')\n",
    "print(f'These become the K and V inputs to cross-attention.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: U-Net forward pass â€” predict the noise.\n",
    "# Only LoRA adapter parameters have gradients. Base weights are frozen.\n",
    "\n",
    "epsilon_hat = unet_lora(z_t, timestep, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "print(f'Predicted noise shape: {list(epsilon_hat.shape)}')\n",
    "print(f'Target noise shape:    {list(epsilon.shape)}')\n",
    "print(f'Shapes match: {epsilon_hat.shape == epsilon.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Compute MSE loss â€” same loss from DDPM training.\n",
    "loss = F.mse_loss(epsilon_hat, epsilon)\n",
    "\n",
    "print(f'MSE Loss: {loss.item():.4f}')\n",
    "print(f'\\nThis is the same loss from \"Learning to Denoise\":')\n",
    "print(f'  L = MSE(epsilon, epsilon_hat)')\n",
    "print(f'The model predicted the noise. The loss measures how close it was.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Backprop and verify gradient flow.\n",
    "# This is the critical verification: gradients should flow ONLY through\n",
    "# LoRA adapter parameters, NOT through base U-Net weights.\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# Check LoRA parameters: they SHOULD have gradients.\n",
    "lora_with_grad = 0\n",
    "lora_without_grad = 0\n",
    "for name, param in unet_lora.named_parameters():\n",
    "    if 'lora_' in name and param.requires_grad:\n",
    "        if param.grad is not None and param.grad.abs().sum() > 0:\n",
    "            lora_with_grad += 1\n",
    "        else:\n",
    "            lora_without_grad += 1\n",
    "\n",
    "# Check base parameters: they should NOT have gradients.\n",
    "base_with_grad = 0\n",
    "base_without_grad = 0\n",
    "for name, param in unet_lora.named_parameters():\n",
    "    if 'lora_' not in name:\n",
    "        if param.grad is not None and param.grad.abs().sum() > 0:\n",
    "            base_with_grad += 1\n",
    "        else:\n",
    "            base_without_grad += 1\n",
    "\n",
    "print('=== Gradient Flow Verification ===')\n",
    "print(f'\\nLoRA adapter parameters:')\n",
    "print(f'  With gradients:    {lora_with_grad}')\n",
    "print(f'  Without gradients: {lora_without_grad}')\n",
    "print(f'\\nBase U-Net parameters:')\n",
    "print(f'  With gradients:    {base_with_grad}  (should be 0)')\n",
    "print(f'  Without gradients: {base_without_grad}')\n",
    "\n",
    "if base_with_grad == 0 and lora_with_grad > 0:\n",
    "    print(f'\\n*** Gradient flow is correct. ***')\n",
    "    print(f'Only LoRA adapters receive gradients. The highway is frozen.')\n",
    "    print(f'The detour is learning.')\n",
    "else:\n",
    "    print(f'\\nSomething unexpected happened. Check the setup.')\n",
    "\n",
    "# Show a sample LoRA gradient to prove it is non-trivial\n",
    "for name, param in unet_lora.named_parameters():\n",
    "    if 'lora_' in name and param.grad is not None and param.grad.abs().sum() > 0:\n",
    "        print(f'\\nSample gradient: {name}')\n",
    "        print(f'  Param shape:     {list(param.shape)}')\n",
    "        print(f'  Grad norm:       {param.grad.norm().item():.6f}')\n",
    "        print(f'  Grad mean:       {param.grad.mean().item():.8f}')\n",
    "        print(f'  Grad std:        {param.grad.std().item():.8f}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You executed one complete diffusion LoRA training step, matching the pseudocode from the lesson:\n",
    "\n",
    "1. **VAE encode** the training image: [3, 512, 512] â†’ [4, 64, 64]. The VAE is frozenâ€”it just compresses the image to latent space.\n",
    "2. **Sample timestep t=500** and add noise using the forward process closed-form formula. The noise Îµ is the target.\n",
    "3. **CLIP encode** the caption: \"a watercolor painting of a village\" â†’ [1, 77, 768]. The text encoder is frozen.\n",
    "4. **U-Net forward pass**: the noised latent z_t, timestep t, and text embeddings go in. The predicted noise ÎµÌ‚ comes out.\n",
    "5. **MSE loss**: L = MSE(Îµ, ÎµÌ‚). Same loss from DDPM training in \"Learning to Denoise.\"\n",
    "6. **Backprop**: gradients flow only through LoRA adapter parameters. Base U-Net weights have **zero gradients**.\n",
    "\n",
    "The critical verification: `base_with_grad == 0`. The highway is frozen. Only the detour learns. This is exactly the LoRA training pattern from Module 4.4, applied to a different highway.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the float32 models to free VRAM before Exercise 3.\n",
    "del unet_lora, unet_f32, vae, text_encoder\n",
    "torch.cuda.empty_cache() if device.type == 'cuda' else None\n",
    "print('Cleaned up float32 models. VRAM freed for Exercise 3.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 3: Train a Style LoRA [Supported]\n\nNow you will train a LoRA end-to-end on a **real dataset** with a visually distinctive style. We will use `lambdalabs/naruto-blip-captions`â€”a dataset of ~1000 anime-style Naruto images with captions. We take a subset of 30 images to keep download and training fast.\n\nThe goal: train a LoRA that shifts the base model's output toward an anime/Naruto art style, then compare before-and-after generations to see the style effect.\n\nYou will:\n1. Load a subset of the naruto-blip-captions dataset from Hugging Face\n2. Set up the LoRA training loop using diffusers + PEFT\n3. Train for a few hundred steps\n4. Generate images with and without the LoRA to see the style effect\n\nThe training loop is the same step you did by hand in Exercise 2, repeated many times with real training images.\n\n**Hints:**\n- Each TODO is 1â€“3 lines of code.\n- The patterns are identical to Exercise 2 (VAE encode, noise, U-Net forward, MSE loss).\n- If you get stuck, the solution is below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL, DDPMScheduler, StableDiffusionPipeline\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from torchvision import transforms\n",
    "import gc\n",
    "\n",
    "# Load the pipeline components fresh for training.\n",
    "print('Loading pipeline components for training...')\n",
    "unet = UNet2DConditionModel.from_pretrained(model_id, subfolder='unet', torch_dtype=torch.float32).to(device)\n",
    "vae = AutoencoderKL.from_pretrained(model_id, subfolder='vae', torch_dtype=torch.float32).to(device)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder='tokenizer')\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder='text_encoder', torch_dtype=torch.float32).to(device)\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(model_id, subfolder='scheduler')\n",
    "\n",
    "# Freeze VAE and text encoder â€” they do not train.\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "print('Components loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load a real style dataset from Hugging Face.\n# lambdalabs/naruto-blip-captions has ~1000 anime-style images with captions.\n# We take a subset of 30 images to keep training fast while still learning\n# a visible style shift.\n\nfrom datasets import load_dataset\nfrom torchvision import transforms\n\nprint('Loading naruto-blip-captions dataset...')\ndataset = load_dataset('lambdalabs/naruto-blip-captions', split='train')\n\n# Take a subset of 30 images for fast training.\nnum_train_images = 30\ndataset = dataset.shuffle(seed=42).select(range(num_train_images))\n\n# Show a few examples to see the style we are training on.\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\nfor i, ax in enumerate(axes):\n    ax.imshow(dataset[i]['image'])\n    ax.set_title(dataset[i]['text'][:40] + '...', fontsize=8)\n    ax.axis('off')\nplt.suptitle('Training Data: Naruto Anime Style', fontsize=13)\nplt.tight_layout()\nplt.show()\n\n# Preprocess: resize to 512x512 and normalize to [-1, 1].\nimage_transform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5]),\n])\n\ntrain_images = []\ntrain_captions = []\nfor sample in dataset:\n    img_tensor = image_transform(sample['image'].convert('RGB'))\n    train_images.append(img_tensor)\n    train_captions.append(sample['text'])\n\nprint(f'\\nLoaded {len(train_images)} training images.')\nprint(f'Image tensor shape: {list(train_images[0].shape)}')\nprint(f'Value range: [{train_images[0].min():.2f}, {train_images[0].max():.2f}]')\nprint(f'Sample caption: \"{train_captions[0]}\"')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the U-Net â€” same config as Exercise 2.\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=4,\n",
    "    target_modules=['to_q', 'to_k', 'to_v', 'to_out.0'],\n",
    "    lora_dropout=0.0,\n",
    ")\n",
    "\n",
    "unet_lora = get_peft_model(unet, lora_config)\n",
    "unet_lora.print_trainable_parameters()\n",
    "\n",
    "# Set up the optimizer â€” only LoRA parameters are trainable.\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in unet_lora.parameters() if p.requires_grad],\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-2,\n",
    ")\n",
    "\n",
    "print(f'\\nOptimizer targets {sum(1 for p in unet_lora.parameters() if p.requires_grad)} parameter groups.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training loop--this is Exercise 2's single step, repeated.\n# Fill in the TODOs. Each is 1-3 lines.\n\nnum_steps = 300\nlosses = []\n\nunet_lora.train()\nprint(f'Training for {num_steps} steps on naruto anime-style images...')\n\nfor step in range(num_steps):\n    # Pick a random training sample\n    idx = step % len(train_images)\n    pixel_values = train_images[idx].unsqueeze(0).to(device)  # [1, 3, 512, 512]\n    caption = train_captions[idx]\n\n    # TODO 1: Encode the image with the frozen VAE.\n    # Use vae.encode(pixel_values), get the latent distribution,\n    # sample from it, and multiply by vae.config.scaling_factor.\n    # Wrap in torch.no_grad() since the VAE is frozen.\n    # (This is identical to Exercise 2, Step 3.)\n    z_0 = None  # Replace this line\n\n    # TODO 2: Sample a random timestep and add noise.\n    # Sample a random timestep between 0 and num_train_timesteps.\n    # Generate random noise with torch.randn_like(z_0).\n    # Use noise_scheduler.add_noise(z_0, noise, timesteps) to create z_t.\n    noise = None     # Replace this line\n    timesteps = None  # Replace this line\n    z_t = None       # Replace this line\n\n    # Encode the caption with frozen CLIP.\n    tokens = tokenizer(\n        caption,\n        padding='max_length',\n        max_length=tokenizer.model_max_length,\n        truncation=True,\n        return_tensors='pt',\n    )\n    with torch.no_grad():\n        text_emb = text_encoder(tokens.input_ids.to(device))[0]\n\n    # TODO 3: U-Net forward pass and MSE loss.\n    # Pass z_t, timesteps, and text_emb through the U-Net.\n    # Compute MSE loss between the predicted noise and the actual noise.\n    # (This is identical to Exercise 2, Steps 6-7.)\n    noise_pred = None  # Replace this line\n    loss = None        # Replace this line\n\n    # Backprop and optimizer step\n    if loss is not None:\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n\n        if step % 50 == 0 or step == num_steps - 1:\n            print(f'  Step {step:>4d}/{num_steps}: loss = {loss.item():.4f}')\n    else:\n        print('Fill in the TODOs above to start training.')\n        break\n\nif losses:\n    print(f'\\nTraining complete. Final loss: {losses[-1]:.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The training loop is Exercise 2's single step, repeated with real data. Every TODO maps to something you already did.\n",
    "\n",
    "**TODO 1 â€” VAE encode:**\n",
    "```python\n",
    "    with torch.no_grad():\n",
    "        z_0 = vae.encode(pixel_values).latent_dist.sample() * vae.config.scaling_factor\n",
    "```\n",
    "Why `torch.no_grad()`? The VAE is frozenâ€”we do not need gradients through it. The `latent_dist.sample()` draws from the learned Gaussian, and the scaling factor normalizes the latent space.\n",
    "\n",
    "**TODO 2 â€” Sample timestep and add noise:**\n",
    "```python\n",
    "    noise = torch.randn_like(z_0)\n",
    "    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (1,), device=device)\n",
    "    z_t = noise_scheduler.add_noise(z_0, noise, timesteps)\n",
    "```\n",
    "The timestep is sampled uniformly from the schedule. `add_noise` applies the closed-form forward process: z_t = sqrt(alpha_bar_t) * z_0 + sqrt(1 - alpha_bar_t) * noise.\n",
    "\n",
    "**TODO 3 â€” Forward pass and loss:**\n",
    "```python\n",
    "    noise_pred = unet_lora(z_t, timesteps, encoder_hidden_states=text_emb).sample\n",
    "    loss = F.mse_loss(noise_pred, noise)\n",
    "```\n",
    "The U-Net predicts the noise that was added. MSE measures how close the prediction is to the actual noise. Only LoRA params have gradients, so backprop only updates the detour.\n",
    "\n",
    "**Common mistakes:**\n",
    "- Forgetting `torch.no_grad()` around the VAE encode. This wastes memory tracking gradients through a frozen model and can cause OOM errors.\n",
    "- Using `timesteps` as an integer instead of a tensor. The scheduler expects a tensor.\n",
    "- Confusing the loss direction: `F.mse_loss(noise_pred, noise)`, not `F.mse_loss(noise, noise_pred)`. Both give the same MSE, but the convention is (predicted, target).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot the training loss curve.\nif losses:\n    fig, ax = plt.subplots(figsize=(10, 4))\n    ax.plot(losses, alpha=0.3, color='cyan', label='Per-step loss')\n    # Smoothed version\n    window = min(20, len(losses))\n    if len(losses) >= window:\n        smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')\n        ax.plot(range(window-1, len(losses)), smoothed, color='cyan', linewidth=2, label=f'Smoothed ({window}-step avg)')\n    ax.set_xlabel('Training Step')\n    ax.set_ylabel('MSE Loss')\n    ax.set_title('LoRA Training Loss (Naruto Style)')\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n    \n    print(f'Starting loss: {losses[0]:.4f}')\n    print(f'Final loss:    {losses[-1]:.4f}')\n    print(f'\\nThe loss should decrease, showing the LoRA adapters are learning')\n    print(f'the anime art style from the naruto dataset.')\nelse:\n    print('No losses recorded. Fill in the TODOs in the training loop.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate images WITH the LoRA to see the style effect.\nif losses:\n    from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n    import gc\n\n    # VRAM-conscious sequencing: free training components before building\n    # inference pipelines. On a T4 (16 GB), having training tensors plus\n    # two full pipelines simultaneously risks OOM.\n    del unet, vae, text_encoder\n    gc.collect()\n    torch.cuda.empty_cache() if device.type == 'cuda' else None\n\n    # Build a pipeline using our LoRA-adapted U-Net.\n    unet_lora.eval()\n\n    # Merge the LoRA weights into the base model for inference.\n    # This is the merge-at-inference pattern from Module 4.4:\n    #   W_merged = W + BA * (alpha/r)\n    # After merging, there is zero inference overhead.\n    unet_lora.merge_adapter()\n\n    # Create a pipeline with the LoRA-merged U-Net.\n    lora_pipe = StableDiffusionPipeline.from_pretrained(\n        model_id,\n        unet=unet_lora,\n        torch_dtype=torch.float32,\n        safety_checker=None,\n        requires_safety_checker=False,\n    ).to(device)\n    lora_pipe.scheduler = DPMSolverMultistepScheduler.from_config(lora_pipe.scheduler.config)\n\n    # Generate with the same seed, same prompt--a prompt unrelated to the\n    # training data, so the style shift is clearly from the LoRA, not the prompt.\n    prompts = [\n        \"a landscape with mountains and a river\",\n        \"a cat sitting in a garden\",\n    ]\n\n    # Generate LoRA images first, then free the pipeline before loading the\n    # clean baseline. This avoids two full pipelines in VRAM simultaneously.\n    lora_images = {}\n    for prompt in prompts:\n        seed = 42\n        gen = torch.Generator(device=device).manual_seed(seed)\n        lora_images[prompt] = lora_pipe(\n            prompt, guidance_scale=7.5, num_inference_steps=25,\n            generator=gen, height=512, width=512,\n        ).images[0]\n\n    # Save the LoRA weights for Exercise 4 (composition).\n    # Unmerge first so we have the adapter weights separate from the base.\n    unet_lora.unmerge_adapter()\n    unet_lora.save_pretrained('/tmp/naruto_lora')\n    print('LoRA adapter saved to /tmp/naruto_lora for Exercise 4.')\n\n    # Free the LoRA pipeline before loading the clean one.\n    del lora_pipe\n    gc.collect()\n    torch.cuda.empty_cache() if device.type == 'cuda' else None\n\n    # Now load a clean pipeline without LoRA for baseline comparison.\n    clean_pipe = StableDiffusionPipeline.from_pretrained(\n        model_id,\n        torch_dtype=dtype,\n        safety_checker=None,\n        requires_safety_checker=False,\n    ).to(device)\n    clean_pipe.scheduler = DPMSolverMultistepScheduler.from_config(clean_pipe.scheduler.config)\n\n    for prompt in prompts:\n        seed = 42\n        gen = torch.Generator(device=device).manual_seed(seed)\n        img_without = clean_pipe(\n            prompt, guidance_scale=7.5, num_inference_steps=25,\n            generator=gen, height=512, width=512,\n        ).images[0]\n\n        show_images(\n            [img_without, lora_images[prompt]],\n            ['Without LoRA (base model)', 'With LoRA (naruto anime style)'],\n            figsize=(12, 6),\n        )\n        print(f'Prompt: \"{prompt}\"\\n')\n\n    print('The LoRA was trained on 30 naruto anime-style images.')\n    print('Even with this small dataset and short training, the style shift')\n    print('should be visible: flatter colors, anime-like rendering, stylized shapes.')\n    print('The prompt mentions landscapes and cats--subjects NOT in the training data.')\n    print('The style transferred because the LoRA modified the cross-attention')\n    print('projections that process ALL text-to-image mappings, not just anime prompts.')\n\n    del clean_pipe\n    torch.cuda.empty_cache() if device.type == 'cuda' else None\nelse:\n    print('Complete the training loop first.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clean up training artifacts before Exercise 4.\nimport gc\n\ntry:\n    del unet_lora\nexcept NameError:\n    pass\ntry:\n    del unet\nexcept NameError:\n    pass\ntry:\n    del lora_pipe\nexcept NameError:\n    pass\ntry:\n    del vae\nexcept NameError:\n    pass\ntry:\n    del text_encoder\nexcept NameError:\n    pass\n\ngc.collect()\ntorch.cuda.empty_cache() if device.type == 'cuda' else None\nprint('Cleaned up. Ready for Exercise 4.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 4: LoRA Composition Experiment [Independent]\n\nThe lesson explained that multiple LoRA adapters can be applied simultaneously:\n\n$$W_{\\text{combined}} = W + BA_{\\text{style}} \\cdot \\frac{\\alpha_1}{r_1} + BA_{\\text{subject}} \\cdot \\frac{\\alpha_2}{r_2}$$\n\nYou already have one trained LoRA from Exercise 3 (saved to `/tmp/naruto_lora`). Now you will train a **second** LoRA on a different style, then compose both to explore how composition works.\n\nYour task:\n1. Train a second LoRA on a different subset of images (we will use a different slice of the same dataset with different captions, or you can load a different small dataset)\n2. Load both LoRA adapters into a pipeline\n3. Apply each adapter **individually** and generate an image\n4. Apply **both adapters together** at different weight ratios\n5. Display all results in a grid and write observations\n\n**Key diffusers APIs you will need:**\n- `pipe.load_lora_weights(\"/tmp/naruto_lora\", adapter_name=\"name\")` â€” loads a saved LoRA\n- `pipe.set_adapters([\"name1\", \"name2\"], adapter_weights=[w1, w2])` â€” activates multiple adapters with weights\n- `pipe.set_adapters([\"name1\"], adapter_weights=[1.0])` â€” activates only one adapter\n\n**What to observe:**\n- Do the two LoRAs compose cleanly, or do they interfere?\n- How does adjusting the adapter weights change the visual output?\n- Is the composition a smooth blend, or does one LoRA dominate?\n\nThis is your experiment. There is no single correct answer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR EXPERIMENT\n#\n# Step 1: Train a second LoRA on a different style.\n#   - Load a different subset or dataset (e.g., a different slice of\n#     naruto-blip-captions, or try \"huggan/wikiart\" for fine art).\n#   - Train for ~100 steps with rank-4.\n#   - Save to /tmp/second_lora.\n#\n# Step 2: Load both LoRAs and compose.\n#   - Load the base pipeline.\n#   - Load both adapters with pipe.load_lora_weights().\n#   - Generate with each individually and both composed.\n#   - Try weight ratios: (1.0, 1.0), (0.5, 0.5), (0.8, 0.3), (0.3, 0.8).\n#   - Display in a grid and write observations.\n#\n# Starter code for training:\n#   unet2 = UNet2DConditionModel.from_pretrained(model_id, subfolder='unet',\n#               torch_dtype=torch.float32).to(device)\n#   lora_config = LoraConfig(r=4, lora_alpha=4,\n#               target_modules=['to_q', 'to_k', 'to_v', 'to_out.0'])\n#   unet2_lora = get_peft_model(unet2, lora_config)\n#   # ... training loop (same pattern as Exercise 3) ...\n#   unet2_lora.save_pretrained('/tmp/second_lora')\n#\n# Starter code for composition:\n#   pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=dtype,\n#              safety_checker=None, requires_safety_checker=False).to(device)\n#   pipe.load_lora_weights('/tmp/naruto_lora', adapter_name='naruto')\n#   pipe.load_lora_weights('/tmp/second_lora', adapter_name='second')\n#\n#   pipe.set_adapters(['naruto'], adapter_weights=[1.0])\n#   img_naruto = pipe(prompt, ...).images[0]\n#\n#   pipe.set_adapters(['second'], adapter_weights=[1.0])\n#   img_second = pipe(prompt, ...).images[0]\n#\n#   pipe.set_adapters(['naruto', 'second'], adapter_weights=[0.7, 0.7])\n#   img_both = pipe(prompt, ...).images[0]\n\nprint('Observations:')\nprint('  (Write your observations here after running the experiment.)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\n**Experimental design reasoning:** To make composition comparisons meaningful, we need controlled conditions. First, use the **same seed** for every generation so the initial noise z_T is identical across all runs--any visual difference must come from the LoRA adapters, not randomness. Second, test **each adapter individually** first to establish baselines--you need to know what each LoRA does alone before you can evaluate their interaction. Third, vary the **weight ratios** systematically (1.0/1.0, 0.5/0.5, and asymmetric like 0.8/0.3) to see whether composition blends smoothly or whether one adapter dominates. The weight grid reveals whether composition is additive and linear or whether there are nonlinear interactions when both adapters modify the same projections.\n\n```python\nfrom peft import LoraConfig, get_peft_model\nfrom diffusers import (UNet2DConditionModel, AutoencoderKL, DDPMScheduler,\n                       StableDiffusionPipeline, DPMSolverMultistepScheduler)\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom datasets import load_dataset\nfrom torchvision import transforms\n\n# --- Step 1: Train a second LoRA on a different dataset subset ---\n# Use a different slice of naruto images so the two LoRAs learn from\n# different examples, giving them partially overlapping but distinct styles.\n\nprint('Loading a different slice of the naruto dataset...')\nfull_dataset = load_dataset('lambdalabs/naruto-blip-captions', split='train')\nsecond_dataset = full_dataset.shuffle(seed=99).select(range(30))\n\nimage_transform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5]),\n])\n\ntrain_images_2 = []\ntrain_captions_2 = []\nfor sample in second_dataset:\n    train_images_2.append(image_transform(sample['image'].convert('RGB')))\n    train_captions_2.append(sample['text'])\n\n# Load fresh components for training.\nunet2 = UNet2DConditionModel.from_pretrained(model_id, subfolder='unet',\n            torch_dtype=torch.float32).to(device)\nvae2 = AutoencoderKL.from_pretrained(model_id, subfolder='vae',\n            torch_dtype=torch.float32).to(device)\ntokenizer2 = CLIPTokenizer.from_pretrained(model_id, subfolder='tokenizer')\ntext_enc2 = CLIPTextModel.from_pretrained(model_id, subfolder='text_encoder',\n            torch_dtype=torch.float32).to(device)\nscheduler2 = DDPMScheduler.from_pretrained(model_id, subfolder='scheduler')\nvae2.requires_grad_(False)\ntext_enc2.requires_grad_(False)\n\n# Use rank-8 for the second LoRA to make composition more interesting:\n# different rank = different capacity = different style encoding.\nlora_config_2 = LoraConfig(r=8, lora_alpha=8,\n    target_modules=['to_q', 'to_k', 'to_v', 'to_out.0'])\nunet2_lora = get_peft_model(unet2, lora_config_2)\n\noptimizer2 = torch.optim.AdamW(\n    [p for p in unet2_lora.parameters() if p.requires_grad], lr=1e-4)\n\n# Training is shorter here because the goal is composition exploration,\n# not training quality. 100 steps is enough to produce a distinct adapter\n# for comparison.\nunet2_lora.train()\nnum_steps_2 = 100\nprint(f'Training second LoRA (rank-8, {num_steps_2} steps)...')\nfor step in range(num_steps_2):\n    idx = step % len(train_images_2)\n    pv = train_images_2[idx].unsqueeze(0).to(device)\n    cap = train_captions_2[idx]\n    with torch.no_grad():\n        z0 = vae2.encode(pv).latent_dist.sample() * vae2.config.scaling_factor\n    noise = torch.randn_like(z0)\n    ts = torch.randint(0, scheduler2.config.num_train_timesteps, (1,), device=device)\n    zt = scheduler2.add_noise(z0, noise, ts)\n    toks = tokenizer2(cap, padding='max_length', max_length=tokenizer2.model_max_length,\n                      truncation=True, return_tensors='pt')\n    with torch.no_grad():\n        te = text_enc2(toks.input_ids.to(device))[0]\n    pred = unet2_lora(zt, ts, encoder_hidden_states=te).sample\n    loss = F.mse_loss(pred, noise)\n    optimizer2.zero_grad()\n    loss.backward()\n    optimizer2.step()\n    if step % 50 == 0:\n        print(f'  Step {step}/{num_steps_2}: loss = {loss.item():.4f}')\n\nunet2_lora.save_pretrained('/tmp/second_lora')\nprint('Second LoRA saved to /tmp/second_lora.')\n\n# Clean up training components.\ndel unet2_lora, unet2, vae2, text_enc2\ngc.collect()\ntorch.cuda.empty_cache() if device.type == 'cuda' else None\n\n# --- Step 2: Load both LoRAs and compose ---\npipe = StableDiffusionPipeline.from_pretrained(\n    model_id, torch_dtype=dtype, safety_checker=None,\n    requires_safety_checker=False).to(device)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n\npipe.load_lora_weights('/tmp/naruto_lora', adapter_name='naruto_a')\npipe.load_lora_weights('/tmp/second_lora', adapter_name='naruto_b')\n\nprompt = \"a cat sitting in a garden\"\nseed = 42\n\n# Baseline: no LoRA\npipe.set_adapters([], adapter_weights=[])\ngen = torch.Generator(device=device).manual_seed(seed)\nimg_base = pipe(prompt, guidance_scale=7.5, num_inference_steps=25,\n                generator=gen, height=512, width=512).images[0]\n\n# LoRA A only (Exercise 3's naruto LoRA, rank-4)\npipe.set_adapters(['naruto_a'], adapter_weights=[1.0])\ngen = torch.Generator(device=device).manual_seed(seed)\nimg_a = pipe(prompt, guidance_scale=7.5, num_inference_steps=25,\n             generator=gen, height=512, width=512).images[0]\n\n# LoRA B only (second naruto LoRA, rank-8)\npipe.set_adapters(['naruto_b'], adapter_weights=[1.0])\ngen = torch.Generator(device=device).manual_seed(seed)\nimg_b = pipe(prompt, guidance_scale=7.5, num_inference_steps=25,\n             generator=gen, height=512, width=512).images[0]\n\n# Both at full weight\npipe.set_adapters(['naruto_a', 'naruto_b'], adapter_weights=[1.0, 1.0])\ngen = torch.Generator(device=device).manual_seed(seed)\nimg_both_full = pipe(prompt, guidance_scale=7.5, num_inference_steps=25,\n                     generator=gen, height=512, width=512).images[0]\n\n# Both at half weight\npipe.set_adapters(['naruto_a', 'naruto_b'], adapter_weights=[0.5, 0.5])\ngen = torch.Generator(device=device).manual_seed(seed)\nimg_both_half = pipe(prompt, guidance_scale=7.5, num_inference_steps=25,\n                     generator=gen, height=512, width=512).images[0]\n\n# Weighted blend (favoring A)\npipe.set_adapters(['naruto_a', 'naruto_b'], adapter_weights=[0.8, 0.3])\ngen = torch.Generator(device=device).manual_seed(seed)\nimg_blend = pipe(prompt, guidance_scale=7.5, num_inference_steps=25,\n                 generator=gen, height=512, width=512).images[0]\n\nshow_image_grid(\n    [img_base, img_a, img_b, img_both_full, img_both_half, img_blend],\n    ['Base (no LoRA)', 'LoRA A only (r=4)', 'LoRA B only (r=8)',\n     'Both (1.0, 1.0)', 'Both (0.5, 0.5)', 'Blend (0.8, 0.3)'],\n    nrows=2, ncols=3, figsize=(18, 12),\n    suptitle='LoRA Composition Experiment',\n)\n\nprint('Observations:')\nprint('- Each LoRA individually shifts the style toward anime rendering.')\nprint('- Both at full weight (1.0, 1.0) doubles the style delta, which may')\nprint('  overshoot and produce artifacts or oversaturated anime effects.')\nprint('- Both at half weight (0.5, 0.5) blends more gently.')\nprint('- Weighted blend lets you favor one LoRA over the other.')\nprint('- Since both were trained on naruto data (different subsets), they')\nprint('  push in similar directions and compose relatively cleanly.')\nprint('- More divergent styles (e.g., anime + photorealism) would show')\nprint('  more interference when composed.')\n```\n\n**Why adapter weights matter:** Each adapter's contribution is scaled by its weight. At (1.0, 1.0), both LoRAs apply at full strength, which can overshoot--the combined delta may be too large. At (0.5, 0.5), each contributes half its learned delta, producing a gentler blend. This is the alpha scaling from the lesson: you are controlling how much each detour influences the highway.\n\n**Common mistakes:**\n1. Forgetting to re-seed the generator for each run. Without the same z_T, comparisons are meaningless--you cannot tell if differences come from the LoRA or the initial noise.\n2. Expecting perfect composition. LoRA composition is a sum of independent adaptations--there is no guarantee they are compatible.\n3. Loading adapters from different SD versions (e.g., SDXL LoRA on SD v1.5). The projection dimensions will not match.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Cross-attention projections are the LoRA targets.** The SD v1.5 U-Net's cross-attention layers (`attn2`) contain `to_q`, `to_k`, `to_v`, and `to_out` projectionsâ€”these are where text meaning meets spatial features. A rank-4 LoRA on these layers adds roughly 0.1â€“0.3% of the total U-Net parameters.\n",
    "\n",
    "2. **The diffusion LoRA training step is DDPM training with frozen base weights.** VAE encode â†’ sample timestep â†’ add noise â†’ U-Net predicts noise â†’ MSE loss â†’ backprop into LoRA params only. Every piece was familiar from prior lessons.\n",
    "\n",
    "3. **Gradient flow verification is the critical check.** Base U-Net parameters should have zero gradients. LoRA adapter parameters should have non-zero gradients. If this is wrong, the training is wrong.\n",
    "\n",
    "4. **LoRA composition is additive but not always clean.** Two LoRA adapters sum their bypass outputs. Adapter weights control the blend. Conflicting adaptations can interfereâ€”scale the weights to find a balance.\n",
    "\n",
    "5. **Same detour, different highway.** The LoRA mechanism from Module 4.4 transferred directly. What changed was the context: which layers to target (cross-attention projections), what the training data looks like (images + captions), and what the loss measures (noise prediction MSE)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}