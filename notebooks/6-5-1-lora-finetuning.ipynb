{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Fine-Tuning for Diffusion Models\n",
    "\n",
    "**Module 6.5, Lesson 1** | CourseAI\n",
    "\n",
    "You know LoRA from Module 4.4 (highway-and-detour bypass, B=0 initialization, merge at inference). You know the Stable Diffusion pipeline from Module 6.4 (text â†’ CLIP â†’ U-Net denoising loop â†’ VAE decode). This notebook connects them: applying LoRA to the diffusion U-Net for style and subject customization.\n",
    "\n",
    "**What you will do:**\n",
    "- Inspect a real SD U-Net to find cross-attention projection layers and compute LoRA parameter counts for different ranks\n",
    "- Execute one LoRA training step by hand: VAE encode, noise, U-Net forward, MSE loss, verify gradient flow\n",
    "- Train a style LoRA end-to-end using diffusers + PEFT on a small style dataset\n",
    "- Load and compose two pre-trained LoRA adapters, experimenting with alpha scaling to control the blend\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "**Estimated time:** 45â€“60 minutes (Exercises 3â€“4 involve training and download time).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies and import everything. This notebook requires a GPU for reasonable training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate peft datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float16 if device.type == 'cuda' else torch.float32\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB')\n",
    "\n",
    "print('\\nSetup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Shared Helpers\n\nDefine display helpers and the model ID. Each exercise loads only the components it needs and cleans up afterward to stay within free-tier Colab VRAM (~16 GB on a T4).\n\n> **VRAM tip:** If you encounter an out-of-memory error, go to Runtime â†’ Restart runtime and rerun from Setup. Exercise 3 uses the most VRAM."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model_id = 'stable-diffusion-v1-5/stable-diffusion-v1-5'\n\n\ndef show_images(images, titles, figsize=None):\n    \"\"\"Display a list of PIL images side by side.\"\"\"\n    n = len(images)\n    if figsize is None:\n        figsize = (5 * n, 5)\n    fig, axes = plt.subplots(1, n, figsize=figsize)\n    if n == 1:\n        axes = [axes]\n    for ax, img, title in zip(axes, images, titles):\n        ax.imshow(np.array(img))\n        ax.set_title(title, fontsize=10)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n\ndef show_image_grid(images, titles, nrows, ncols, figsize=None, suptitle=None):\n    \"\"\"Display images in a grid with the given number of rows and columns.\"\"\"\n    if figsize is None:\n        figsize = (4 * ncols, 4 * nrows)\n    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n    axes_flat = axes.flat if nrows > 1 or ncols > 1 else [axes]\n    for ax, img, title in zip(axes_flat, images, titles):\n        ax.imshow(np.array(img))\n        ax.set_title(title, fontsize=10)\n        ax.axis('off')\n    for ax in list(axes_flat)[len(images):]:\n        ax.axis('off')\n    if suptitle:\n        plt.suptitle(suptitle, fontsize=13)\n    plt.tight_layout()\n    plt.show()\n\n\nprint('Helpers defined.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Inspect LoRA Target Layers [Guided]\n",
    "\n",
    "The lesson explained that LoRA for diffusion targets the **cross-attention projection matrices** (W_Q, W_K, W_V, W_out) because that is where text meaning meets spatial features. But which layers are those, concretely? How many parameters would a rank-4 LoRA add versus rank-16?\n",
    "\n",
    "This exercise inspects the real U-Net to ground the architectural understanding in actual layer names and parameter counts.\n",
    "\n",
    "**Before running, predict:**\n",
    "- How many cross-attention blocks does the SD v1.5 U-Net have? (Hint: cross-attention occurs at attention resolutions 16Ã—16 and 32Ã—32, in both the downsampling and upsampling paths.)\n",
    "- For a projection matrix of shape [320, 768], how many LoRA parameters does rank-4 add? (Think: B is [320, 4], A is [4, 768]. Total = ?)\n",
    "- What fraction of total U-Net parameters will a rank-4 LoRA represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = pipe.unet\n",
    "\n",
    "# Step 1: Find all cross-attention projection layers.\n",
    "# In the SD U-Net, cross-attention layers have names containing 'attn2'\n",
    "# (self-attention is 'attn1'). The projection matrices are:\n",
    "#   to_q, to_k, to_v  (input projections)\n",
    "#   to_out.0           (output projection)\n",
    "\n",
    "cross_attn_layers = {}\n",
    "for name, param in unet.named_parameters():\n",
    "    if 'attn2' in name and any(proj in name for proj in ['to_q', 'to_k', 'to_v', 'to_out.0.weight']):\n",
    "        cross_attn_layers[name] = param.shape\n",
    "\n",
    "print(f'Cross-attention projection layers found: {len(cross_attn_layers)}')\n",
    "print(f'\\n{\"Layer Name\":<75} {\"Shape\":>15}')\n",
    "print('-' * 92)\n",
    "for name, shape in sorted(cross_attn_layers.items()):\n",
    "    print(f'{name:<75} {str(list(shape)):>15}')\n",
    "\n",
    "# Step 2: Count LoRA parameters for different ranks.\n",
    "# For each weight matrix W of shape [out_features, in_features],\n",
    "# LoRA adds B [out_features, r] and A [r, in_features].\n",
    "# Total LoRA params per layer = r * (out_features + in_features)\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('LoRA Parameter Counts by Rank')\n",
    "print('=' * 60)\n",
    "\n",
    "for rank in [4, 8, 16]:\n",
    "    total_lora_params = 0\n",
    "    for name, shape in cross_attn_layers.items():\n",
    "        out_features, in_features = shape[0], shape[1]\n",
    "        lora_params = rank * (out_features + in_features)\n",
    "        total_lora_params += lora_params\n",
    "    print(f'\\nRank {rank}:')\n",
    "    print(f'  Total LoRA params:  {total_lora_params:>12,}')\n",
    "    print(f'  In MB (fp16):       {total_lora_params * 2 / 1024**2:>12.2f} MB')\n",
    "\n",
    "# Step 3: Compare to total U-Net parameters.\n",
    "total_unet_params = sum(p.numel() for p in unet.parameters())\n",
    "cross_attn_params = sum(shape[0] * shape[1] for shape in cross_attn_layers.values())\n",
    "\n",
    "print(f'\\n{\"=\" * 60}')\n",
    "print('U-Net Parameter Breakdown')\n",
    "print(f'{\"=\" * 60}')\n",
    "print(f'  Total U-Net params:         {total_unet_params:>12,}')\n",
    "print(f'  Cross-attn projection params:{cross_attn_params:>12,} ({100*cross_attn_params/total_unet_params:.1f}%)')\n",
    "print(f'  Everything else (frozen):    {total_unet_params - cross_attn_params:>12,} ({100*(total_unet_params - cross_attn_params)/total_unet_params:.1f}%)')\n",
    "\n",
    "# LoRA as fraction of total U-Net\n",
    "for rank in [4, 8, 16]:\n",
    "    total_lora = sum(\n",
    "        rank * (s[0] + s[1]) for s in cross_attn_layers.values()\n",
    "    )\n",
    "    pct = 100 * total_lora / total_unet_params\n",
    "    print(f'  Rank-{rank} LoRA params:       {total_lora:>12,} ({pct:.3f}% of U-Net)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You inspected the real SD v1.5 U-Net and found its cross-attention projection layers by name. Key observations:\n",
    "\n",
    "1. **Cross-attention layers are named `attn2`** in the diffusers implementation (self-attention is `attn1`). The projections are `to_q`, `to_k`, `to_v`, and `to_out.0`. This maps directly to the W_Q, W_K, W_V, and W_out from the lesson.\n",
    "\n",
    "2. **The projection shapes vary by resolution.** At lower resolutions (deeper in the U-Net), the channel dimension is larger (640, 1280), so the projection matrices are bigger. At higher resolutions (32Ã—32), the channel dimension is smaller (320).\n",
    "\n",
    "3. **LoRA adds a tiny fraction of the total parameters.** A rank-4 LoRA on all cross-attention projections adds roughly 0.1â€“0.3% of the U-Net's total parameters. That is the \"surgical\" nature of LoRAâ€”a small detour on a massive highway. This is why LoRA files are typically just 2â€“50 MB.\n",
    "\n",
    "4. **Rank scales linearly.** Rank-16 has exactly 4Ã— the parameters of rank-4 (four times as many columns in B and rows in A). The lesson noted that rank 4â€“8 is the sweet spot for diffusionâ€”enough capacity for style adaptation without overfitting on small datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: One LoRA Training Step by Hand [Guided]\n",
    "\n",
    "The lesson traced one training step in pseudocode: load image â†’ VAE encode â†’ sample timestep â†’ add noise â†’ U-Net predicts noise (only LoRA params have gradients) â†’ MSE loss â†’ backprop into LoRA params only.\n",
    "\n",
    "Now you will do it for real. This exercise verifies the training loop from the lesson with actual tensors and real gradient flow.\n",
    "\n",
    "**Before running, predict:**\n",
    "- After VAE encoding, what shape will the latent z_0 have for a 512Ã—512 image? (Think: 512/8 = 64, and the VAE's latent space has 4 channels.)\n",
    "- After the forward pass, should the U-Net's **base** weight parameters have gradients? Should the LoRA adapter parameters have gradients?\n",
    "- What is the loss function? What are its two arguments? (The model predicts ÎµÌ‚, the target is the actual noise Îµ.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# We will work in float32 for this exercise to get clean gradients.\n",
    "# Reload the U-Net in float32 for the training step.\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "print('Loading components in float32 for training step...')\n",
    "unet_f32 = UNet2DConditionModel.from_pretrained(model_id, subfolder='unet', torch_dtype=torch.float32).to(device)\n",
    "vae = AutoencoderKL.from_pretrained(model_id, subfolder='vae', torch_dtype=torch.float32).to(device)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder='tokenizer')\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder='text_encoder', torch_dtype=torch.float32).to(device)\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(model_id, subfolder='scheduler')\n",
    "\n",
    "# Freeze everything: VAE, text encoder, and U-Net base weights.\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "print('\\nComponents loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply LoRA to the U-Net.\n",
    "# We target the cross-attention projections (attn2) with rank 4.\n",
    "# PEFT uses target_modules to specify which layers get LoRA adapters.\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                           # Rank 4 â€” the small detour\n",
    "    lora_alpha=4,                  # Alpha = rank, so scaling is 1.0\n",
    "    target_modules=[\n",
    "        'to_q', 'to_k', 'to_v', 'to_out.0',  # Cross-attention projections\n",
    "    ],\n",
    "    lora_dropout=0.0,\n",
    ")\n",
    "\n",
    "unet_lora = get_peft_model(unet_f32, lora_config)\n",
    "\n",
    "# Print trainable vs total parameters.\n",
    "trainable_params = sum(p.numel() for p in unet_lora.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in unet_lora.parameters())\n",
    "print(f'Trainable LoRA params: {trainable_params:,}')\n",
    "print(f'Total U-Net params:    {total_params:,}')\n",
    "print(f'Trainable fraction:    {100 * trainable_params / total_params:.3f}%')\n",
    "print(f'\\nThis matches Exercise 1\\'s rank-4 count. PEFT applied LoRA to exactly')\n",
    "print(f'the cross-attention projections we identified.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a synthetic training sample.\n",
    "# In a real training run, this would be a (image, caption) pair from your dataset.\n",
    "# Here we use a random image to focus on the mechanics of the training step.\n",
    "\n",
    "# \"Training image\" â€” a random 512x512 image (standing in for a real one)\n",
    "train_image = torch.randn(1, 3, 512, 512, device=device)\n",
    "caption = \"a watercolor painting of a village\"\n",
    "\n",
    "# Step 3: VAE encode â€” compress the image to latent space.\n",
    "# The VAE is frozen. We detach and scale by the VAE's scaling factor.\n",
    "with torch.no_grad():\n",
    "    latent_dist = vae.encode(train_image)\n",
    "    z_0 = latent_dist.latent_dist.sample() * vae.config.scaling_factor\n",
    "\n",
    "print(f'Input image shape:  {list(train_image.shape)}')\n",
    "print(f'Latent z_0 shape:   {list(z_0.shape)}')\n",
    "print(f'VAE scaling factor: {vae.config.scaling_factor}')\n",
    "print(f'\\n8x spatial compression: 512/8 = 64. Four latent channels.')\n",
    "print(f'This is the same VAE encoding from \"From Pixels to Latents.\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Sample a random timestep and add noise.\n",
    "# This is the forward process from \"The Forward Process\":\n",
    "#   z_t = sqrt(alpha_bar_t) * z_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
    "\n",
    "timestep = torch.tensor([500], device=device)  # Middle of the schedule\n",
    "\n",
    "# Sample the noise that we will try to predict\n",
    "epsilon = torch.randn_like(z_0)\n",
    "\n",
    "# Add noise using the scheduler's closed-form formula\n",
    "z_t = noise_scheduler.add_noise(z_0, epsilon, timestep)\n",
    "\n",
    "print(f'Timestep:       t = {timestep.item()}')\n",
    "print(f'Noise shape:    {list(epsilon.shape)}')\n",
    "print(f'Noised latent:  {list(z_t.shape)}')\n",
    "print(f'\\nThe noise epsilon is the TARGET. The U-Net will try to predict it.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Encode the caption with frozen CLIP.\n",
    "\n",
    "tokens = tokenizer(\n",
    "    caption,\n",
    "    padding='max_length',\n",
    "    max_length=tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(tokens.input_ids.to(device))[0]\n",
    "\n",
    "print(f'Caption: \"{caption}\"')\n",
    "print(f'Token IDs shape:      {list(tokens.input_ids.shape)}')\n",
    "print(f'Text embeddings shape: {list(text_embeddings.shape)}')\n",
    "print(f'\\n77 tokens (padded), 768-dim CLIP embeddings.')\n",
    "print(f'These become the K and V inputs to cross-attention.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: U-Net forward pass â€” predict the noise.\n",
    "# Only LoRA adapter parameters have gradients. Base weights are frozen.\n",
    "\n",
    "epsilon_hat = unet_lora(z_t, timestep, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "print(f'Predicted noise shape: {list(epsilon_hat.shape)}')\n",
    "print(f'Target noise shape:    {list(epsilon.shape)}')\n",
    "print(f'Shapes match: {epsilon_hat.shape == epsilon.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Compute MSE loss â€” same loss from DDPM training.\n",
    "loss = F.mse_loss(epsilon_hat, epsilon)\n",
    "\n",
    "print(f'MSE Loss: {loss.item():.4f}')\n",
    "print(f'\\nThis is the same loss from \"Learning to Denoise\":')\n",
    "print(f'  L = MSE(epsilon, epsilon_hat)')\n",
    "print(f'The model predicted the noise. The loss measures how close it was.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Backprop and verify gradient flow.\n",
    "# This is the critical verification: gradients should flow ONLY through\n",
    "# LoRA adapter parameters, NOT through base U-Net weights.\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# Check LoRA parameters: they SHOULD have gradients.\n",
    "lora_with_grad = 0\n",
    "lora_without_grad = 0\n",
    "for name, param in unet_lora.named_parameters():\n",
    "    if 'lora_' in name and param.requires_grad:\n",
    "        if param.grad is not None and param.grad.abs().sum() > 0:\n",
    "            lora_with_grad += 1\n",
    "        else:\n",
    "            lora_without_grad += 1\n",
    "\n",
    "# Check base parameters: they should NOT have gradients.\n",
    "base_with_grad = 0\n",
    "base_without_grad = 0\n",
    "for name, param in unet_lora.named_parameters():\n",
    "    if 'lora_' not in name:\n",
    "        if param.grad is not None and param.grad.abs().sum() > 0:\n",
    "            base_with_grad += 1\n",
    "        else:\n",
    "            base_without_grad += 1\n",
    "\n",
    "print('=== Gradient Flow Verification ===')\n",
    "print(f'\\nLoRA adapter parameters:')\n",
    "print(f'  With gradients:    {lora_with_grad}')\n",
    "print(f'  Without gradients: {lora_without_grad}')\n",
    "print(f'\\nBase U-Net parameters:')\n",
    "print(f'  With gradients:    {base_with_grad}  (should be 0)')\n",
    "print(f'  Without gradients: {base_without_grad}')\n",
    "\n",
    "if base_with_grad == 0 and lora_with_grad > 0:\n",
    "    print(f'\\n*** Gradient flow is correct. ***')\n",
    "    print(f'Only LoRA adapters receive gradients. The highway is frozen.')\n",
    "    print(f'The detour is learning.')\n",
    "else:\n",
    "    print(f'\\nSomething unexpected happened. Check the setup.')\n",
    "\n",
    "# Show a sample LoRA gradient to prove it is non-trivial\n",
    "for name, param in unet_lora.named_parameters():\n",
    "    if 'lora_' in name and param.grad is not None and param.grad.abs().sum() > 0:\n",
    "        print(f'\\nSample gradient: {name}')\n",
    "        print(f'  Param shape:     {list(param.shape)}')\n",
    "        print(f'  Grad norm:       {param.grad.norm().item():.6f}')\n",
    "        print(f'  Grad mean:       {param.grad.mean().item():.8f}')\n",
    "        print(f'  Grad std:        {param.grad.std().item():.8f}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You executed one complete diffusion LoRA training step, matching the pseudocode from the lesson:\n",
    "\n",
    "1. **VAE encode** the training image: [3, 512, 512] â†’ [4, 64, 64]. The VAE is frozenâ€”it just compresses the image to latent space.\n",
    "2. **Sample timestep t=500** and add noise using the forward process closed-form formula. The noise Îµ is the target.\n",
    "3. **CLIP encode** the caption: \"a watercolor painting of a village\" â†’ [1, 77, 768]. The text encoder is frozen.\n",
    "4. **U-Net forward pass**: the noised latent z_t, timestep t, and text embeddings go in. The predicted noise ÎµÌ‚ comes out.\n",
    "5. **MSE loss**: L = MSE(Îµ, ÎµÌ‚). Same loss from DDPM training in \"Learning to Denoise.\"\n",
    "6. **Backprop**: gradients flow only through LoRA adapter parameters. Base U-Net weights have **zero gradients**.\n",
    "\n",
    "The critical verification: `base_with_grad == 0`. The highway is frozen. Only the detour learns. This is exactly the LoRA training pattern from Module 4.4, applied to a different highway.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the float32 models to free VRAM before Exercise 3.\n",
    "del unet_lora, unet_f32, vae, text_encoder\n",
    "torch.cuda.empty_cache() if device.type == 'cuda' else None\n",
    "print('Cleaned up float32 models. VRAM freed for Exercise 3.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Train a Style LoRA [Supported]\n",
    "\n",
    "Now you will train a LoRA end-to-end. Instead of gathering a real dataset of 50â€“200 style images, we will use a small publicly available dataset of artistic images from Hugging Face. The goal is to see the **workflow and the effect**, not to produce a gallery-quality LoRA.\n",
    "\n",
    "You will:\n",
    "1. Load a small art dataset from Hugging Face\n",
    "2. Set up the LoRA training loop using diffusers + PEFT\n",
    "3. Train for a few hundred steps\n",
    "4. Generate images with and without the LoRA to see the style effect\n",
    "5. Compare rank-4 vs rank-8\n",
    "\n",
    "The training loop is the same step you did by hand in Exercise 2, repeated many times with real training images.\n",
    "\n",
    "**Hints:**\n",
    "- Each TODO is 1â€“3 lines of code.\n",
    "- The patterns are identical to Exercise 2 (VAE encode, noise, U-Net forward, MSE loss).\n",
    "- If you get stuck, the solution is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL, DDPMScheduler, StableDiffusionPipeline\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from torchvision import transforms\n",
    "import gc\n",
    "\n",
    "# Load the pipeline components fresh for training.\n",
    "print('Loading pipeline components for training...')\n",
    "unet = UNet2DConditionModel.from_pretrained(model_id, subfolder='unet', torch_dtype=torch.float32).to(device)\n",
    "vae = AutoencoderKL.from_pretrained(model_id, subfolder='vae', torch_dtype=torch.float32).to(device)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder='tokenizer')\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder='text_encoder', torch_dtype=torch.float32).to(device)\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(model_id, subfolder='scheduler')\n",
    "\n",
    "# Freeze VAE and text encoder â€” they do not train.\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "print('Components loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small synthetic \"style dataset.\"\n",
    "# In a real workflow, you would use actual images in your target style.\n",
    "# Here we create simple synthetic images with a consistent visual pattern\n",
    "# (warm-toned gradients) so we can verify the LoRA learns *something*\n",
    "# without needing to download a large dataset.\n",
    "\n",
    "num_train_images = 20\n",
    "train_captions = [\n",
    "    \"a warm sunset painting\",\n",
    "    \"a golden landscape with warm colors\",\n",
    "    \"an orange and red abstract composition\",\n",
    "    \"a warm-toned watercolor scene\",\n",
    "    \"a painting with amber and crimson hues\",\n",
    "] * 4  # Repeat to get 20 samples\n",
    "\n",
    "# Create synthetic images with a warm color bias.\n",
    "# These are not art, but they have a consistent \"warm\" signal.\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),  # Scale to [-1, 1]\n",
    "])\n",
    "\n",
    "train_images = []\n",
    "for i in range(num_train_images):\n",
    "    # Create a warm-biased random image (more red/yellow, less blue)\n",
    "    img = np.random.rand(512, 512, 3).astype(np.float32)\n",
    "    img[:, :, 0] = np.clip(img[:, :, 0] + 0.3, 0, 1)  # Boost red\n",
    "    img[:, :, 1] = np.clip(img[:, :, 1] + 0.1, 0, 1)  # Slight green boost\n",
    "    img[:, :, 2] = np.clip(img[:, :, 2] - 0.2, 0, 1)  # Reduce blue\n",
    "    pil_img = Image.fromarray((img * 255).astype(np.uint8))\n",
    "    train_images.append(image_transform(pil_img))\n",
    "\n",
    "print(f'Created {len(train_images)} synthetic training images.')\n",
    "print(f'Image tensor shape: {list(train_images[0].shape)}')\n",
    "print(f'Value range: [{train_images[0].min():.2f}, {train_images[0].max():.2f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the U-Net â€” same config as Exercise 2.\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=4,\n",
    "    target_modules=['to_q', 'to_k', 'to_v', 'to_out.0'],\n",
    "    lora_dropout=0.0,\n",
    ")\n",
    "\n",
    "unet_lora = get_peft_model(unet, lora_config)\n",
    "unet_lora.print_trainable_parameters()\n",
    "\n",
    "# Set up the optimizer â€” only LoRA parameters are trainable.\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in unet_lora.parameters() if p.requires_grad],\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-2,\n",
    ")\n",
    "\n",
    "print(f'\\nOptimizer targets {sum(1 for p in unet_lora.parameters() if p.requires_grad)} parameter groups.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop â€” this is Exercise 2's single step, repeated.\n",
    "# Fill in the TODOs. Each is 1-3 lines.\n",
    "\n",
    "num_steps = 200\n",
    "losses = []\n",
    "\n",
    "unet_lora.train()\n",
    "print(f'Training for {num_steps} steps...')\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick a random training sample\n",
    "    idx = step % len(train_images)\n",
    "    pixel_values = train_images[idx].unsqueeze(0).to(device)  # [1, 3, 512, 512]\n",
    "    caption = train_captions[idx]\n",
    "\n",
    "    # TODO 1: Encode the image with the frozen VAE.\n",
    "    # Use vae.encode(pixel_values), get the latent distribution,\n",
    "    # sample from it, and multiply by vae.config.scaling_factor.\n",
    "    # Wrap in torch.no_grad() since the VAE is frozen.\n",
    "    # (This is identical to Exercise 2, Step 3.)\n",
    "    z_0 = None  # Replace this line\n",
    "\n",
    "    # TODO 2: Sample a random timestep and add noise.\n",
    "    # Sample a random timestep between 0 and num_train_timesteps.\n",
    "    # Generate random noise with torch.randn_like(z_0).\n",
    "    # Use noise_scheduler.add_noise(z_0, noise, timesteps) to create z_t.\n",
    "    noise = None     # Replace this line\n",
    "    timesteps = None  # Replace this line\n",
    "    z_t = None       # Replace this line\n",
    "\n",
    "    # Encode the caption with frozen CLIP.\n",
    "    tokens = tokenizer(\n",
    "        caption,\n",
    "        padding='max_length',\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        text_emb = text_encoder(tokens.input_ids.to(device))[0]\n",
    "\n",
    "    # TODO 3: U-Net forward pass and MSE loss.\n",
    "    # Pass z_t, timesteps, and text_emb through the U-Net.\n",
    "    # Compute MSE loss between the predicted noise and the actual noise.\n",
    "    # (This is identical to Exercise 2, Steps 6-7.)\n",
    "    noise_pred = None  # Replace this line\n",
    "    loss = None        # Replace this line\n",
    "\n",
    "    # Backprop and optimizer step\n",
    "    if loss is not None:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if step % 50 == 0 or step == num_steps - 1:\n",
    "            print(f'  Step {step:>4d}/{num_steps}: loss = {loss.item():.4f}')\n",
    "    else:\n",
    "        print('Fill in the TODOs above to start training.')\n",
    "        break\n",
    "\n",
    "if losses:\n",
    "    print(f'\\nTraining complete. Final loss: {losses[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The training loop is Exercise 2's single step, repeated with real data. Every TODO maps to something you already did.\n",
    "\n",
    "**TODO 1 â€” VAE encode:**\n",
    "```python\n",
    "    with torch.no_grad():\n",
    "        z_0 = vae.encode(pixel_values).latent_dist.sample() * vae.config.scaling_factor\n",
    "```\n",
    "Why `torch.no_grad()`? The VAE is frozenâ€”we do not need gradients through it. The `latent_dist.sample()` draws from the learned Gaussian, and the scaling factor normalizes the latent space.\n",
    "\n",
    "**TODO 2 â€” Sample timestep and add noise:**\n",
    "```python\n",
    "    noise = torch.randn_like(z_0)\n",
    "    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (1,), device=device)\n",
    "    z_t = noise_scheduler.add_noise(z_0, noise, timesteps)\n",
    "```\n",
    "The timestep is sampled uniformly from the schedule. `add_noise` applies the closed-form forward process: z_t = sqrt(alpha_bar_t) * z_0 + sqrt(1 - alpha_bar_t) * noise.\n",
    "\n",
    "**TODO 3 â€” Forward pass and loss:**\n",
    "```python\n",
    "    noise_pred = unet_lora(z_t, timesteps, encoder_hidden_states=text_emb).sample\n",
    "    loss = F.mse_loss(noise_pred, noise)\n",
    "```\n",
    "The U-Net predicts the noise that was added. MSE measures how close the prediction is to the actual noise. Only LoRA params have gradients, so backprop only updates the detour.\n",
    "\n",
    "**Common mistakes:**\n",
    "- Forgetting `torch.no_grad()` around the VAE encode. This wastes memory tracking gradients through a frozen model and can cause OOM errors.\n",
    "- Using `timesteps` as an integer instead of a tensor. The scheduler expects a tensor.\n",
    "- Confusing the loss direction: `F.mse_loss(noise_pred, noise)`, not `F.mse_loss(noise, noise_pred)`. Both give the same MSE, but the convention is (predicted, target).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss curve.\n",
    "if losses:\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.plot(losses, alpha=0.3, color='cyan', label='Per-step loss')\n",
    "    # Smoothed version\n",
    "    window = min(20, len(losses))\n",
    "    if len(losses) >= window:\n",
    "        smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "        ax.plot(range(window-1, len(losses)), smoothed, color='cyan', linewidth=2, label=f'Smoothed ({window}-step avg)')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('MSE Loss')\n",
    "    ax.set_title('LoRA Training Loss')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Starting loss: {losses[0]:.4f}')\n",
    "    print(f'Final loss:    {losses[-1]:.4f}')\n",
    "    print(f'\\nThe loss should decrease, showing the LoRA adapters are learning.')\n",
    "    print(f'On synthetic data, the effect will be subtle but measurable.')\n",
    "else:\n",
    "    print('No losses recorded. Fill in the TODOs in the training loop.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images WITH the LoRA to see the effect.\n",
    "if losses:\n",
    "    from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "    # Build a pipeline using our LoRA-adapted U-Net.\n",
    "    unet_lora.eval()\n",
    "\n",
    "    # Merge the LoRA weights into the base model for inference.\n",
    "    # This is the merge-at-inference pattern from Module 4.4:\n",
    "    #   W_merged = W + BA * (alpha/r)\n",
    "    # After merging, there is zero inference overhead.\n",
    "    unet_lora.merge_adapter()\n",
    "\n",
    "    # Create a fresh pipeline with the LoRA-merged U-Net.\n",
    "    lora_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        unet=unet_lora,\n",
    "        torch_dtype=torch.float32,\n",
    "        safety_checker=None,\n",
    "        requires_safety_checker=False,\n",
    "    ).to(device)\n",
    "    lora_pipe.scheduler = DPMSolverMultistepScheduler.from_config(lora_pipe.scheduler.config)\n",
    "\n",
    "    # Also load a clean pipeline without LoRA for comparison.\n",
    "    clean_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype,\n",
    "        safety_checker=None,\n",
    "        requires_safety_checker=False,\n",
    "    ).to(device)\n",
    "    clean_pipe.scheduler = DPMSolverMultistepScheduler.from_config(clean_pipe.scheduler.config)\n",
    "\n",
    "    # Generate with the same seed, same prompt.\n",
    "    prompt = \"a landscape with mountains and a river\"\n",
    "    seed = 42\n",
    "\n",
    "    gen = torch.Generator(device=device).manual_seed(seed)\n",
    "    img_without = clean_pipe(\n",
    "        prompt, guidance_scale=7.5, num_inference_steps=25,\n",
    "        generator=gen, height=512, width=512,\n",
    "    ).images[0]\n",
    "\n",
    "    gen = torch.Generator(device=device).manual_seed(seed)\n",
    "    img_with = lora_pipe(\n",
    "        prompt, guidance_scale=7.5, num_inference_steps=25,\n",
    "        generator=gen, height=512, width=512,\n",
    "    ).images[0]\n",
    "\n",
    "    show_images(\n",
    "        [img_without, img_with],\n",
    "        ['Without LoRA (base model)', 'With LoRA (trained on warm tones)'],\n",
    "        figsize=(12, 6),\n",
    "    )\n",
    "\n",
    "    print('The LoRA was trained on a small synthetic dataset with warm-toned images.')\n",
    "    print('The effect may be subtle (synthetic data, short training), but it')\n",
    "    print('demonstrates the full workflow: train LoRA -> merge -> generate.')\n",
    "    print('\\nWith a real style dataset (50-200 images of actual artwork),')\n",
    "    print('the style transfer would be dramatically more visible.')\n",
    "else:\n",
    "    print('Complete the training loop first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up training artifacts before Exercise 4.\n",
    "del unet_lora, unet\n",
    "if 'lora_pipe' in dir():\n",
    "    del lora_pipe\n",
    "if 'clean_pipe' in dir():\n",
    "    del clean_pipe\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if device.type == 'cuda' else None\n",
    "print('Cleaned up. Ready for Exercise 4.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: LoRA Composition Experiment [Independent]\n",
    "\n",
    "The lesson explained that multiple LoRA adapters can be applied simultaneously:\n",
    "\n",
    "$$W_{\\text{combined}} = W + BA_{\\text{style}} \\cdot \\frac{\\alpha_1}{r_1} + BA_{\\text{subject}} \\cdot \\frac{\\alpha_2}{r_2}$$\n",
    "\n",
    "Your task:\n",
    "1. Load two **pre-trained community LoRA adapters** from Hugging Face (suggestions below)\n",
    "2. Apply each adapter **individually** and generate an image\n",
    "3. Apply **both adapters together** and generate an image\n",
    "4. **Experiment with alpha scaling**: try different adapter weights (e.g., 0.5 and 1.0, or 0.3 and 0.7) and observe how the blend changes\n",
    "5. Display all results in a grid and write observations\n",
    "\n",
    "**Suggested LoRA adapters from Hugging Face:**\n",
    "- Use any two community LoRA adapters compatible with SD v1.5. Search [huggingface.co/models](https://huggingface.co/models?pipeline_tag=text-to-image&sort=downloads) filtered by \"lora\" and \"stable-diffusion\".\n",
    "- Some well-known examples: `nerijs/pixel-art-xl` (pixel art style, but check SD version compatibility), or search for \"lora sd-1.5\" on the Hub.\n",
    "\n",
    "**Key diffusers APIs you will need:**\n",
    "- `pipe.load_lora_weights(\"hf-repo-id\", adapter_name=\"name\")` â€” loads a LoRA from Hugging Face\n",
    "- `pipe.set_adapters([\"name1\", \"name2\"], adapter_weights=[w1, w2])` â€” activates multiple adapters with weights\n",
    "- `pipe.set_adapters([\"name1\"], adapter_weights=[1.0])` â€” activates only one adapter\n",
    "- `pipe.unload_lora_weights()` â€” removes all adapters\n",
    "\n",
    "**What to observe:**\n",
    "- Do the two LoRAs compose cleanly, or do they interfere?\n",
    "- How does adjusting the adapter weights change the visual output?\n",
    "- Is the composition a smooth blend, or does one LoRA dominate?\n",
    "\n",
    "This is your experiment. There is no single correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR EXPERIMENT\n",
    "#\n",
    "# 1. Load the base SD v1.5 pipeline\n",
    "# 2. Load two LoRA adapters with pipe.load_lora_weights()\n",
    "# 3. Generate with each adapter individually\n",
    "# 4. Generate with both adapters composed at different weight ratios\n",
    "# 5. Display results and write observations\n",
    "#\n",
    "# Starter code:\n",
    "#   pipe = StableDiffusionPipeline.from_pretrained(\n",
    "#       model_id, torch_dtype=dtype, safety_checker=None,\n",
    "#       requires_safety_checker=False,\n",
    "#   ).to(device)\n",
    "#\n",
    "#   pipe.load_lora_weights(\"some-hf-repo\", adapter_name=\"style_a\")\n",
    "#   pipe.load_lora_weights(\"another-hf-repo\", adapter_name=\"style_b\")\n",
    "#\n",
    "#   # Generate with only style_a:\n",
    "#   pipe.set_adapters([\"style_a\"], adapter_weights=[1.0])\n",
    "#   img_a = pipe(prompt, ...).images[0]\n",
    "#\n",
    "#   # Generate with only style_b:\n",
    "#   pipe.set_adapters([\"style_b\"], adapter_weights=[1.0])\n",
    "#   img_b = pipe(prompt, ...).images[0]\n",
    "#\n",
    "#   # Generate with both:\n",
    "#   pipe.set_adapters([\"style_a\", \"style_b\"], adapter_weights=[0.7, 0.7])\n",
    "#   img_both = pipe(prompt, ...).images[0]\n",
    "#\n",
    "#   show_images([img_a, img_b, img_both], [\"Style A\", \"Style B\", \"Both\"])\n",
    "\n",
    "print('Observations:')\n",
    "print('  (Write your observations here after running the experiment.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The key insight is that LoRA composition is additive: the two bypass outputs are summed with their respective alpha scaling. Whether they compose cleanly depends on whether they modify the same projections in compatible directions.\n",
    "\n",
    "```python\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "# Load the base pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=dtype,\n",
    "    safety_checker=None,\n",
    "    requires_safety_checker=False,\n",
    ").to(device)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Load two LoRA adapters.\n",
    "# Replace these with actual LoRA repos from Hugging Face.\n",
    "# Example repos (verify availability and SD v1.5 compatibility):\n",
    "lora_a_repo = \"YOUR_FIRST_LORA_REPO\"   # e.g., a style LoRA\n",
    "lora_b_repo = \"YOUR_SECOND_LORA_REPO\"  # e.g., another style LoRA\n",
    "\n",
    "pipe.load_lora_weights(lora_a_repo, adapter_name=\"style_a\")\n",
    "pipe.load_lora_weights(lora_b_repo, adapter_name=\"style_b\")\n",
    "\n",
    "prompt = \"a cat sitting in a garden\"\n",
    "seed = 42\n",
    "\n",
    "# Generate: no LoRA (baseline)\n",
    "pipe.set_adapters([], adapter_weights=[])\n",
    "gen = torch.Generator(device=device).manual_seed(seed)\n",
    "img_base = pipe(prompt, guidance_scale=7.5, num_inference_steps=25,\n",
    "                generator=gen, height=512, width=512).images[0]\n",
    "\n",
    "# Generate: style_a only\n",
    "pipe.set_adapters([\"style_a\"], adapter_weights=[1.0])\n",
    "gen = torch.Generator(device=device).manual_seed(seed)\n",
    "img_a = pipe(prompt, guidance_scale=7.5, num_inference_steps=25,\n",
    "             generator=gen, height=512, width=512).images[0]\n",
    "\n",
    "# Generate: style_b only\n",
    "pipe.set_adapters([\"style_b\"], adapter_weights=[1.0])\n",
    "gen = torch.Generator(device=device).manual_seed(seed)\n",
    "img_b = pipe(prompt, guidance_scale=7.5, num_inference_steps=25,\n",
    "             generator=gen, height=512, width=512).images[0]\n",
    "\n",
    "# Generate: both at full weight\n",
    "pipe.set_adapters([\"style_a\", \"style_b\"], adapter_weights=[1.0, 1.0])\n",
    "gen = torch.Generator(device=device).manual_seed(seed)\n",
    "img_both_full = pipe(prompt, guidance_scale=7.5, num_inference_steps=25,\n",
    "                     generator=gen, height=512, width=512).images[0]\n",
    "\n",
    "# Generate: both at half weight\n",
    "pipe.set_adapters([\"style_a\", \"style_b\"], adapter_weights=[0.5, 0.5])\n",
    "gen = torch.Generator(device=device).manual_seed(seed)\n",
    "img_both_half = pipe(prompt, guidance_scale=7.5, num_inference_steps=25,\n",
    "                     generator=gen, height=512, width=512).images[0]\n",
    "\n",
    "# Generate: weighted blend (favoring style_a)\n",
    "pipe.set_adapters([\"style_a\", \"style_b\"], adapter_weights=[0.8, 0.3])\n",
    "gen = torch.Generator(device=device).manual_seed(seed)\n",
    "img_blend = pipe(prompt, guidance_scale=7.5, num_inference_steps=25,\n",
    "                 generator=gen, height=512, width=512).images[0]\n",
    "\n",
    "# Display results\n",
    "show_image_grid(\n",
    "    [img_base, img_a, img_b, img_both_full, img_both_half, img_blend],\n",
    "    ['Base (no LoRA)', 'Style A only', 'Style B only',\n",
    "     'Both (1.0, 1.0)', 'Both (0.5, 0.5)', 'Blend (0.8, 0.3)'],\n",
    "    nrows=2, ncols=3, figsize=(18, 12),\n",
    "    suptitle='LoRA Composition Experiment',\n",
    ")\n",
    "\n",
    "print('Observations:')\n",
    "print('- Each LoRA individually produces a consistent style shift from the base.')\n",
    "print('- Both at full weight (1.0, 1.0) may produce artifacts if the styles conflict.')\n",
    "print('- Both at half weight (0.5, 0.5) often blends more smoothly.')\n",
    "print('- Weighted blend lets you favor one style over the other.')\n",
    "print('- Composition is not always clean: if both LoRAs push cross-attention')\n",
    "print('  projections in conflicting directions, the result may be incoherent.')\n",
    "```\n",
    "\n",
    "**Why adapter weights matter:** Each adapter's contribution is scaled by its weight. At (1.0, 1.0), both LoRAs apply at full strength, which can overshootâ€”the combined delta may be too large. At (0.5, 0.5), each contributes half its learned delta, producing a gentler blend. This is the alpha scaling from the lesson: you are controlling how much each detour influences the highway.\n",
    "\n",
    "**Common mistakes:**\n",
    "1. Forgetting to re-seed the generator for each run. Without the same z_T, comparisons are meaningless.\n",
    "2. Using LoRAs trained for different SD versions (e.g., SDXL LoRA on SD v1.5). The projection dimensions will not match.\n",
    "3. Expecting perfect composition. LoRA composition is a sum of independent adaptationsâ€”there is no guarantee they are compatible.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Cross-attention projections are the LoRA targets.** The SD v1.5 U-Net's cross-attention layers (`attn2`) contain `to_q`, `to_k`, `to_v`, and `to_out` projectionsâ€”these are where text meaning meets spatial features. A rank-4 LoRA on these layers adds roughly 0.1â€“0.3% of the total U-Net parameters.\n",
    "\n",
    "2. **The diffusion LoRA training step is DDPM training with frozen base weights.** VAE encode â†’ sample timestep â†’ add noise â†’ U-Net predicts noise â†’ MSE loss â†’ backprop into LoRA params only. Every piece was familiar from prior lessons.\n",
    "\n",
    "3. **Gradient flow verification is the critical check.** Base U-Net parameters should have zero gradients. LoRA adapter parameters should have non-zero gradients. If this is wrong, the training is wrong.\n",
    "\n",
    "4. **LoRA composition is additive but not always clean.** Two LoRA adapters sum their bypass outputs. Adapter weights control the blend. Conflicting adaptations can interfereâ€”scale the weights to find a balance.\n",
    "\n",
    "5. **Same detour, different highway.** The LoRA mechanism from Module 4.4 transferred directly. What changed was the context: which layers to target (cross-attention projections), what the training data looks like (images + captions), and what the loss measures (noise prediction MSE)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}