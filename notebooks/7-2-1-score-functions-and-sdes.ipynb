{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Functions & SDEs\n",
    "\n",
    "**Module 7.2, Lesson 1** | CourseAI\n",
    "\n",
    "You know the theory\u2014the score function is the gradient of log probability, it points toward regions of higher probability, and DDPM's noise prediction is a scaled version of the score. This notebook makes that concrete. You will compute score functions by hand, visualize them as vector fields, verify the score-noise equivalence with a real model, and compare SDE vs ODE sampling trajectories.\n",
    "\n",
    "**What you will do:**\n",
    "- Compute score functions analytically for 1D Gaussians and a Gaussian mixture, plot them alongside the PDF\n",
    "- Visualize the score as a 2D vector field for a Gaussian mixture and observe how noise smooths the field\n",
    "- Verify the score-noise equivalence using a pre-trained diffusion model on a toy 2D distribution\n",
    "- Compare SDE (stochastic, wiggly) vs ODE (deterministic, smooth) sampling trajectories\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "Every concept in this notebook comes from the lesson. Score function as compass toward data, the score-noise equivalence, the reverse SDE vs probability flow ODE. No new theory\u2014just hands-on practice with the math and models.\n",
    "\n",
    "**Estimated time:** 30\u201345 minutes. Exercises 1\u20132 are pure math (no GPU needed). Exercises 3\u20134 train a small model on a toy 2D distribution (~1 minute on CPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies, import everything, and configure the environment.\n",
    "\n",
    "No GPU required for this notebook. Everything runs on CPU. The diffusion model in Exercises 3\u20134 is a tiny MLP trained on a 2D Gaussian mixture\u2014not an image model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print('Setup complete. No GPU needed for this notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Helpers\n",
    "\n",
    "A toy diffusion model for Exercises 3\u20134. This trains a small MLP to predict noise on a 2D Gaussian mixture\u2014the same DDPM training objective you already know, just on 2D points instead of images.\n",
    "\n",
    "Run this cell now. It defines the model architecture, the noise schedule, and a training function. The actual training happens in Exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Toy 2D diffusion model\n",
    "# ============================================================\n",
    "# This is DDPM on 2D points instead of images. Same concepts:\n",
    "#   - Forward process: x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
    "#   - Training: predict epsilon from (x_t, t)\n",
    "#   - The model is a small MLP, not a U-Net (2D points don't need convolutions)\n",
    "\n",
    "def make_noise_schedule(T=100, beta_start=1e-4, beta_end=0.02):\n",
    "    \"\"\"Linear noise schedule, same as DDPM.\"\"\"\n",
    "    betas = torch.linspace(beta_start, beta_end, T)\n",
    "    alphas = 1.0 - betas\n",
    "    alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "    return betas, alphas, alpha_bars\n",
    "\n",
    "\n",
    "class ToyDiffusionModel(nn.Module):\n",
    "    \"\"\"MLP that predicts noise epsilon given (x_t, t).\n",
    "    \n",
    "    Input: 2D point x_t concatenated with timestep embedding.\n",
    "    Output: 2D noise prediction epsilon_theta.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=128, T=100):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        # Timestep embedding: map integer t to a learned vector\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Embedding(T, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        # Main network: takes (x_t, time_embedding) -> epsilon prediction\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2 + hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 2),  # Output: 2D noise prediction\n",
    "        )\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        \"\"\"Predict the noise that was added to create x_t at timestep t.\"\"\"\n",
    "        t_emb = self.time_embed(t)\n",
    "        inp = torch.cat([x_t, t_emb], dim=-1)\n",
    "        return self.net(inp)\n",
    "\n",
    "\n",
    "def sample_gaussian_mixture(n, centers=None, std=0.3):\n",
    "    \"\"\"Sample from a 2D Gaussian mixture.\n",
    "    \n",
    "    Default: 4 clusters arranged in a square pattern.\n",
    "    This is the \"data distribution\" our toy model learns.\n",
    "    \"\"\"\n",
    "    if centers is None:\n",
    "        centers = torch.tensor([\n",
    "            [-2.0, -2.0],\n",
    "            [-2.0,  2.0],\n",
    "            [ 2.0, -2.0],\n",
    "            [ 2.0,  2.0],\n",
    "        ])\n",
    "    k = centers.shape[0]\n",
    "    # Randomly assign each point to a cluster\n",
    "    assignments = torch.randint(0, k, (n,))\n",
    "    samples = centers[assignments] + std * torch.randn(n, 2)\n",
    "    return samples\n",
    "\n",
    "\n",
    "def train_toy_diffusion(model, alpha_bars, n_epochs=200, batch_size=512, lr=3e-4):\n",
    "    \"\"\"Train the toy diffusion model with the DDPM objective.\n",
    "    \n",
    "    Same training loop as DDPM on images:\n",
    "    1. Sample clean data x_0\n",
    "    2. Sample random timestep t and noise epsilon\n",
    "    3. Create noisy x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
    "    4. Predict epsilon from (x_t, t)\n",
    "    5. Loss = MSE(epsilon, epsilon_predicted)\n",
    "    \"\"\"\n",
    "    T = len(alpha_bars)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        # 1. Sample clean data\n",
    "        x_0 = sample_gaussian_mixture(batch_size)\n",
    "        \n",
    "        # 2. Sample random timesteps and noise\n",
    "        t = torch.randint(0, T, (batch_size,))\n",
    "        epsilon = torch.randn_like(x_0)\n",
    "        \n",
    "        # 3. Create noisy samples: x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
    "        ab_t = alpha_bars[t].unsqueeze(-1)  # (batch, 1)\n",
    "        x_t = torch.sqrt(ab_t) * x_0 + torch.sqrt(1 - ab_t) * epsilon\n",
    "        \n",
    "        # 4. Predict noise\n",
    "        epsilon_pred = model(x_t, t)\n",
    "        \n",
    "        # 5. MSE loss\n",
    "        loss = nn.functional.mse_loss(epsilon_pred, epsilon)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'  Epoch {epoch+1}/{n_epochs}, loss: {loss.item():.4f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "print('Toy diffusion model defined.')\n",
    "print('Architecture: MLP with timestep embedding (same DDPM objective, just on 2D points).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Compute Score Functions by Hand `[Guided]`\n",
    "\n",
    "From the lesson: the score function is $\\text{score}(x) = \\nabla_x \\log p(x)$. It is a compass toward likely data\u2014it points in the direction of increasing probability. For a standard Gaussian $\\mathcal{N}(0,1)$, the score is simply $-x$.\n",
    "\n",
    "We will compute the score function analytically for three distributions:\n",
    "1. $\\mathcal{N}(0, 1)$\u2014the simplest case\n",
    "2. $\\mathcal{N}(3, 2)$\u2014shifted and wider\n",
    "3. A mixture of two Gaussians\u2014two peaks, where things get interesting\n",
    "\n",
    "For each, we plot the PDF and the score function side by side. The score should be zero at peaks (you are already at high probability\u2014nowhere better to go) and large far from peaks (strong pull back toward the data).\n",
    "\n",
    "**Before running, predict:**\n",
    "- For the Gaussian mixture with peaks at $\\mu_1 = -2$ and $\\mu_2 = 3$, where will the score cross zero? How many zero crossings will there be?\n",
    "- Between the two peaks, which direction will the score point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1: Compute score functions analytically\n",
    "# ============================================================\n",
    "\n",
    "x = np.linspace(-8, 8, 1000)\n",
    "\n",
    "# --- Distribution 1: N(0, 1) ---\n",
    "# log p(x) = -x^2/2 + const\n",
    "# score(x) = d/dx log p(x) = -x\n",
    "pdf_1 = norm.pdf(x, loc=0, scale=1)\n",
    "score_1 = -x  # Analytic score for N(0,1)\n",
    "\n",
    "# --- Distribution 2: N(3, 2) ---\n",
    "# For N(mu, sigma^2): log p(x) = -(x-mu)^2 / (2*sigma^2) + const\n",
    "# score(x) = -(x - mu) / sigma^2\n",
    "mu_2, sigma_2 = 3.0, 2.0\n",
    "pdf_2 = norm.pdf(x, loc=mu_2, scale=sigma_2)\n",
    "score_2 = -(x - mu_2) / (sigma_2 ** 2)  # Analytic score for N(3, 2)\n",
    "\n",
    "# --- Distribution 3: Mixture of two Gaussians ---\n",
    "# p(x) = 0.5 * N(x; -2, 0.8) + 0.5 * N(x; 3, 1.0)\n",
    "# score(x) = (d/dx p(x)) / p(x)  (because d/dx log p = (d/dx p) / p)\n",
    "mu_a, sigma_a = -2.0, 0.8\n",
    "mu_b, sigma_b = 3.0, 1.0\n",
    "weight_a, weight_b = 0.5, 0.5\n",
    "\n",
    "pdf_a = norm.pdf(x, loc=mu_a, scale=sigma_a)\n",
    "pdf_b = norm.pdf(x, loc=mu_b, scale=sigma_b)\n",
    "pdf_3 = weight_a * pdf_a + weight_b * pdf_b\n",
    "\n",
    "# Derivative of each Gaussian component:\n",
    "# d/dx N(x; mu, sigma) = -(x - mu) / sigma^2 * N(x; mu, sigma)\n",
    "dpdf_a = -(x - mu_a) / (sigma_a ** 2) * pdf_a\n",
    "dpdf_b = -(x - mu_b) / (sigma_b ** 2) * pdf_b\n",
    "dpdf_3 = weight_a * dpdf_a + weight_b * dpdf_b\n",
    "\n",
    "# Score = (d/dx p(x)) / p(x)\n",
    "score_3 = dpdf_3 / (pdf_3 + 1e-10)  # Small epsilon to avoid division by zero\n",
    "\n",
    "print('Score functions computed analytically.')\n",
    "print('')\n",
    "print('Key formulas:')\n",
    "print('  N(0, 1):      score(x) = -x')\n",
    "print('  N(mu, sig^2): score(x) = -(x - mu) / sig^2')\n",
    "print('  Mixture:      score(x) = (d/dx p(x)) / p(x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PDF and score function side by side for all three distributions\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "\n",
    "distributions = [\n",
    "    ('$\\\\mathcal{N}(0, 1)$', pdf_1, score_1, [0]),\n",
    "    ('$\\\\mathcal{N}(3, 2)$', pdf_2, score_2, [3]),\n",
    "    ('Mixture: $0.5\\\\,\\\\mathcal{N}(-2, 0.8) + 0.5\\\\,\\\\mathcal{N}(3, 1.0)$', pdf_3, score_3, [-2, 3]),\n",
    "]\n",
    "\n",
    "for i, (title, pdf, score, peaks) in enumerate(distributions):\n",
    "    # Left column: PDF\n",
    "    axes[i, 0].plot(x, pdf, color='#60a5fa', linewidth=2)\n",
    "    axes[i, 0].fill_between(x, pdf, alpha=0.2, color='#60a5fa')\n",
    "    for p in peaks:\n",
    "        axes[i, 0].axvline(p, color='#f97316', linestyle='--', alpha=0.6, label=f'peak at x={p}')\n",
    "    axes[i, 0].set_title(f'{title}\\nProbability Density', fontsize=11)\n",
    "    axes[i, 0].set_xlabel('x')\n",
    "    axes[i, 0].set_ylabel('p(x)')\n",
    "    axes[i, 0].legend(fontsize=8)\n",
    "    \n",
    "    # Right column: Score function\n",
    "    axes[i, 1].plot(x, score, color='#a78bfa', linewidth=2)\n",
    "    axes[i, 1].axhline(0, color='white', linewidth=0.5, alpha=0.3)\n",
    "    for p in peaks:\n",
    "        axes[i, 1].axvline(p, color='#f97316', linestyle='--', alpha=0.6)\n",
    "    # Mark zero crossings\n",
    "    zero_crossings = np.where(np.diff(np.sign(score)))[0]\n",
    "    for zc in zero_crossings:\n",
    "        axes[i, 1].plot(x[zc], 0, 'o', color='#f97316', markersize=8)\n",
    "    axes[i, 1].set_title(f'{title}\\nScore Function $\\\\nabla_x \\\\log p(x)$', fontsize=11)\n",
    "    axes[i, 1].set_xlabel('x')\n",
    "    axes[i, 1].set_ylabel('score(x)')\n",
    "    axes[i, 1].set_ylim(-6, 6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Observations:')\n",
    "print('- N(0,1): Score is a straight line through the origin. score(x) = -x.')\n",
    "print('  At x=3, score=-3 (strong pull left). At x=0, score=0 (at the peak).')\n",
    "print('')\n",
    "print('- N(3,2): Score is a gentler line through x=3. score(x) = -(x-3)/4.')\n",
    "print('  The wider variance makes the score shallower\\u2014the probability landscape is flatter.')\n",
    "print('')\n",
    "print('- Mixture: THREE zero crossings! One at each peak (score=0 at the top of each')\n",
    "print('  Gaussian), and one BETWEEN the peaks (a saddle point where the pulls balance).')\n",
    "print('  Between the peaks, the score points toward the NEAREST peak.')\n",
    "print('  Far from both peaks, the score points toward the nearest one.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You computed the score function analytically for three distributions and verified the lesson's core claims:\n",
    "\n",
    "- **The score is zero at peaks.** At the mean of each Gaussian, the score crosses zero. This is the same as gradient = 0 at a minimum in optimization. Zero score means \"you are at a peak\u2014no better direction to go.\"\n",
    "- **The score is large far from peaks.** Points far from the data experience a strong pull back toward high-probability regions. The further away, the stronger the pull.\n",
    "- **For the mixture, the score is more interesting.** There are three zero crossings: one at each peak, and one between them where the opposing pulls balance. Between the peaks, the score points toward the nearest peak\u2014it is a compass toward the closest mode of the distribution.\n",
    "- **Wider variance = shallower score.** The $\\mathcal{N}(3, 2)$ score is gentler than the $\\mathcal{N}(0, 1)$ score because the probability landscape is flatter. The score magnitude reflects how steeply probability changes.\n",
    "\n",
    "The score function is a direction at every point\u2014not a value. It says \"which way is up,\" not \"how high am I.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Visualize the Score as a 2D Vector Field `[Guided]`\n",
    "\n",
    "From the lesson: in 2D, the score at every point is a 2D vector. For a Gaussian mixture with multiple peaks, the score field shows arrows converging on each peak. At different noise levels, the field changes\u2014high noise simplifies the field (nearly Gaussian, nearly linear arrows), low noise reveals the complex structure (distinct basins of attraction).\n",
    "\n",
    "We will:\n",
    "1. Create a 2D Gaussian mixture with two peaks\n",
    "2. Plot the score field as a quiver plot (arrows at every point)\n",
    "3. Add increasing Gaussian noise and re-plot the score field at each noise level\n",
    "\n",
    "**Before running, predict:**\n",
    "- At very high noise ($\\sigma = 3.0$), will the score field show two distinct basins, or will it look like a single Gaussian?\n",
    "- At very low noise ($\\sigma = 0.1$), will the arrows between the two peaks be simple or complex?\n",
    "- How does this connect to the noise schedule? (Think about $\\bar\\alpha$ near 0 vs near 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 2: Score function as a 2D vector field\n",
    "# ============================================================\n",
    "\n",
    "def gaussian_2d_pdf(x, y, mu, cov_inv, det_cov):\n",
    "    \"\"\"Compute the PDF of a 2D Gaussian at grid points (x, y).\"\"\"\n",
    "    dx = x - mu[0]\n",
    "    dy = y - mu[1]\n",
    "    exponent = -0.5 * (cov_inv[0,0]*dx**2 + 2*cov_inv[0,1]*dx*dy + cov_inv[1,1]*dy**2)\n",
    "    return (1.0 / (2 * np.pi * np.sqrt(det_cov))) * np.exp(exponent)\n",
    "\n",
    "\n",
    "def score_2d_gaussian_mixture(x, y, means, covariances, weights):\n",
    "    \"\"\"Compute the score field for a 2D Gaussian mixture.\n",
    "    \n",
    "    score(x) = nabla_x log p(x) = (nabla_x p(x)) / p(x)\n",
    "    \n",
    "    For a mixture, nabla_x p(x) = sum_k w_k * nabla_x N(x; mu_k, Sigma_k)\n",
    "    and nabla_x N(x; mu_k, Sigma_k) = -Sigma_k^{-1} (x - mu_k) * N(x; mu_k, Sigma_k)\n",
    "    \"\"\"\n",
    "    p_total = np.zeros_like(x)\n",
    "    grad_x_total = np.zeros_like(x)\n",
    "    grad_y_total = np.zeros_like(x)\n",
    "    \n",
    "    for mu, cov, w in zip(means, covariances, weights):\n",
    "        cov_inv = np.linalg.inv(cov)\n",
    "        det_cov = np.linalg.det(cov)\n",
    "        \n",
    "        pdf = gaussian_2d_pdf(x, y, mu, cov_inv, det_cov)\n",
    "        p_total += w * pdf\n",
    "        \n",
    "        # Gradient of the k-th component: -Sigma_k^{-1} (x - mu_k) * N(x; mu_k, Sigma_k)\n",
    "        dx = x - mu[0]\n",
    "        dy = y - mu[1]\n",
    "        grad_x_total += w * pdf * (-(cov_inv[0,0]*dx + cov_inv[0,1]*dy))\n",
    "        grad_y_total += w * pdf * (-(cov_inv[1,0]*dx + cov_inv[1,1]*dy))\n",
    "    \n",
    "    # score = grad(log p) = grad(p) / p\n",
    "    score_x = grad_x_total / (p_total + 1e-10)\n",
    "    score_y = grad_y_total / (p_total + 1e-10)\n",
    "    \n",
    "    return score_x, score_y, p_total\n",
    "\n",
    "\n",
    "# Define a 2D Gaussian mixture with two peaks\n",
    "means = [np.array([-2.0, 0.0]), np.array([2.0, 0.0])]\n",
    "covariances = [np.array([[0.5, 0.0], [0.0, 0.5]]),\n",
    "               np.array([[0.5, 0.0], [0.0, 0.5]])]\n",
    "weights = [0.5, 0.5]\n",
    "\n",
    "# Create grid for plotting\n",
    "grid_range = 5.0\n",
    "n_grid = 20  # Number of arrows per axis (keep sparse for readability)\n",
    "n_density = 200  # Number of points for density contours\n",
    "\n",
    "xg = np.linspace(-grid_range, grid_range, n_grid)\n",
    "yg = np.linspace(-grid_range, grid_range, n_grid)\n",
    "X, Y = np.meshgrid(xg, yg)\n",
    "\n",
    "xd = np.linspace(-grid_range, grid_range, n_density)\n",
    "yd = np.linspace(-grid_range, grid_range, n_density)\n",
    "XD, YD = np.meshgrid(xd, yd)\n",
    "\n",
    "print('2D Gaussian mixture defined:')\n",
    "print(f'  Peak 1: mean = {means[0]}, covariance = {covariances[0][0,0]:.1f} * I')\n",
    "print(f'  Peak 2: mean = {means[1]}, covariance = {covariances[1][0,0]:.1f} * I')\n",
    "print(f'  Equal weights: {weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the score field at the original (clean) distribution\n",
    "\n",
    "score_x, score_y, p_total = score_2d_gaussian_mixture(X, Y, means, covariances, weights)\n",
    "_, _, p_density = score_2d_gaussian_mixture(XD, YD, means, covariances, weights)\n",
    "\n",
    "# Compute arrow magnitude for coloring\n",
    "magnitude = np.sqrt(score_x**2 + score_y**2)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
    "\n",
    "# Density contours in the background\n",
    "ax.contourf(XD, YD, p_density, levels=20, cmap='Blues', alpha=0.3)\n",
    "\n",
    "# Score arrows (quiver plot)\n",
    "# Normalize arrow lengths for visual clarity\n",
    "norm_factor = magnitude.max()\n",
    "quiv = ax.quiver(X, Y, score_x/norm_factor, score_y/norm_factor, magnitude,\n",
    "                 cmap='cool', scale=25, width=0.004, alpha=0.9)\n",
    "\n",
    "# Mark the peaks\n",
    "for mu in means:\n",
    "    ax.plot(mu[0], mu[1], '*', color='#f97316', markersize=15, markeredgecolor='white', markeredgewidth=1)\n",
    "\n",
    "ax.set_title('Score Field of a 2D Gaussian Mixture (Clean Distribution)', fontsize=13)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_aspect('equal')\n",
    "plt.colorbar(quiv, ax=ax, label='Score magnitude')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('The arrows point toward the nearest peak everywhere.')\n",
    "print('Between the peaks, the field splits\\u2014some points are pulled left, others right.')\n",
    "print('At each peak (orange stars), the arrows converge and the score approaches zero.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add increasing noise and observe how the score field changes.\n",
    "#\n",
    "# Adding Gaussian noise N(0, sigma^2 * I) to the data distribution is equivalent\n",
    "# to convolving the distribution with a Gaussian. This smooths the distribution\n",
    "# and simplifies the score field.\n",
    "#\n",
    "# This is exactly what happens in the DDPM forward process at different timesteps.\n",
    "# High noise (large sigma, alpha_bar near 0) -> simple score field.\n",
    "# Low noise (small sigma, alpha_bar near 1) -> complex score field.\n",
    "\n",
    "noise_levels = [0.0, 0.3, 1.0, 3.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for idx, sigma_noise in enumerate(noise_levels):\n",
    "    # Adding noise sigma to each component increases its covariance by sigma^2 * I\n",
    "    noisy_covariances = [cov + sigma_noise**2 * np.eye(2) for cov in covariances]\n",
    "    \n",
    "    sx, sy, _ = score_2d_gaussian_mixture(X, Y, means, noisy_covariances, weights)\n",
    "    _, _, pd = score_2d_gaussian_mixture(XD, YD, means, noisy_covariances, weights)\n",
    "    \n",
    "    mag = np.sqrt(sx**2 + sy**2)\n",
    "    nf = max(mag.max(), 1e-6)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.contourf(XD, YD, pd, levels=20, cmap='Blues', alpha=0.3)\n",
    "    ax.quiver(X, Y, sx/nf, sy/nf, mag, cmap='cool', scale=25, width=0.005, alpha=0.9)\n",
    "    for mu in means:\n",
    "        ax.plot(mu[0], mu[1], '*', color='#f97316', markersize=12,\n",
    "                markeredgecolor='white', markeredgewidth=1)\n",
    "    \n",
    "    noise_label = 'Clean' if sigma_noise == 0 else f'$\\\\sigma$ = {sigma_noise}'\n",
    "    ax.set_title(f'{noise_label}', fontsize=12)\n",
    "    ax.set_xlim(-grid_range, grid_range)\n",
    "    ax.set_ylim(-grid_range, grid_range)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel('$x_2$')\n",
    "\n",
    "plt.suptitle(\n",
    "    'Score Field at Increasing Noise Levels\\n'\n",
    "    'High noise \\u2192 simple field (easy task) | Low noise \\u2192 complex field (hard task)',\n",
    "    fontsize=13, y=1.05\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Key observations:')\n",
    "print('')\n",
    "print('- Clean (sigma=0): Two distinct basins. Sharp boundary between them.')\n",
    "print('  The score field is complex\\u2014the model must capture fine structure.')\n",
    "print('')\n",
    "print('- Sigma=0.3: The basins start to merge. The boundary softens.')\n",
    "print('')\n",
    "print('- Sigma=1.0: The two peaks are barely distinguishable.')\n",
    "print('  The score field is becoming simpler\\u2014nearly radial.')\n",
    "print('')\n",
    "print('- Sigma=3.0: The distribution is effectively a single Gaussian.')\n",
    "print('  The score field is nearly linear\\u2014all arrows point toward the center.')\n",
    "print('  A model can capture this trivially.')\n",
    "print('')\n",
    "print('Connect to alpha-bar: This is why the noise schedule starts with easy tasks')\n",
    "print('(high noise, alpha_bar near 0, simple scores) and progresses to hard tasks')\n",
    "print('(low noise, alpha_bar near 1, complex scores). The denoising model learns')\n",
    "print('the easy landscape first, then gradually handles finer structure.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You visualized the score function as a 2D vector field and observed its behavior at different noise levels:\n",
    "\n",
    "- **Clean distribution:** Two distinct basins of attraction. The score field is complex\u2014sharp boundaries between regions pulled toward different peaks. A model must capture this fine structure.\n",
    "- **Low noise ($\\sigma = 0.3$):** The basins begin to merge. The boundary between them softens. Still two peaks, but the transition is smoother.\n",
    "- **Medium noise ($\\sigma = 1.0$):** The two peaks are barely distinguishable. The score field is becoming nearly radial\u2014almost like a single Gaussian.\n",
    "- **High noise ($\\sigma = 3.0$):** Effectively a single Gaussian. The score field is nearly linear. All arrows point toward the combined center. Trivial for a model to learn.\n",
    "\n",
    "This is exactly what the denoising model sees at different timesteps. At high noise ($\\bar\\alpha$ near 0), the model's job is easy\u2014the noisy distribution is nearly Gaussian, the score field is simple. At low noise ($\\bar\\alpha$ near 1), the model must capture the fine structure of the data distribution\u2014multiple modes, sharp boundaries, complex geometry. The noise schedule controls the difficulty progression.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Verify the Score-Noise Equivalence `[Supported]`\n",
    "\n",
    "From the lesson: the DDPM noise prediction $\\epsilon_\\theta(x_t, t)$ is related to the score by:\n",
    "\n",
    "$$\\text{score}(x_t, t) \\approx -\\frac{\\epsilon_\\theta(x_t, t)}{\\sqrt{1 - \\bar\\alpha_t}}$$\n",
    "\n",
    "The noise prediction points **away** from data (it estimates the noise that corrupted the sample). The score points **toward** data (it estimates the direction of increasing probability). They are the same direction, opposite sign, with a scaling factor.\n",
    "\n",
    "We will:\n",
    "1. Train the toy diffusion model from the Helpers section on our 2D Gaussian mixture\n",
    "2. For given noisy samples $x_t$ at specific timesteps, get the model's noise prediction\n",
    "3. Compute the implied score from the noise prediction\n",
    "4. Verify that the score points from $x_t$ toward the data clusters\n",
    "\n",
    "Your task: fill in the TODOs to compute the implied score and verify its direction.\n",
    "\n",
    "**Before running, predict:**\n",
    "- At a high timestep (lots of noise), will the implied score arrows be large or small?\n",
    "- At a low timestep (little noise), will the score arrows point more precisely toward the nearest cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train the toy diffusion model (~30 seconds)\n",
    "\n",
    "T = 100  # Number of diffusion timesteps\n",
    "betas, alphas, alpha_bars = make_noise_schedule(T=T)\n",
    "\n",
    "model = ToyDiffusionModel(hidden_dim=128, T=T)\n",
    "\n",
    "print('Training toy diffusion model on 2D Gaussian mixture...')\n",
    "print('(Same DDPM objective: predict the noise epsilon from noisy samples x_t)')\n",
    "print()\n",
    "losses = train_toy_diffusion(model, alpha_bars, n_epochs=300, batch_size=512)\n",
    "\n",
    "# Plot training loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 3))\n",
    "ax.plot(losses, color='#60a5fa', linewidth=1, alpha=0.7)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title('Training Loss (DDPM on 2D Gaussian Mixture)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nFinal loss: {losses[-1]:.4f}')\n",
    "print('The model has learned to predict noise\\u2014which means it has implicitly')\n",
    "print('learned the score function at every noise level.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Verify the score-noise equivalence\n",
    "#\n",
    "# For several timesteps, we will:\n",
    "#   a) Create noisy samples x_t from clean data x_0\n",
    "#   b) Get the model's noise prediction epsilon_theta(x_t, t)\n",
    "#   c) Compute the implied score: score = -epsilon_theta / sqrt(1 - alpha_bar_t)\n",
    "#   d) Verify: the score should point from x_t toward the data clusters\n",
    "\n",
    "model.eval()\n",
    "\n",
    "timesteps_to_check = [10, 30, 60, 90]  # Low noise to high noise\n",
    "\n",
    "# Generate some clean data points\n",
    "torch.manual_seed(123)\n",
    "x_0 = sample_gaussian_mixture(200)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(timesteps_to_check), figsize=(20, 5))\n",
    "\n",
    "for idx, t_val in enumerate(timesteps_to_check):\n",
    "    t_tensor = torch.full((x_0.shape[0],), t_val, dtype=torch.long)\n",
    "    \n",
    "    # Create noisy samples using the forward process\n",
    "    ab_t = alpha_bars[t_val]\n",
    "    epsilon = torch.randn_like(x_0)\n",
    "    x_t = torch.sqrt(ab_t) * x_0 + torch.sqrt(1 - ab_t) * epsilon\n",
    "    \n",
    "    # Get model's noise prediction\n",
    "    with torch.no_grad():\n",
    "        eps_pred = model(x_t, t_tensor)\n",
    "    \n",
    "    # TODO: Compute the implied score from the noise prediction.\n",
    "    # Remember the formula from the lesson:\n",
    "    #   score = -epsilon_theta / sqrt(1 - alpha_bar_t)\n",
    "    #\n",
    "    # Hint: ab_t is alpha_bar at this timestep. eps_pred is the model's noise prediction.\n",
    "    # The result should be a tensor with the same shape as eps_pred.\n",
    "    implied_score = None  # <-- Replace this line\n",
    "    \n",
    "    # TODO: Compute the direction from x_t toward the nearest clean data center.\n",
    "    # This is the \"ground truth\" direction the score should approximately point.\n",
    "    #\n",
    "    # For each noisy point x_t[i], find which cluster center it is closest to,\n",
    "    # then compute the direction vector (center - x_t[i]).\n",
    "    #\n",
    "    # Hint: The cluster centers are at [-2, -2], [-2, 2], [2, -2], [2, 2].\n",
    "    # Use torch.cdist to find distances, then torch.argmin for closest center.\n",
    "    centers = torch.tensor([[-2., -2.], [-2., 2.], [2., -2.], [2., 2.]])\n",
    "    gt_direction = None  # <-- Replace this line\n",
    "    \n",
    "    # Plotting (provided\\u2014no TODO here)\n",
    "    ax = axes[idx]\n",
    "    x_np = x_t.numpy()\n",
    "    \n",
    "    # Plot noisy samples\n",
    "    ax.scatter(x_np[:, 0], x_np[:, 1], s=3, alpha=0.3, color='#94a3b8')\n",
    "    \n",
    "    # Plot implied score arrows (subsample for clarity)\n",
    "    step = 5\n",
    "    if implied_score is not None:\n",
    "        score_np = implied_score.numpy()\n",
    "        # Normalize for visual clarity\n",
    "        score_mag = np.sqrt(score_np[:, 0]**2 + score_np[:, 1]**2).reshape(-1, 1)\n",
    "        score_normalized = score_np / (score_mag + 1e-6)\n",
    "        ax.quiver(x_np[::step, 0], x_np[::step, 1],\n",
    "                  score_normalized[::step, 0], score_normalized[::step, 1],\n",
    "                  color='#a78bfa', scale=20, width=0.005, alpha=0.7,\n",
    "                  label='Implied score')\n",
    "    \n",
    "    if gt_direction is not None:\n",
    "        gt_np = gt_direction.numpy()\n",
    "        gt_mag = np.sqrt(gt_np[:, 0]**2 + gt_np[:, 1]**2).reshape(-1, 1)\n",
    "        gt_normalized = gt_np / (gt_mag + 1e-6)\n",
    "        ax.quiver(x_np[::step, 0], x_np[::step, 1],\n",
    "                  gt_normalized[::step, 0], gt_normalized[::step, 1],\n",
    "                  color='#f97316', scale=20, width=0.005, alpha=0.4,\n",
    "                  label='Toward nearest center')\n",
    "    \n",
    "    # Mark cluster centers\n",
    "    for c in centers.numpy():\n",
    "        ax.plot(c[0], c[1], '*', color='#f97316', markersize=12,\n",
    "                markeredgecolor='white', markeredgewidth=1)\n",
    "    \n",
    "    ab_val = ab_t.item()\n",
    "    ax.set_title(f't = {t_val}\\n$\\\\bar\\\\alpha_t$ = {ab_val:.3f}', fontsize=11)\n",
    "    ax.set_xlim(-6, 6)\n",
    "    ax.set_ylim(-6, 6)\n",
    "    ax.set_aspect('equal')\n",
    "    if idx == 0:\n",
    "        ax.legend(fontsize=8, loc='lower left')\n",
    "\n",
    "plt.suptitle(\n",
    "    'Score-Noise Equivalence: Implied Score (purple) vs Direction to Nearest Center (orange)\\n'\n",
    "    'Low t = low noise (clustered) | High t = high noise (spread out)',\n",
    "    fontsize=12, y=1.05\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if implied_score is not None:\n",
    "    print('Observations:')\n",
    "    print('- The purple (implied score) and orange (ground truth direction) arrows')\n",
    "    print('  point in similar directions\\u2014the score-noise equivalence works!')\n",
    "    print('')\n",
    "    print('- At low t (little noise): samples are clustered near the data.')\n",
    "    print('  The score points precisely toward the nearest cluster center.')\n",
    "    print('')\n",
    "    print('- At high t (lots of noise): samples are spread out.')\n",
    "    print('  The score points generally \"inward\" toward the data region.')\n",
    "    print('  Less precise, but still directionally correct.')\n",
    "else:\n",
    "    print('implied_score is None\\u2014go back and fill in the TODOs.')\n",
    "    print('Check the solution below if you get stuck.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\ud83d\udca1 Solution</summary>\n",
    "\n",
    "The key insight is that the score-noise equivalence is a direct algebraic relationship. The noise prediction and the score point in opposite directions (noise points away from data, score points toward data), with a scaling factor that depends on the noise level.\n",
    "\n",
    "```python\n",
    "# Implied score from noise prediction:\n",
    "# score = -epsilon_theta / sqrt(1 - alpha_bar_t)\n",
    "implied_score = -eps_pred / torch.sqrt(1 - ab_t)\n",
    "\n",
    "# Direction toward nearest center:\n",
    "# For each point, find the closest cluster center and compute the direction vector.\n",
    "dists = torch.cdist(x_t, centers)  # (n_points, n_centers)\n",
    "closest = torch.argmin(dists, dim=1)  # Index of nearest center for each point\n",
    "gt_direction = centers[closest] - x_t  # Vector from x_t toward nearest center\n",
    "```\n",
    "\n",
    "**Why the negative sign:** The model predicts $\\epsilon$\u2014the noise that was ADDED to corrupt the data. Noise goes from clean to noisy, so it points AWAY from the data. The score points TOWARD the data. Hence the negative sign: `score = -epsilon / scale`.\n",
    "\n",
    "**Why the scaling factor:** At high noise ($\\bar\\alpha_t$ small, $1 - \\bar\\alpha_t$ large), the scaling factor $1/\\sqrt{1 - \\bar\\alpha_t}$ is close to 1\u2014the noise prediction and score have similar magnitude. At low noise ($\\bar\\alpha_t$ large, $1 - \\bar\\alpha_t$ small), the factor becomes large\u2014small noise predictions map to large scores. This makes sense: near-clean data has sharply peaked probability, so the score gradient is steep.\n",
    "\n",
    "**Common mistake:** Forgetting the negative sign. Without it, the arrows point away from the data instead of toward it.\n",
    "\n",
    "</details>\n",
    "\n",
    "### What Just Happened\n",
    "\n",
    "You verified the lesson's core reveal: the noise prediction IS a scaled version of the score function. Specifically:\n",
    "\n",
    "- The **implied score** (computed from the model's noise prediction using the equivalence formula) consistently points toward the data clusters. The model was learning the score function all along\u2014it just looked like it was predicting noise.\n",
    "- At **low noise** (low t, $\\bar\\alpha$ near 1), the score points precisely toward the nearest cluster. The model captures the fine structure of the data distribution.\n",
    "- At **high noise** (high t, $\\bar\\alpha$ near 0), the score points generally inward. The model captures the broad shape but not the fine details.\n",
    "- The **orange arrows** (ground truth direction to nearest center) and **purple arrows** (implied score) align, confirming the equivalence.\n",
    "\n",
    "This is the practical proof of $\\text{score}(x_t, t) \\approx -\\epsilon_\\theta(x_t, t) / \\sqrt{1 - \\bar\\alpha_t}$. The noise prediction network was a score estimator all along.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: SDE vs ODE Trajectories `[Independent]`\n",
    "\n",
    "From the lesson: the reverse SDE and probability flow ODE produce the same marginal distributions but follow different paths. The reverse SDE adds stochastic noise at each step (like DDPM sampling)\u2014paths are wiggly and diverse. The probability flow ODE is deterministic (like DDIM sampling)\u2014paths are smooth, and the same starting point always gives the same endpoint.\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Using the trained toy model from Exercise 3, implement both sampling methods:\n",
    "\n",
    "1. **Reverse SDE sampling** (DDPM-like): At each step, predict noise, remove it, AND add fresh stochastic noise. Different runs from the same starting point should give different endpoints.\n",
    "\n",
    "2. **Probability flow ODE sampling** (DDIM-like): At each step, predict noise and move deterministically toward clean data. NO stochastic noise injection. Same starting point should always give the same endpoint.\n",
    "\n",
    "Then:\n",
    "3. Plot several trajectories from the **same starting points** for both methods.\n",
    "4. Observe: SDE paths are wiggly and diverse. ODE paths are smooth and deterministic.\n",
    "5. Run the ODE twice from the same start\u2014verify identical endpoints. Run the SDE twice\u2014verify different endpoints.\n",
    "\n",
    "**Implementation hints:**\n",
    "\n",
    "The DDPM reverse step (reverse SDE, discrete approximation):\n",
    "```\n",
    "x_{t-1} = (1/sqrt(alpha_t)) * (x_t - (beta_t / sqrt(1 - alpha_bar_t)) * epsilon_theta(x_t, t)) + sigma_t * z\n",
    "```\n",
    "where `sigma_t = sqrt(beta_t)` and `z ~ N(0, I)` (set `z = 0` for the last step).\n",
    "\n",
    "The DDIM step (probability flow ODE, discrete approximation):\n",
    "```\n",
    "predicted_x0 = (x_t - sqrt(1 - alpha_bar_t) * epsilon_theta) / sqrt(alpha_bar_t)\n",
    "x_{t-1} = sqrt(alpha_bar_{t-1}) * predicted_x0 + sqrt(1 - alpha_bar_{t-1}) * epsilon_theta\n",
    "```\n",
    "\n",
    "**Before running, predict:**\n",
    "- If you run the ODE from the same starting noise twice (same seed), will the trajectories overlap exactly?\n",
    "- If you run the SDE from the same starting noise twice (same seed), will the endpoints be the same?\n",
    "- Which method produces more diverse samples from the same starting points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your SDE vs ODE trajectory code goes here.\n",
    "#\n",
    "# Suggested structure:\n",
    "#\n",
    "# 1. Define a function sample_sde(model, alpha_bars, betas, alphas, x_T, T)\n",
    "#    that implements DDPM-style reverse sampling with stochastic noise.\n",
    "#    Return both the final samples AND the full trajectory (list of x at each step).\n",
    "#\n",
    "# 2. Define a function sample_ode(model, alpha_bars, x_T, T)\n",
    "#    that implements DDIM-style deterministic sampling (no stochastic noise).\n",
    "#    Return both the final samples AND the full trajectory.\n",
    "#\n",
    "# 3. Generate N starting points from pure noise (x_T ~ N(0, I)).\n",
    "#\n",
    "# 4. Run both methods from the SAME starting points.\n",
    "#    For the SDE, run it TWICE to show different endpoints.\n",
    "#    For the ODE, run it TWICE to show identical endpoints.\n",
    "#\n",
    "# 5. Plot:\n",
    "#    - Panel 1: SDE trajectories (wiggly, different colors per trajectory)\n",
    "#    - Panel 2: ODE trajectories (smooth, different colors per trajectory)\n",
    "#    - Panel 3: SDE run 1 vs run 2 endpoints (should differ)\n",
    "#    - Panel 4: ODE run 1 vs run 2 endpoints (should match)\n",
    "#    - Mark the data cluster centers for reference.\n",
    "#\n",
    "# Remember:\n",
    "# - The model, alpha_bars, betas, alphas are already defined from Exercise 3.\n",
    "# - model.eval() is already set.\n",
    "# - Use torch.no_grad() for inference.\n",
    "# - For the SDE last step (t=0), set the noise z = 0.\n",
    "# - For the ODE, clip predicted_x0 to a reasonable range (e.g., -5 to 5)\n",
    "#   to prevent numerical instability at early timesteps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\ud83d\udca1 Solution</summary>\n",
    "\n",
    "The core insight is that the SDE and ODE are two ways to traverse the same generative landscape. The SDE adds exploration (random noise at each step), making paths wiggly and diverse. The ODE commits to a deterministic path, making it smooth and repeatable. This is the same distinction as DDPM vs DDIM\u2014now seen through the formal SDE/ODE lens.\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "def sample_sde(model, alpha_bars, betas, alphas, x_T, T):\n",
    "    \"\"\"Reverse SDE sampling (DDPM-like): deterministic denoising + stochastic noise.\"\"\"\n",
    "    trajectory = [x_T.clone()]\n",
    "    x = x_T.clone()\n",
    "    \n",
    "    for t_val in reversed(range(T)):\n",
    "        t_tensor = torch.full((x.shape[0],), t_val, dtype=torch.long)\n",
    "        eps_pred = model(x, t_tensor)\n",
    "        \n",
    "        alpha_t = alphas[t_val]\n",
    "        beta_t = betas[t_val]\n",
    "        ab_t = alpha_bars[t_val]\n",
    "        \n",
    "        # DDPM reverse step: remove predicted noise, add fresh stochastic noise\n",
    "        mean = (1 / torch.sqrt(alpha_t)) * (x - (beta_t / torch.sqrt(1 - ab_t)) * eps_pred)\n",
    "        \n",
    "        # Stochastic noise (zero at last step)\n",
    "        if t_val > 0:\n",
    "            sigma_t = torch.sqrt(beta_t)\n",
    "            z = torch.randn_like(x)\n",
    "            x = mean + sigma_t * z\n",
    "        else:\n",
    "            x = mean\n",
    "        \n",
    "        if t_val % 5 == 0:  # Store every 5th step for trajectory plotting\n",
    "            trajectory.append(x.clone())\n",
    "    \n",
    "    return x, trajectory\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_ode(model, alpha_bars, x_T, T):\n",
    "    \"\"\"Probability flow ODE sampling (DDIM-like): fully deterministic.\"\"\"\n",
    "    trajectory = [x_T.clone()]\n",
    "    x = x_T.clone()\n",
    "    \n",
    "    for t_val in reversed(range(T)):\n",
    "        t_tensor = torch.full((x.shape[0],), t_val, dtype=torch.long)\n",
    "        eps_pred = model(x, t_tensor)\n",
    "        \n",
    "        ab_t = alpha_bars[t_val]\n",
    "        ab_prev = alpha_bars[t_val - 1] if t_val > 0 else torch.tensor(1.0)\n",
    "        \n",
    "        # DDIM step: predict x_0, then jump to x_{t-1}\n",
    "        pred_x0 = (x - torch.sqrt(1 - ab_t) * eps_pred) / torch.sqrt(ab_t)\n",
    "        pred_x0 = pred_x0.clamp(-5, 5)  # Numerical stability\n",
    "        \n",
    "        # Deterministic step toward predicted x_0 (no noise!)\n",
    "        x = torch.sqrt(ab_prev) * pred_x0 + torch.sqrt(1 - ab_prev) * eps_pred\n",
    "        \n",
    "        if t_val % 5 == 0:\n",
    "            trajectory.append(x.clone())\n",
    "    \n",
    "    # Final x is the predicted clean sample\n",
    "    return pred_x0, trajectory\n",
    "\n",
    "\n",
    "# Generate starting points\n",
    "n_trajectories = 8\n",
    "torch.manual_seed(42)\n",
    "x_T = torch.randn(n_trajectories, 2) * 2  # Starting noise\n",
    "\n",
    "# Run SDE twice (different endpoints expected)\n",
    "torch.manual_seed(100)\n",
    "sde_final_1, sde_traj_1 = sample_sde(model, alpha_bars, betas, alphas, x_T.clone(), T)\n",
    "torch.manual_seed(200)  # Different seed for stochastic noise\n",
    "sde_final_2, sde_traj_2 = sample_sde(model, alpha_bars, betas, alphas, x_T.clone(), T)\n",
    "\n",
    "# Run ODE twice (identical endpoints expected)\n",
    "ode_final_1, ode_traj_1 = sample_ode(model, alpha_bars, x_T.clone(), T)\n",
    "ode_final_2, ode_traj_2 = sample_ode(model, alpha_bars, x_T.clone(), T)\n",
    "\n",
    "# Plot\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, n_trajectories))\n",
    "centers_np = np.array([[-2, -2], [-2, 2], [2, -2], [2, 2]])\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "# Panel 1: SDE trajectories (run 1)\n",
    "ax = axes[0]\n",
    "for i in range(n_trajectories):\n",
    "    pts = np.array([t[i].numpy() for t in sde_traj_1])\n",
    "    ax.plot(pts[:, 0], pts[:, 1], '-', color=colors[i], alpha=0.6, linewidth=1)\n",
    "    ax.plot(pts[0, 0], pts[0, 1], 'o', color=colors[i], markersize=6)\n",
    "    ax.plot(pts[-1, 0], pts[-1, 1], 's', color=colors[i], markersize=6)\n",
    "for c in centers_np:\n",
    "    ax.plot(c[0], c[1], '*', color='#f97316', markersize=15, markeredgecolor='white')\n",
    "ax.set_title('SDE Trajectories\\n(wiggly, stochastic)', fontsize=11)\n",
    "ax.set_xlim(-5, 5); ax.set_ylim(-5, 5); ax.set_aspect('equal')\n",
    "\n",
    "# Panel 2: ODE trajectories\n",
    "ax = axes[1]\n",
    "for i in range(n_trajectories):\n",
    "    pts = np.array([t[i].numpy() for t in ode_traj_1])\n",
    "    ax.plot(pts[:, 0], pts[:, 1], '-', color=colors[i], alpha=0.6, linewidth=1)\n",
    "    ax.plot(pts[0, 0], pts[0, 1], 'o', color=colors[i], markersize=6)\n",
    "    ax.plot(pts[-1, 0], pts[-1, 1], 's', color=colors[i], markersize=6)\n",
    "for c in centers_np:\n",
    "    ax.plot(c[0], c[1], '*', color='#f97316', markersize=15, markeredgecolor='white')\n",
    "ax.set_title('ODE Trajectories\\n(smooth, deterministic)', fontsize=11)\n",
    "ax.set_xlim(-5, 5); ax.set_ylim(-5, 5); ax.set_aspect('equal')\n",
    "\n",
    "# Panel 3: SDE run 1 vs run 2\n",
    "ax = axes[2]\n",
    "s1 = sde_final_1.numpy()\n",
    "s2 = sde_final_2.numpy()\n",
    "ax.scatter(s1[:, 0], s1[:, 1], s=60, c='#a78bfa', marker='o', label='SDE run 1', zorder=3)\n",
    "ax.scatter(s2[:, 0], s2[:, 1], s=60, c='#f97316', marker='^', label='SDE run 2', zorder=3)\n",
    "for i in range(n_trajectories):\n",
    "    ax.plot([s1[i, 0], s2[i, 0]], [s1[i, 1], s2[i, 1]], '--', color='white', alpha=0.3)\n",
    "for c in centers_np:\n",
    "    ax.plot(c[0], c[1], '*', color='#f97316', markersize=15, markeredgecolor='white')\n",
    "ax.set_title('SDE: Same Start, Different Ends\\n(stochastic = diverse)', fontsize=11)\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xlim(-5, 5); ax.set_ylim(-5, 5); ax.set_aspect('equal')\n",
    "\n",
    "# Panel 4: ODE run 1 vs run 2\n",
    "ax = axes[3]\n",
    "o1 = ode_final_1.numpy()\n",
    "o2 = ode_final_2.numpy()\n",
    "ax.scatter(o1[:, 0], o1[:, 1], s=60, c='#a78bfa', marker='o', label='ODE run 1', zorder=3)\n",
    "ax.scatter(o2[:, 0], o2[:, 1], s=60, c='#f97316', marker='^', label='ODE run 2', zorder=3)\n",
    "for c in centers_np:\n",
    "    ax.plot(c[0], c[1], '*', color='#f97316', markersize=15, markeredgecolor='white')\n",
    "ax.set_title('ODE: Same Start, Same Ends\\n(deterministic = repeatable)', fontsize=11)\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xlim(-5, 5); ax.set_ylim(-5, 5); ax.set_aspect('equal')\n",
    "\n",
    "plt.suptitle(\n",
    "    'Reverse SDE (DDPM-like) vs Probability Flow ODE (DDIM-like)\\n'\n",
    "    'Same trained model, same starting noise, different sampling strategies',\n",
    "    fontsize=13, y=1.05,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quantify the difference\n",
    "ode_diff = torch.norm(ode_final_1 - ode_final_2, dim=1).mean().item()\n",
    "sde_diff = torch.norm(sde_final_1 - sde_final_2, dim=1).mean().item()\n",
    "print(f'ODE: average endpoint difference between runs: {ode_diff:.6f} (should be ~0)')\n",
    "print(f'SDE: average endpoint difference between runs: {sde_diff:.4f} (should be > 0)')\n",
    "print()\n",
    "print('The ODE is deterministic: same start -> same end, every time.')\n",
    "print('The SDE is stochastic: same start -> different ends, every time.')\n",
    "print()\n",
    "print('This is DDPM vs DDIM, seen through the SDE/ODE lens.')\n",
    "print('Same model. Same score function. Different sampling strategy.')\n",
    "```\n",
    "\n",
    "**Key decisions:**\n",
    "- Storing every 5th step keeps trajectories visible without overwhelming the plot.\n",
    "- Clamping `pred_x0` to [-5, 5] in the ODE prevents numerical instability at early timesteps where $\\bar\\alpha_t$ is very small.\n",
    "- Using different random seeds for the two SDE runs ensures different stochastic noise, producing different endpoints.\n",
    "- The ODE does not use any random noise, so no seed matters after the starting points are fixed.\n",
    "\n",
    "**Common mistakes:**\n",
    "- Adding stochastic noise in the ODE sampler. The ODE is deterministic\u2014no `z * sigma` term.\n",
    "- Forgetting to set `z = 0` for the last SDE step (t=0). Adding noise at the very end corrupts the final sample.\n",
    "- Not using `torch.no_grad()`. This wastes memory by storing gradients during inference.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The score function is the gradient of log probability.** $\\text{score}(x) = \\nabla_x \\log p(x)$. At every point in data space, it gives you a direction toward higher probability. It is zero at peaks (nowhere better to go) and large far from peaks (strong pull toward data). It is a compass toward likely data.\n",
    "\n",
    "2. **The score field simplifies with noise.** Adding noise to the data distribution smooths the score field. At high noise, the field is nearly linear (easy for a model to learn). At low noise, the field is complex (hard to learn). This is why the noise schedule matters\u2014it controls the difficulty progression from easy tasks to hard tasks.\n",
    "\n",
    "3. **DDPM's noise prediction IS the score function (up to scaling).** $\\text{score}(x_t, t) \\approx -\\epsilon_\\theta(x_t, t) / \\sqrt{1 - \\bar\\alpha_t}$. The model was always learning to point toward data. Nothing new was trained\u2014the score was hiding inside DDPM all along.\n",
    "\n",
    "4. **The reverse SDE (DDPM-like) and probability flow ODE (DDIM-like) are two ways to sample.** Both use the same trained model and the same score function. The SDE adds stochastic exploration\u2014wiggly paths, diverse outputs. The ODE commits deterministically\u2014smooth paths, repeatable outputs. Same landscape, different traversal strategy.\n",
    "\n",
    "5. **This is the unifying framework.** DDPM, DDIM, DPM-Solver\u2014different vocabulary for the same underlying process. A trained noise prediction network is a score estimator. Samplers are different strategies for following the score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
