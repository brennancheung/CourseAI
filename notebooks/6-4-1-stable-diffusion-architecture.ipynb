{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Stable Diffusion Pipeline\n",
    "\n",
    "**Module 6.4, Lesson 1** | CourseAI\n",
    "\n",
    "In the lesson, you traced the complete Stable Diffusion pipeline on paper: text prompt -> CLIP tokenizer -> CLIP text encoder -> denoising loop (U-Net x2 for CFG, timestep conditioning, cross-attention) -> VAE decoder -> pixel image. Now you will do it hands-on with real pre-trained components.\n",
    "\n",
    "**What you will do:**\n",
    "- Load the three SD components separately (CLIP, VAE, U-Net) and inspect their parameter counts\n",
    "- Trace the CLIP stage: tokenize a prompt, inspect token IDs and padding, verify the output embedding shape\n",
    "- Trace one denoising step manually: two U-Net forward passes, CFG combination, scheduler step\n",
    "- Execute the complete pipeline manually from text prompt to generated image, verifying every tensor shape\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "This is a CONSOLIDATE notebook. No new algorithms, no new math. Every concept here is something you already know from Modules 6.1\u20136.3. The exercises verify that you can trace the pipeline and identify components, not implement anything from scratch.\n",
    "\n",
    "**Estimated time:** 30\u201345 minutes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies and import everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float16 if device.type == 'cuda' else torch.float32\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB')\n",
    "\n",
    "print('\\nSetup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Load and Inspect Components [Guided]\n",
    "\n",
    "Stable Diffusion is not one big model. It is **three independently trained models** connected by tensor handoffs:\n",
    "\n",
    "1. **CLIP text encoder** \u2014 translates text to embedding vectors\n",
    "2. **VAE** \u2014 translates between pixel space and latent space\n",
    "3. **U-Net** \u2014 denoises latent tensors, conditioned on text and timestep\n",
    "\n",
    "Each was trained separately, with a different loss, on different data. They were never trained together.\n",
    "\n",
    "In this exercise, you will load each component separately from `diffusers` and inspect their parameter counts. The lesson predicted: CLIP ~123M, VAE ~84M, U-Net ~860M, total ~1.07B.\n",
    "\n",
    "**Before running, predict:**\n",
    "- How many parameters does the U-Net have? (The lesson said ~860M. It is by far the largest component because denoising at multiple scales is a hard task.)\n",
    "- What is the total parameter count across all three components? (~1.07B)\n",
    "- Which component has the fewest parameters? (The VAE at ~84M. It only needs to encode/decode images, not generate them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "\n",
    "model_id = 'stable-diffusion-v1-5/stable-diffusion-v1-5'\n",
    "\n",
    "# Load each component separately -- they live in different subfolders\n",
    "# of the same model repository. This mirrors the modular architecture:\n",
    "# each component is independent and swappable.\n",
    "\n",
    "print('Loading CLIP tokenizer and text encoder...')\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder='tokenizer')\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    model_id, subfolder='text_encoder', torch_dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print('Loading VAE...')\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    model_id, subfolder='vae', torch_dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print('Loading U-Net...')\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    model_id, subfolder='unet', torch_dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "# Set all to eval mode -- no training, no gradients\n",
    "text_encoder.eval()\n",
    "vae.eval()\n",
    "unet.eval()\n",
    "\n",
    "# Count parameters for each component\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "clip_params = count_params(text_encoder)\n",
    "vae_params = count_params(vae)\n",
    "unet_params = count_params(unet)\n",
    "total_params = clip_params + vae_params + unet_params\n",
    "\n",
    "print('\\n=== Component Parameter Counts ===')\n",
    "print(f'CLIP text encoder: {clip_params / 1e6:>8.1f}M parameters')\n",
    "print(f'VAE:               {vae_params / 1e6:>8.1f}M parameters')\n",
    "print(f'U-Net:             {unet_params / 1e6:>8.1f}M parameters')\n",
    "print(f'{\"-\" * 42}')\n",
    "print(f'Total:             {total_params / 1e6:>8.1f}M parameters')\n",
    "print()\n",
    "print(f'The U-Net is {unet_params / clip_params:.1f}x larger than CLIP')\n",
    "print(f'The U-Net is {unet_params / vae_params:.1f}x larger than the VAE')\n",
    "print(f'The U-Net accounts for {unet_params / total_params * 100:.0f}% of total parameters')\n",
    "print()\n",
    "print('Each component was trained independently:')\n",
    "print('  CLIP:  contrastive loss on 400M text-image pairs (by OpenAI)')\n",
    "print('  VAE:   perceptual + adversarial loss on image reconstruction')\n",
    "print('  U-Net: MSE loss on noise prediction in latent space')\n",
    "print()\n",
    "print('They communicate through tensor shapes, not shared weights.')\n",
    "print('That is why you can swap any component independently.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You loaded the three components of Stable Diffusion separately and verified their parameter counts:\n",
    "\n",
    "- **CLIP text encoder:** ~123M parameters. Translates text prompts into 768-dimensional embedding vectors.\n",
    "- **VAE:** ~84M parameters. Translates between pixel space (512x512x3) and latent space (64x64x4).\n",
    "- **U-Net:** ~860M parameters. The denoising workhorse. Accounts for ~80% of the total parameters.\n",
    "\n",
    "The parameter counts confirm the lesson's predictions. The U-Net is by far the largest component because denoising at multiple scales \u2014 with cross-attention at each resolution, adaptive group normalization, and skip connections \u2014 is a complex task. But parameter count does not equal importance: without CLIP, you lose text control; without the VAE, you lose the 48x speed advantage.\n",
    "\n",
    "The key insight: **three independently trained models, connected by tensor handoffs.** Each was trained with a different loss on different data. They were never trained together.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Trace the CLIP Stage [Guided]\n",
    "\n",
    "The pipeline starts with text. But the U-Net cannot read text \u2014 it operates on tensors. CLIP is the translator.\n",
    "\n",
    "The CLIP stage has two parts:\n",
    "1. **Tokenizer:** splits text into subword tokens, adds special tokens (SOT/EOT), pads to a fixed length of **77 tokens**\n",
    "2. **Text encoder:** a transformer that processes the 77 token IDs and outputs 77 contextual embedding vectors, each 768 dimensions\n",
    "\n",
    "The lesson predicted: any prompt, regardless of length, produces a **[batch, 77, 768]** output tensor. The 77 comes from padding; the 768 comes from CLIP's embedding dimension. This [77, 768] tensor is the **only thing** the rest of the pipeline sees.\n",
    "\n",
    "**Before running, predict:**\n",
    "- How many token positions will the tokenizer output? (77 \u2014 always, regardless of prompt length. Shorter prompts are padded.)\n",
    "- What shape will the CLIP text encoder output? ([1, 77, 768] \u2014 batch size 1, 77 token positions, 768 embedding dimensions.)\n",
    "- If you tokenize a completely different prompt (shorter or longer), will the output shape change? (No \u2014 both will be [1, 77, 768]. The padding ensures a fixed-size output.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Part 1: Tokenize the prompt ----\n",
    "prompt = \"a cat sitting on a beach at sunset\"\n",
    "\n",
    "# Tokenize with padding to 77 (CLIP's fixed context length)\n",
    "tokens = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',\n",
    "    max_length=77,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "token_ids = tokens.input_ids  # (1, 77)\n",
    "\n",
    "print(f'Prompt: \"{prompt}\"')\n",
    "print(f'Token IDs shape: {token_ids.shape}')\n",
    "print(f'Token IDs: {token_ids[0].tolist()}')\n",
    "print()\n",
    "\n",
    "# Decode individual tokens to see what the tokenizer did\n",
    "decoded_tokens = [tokenizer.decode([tid]) for tid in token_ids[0].tolist()]\n",
    "non_pad_count = sum(1 for tid in token_ids[0].tolist() if tid != tokenizer.pad_token_id)\n",
    "\n",
    "print(f'First {non_pad_count} tokens (non-padding):')\n",
    "for i in range(non_pad_count):\n",
    "    tid = token_ids[0, i].item()\n",
    "    print(f'  Position {i:2d}: ID {tid:>5d} -> \"{decoded_tokens[i]}\"')\n",
    "\n",
    "print(f'\\nRemaining {77 - non_pad_count} positions are padding (ID {tokenizer.pad_token_id})')\n",
    "print(f'Total: {token_ids.shape[1]} token positions (always 77, regardless of prompt length)')\n",
    "print()\n",
    "print('Notice the special tokens:')\n",
    "print(f'  Position 0: SOT (start-of-text) token ID {token_ids[0, 0].item()}')\n",
    "print(f'  Position {non_pad_count - 1}: EOT (end-of-text) token ID {token_ids[0, non_pad_count - 1].item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Part 2: Run the CLIP text encoder ----\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(\n",
    "        token_ids.to(device)\n",
    "    ).last_hidden_state  # (1, 77, 768)\n",
    "\n",
    "print(f'Text embeddings shape: {text_embeddings.shape}')\n",
    "print(f'Text embeddings dtype: {text_embeddings.dtype}')\n",
    "print(f'Value range: [{text_embeddings.min():.3f}, {text_embeddings.max():.3f}]')\n",
    "print()\n",
    "print('This [1, 77, 768] tensor is what the U-Net receives via cross-attention.')\n",
    "print('Each of the 77 positions is a 768-dimensional contextual embedding.')\n",
    "print('These are NOT simple lookup embeddings -- each token\\'s representation')\n",
    "print('includes context from all other tokens via self-attention inside CLIP.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Part 3: Verify shape is prompt-independent ----\n",
    "# The lesson claimed: 77x768 regardless of prompt length. Let's verify.\n",
    "\n",
    "short_prompt = \"dog\"\n",
    "long_prompt = \"a highly detailed oil painting of a golden retriever puppy playing in a field of wildflowers under a dramatic sky with rays of light\"\n",
    "\n",
    "for name, p in [(\"short\", short_prompt), (\"long\", long_prompt)]:\n",
    "    t = tokenizer(p, return_tensors='pt', padding='max_length', max_length=77, truncation=True)\n",
    "    non_pad = sum(1 for tid in t.input_ids[0].tolist() if tid != tokenizer.pad_token_id)\n",
    "    with torch.no_grad():\n",
    "        emb = text_encoder(t.input_ids.to(device)).last_hidden_state\n",
    "    print(f'{name:5s} prompt: \"{p}\"')\n",
    "    print(f'       Non-padding tokens: {non_pad}/77, Embedding shape: {emb.shape}')\n",
    "    print()\n",
    "\n",
    "print('Both produce [1, 77, 768] -- the shape is always the same.')\n",
    "print()\n",
    "print('Why? The tokenizer PADS short prompts to 77 and TRUNCATES long ones to 77.')\n",
    "print('CLIP was trained with a fixed context length of 77 tokens.')\n",
    "print('This fixed shape is what makes the interface between CLIP and the U-Net')\n",
    "print('standardized: the U-Net always receives [batch, 77, 768] as K/V for cross-attention,')\n",
    "print('regardless of what the user typed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You traced the complete CLIP stage:\n",
    "\n",
    "1. **Tokenizer:** The prompt \"a cat sitting on a beach at sunset\" was split into subword tokens, with SOT/EOT special tokens added and padding to exactly 77 positions. This is the same BPE-style tokenization you learned in **Tokenization** (Series 4), just with CLIP's vocabulary.\n",
    "\n",
    "2. **Text encoder:** CLIP's transformer processed the 77 token IDs and output a [1, 77, 768] tensor of contextual embeddings. These are the K and V inputs for cross-attention inside the U-Net.\n",
    "\n",
    "3. **Shape invariance:** Different prompts (short, long) all produce the same [1, 77, 768] output. The tokenizer handles padding/truncation. This fixed interface is what makes the pipeline modular: the U-Net always expects [batch, 77, 768], no matter what.\n",
    "\n",
    "The U-Net will never see the text \"a cat sitting on a beach at sunset.\" It will only see a 77x768 tensor of floating-point numbers. **CLIP is the translator from human language to the geometric representation space the U-Net operates in.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Trace One Denoising Step [Supported]\n",
    "\n",
    "The denoising loop is the heart of the pipeline. At each step, the U-Net runs **twice** (once unconditional, once with text), and the results are combined with the CFG formula.\n",
    "\n",
    "Your task: trace a single denoising step manually. You will:\n",
    "1. Sample random noise z_T in latent space\n",
    "2. Run one U-Net forward pass with the text embeddings (conditional)\n",
    "3. Run a second U-Net forward pass with empty-string embeddings (unconditional)\n",
    "4. Apply the CFG formula: eps_cfg = eps_uncond + w * (eps_cond - eps_uncond)\n",
    "5. Apply one scheduler step to get z_{T-1}\n",
    "6. Compare z_T and z_{T-1}\n",
    "\n",
    "**Hints:**\n",
    "- z_T has shape [1, 4, 64, 64] (batch=1, 4 latent channels, 64x64 spatial)\n",
    "- The U-Net expects: `unet(sample, timestep, encoder_hidden_states=...).sample`\n",
    "- Empty-string embeddings: tokenize `\"\"` and run through the text encoder, same as any other prompt\n",
    "- The scheduler needs `scheduler.set_timesteps(num_steps)` before use, and `scheduler.step(noise_pred, t, z_t).prev_sample` for a step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "\n",
    "# Load the DDPM scheduler (the same algorithm from Module 6.2)\n",
    "scheduler = DDPMScheduler.from_pretrained(model_id, subfolder='scheduler')\n",
    "num_steps = 50\n",
    "scheduler.set_timesteps(num_steps)\n",
    "\n",
    "print(f'Scheduler: {scheduler.__class__.__name__}')\n",
    "print(f'Number of denoising steps: {num_steps}')\n",
    "print(f'Timesteps: {scheduler.timesteps[:5].tolist()} ... {scheduler.timesteps[-3:].tolist()}')\n",
    "print(f'  (From high noise to low noise, just like the sampling loop in Module 6.2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 1: Sample z_T (pure noise in latent space) ----\n",
    "\n",
    "# TODO: Create a random tensor of shape (1, 4, 64, 64) on the correct device and dtype.\n",
    "# Hint: torch.randn(shape, device=device, dtype=dtype)\n",
    "z_t = None  # TODO\n",
    "\n",
    "if z_t is not None:\n",
    "    print(f'z_T shape: {z_t.shape}')   # Should be (1, 4, 64, 64)\n",
    "    print(f'z_T dtype: {z_t.dtype}')\n",
    "    print(f'z_T value range: [{z_t.min():.3f}, {z_t.max():.3f}]')\n",
    "    print(f'z_T mean: {z_t.mean():.4f} (should be near 0)')\n",
    "    print(f'z_T std: {z_t.std():.4f} (should be near 1)')\n",
    "    print()\n",
    "    print('This is the starting point for generation: pure random noise in latent space.')\n",
    "    print('No input image, no VAE encoder. Just N(0, I).')\n",
    "else:\n",
    "    print('Fill in the TODO above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 2: Prepare text embeddings (conditional and unconditional) ----\n",
    "\n",
    "# The conditional embeddings are the ones we computed in Exercise 2.\n",
    "# We already have `text_embeddings` from that exercise: shape (1, 77, 768).\n",
    "\n",
    "# TODO: Create the unconditional embeddings by encoding an empty string \"\".\n",
    "# Hint: Use the same tokenizer + text_encoder pattern from Exercise 2.\n",
    "# 1. Tokenize \"\" with the same padding settings\n",
    "# 2. Run through text_encoder to get the embeddings\n",
    "uncond_tokens = None  # TODO: tokenizer(\"\", return_tensors='pt', padding='max_length', max_length=77, truncation=True)\n",
    "uncond_embeddings = None  # TODO: text_encoder(uncond_tokens.input_ids.to(device)).last_hidden_state\n",
    "\n",
    "if uncond_embeddings is not None:\n",
    "    print(f'Conditional embeddings shape:   {text_embeddings.shape}')\n",
    "    print(f'Unconditional embeddings shape: {uncond_embeddings.shape}')\n",
    "    print()\n",
    "    print('Both are [1, 77, 768] -- same shape, different content.')\n",
    "    print('The conditional embeddings encode \"a cat sitting on a beach at sunset\".')\n",
    "    print('The unconditional embeddings encode \"\" (empty string).')\n",
    "    print('The U-Net will run once with each, and CFG will combine them.')\n",
    "else:\n",
    "    print('Fill in the TODOs above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 3: Run two U-Net forward passes ----\n",
    "\n",
    "# We will use the FIRST timestep from the scheduler (highest noise level).\n",
    "t = scheduler.timesteps[0]\n",
    "print(f'Timestep: {t}')\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Unconditional pass: U-Net with empty-string embeddings\n",
    "    eps_uncond = unet(z_t, t, encoder_hidden_states=uncond_embeddings).sample\n",
    "\n",
    "    # Conditional pass: U-Net with real text embeddings\n",
    "    eps_cond = unet(z_t, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "print(f'eps_uncond shape: {eps_uncond.shape}')  # Should match z_t: (1, 4, 64, 64)\n",
    "print(f'eps_cond shape:   {eps_cond.shape}')    # Should match z_t: (1, 4, 64, 64)\n",
    "print(f'z_t shape:        {z_t.shape}')\n",
    "print()\n",
    "print('Both noise predictions have the SAME shape as z_t.')\n",
    "print('The U-Net predicts: \"this is how much noise I think is in z_t.\"')\n",
    "print('Same architecture, same weights, same z_t, same timestep.')\n",
    "print('The ONLY difference: which text embeddings cross-attention used.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 4: Apply the CFG formula ----\n",
    "\n",
    "guidance_scale = 7.5  # Typical value from the lesson\n",
    "\n",
    "# TODO: Apply the CFG formula.\n",
    "# eps_cfg = eps_uncond + guidance_scale * (eps_cond - eps_uncond)\n",
    "eps_cfg = None  # TODO\n",
    "\n",
    "if eps_cfg is not None:\n",
    "    print(f'eps_cfg shape: {eps_cfg.shape}')  # Same as z_t: (1, 4, 64, 64)\n",
    "    print(f'guidance_scale: {guidance_scale}')\n",
    "    print()\n",
    "    \n",
    "    # How different are the conditional and unconditional predictions?\n",
    "    diff = (eps_cond - eps_uncond).float()\n",
    "    print(f'Mean |eps_cond - eps_uncond|: {diff.abs().mean():.4f}')\n",
    "    print(f'This is the \"text direction\" -- the signal that CFG amplifies by {guidance_scale}x.')\n",
    "    print()\n",
    "    print('The CFG formula steers TOWARD the text prompt.')\n",
    "    print('At w=1.0: no amplification (just eps_cond).')\n",
    "    print('At w=7.5: strong amplification of the text direction.')\n",
    "    print('At w=20+: oversaturated, artifacts.')\n",
    "else:\n",
    "    print('Fill in the TODO above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 5: Apply one scheduler step ----\n",
    "\n",
    "# TODO: Use the scheduler to compute z_{t-1} from z_t and the CFG noise prediction.\n",
    "# Hint: scheduler.step(eps_cfg, t, z_t).prev_sample\n",
    "z_t_minus_1 = None  # TODO\n",
    "\n",
    "if z_t_minus_1 is not None:\n",
    "    print(f'z_t shape:     {z_t.shape}')\n",
    "    print(f'z_{{t-1}} shape: {z_t_minus_1.shape}')\n",
    "    print()\n",
    "    \n",
    "    # Compare z_t and z_{t-1}\n",
    "    z_diff = (z_t.float() - z_t_minus_1.float()).abs()\n",
    "    print(f'Mean |z_t - z_{{t-1}}|: {z_diff.mean():.4f}')\n",
    "    print(f'z_t std:     {z_t.float().std():.4f}')\n",
    "    print(f'z_{{t-1}} std: {z_t_minus_1.float().std():.4f}')\n",
    "    print()\n",
    "    print('z_{t-1} is slightly different from z_t. One small step of denoising.')\n",
    "    print('This is the reverse step formula from Sampling and Generation (Module 6.2).')\n",
    "    print('Repeat this 49 more times and you get z_0 -- a clean latent.')\n",
    "else:\n",
    "    print('Fill in the TODO above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The key insight: one denoising step requires **two full U-Net forward passes** (unconditional + conditional), a CFG combination, and a scheduler step. With 50 steps, that is 100 U-Net forward passes total. CFG is not post-processing -- it is woven into every step.\n",
    "\n",
    "**Step 1: Sample z_T**\n",
    "```python\n",
    "z_t = torch.randn(1, 4, 64, 64, device=device, dtype=dtype)\n",
    "```\n",
    "\n",
    "**Step 2: Unconditional embeddings**\n",
    "```python\n",
    "uncond_tokens = tokenizer(\"\", return_tensors='pt', padding='max_length', max_length=77, truncation=True)\n",
    "with torch.no_grad():\n",
    "    uncond_embeddings = text_encoder(uncond_tokens.input_ids.to(device)).last_hidden_state\n",
    "```\n",
    "\n",
    "**Step 4: CFG formula**\n",
    "```python\n",
    "eps_cfg = eps_uncond + guidance_scale * (eps_cond - eps_uncond)\n",
    "```\n",
    "This is the exact formula from **Text Conditioning & Guidance**: amplify the direction that the text embeddings push the prediction. At w=7.5, the text direction is amplified 7.5x.\n",
    "\n",
    "**Step 5: Scheduler step**\n",
    "```python\n",
    "z_t_minus_1 = scheduler.step(eps_cfg, t, z_t).prev_sample\n",
    "```\n",
    "This applies the reverse step formula from Module 6.2. The scheduler uses eps_cfg and the noise schedule to compute z_{t-1}.\n",
    "\n",
    "Common mistake: forgetting the `.prev_sample` accessor -- `scheduler.step()` returns a named tuple, not a tensor.\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Full Pipeline Trace [Independent]\n",
    "\n",
    "You now have all the pieces. In this exercise, you will execute the **complete Stable Diffusion pipeline** manually:\n",
    "\n",
    "1. **Tokenize** the prompt\n",
    "2. **CLIP encode** the tokens (conditional + unconditional embeddings)\n",
    "3. **Sample z_T** from N(0, I) in latent space\n",
    "4. **Run the full denoising loop** (50 steps, each with two U-Net passes + CFG + scheduler step)\n",
    "5. **VAE decode** z_0 to get a pixel image\n",
    "6. **Display** the generated image\n",
    "\n",
    "At every stage, print the tensor shape and verify it matches the lesson's predictions:\n",
    "- After tokenizer: [1, 77] int IDs\n",
    "- After CLIP: [1, 77, 768] float embeddings\n",
    "- z_T: [1, 4, 64, 64]\n",
    "- U-Net output at each step: [1, 4, 64, 64]\n",
    "- After VAE decode: [1, 3, 512, 512]\n",
    "\n",
    "**Time each stage** and answer the reflection question: which component took the longest to run? Why?\n",
    "\n",
    "**Key tips:**\n",
    "- Use all the components already loaded: `tokenizer`, `text_encoder`, `unet`, `vae`, `scheduler`\n",
    "- Re-initialize the scheduler timesteps with `scheduler.set_timesteps(50)`\n",
    "- The denoising loop iterates over `scheduler.timesteps`\n",
    "- VAE decode: `vae.decode(z_0 / vae.config.scaling_factor).sample`\n",
    "- To display: convert tensor to PIL with values clipped to [0, 1]\n",
    "- Wrap everything in `torch.no_grad()` for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#\n",
    "# Execute the complete Stable Diffusion pipeline manually:\n",
    "#   1. Tokenize the prompt \"a cat sitting on a beach at sunset\"\n",
    "#   2. Encode with CLIP (conditional + unconditional embeddings)\n",
    "#   3. Sample z_T from N(0, I)\n",
    "#   4. Denoising loop: for each timestep, two U-Net passes + CFG + scheduler step\n",
    "#   5. VAE decode z_0 to pixel image\n",
    "#   6. Display the result\n",
    "#\n",
    "# Print tensor shapes at every stage.\n",
    "# Time each stage (CLIP, denoising loop, VAE decode).\n",
    "#\n",
    "# Reflection: Which component took the longest? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The key insight: you are manually executing the exact pipeline that the lesson traced on paper. Every tensor shape matches the predictions. The denoising loop dominates compute time because it runs 100 U-Net forward passes (50 steps x 2 for CFG), while CLIP and the VAE decoder each run only once.\n",
    "\n",
    "```python\n",
    "prompt = \"a cat sitting on a beach at sunset\"\n",
    "num_steps = 50\n",
    "guidance_scale = 7.5\n",
    "\n",
    "# Re-seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Re-initialize scheduler\n",
    "scheduler.set_timesteps(num_steps)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # ---- Stage 1: CLIP encoding ----\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer(\n",
    "        prompt, return_tensors='pt',\n",
    "        padding='max_length', max_length=77, truncation=True\n",
    "    )\n",
    "    print(f'Token IDs shape: {tokens.input_ids.shape}')  # [1, 77]\n",
    "    \n",
    "    # Conditional embeddings\n",
    "    cond_emb = text_encoder(tokens.input_ids.to(device)).last_hidden_state\n",
    "    print(f'Conditional embeddings shape: {cond_emb.shape}')  # [1, 77, 768]\n",
    "    \n",
    "    # Unconditional embeddings\n",
    "    uncond_tok = tokenizer(\n",
    "        \"\", return_tensors='pt',\n",
    "        padding='max_length', max_length=77, truncation=True\n",
    "    )\n",
    "    uncond_emb = text_encoder(uncond_tok.input_ids.to(device)).last_hidden_state\n",
    "    print(f'Unconditional embeddings shape: {uncond_emb.shape}')  # [1, 77, 768]\n",
    "    \n",
    "    clip_time = time.time() - t0\n",
    "    print(f'\\nCLIP stage time: {clip_time:.2f}s')\n",
    "    \n",
    "    # ---- Stage 2: Sample z_T ----\n",
    "    z = torch.randn(1, 4, 64, 64, device=device, dtype=dtype)\n",
    "    print(f'\\nz_T shape: {z.shape}')  # [1, 4, 64, 64]\n",
    "    \n",
    "    # ---- Stage 3: Denoising loop ----\n",
    "    t0 = time.time()\n",
    "    \n",
    "    for i, t in enumerate(scheduler.timesteps):\n",
    "        # Two U-Net passes for CFG\n",
    "        eps_uncond = unet(z, t, encoder_hidden_states=uncond_emb).sample\n",
    "        eps_cond = unet(z, t, encoder_hidden_states=cond_emb).sample\n",
    "        \n",
    "        # CFG combine\n",
    "        eps = eps_uncond + guidance_scale * (eps_cond - eps_uncond)\n",
    "        \n",
    "        # Scheduler step\n",
    "        z = scheduler.step(eps, t, z).prev_sample\n",
    "        \n",
    "        if i == 0:\n",
    "            print(f'\\nStep {i}: U-Net output shape: {eps_cond.shape}, z shape: {z.shape}')\n",
    "    \n",
    "    denoise_time = time.time() - t0\n",
    "    print(f'\\nDenoising loop time ({num_steps} steps, {num_steps * 2} U-Net passes): {denoise_time:.2f}s')\n",
    "    print(f'z_0 shape: {z.shape}')  # [1, 4, 64, 64]\n",
    "    \n",
    "    # ---- Stage 4: VAE decode ----\n",
    "    t0 = time.time()\n",
    "    \n",
    "    image = vae.decode(z / vae.config.scaling_factor).sample\n",
    "    \n",
    "    vae_time = time.time() - t0\n",
    "    print(f'\\nVAE decode time: {vae_time:.2f}s')\n",
    "    print(f'Image shape: {image.shape}')  # [1, 3, 512, 512]\n",
    "\n",
    "# ---- Display ----\n",
    "img = image.detach().cpu().float().squeeze(0).permute(1, 2, 0).numpy()\n",
    "img = np.clip((img + 1.0) / 2.0, 0.0, 1.0)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.title(f'\"{prompt}\"\\n{num_steps} steps, guidance_scale={guidance_scale}', fontsize=11)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- Timing summary ----\n",
    "total_time = clip_time + denoise_time + vae_time\n",
    "print(f'\\n=== Timing Summary ===')\n",
    "print(f'CLIP encoding:   {clip_time:>6.2f}s  ({clip_time/total_time*100:>4.1f}%)')\n",
    "print(f'Denoising loop:  {denoise_time:>6.2f}s  ({denoise_time/total_time*100:>4.1f}%)')\n",
    "print(f'VAE decode:      {vae_time:>6.2f}s  ({vae_time/total_time*100:>4.1f}%)')\n",
    "print(f'Total:           {total_time:>6.2f}s')\n",
    "print()\n",
    "print(f'The denoising loop took {denoise_time/total_time*100:.0f}% of total time.')\n",
    "print(f'Why? It runs {num_steps * 2} U-Net forward passes ({num_steps} steps x 2 for CFG).')\n",
    "print(f'The U-Net has ~860M parameters. Each forward pass processes a [1, 4, 64, 64]')\n",
    "print(f'tensor through the full encoder-decoder architecture with cross-attention.')\n",
    "print(f'CLIP (once) and VAE decode (once) are negligible in comparison.')\n",
    "```\n",
    "\n",
    "**Shape verification summary:**\n",
    "| Stage | Shape | Matches lesson? |\n",
    "|-------|-------|-----------------|\n",
    "| Token IDs | [1, 77] | Yes |\n",
    "| CLIP embeddings | [1, 77, 768] | Yes |\n",
    "| z_T | [1, 4, 64, 64] | Yes |\n",
    "| U-Net output | [1, 4, 64, 64] | Yes |\n",
    "| z_0 | [1, 4, 64, 64] | Yes |\n",
    "| Decoded image | [1, 3, 512, 512] | Yes |\n",
    "\n",
    "**Reflection:** The denoising loop dominates because it runs 100 U-Net forward passes through a ~860M parameter network. CLIP runs once (~123M params). The VAE decoder runs once (~84M params, decoder only). The U-Net is both the largest component AND runs the most times. This is why faster samplers (next lesson) are so valuable -- reducing 50 steps to 20 steps cuts the U-Net passes from 100 to 40.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Stable Diffusion is three independently trained models connected by tensor handoffs.** CLIP (~123M params) translates text to [77, 768] embeddings. The U-Net (~860M params) denoises [4, 64, 64] latents using cross-attention for text, adaptive group norm for timestep, and CFG for amplification. The VAE (~84M params) decoder translates the final [4, 64, 64] latent to a [3, 512, 512] pixel image.\n",
    "\n",
    "2. **Every tensor shape matches the lesson's predictions.** Token IDs: [77]. CLIP embeddings: [77, 768]. Latent tensors: [4, 64, 64]. U-Net output: [4, 64, 64]. Decoded image: [3, 512, 512]. The shapes are the interface contract between components.\n",
    "\n",
    "3. **CFG requires two U-Net forward passes per step.** 50 steps means 100 U-Net forward passes. This is why the denoising loop dominates compute time. CFG is not post-processing -- it is woven into every step.\n",
    "\n",
    "4. **CLIP always produces [77, 768] regardless of prompt length.** Short prompts are padded, long prompts are truncated. This fixed interface is what makes the pipeline modular.\n",
    "\n",
    "5. **Nothing in this pipeline is new to you.** The tokenizer is from Series 4. CLIP is from Module 6.3. The U-Net architecture, timestep conditioning, cross-attention, and CFG are from Modules 6.2-6.3. The VAE is from Module 6.1. The latent diffusion concept is from Module 6.3. Every piece is something you built or deeply studied."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
