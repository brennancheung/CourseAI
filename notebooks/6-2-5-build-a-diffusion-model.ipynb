{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Diffusion Model\n",
    "\n",
    "**Module 6.2, Lesson 5 (Capstone)** | CourseAI\n",
    "\n",
    "You have spent four lessons building up every piece of the diffusion pipeline: the intuition for why it works, the math of noise destruction, the training objective, and the sampling algorithm. You have never written a single line of diffusion code. This notebook closes that gap.\n",
    "\n",
    "**What you will do:**\n",
    "- Implement the noise schedule and compute alpha-bar from scratch\n",
    "- Implement the forward process as a `q_sample()` function\n",
    "- Read and understand a minimal U-Net architecture (provided with annotations)\n",
    "- Fill in the DDPM training loop and train on MNIST\n",
    "- Implement the sampling algorithm and generate images from pure noise\n",
    "- Measure the computational cost of 1,000-step pixel-space sampling\n",
    "- Compare diffusion generation to VAE generation in quality and speed\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "Every line of code in this notebook comes from the last four lessons. The forward process formula is from *The Forward Process*. The training algorithm is from *Learning to Denoise*. The sampling loop is from *Sampling and Generation*. No new theory — just translation from math to PyTorch.\n",
    "\n",
    "**Estimated time:** 60–90 minutes (training takes 20–30 minutes on a Colab GPU).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies, import everything, and configure the environment.\n",
    "\n",
    "**Important:** Set the runtime to GPU before running. In Colab: Runtime → Change runtime type → T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('WARNING: No GPU detected. Training will be very slow.')\n",
    "    print('In Colab: Runtime → Change runtime type → T4 GPU')\n",
    "\n",
    "print('\\nSetup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST\n",
    "\n",
    "We normalize images to [-1, 1]. This connects to the **variance-preserving formulation** from *The Forward Process*: images are centered around 0 with roughly unit variance, so when we add unit-variance Gaussian noise, the total variance stays controlled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST, normalized to [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # maps [0,1] → [-1, 1]\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "print(f'Dataset size: {len(dataset)}')\n",
    "print(f'Image shape: {dataset[0][0].shape}')\n",
    "print(f'Pixel range: [{dataset[0][0].min():.1f}, {dataset[0][0].max():.1f}]')\n",
    "print(f'Batches per epoch: {len(dataloader)}')\n",
    "\n",
    "# Show a few examples\n",
    "fig, axes = plt.subplots(1, 8, figsize=(12, 2))\n",
    "for i in range(8):\n",
    "    img, label = dataset[i]\n",
    "    axes[i].imshow(img.squeeze(), cmap='gray', vmin=-1, vmax=1)\n",
    "    axes[i].set_title(str(label), fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('MNIST samples (normalized to [-1, 1])', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Noise Schedule (Guided)\n",
    "\n",
    "The noise schedule defines how quickly noise is added during the forward process. From *The Forward Process*, you know the chain:\n",
    "\n",
    "$$\\beta_t \\rightarrow \\alpha_t = 1 - \\beta_t \\rightarrow \\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$$\n",
    "\n",
    "$\\bar{\\alpha}_t$ is the **signal-to-noise dial** — the fraction of original signal remaining at timestep $t$. We use a **linear schedule** with $T = 1000$, $\\beta_{\\min} = 0.0001$, $\\beta_{\\max} = 0.02$, exactly as defined in the original DDPM paper.\n",
    "\n",
    "**Before running, predict:**\n",
    "- What will `alpha_bar[0]` be? (Hint: the product of one alpha near 1.)\n",
    "- What will `alpha_bar[999]` be? (Hint: the cumulative product of 1000 values, each slightly less than 1.)\n",
    "- What shape will the alpha_bar curve have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion hyperparameters\n",
    "T = 1000           # Total timesteps\n",
    "beta_min = 0.0001  # Starting noise level\n",
    "beta_max = 0.02    # Ending noise level\n",
    "\n",
    "# Linear noise schedule: beta increases linearly from beta_min to beta_max\n",
    "betas = torch.linspace(beta_min, beta_max, T)\n",
    "\n",
    "# alpha_t = 1 - beta_t (fraction of signal preserved at each step)\n",
    "alphas = 1.0 - betas\n",
    "\n",
    "# alpha_bar_t = cumulative product of alphas (total signal remaining at step t)\n",
    "# This is the key quantity from The Forward Process\n",
    "alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "print(f'beta range:       [{betas[0]:.4f}, {betas[-1]:.4f}]')\n",
    "print(f'alpha range:      [{alphas[0]:.4f}, {alphas[-1]:.4f}]')\n",
    "print(f'alpha_bar[0]:     {alpha_bars[0]:.4f}  (nearly all signal)')\n",
    "print(f'alpha_bar[499]:   {alpha_bars[499]:.4f}  (midpoint)')\n",
    "print(f'alpha_bar[999]:   {alpha_bars[999]:.6f}  (nearly no signal)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the alpha_bar curve — the signal-to-noise dial\n",
    "# Compare this to the widget from The Forward Process\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Alpha-bar curve\n",
    "ax1.plot(alpha_bars.numpy(), color='#c084fc', linewidth=2)\n",
    "ax1.set_xlabel('Timestep t', fontsize=12)\n",
    "ax1.set_ylabel('$\\\\bar{\\\\alpha}_t$', fontsize=14)\n",
    "ax1.set_title('Alpha-bar: signal remaining at each timestep', fontsize=12)\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.4, label='50% signal')\n",
    "ax1.legend(fontsize=10)\n",
    "\n",
    "# Beta schedule\n",
    "ax2.plot(betas.numpy(), color='#86efac', linewidth=2)\n",
    "ax2.set_xlabel('Timestep t', fontsize=12)\n",
    "ax2.set_ylabel('$\\\\beta_t$', fontsize=14)\n",
    "ax2.set_title('Beta schedule (noise added per step)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Does your alpha_bar curve look like the one from the widget in The Forward Process?')\n",
    "print('It should start near 1.0 and drop to near 0.0.')\n",
    "print('With a linear schedule, the curve is not linear — it is concave (drops faster at first).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You computed the complete noise schedule from scratch:\n",
    "- **betas**: linearly increasing noise level at each step\n",
    "- **alphas**: fraction of signal preserved at each step ($1 - \\beta_t$)\n",
    "- **alpha_bars**: cumulative product — total signal remaining at timestep $t$\n",
    "\n",
    "This is the same chain of definitions from *The Forward Process*. Alpha-bar starts near 1.0 (all signal) and drops to near 0.0 (all noise). The linear schedule is not a linear curve in alpha-bar — the cumulative product creates a concave shape that drops faster at first.\n",
    "\n",
    "We will use `alpha_bars` everywhere in this notebook: in the forward process, in the training loop, and in the sampling algorithm.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Forward Process (Guided)\n",
    "\n",
    "Now implement the closed-form formula as a function. From *The Forward Process*:\n",
    "\n",
    "$$x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon$$\n",
    "\n",
    "This is the formula you derived step by step, verified with the 1D pixel walkthrough, and used in the *Learning to Denoise* notebook. Now it becomes a reusable function.\n",
    "\n",
    "**Before running, predict:**\n",
    "- At $t = 0$, what will the image look like? (What are the coefficients?)\n",
    "- At $t = 999$, what will the image look like? (Recall: $\\bar{\\alpha}_{999} \\approx 0$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_sample(x_0, t, alpha_bars, noise=None):\n",
    "    \"\"\"Forward process: create a noisy image at timestep t.\n",
    "\n",
    "    This is the closed-form formula from The Forward Process:\n",
    "    x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
    "\n",
    "    Args:\n",
    "        x_0: clean images [batch_size, 1, 28, 28]\n",
    "        t: timesteps [batch_size] (integer indices into alpha_bars)\n",
    "        alpha_bars: precomputed alpha_bar schedule [T]\n",
    "        noise: optional pre-sampled noise (if None, sample fresh noise)\n",
    "\n",
    "    Returns:\n",
    "        x_t: noisy images [batch_size, 1, 28, 28]\n",
    "        noise: the noise that was added (needed for training loss)\n",
    "    \"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_0)\n",
    "\n",
    "    # Gather alpha_bar for each image's timestep\n",
    "    alpha_bar_t = alpha_bars[t]  # [batch_size]\n",
    "\n",
    "    # Reshape for broadcasting: [batch_size] → [batch_size, 1, 1, 1]\n",
    "    alpha_bar_t = alpha_bar_t.view(-1, 1, 1, 1)\n",
    "\n",
    "    # The closed-form formula\n",
    "    signal_coeff = torch.sqrt(alpha_bar_t)\n",
    "    noise_coeff = torch.sqrt(1.0 - alpha_bar_t)\n",
    "    x_t = signal_coeff * x_0 + noise_coeff * noise\n",
    "\n",
    "    return x_t, noise\n",
    "\n",
    "print('q_sample() defined. This is the formula from The Forward Process.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the noise progression on a real MNIST digit\n",
    "# Compare this to the DiffusionNoiseWidget from The Diffusion Idea\n",
    "\n",
    "x_0, label = dataset[3]  # grab a digit\n",
    "x_0_batch = x_0.unsqueeze(0)  # [1, 1, 28, 28]\n",
    "\n",
    "# Use the same noise across all timesteps so we can see the progression clearly\n",
    "torch.manual_seed(42)\n",
    "fixed_noise = torch.randn_like(x_0_batch)\n",
    "\n",
    "timesteps_to_show = [0, 100, 250, 500, 750, 999]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(timesteps_to_show), figsize=(15, 3))\n",
    "\n",
    "for i, t_val in enumerate(timesteps_to_show):\n",
    "    t_tensor = torch.tensor([t_val])\n",
    "    x_t, _ = q_sample(x_0_batch, t_tensor, alpha_bars, noise=fixed_noise)\n",
    "\n",
    "    axes[i].imshow(x_t.squeeze(), cmap='gray', vmin=-2, vmax=2)\n",
    "    axes[i].set_title(f't={t_val}\\n$\\\\bar{{\\\\alpha}}$={alpha_bars[t_val]:.3f}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f'Forward process: digit {label} at different noise levels', y=1.05, fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('At t=0:   nearly all signal — the digit is clear.')\n",
    "print('At t=250: light noise — still recognizable.')\n",
    "print('At t=500: signal and noise are mixing.')\n",
    "print('At t=750: mostly noise — barely recognizable.')\n",
    "print('At t=999: almost pure noise — the digit is gone.')\n",
    "print()\n",
    "print('This should match what you saw in the DiffusionNoiseWidget from The Diffusion Idea.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You implemented `q_sample()` — the closed-form formula from *The Forward Process* as a reusable Python function. The formula lets you jump directly to any noise level without iterating through intermediate steps.\n",
    "\n",
    "The noise progression should match what you saw in the interactive widgets:\n",
    "- At $t=250$, the digit is still recognizable (high $\\bar{\\alpha}$, mostly signal)\n",
    "- At $t=750$, it is mostly noise (low $\\bar{\\alpha}$, mostly noise)\n",
    "- At $t=999$, it is indistinguishable from pure Gaussian noise\n",
    "\n",
    "This function will be used in the training loop (Part 4) to create noisy training examples on the fly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Simple U-Net (Guided)\n",
    "\n",
    "Now we need a denoising network — the model that takes a noisy image $x_t$ and a timestep $t$ and predicts the noise $\\epsilon$ that was added.\n",
    "\n",
    "The architecture is a **minimal U-Net**: an encoder-decoder with skip connections and timestep conditioning. Think of it as the autoencoder from *Autoencoders* (Module 6.1) with two additions:\n",
    "\n",
    "1. **Skip connections** — the autoencoder's bottleneck forces compression, but for denoising we want the decoder to have access to the encoder's high-resolution features. Skip connections pass them through directly, like giving the decoder a cheat sheet.\n",
    "\n",
    "2. **Timestep embedding** — the network needs to know which noise level it is working at. At $t = 50$, remove a tiny amount of noise. At $t = 950$, hallucinate structure from near-pure static. We embed $t$ into a vector and add it to the features.\n",
    "\n",
    "**This section is the most scaffolded.** The architecture is provided and annotated — your job is to read and understand it, not write it from scratch. Architecture sophistication is not the focus of this lesson. The full U-Net with attention and sinusoidal embeddings comes in Module 6.3.\n",
    "\n",
    "**Before running, predict:**\n",
    "- How many parameters will this network have? (Hint: think small. This is MNIST at 28x28.)\n",
    "- What will the output shape be? (Hint: same as the input — the model predicts noise at every pixel.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ConvBlock(nn.Module):\n    \"\"\"Two convolutions with BatchNorm and ReLU.\n    The same Conv-BN-ReLU pattern from your CNNs in Series 3.\"\"\"\n\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass SimpleUNet(nn.Module):\n    \"\"\"A minimal U-Net for denoising MNIST.\n\n    Architecture:\n    - 3 encoder levels: 1->32->64->128 channels, spatial 28->14->7\n    - Bottleneck at 7x7 with 256 channels\n    - 2 decoder levels with skip connections: enc1->dec2, enc2->dec3\n      (enc3 feeds into the bottleneck without an explicit skip connection)\n    - Timestep embedding: learned linear projection added to bottleneck features\n    - Output: 1 channel (predicted noise, same shape as input)\n\n    This is NOT the full U-Net from real diffusion systems.\n    No attention, no group norm, no sinusoidal embeddings.\n    Just enough to prove diffusion works. Module 6.3 builds the real thing.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        # ---- Timestep embedding ----\n        # The network needs to know which noise level it is working at.\n        # Simplest approach: embed the integer timestep into a vector\n        # using a learned linear layer, then add it to the features.\n        # (The full version uses sinusoidal positional encoding — Module 6.3.)\n        self.time_embed = nn.Sequential(\n            nn.Linear(1, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n        )\n\n        # ---- Encoder (downsampling path) ----\n        # Same structure as the autoencoder encoder from Module 6.1.\n        # Conv blocks extract features, MaxPool reduces spatial resolution.\n        self.enc1 = ConvBlock(1, 32)        # 28x28 → 28x28, 32 channels\n        self.pool1 = nn.MaxPool2d(2)        # 28x28 → 14x14\n        self.enc2 = ConvBlock(32, 64)       # 14x14 → 14x14, 64 channels\n        self.pool2 = nn.MaxPool2d(2)        # 14x14 → 7x7\n        self.enc3 = ConvBlock(64, 128)      # 7x7 → 7x7, 128 channels\n\n        # ---- Bottleneck ----\n        self.bottleneck = ConvBlock(128, 256)  # 7x7, 256 channels\n\n        # ---- Decoder (upsampling path) ----\n        # ConvTranspose2d upsamples (like the autoencoder decoder).\n        # But unlike the autoencoder, the decoder receives SKIP CONNECTIONS\n        # from the encoder — features are concatenated along the channel dimension.\n        # That is why the input channels include both upsampled and skip features.\n        # Note: we have skip connections at 2 of 3 resolution levels.\n        # enc3's features flow through the bottleneck rather than being skipped directly.\n        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)  # 7x7 → 14x14\n        self.dec3 = ConvBlock(128 + 64, 128)   # 128 from upsample + 64 from enc2 skip\n        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)   # 14x14 → 28x28\n        self.dec2 = ConvBlock(64 + 32, 64)     # 64 from upsample + 32 from enc1 skip\n\n        # Final conv: map to 1 output channel (predicted noise)\n        self.final = nn.Sequential(\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 1, kernel_size=1),  # 1x1 conv to get 1 output channel\n        )\n\n    def forward(self, x, t):\n        \"\"\"\n        Args:\n            x: noisy image [batch_size, 1, 28, 28]\n            t: timestep [batch_size] (integer)\n\n        Returns:\n            predicted noise [batch_size, 1, 28, 28]\n        \"\"\"\n        # Embed the timestep\n        # Normalize t to [0, 1] so the network sees a consistent range\n        t_normalized = t.float().unsqueeze(1) / T  # [batch_size, 1]\n        t_emb = self.time_embed(t_normalized)      # [batch_size, 128]\n\n        # ---- Encoder ----\n        e1 = self.enc1(x)               # [B, 32, 28, 28]\n        e2 = self.enc2(self.pool1(e1))   # [B, 64, 14, 14]\n        e3 = self.enc3(self.pool2(e2))   # [B, 128, 7, 7]\n\n        # ---- Bottleneck ----\n        # enc3's features flow into the bottleneck directly (no skip connection here)\n        b = self.bottleneck(e3)          # [B, 256, 7, 7]\n\n        # Add timestep embedding to bottleneck features.\n        # Reshape: [B, 128] → [B, 128, 1, 1] and broadcast across spatial dims.\n        # We only add to the first 128 channels (matching the embedding size).\n        t_emb_spatial = t_emb.view(-1, 128, 1, 1)  # [B, 128, 1, 1]\n        b[:, :128, :, :] = b[:, :128, :, :] + t_emb_spatial\n\n        # ---- Decoder with skip connections ----\n        # Skip connections at 2 of 3 levels: enc2→dec3 and enc1→dec2.\n        # This is the key difference from the autoencoder: the decoder gets a\n        # \"cheat sheet\" of high-resolution features via concatenation.\n        d3 = self.up3(b)                             # [B, 128, 14, 14]\n        d3 = self.dec3(torch.cat([d3, e2], dim=1))   # concat enc2 skip → [B, 128, 14, 14]\n\n        d2 = self.up2(d3)                            # [B, 64, 28, 28]\n        d2 = self.dec2(torch.cat([d2, e1], dim=1))   # concat enc1 skip → [B, 64, 28, 28]\n\n        # Final conv: map from 64 channels to 1 (predicted noise)\n        out = self.final(d2)                         # [B, 1, 28, 28]\n\n        return out\n\nprint('SimpleUNet defined.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the architecture: check shapes and count parameters\n",
    "model = SimpleUNet().to(device)\n",
    "\n",
    "# Test with a dummy input\n",
    "dummy_x = torch.randn(4, 1, 28, 28).to(device)\n",
    "dummy_t = torch.randint(0, T, (4,)).to(device)\n",
    "dummy_out = model(dummy_x, dummy_t)\n",
    "\n",
    "print(f'Input shape:  {dummy_x.shape}')    # [4, 1, 28, 28]\n",
    "print(f'Output shape: {dummy_out.shape}')   # [4, 1, 28, 28] — same as input!\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'\\nTotal parameters:     {total_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,}')\n",
    "print(f'\\nThis is a small model — well under 1M parameters.')\n",
    "print(f'Real diffusion models (Stable Diffusion) have ~860M parameters.')\n",
    "print(f'But this is enough to generate recognizable MNIST digits.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### What Just Happened\n\nYou read through a minimal U-Net architecture with clear annotations. The key pieces:\n\n1. **Encoder-decoder structure** — the same hourglass shape from your autoencoder in Module 6.1. Convolutions extract features, pooling reduces resolution, transposed convolutions upsample.\n\n2. **Skip connections** — encoder features at each resolution level (enc1 at 28x28, enc2 at 14x14) are concatenated with the corresponding decoder level. This is the upgrade over the autoencoder: the decoder does not have to reconstruct fine details from a compressed bottleneck alone.\n\n3. **Timestep embedding** — the integer timestep $t$ is normalized and passed through a small MLP, producing a 128-dimensional vector. This vector is added to the bottleneck features, telling the network which noise level it is working at.\n\n4. **Output shape = input shape** — the network takes a noisy image and outputs a noise prediction of the same size. Every pixel gets a noise estimate.\n\nThe architecture is deliberately minimal. No attention layers, no group normalization, no sinusoidal positional encoding. These improvements come in Module 6.3 when you build the full U-Net.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Loop (Supported)\n",
    "\n",
    "Now you assemble the DDPM training loop. From *Learning to Denoise*, the algorithm has 7 steps:\n",
    "\n",
    "1. **Sample** a training image $x_0$ from the dataset\n",
    "2. **Sample** a random timestep $t \\sim \\text{Uniform}(0, T-1)$\n",
    "3. **Sample** noise $\\epsilon \\sim \\mathcal{N}(0, I)$\n",
    "4. **Create** the noisy image using `q_sample()`\n",
    "5. **Predict**: $\\hat{\\epsilon} = \\text{model}(x_t, t)$\n",
    "6. **Compute loss**: $L = \\text{MSE}(\\hat{\\epsilon}, \\epsilon)$\n",
    "7. **Backpropagate** and update weights\n",
    "\n",
    "The loop skeleton is provided. You fill in the **diffusion-specific parts** (steps 2-4 and step 6). The surrounding code handles the standard training mechanics you already know.\n",
    "\n",
    "**Task:** Fill in the four `# YOUR CODE HERE` markers. Each is 1-2 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, alpha_bars, T, device):\n",
    "    \"\"\"Train for one epoch. Returns the average loss.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch_images, _ in dataloader:\n",
    "        # Step 1: x_0 is the batch of clean images\n",
    "        x_0 = batch_images.to(device)\n",
    "        batch_size = x_0.shape[0]\n",
    "\n",
    "        # Step 2: Sample a random timestep for each image in the batch\n",
    "        # Each image gets its own random t from Uniform(0, T-1)\n",
    "        # Hint: use torch.randint. Range should be [0, T).\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "\n",
    "        # Step 3: Sample noise — the \"answer key\" for this training step\n",
    "        # Fresh Gaussian noise, same shape as x_0\n",
    "        # Hint: use torch.randn_like\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "\n",
    "        # Step 4: Create the noisy image using the forward process\n",
    "        # Hint: use the q_sample() function you implemented in Part 2\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "\n",
    "        # Step 5: Predict the noise (forward pass through the network)\n",
    "        epsilon_hat = model(x_t, t)\n",
    "\n",
    "        # Step 6: Compute MSE loss between predicted and actual noise\n",
    "        # This is the same nn.MSELoss from Series 1 and the Learning to Denoise notebook\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "\n",
    "        # Step 7: Backpropagate and update weights (standard training loop)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "print('train_epoch() defined. Fill in the YOUR CODE HERE markers before running.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\nThe four fills are the diffusion-specific data preparation steps. Everything else is the standard training loop you have used since Series 2.\n\n**Step 2 — Sample random timesteps:**\n```python\nt = torch.randint(0, T, (batch_size,), device=device)\n```\nEach image in the batch gets its own random timestep. This is why the closed-form formula matters — it lets you create a noisy image at any timestep without iterating.\n\n**Step 3 — Sample noise:**\n```python\nnoise = torch.randn_like(x_0)\n```\nFresh Gaussian noise, same shape as the images. This is the \"answer key\" — what the model will try to predict.\n\n**Step 4 — Create noisy image:**\n```python\nx_t, noise = q_sample(x_0, t, alpha_bars, noise=noise)\n```\nUses the `q_sample()` function you built in Part 2. The `alpha_bars` parameter is already on the GPU (we moved it to the device before training), so no `.to(device)` call is needed here.\n\n**Step 6 — Compute loss:**\n```python\nloss = F.mse_loss(epsilon_hat, noise)\n```\nMSE between predicted noise and actual noise. The same loss formula from Series 1. Different question, same math.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Train for 5 Epochs\n",
    "\n",
    "Start with a short training run to see if the loss decreases. This is a sanity check, not full training.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Will 5 epochs be enough to generate recognizable digits? (Recall the misconception from the planning: diffusion training needs more patience than autoencoders.)\n",
    "- What will the initial loss be? (Hint: an untrained model predicts random noise.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fresh model and optimizer\nmodel = SimpleUNet().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n\n# Move alpha_bars to device once (avoids repeated CPU→GPU transfers per batch)\nalpha_bars_device = alpha_bars.to(device)\n\n# Train for 5 epochs\nlosses_phase1 = []\nprint('Phase 1: Training for 5 epochs...')\nprint('-' * 40)\n\nfor epoch in range(1, 6):\n    start_time = time.time()\n    avg_loss = train_epoch(model, dataloader, optimizer, alpha_bars_device, T, device)\n    elapsed = time.time() - start_time\n    losses_phase1.append(avg_loss)\n    print(f'Epoch {epoch:>2d} | Loss: {avg_loss:.4f} | Time: {elapsed:.1f}s')\n\nprint('\\nPhase 1 complete.')\nprint(f'Loss dropped from {losses_phase1[0]:.4f} to {losses_phase1[-1]:.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Phase 1 loss curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, len(losses_phase1) + 1), losses_phase1, 'o-', color='#c084fc', linewidth=2, markersize=6)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Average Loss', fontsize=12)\n",
    "plt.title('Phase 1: First 5 epochs', fontsize=13)\n",
    "plt.axhline(y=2.0, color='gray', linestyle='--', alpha=0.4, label='Random guess (MSE~2)')\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('The loss should be decreasing — the model is learning to predict noise.')\n",
    "print('But 5 epochs is not enough for good generation. The model must learn to')\n",
    "print('denoise at ALL 1000 noise levels simultaneously, and random timestep sampling')\n",
    "print('means it sees each noise level sparsely.')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Is the Model Learning? A Denoising Diagnostic\n\nBefore training further, let's check whether 5 epochs was enough to learn *anything*. We haven't built the sampling loop yet (that's Part 5), so we can't generate from scratch. But we can test the model's denoising ability directly.\n\n**The idea:** Take a real MNIST digit, add noise at several levels, then ask the model: \"what do you think the clean image looks like?\" The model predicts $\\hat{\\epsilon}$, and we recover the clean image estimate:\n\n$$\\hat{x}_0 = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\hat{\\epsilon}}{\\sqrt{\\bar{\\alpha}_t}}$$\n\nThis is just the closed-form formula from *The Forward Process*, rearranged to solve for $x_0$.\n\n**Before running, predict:** At which noise levels will the model's prediction be best — low noise (small $t$) or high noise (large $t$)?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Single-step denoising diagnostic: what does the model think the clean image looks like?\n# This does NOT use the sampling loop (that's Part 5). Instead, we ask the model\n# to predict epsilon at a single timestep, then recover the clean image estimate.\n\n@torch.no_grad()\ndef denoise_diagnostic(model, x_0, timesteps, alpha_bars, device):\n    \"\"\"Show the model's one-step denoising prediction at several noise levels.\n\n    For each timestep t:\n      1. Create noisy image: x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n      2. Predict noise: epsilon_hat = model(x_t, t)\n      3. Recover clean estimate: x_0_hat = (x_t - sqrt(1 - alpha_bar_t) * epsilon_hat) / sqrt(alpha_bar_t)\n\n    This is the closed-form formula rearranged to solve for x_0.\n    \"\"\"\n    model.eval()\n    x_0 = x_0.to(device)\n    results = []\n\n    for t_val in timesteps:\n        t_tensor = torch.tensor([t_val], device=device)\n        noise = torch.randn_like(x_0)\n\n        # Forward process: add noise\n        alpha_bar_t = alpha_bars[t_val]\n        x_t = torch.sqrt(alpha_bar_t) * x_0 + torch.sqrt(1.0 - alpha_bar_t) * noise\n\n        # Model predicts the noise\n        epsilon_hat = model(x_t, t_tensor)\n\n        # Recover clean image estimate (rearranged closed-form formula)\n        x_0_hat = (x_t - torch.sqrt(1.0 - alpha_bar_t) * epsilon_hat) / torch.sqrt(alpha_bar_t)\n\n        results.append({\n            't': t_val,\n            'x_t': x_t.cpu().squeeze(),\n            'x_0_hat': x_0_hat.cpu().squeeze().clamp(-1, 1),\n        })\n\n    return results\n\n\n# Pick a real digit and run the diagnostic\nx_0_sample, label = dataset[3]\nx_0_batch = x_0_sample.unsqueeze(0)  # [1, 1, 28, 28]\n\ndiagnostic_timesteps = [100, 300, 600, 900]\nresults = denoise_diagnostic(model, x_0_batch, diagnostic_timesteps, alpha_bars_device, device)\n\n# Display: original | noisy | model's prediction at each noise level\nn_cols = len(diagnostic_timesteps)\nfig, axes = plt.subplots(3, n_cols, figsize=(3 * n_cols, 8))\n\nfor i, r in enumerate(results):\n    # Row 1: noisy input\n    axes[0, i].imshow(r['x_t'], cmap='gray', vmin=-2, vmax=2)\n    axes[0, i].set_title(f't={r[\"t\"]}\\n$\\\\bar{{\\\\alpha}}$={alpha_bars[r[\"t\"]]:.3f}', fontsize=10)\n    axes[0, i].axis('off')\n\n    # Row 2: model's prediction of clean image\n    axes[1, i].imshow(r['x_0_hat'], cmap='gray', vmin=-1, vmax=1)\n    axes[1, i].set_title('Model prediction', fontsize=10)\n    axes[1, i].axis('off')\n\n    # Row 3: original clean image (same in every column)\n    axes[2, i].imshow(x_0_sample.squeeze(), cmap='gray', vmin=-1, vmax=1)\n    axes[2, i].set_title('Original', fontsize=10)\n    axes[2, i].axis('off')\n\n# Row labels\naxes[0, 0].set_ylabel('Noisy input', fontsize=11, rotation=0, labelpad=70, va='center')\naxes[1, 0].set_ylabel('Model predicts', fontsize=11, rotation=0, labelpad=70, va='center')\naxes[2, 0].set_ylabel('Ground truth', fontsize=11, rotation=0, labelpad=70, va='center')\n\nplt.suptitle(f'Denoising diagnostic after 5 epochs (digit {label})', y=1.02, fontsize=13)\nplt.tight_layout()\nplt.show()\n\nprint('Look at the model\\'s predictions:')\nprint('- At low noise (t=100): the model should do reasonably well — most signal is intact.')\nprint('- At high noise (t=900): the prediction will be rough/blobby — the model must')\nprint('  hallucinate structure from almost pure noise. After only 5 epochs, it is still learning.')\nprint()\nprint('The model IS learning, but 5 epochs is not enough for clean predictions at high noise.')\nprint('Let\\'s keep training and see how much more helps.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Train for 15 More Epochs (Total 20)\n",
    "\n",
    "Now train longer. This will take longer — allow ~15-25 minutes on a Colab GPU.\n",
    "\n",
    "The model is learning a much harder task than your autoencoder: denoising at 1,000 different noise levels with one network. Training loss decreases gradually rather than dropping sharply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Continue training for 15 more epochs (total 20)\nlosses_phase2 = list(losses_phase1)  # keep phase 1 losses\n\nprint('Phase 2: Training for 15 more epochs (total 20)...')\nprint('-' * 40)\n\nfor epoch in range(6, 21):\n    start_time = time.time()\n    avg_loss = train_epoch(model, dataloader, optimizer, alpha_bars_device, T, device)\n    elapsed = time.time() - start_time\n    losses_phase2.append(avg_loss)\n    print(f'Epoch {epoch:>2d} | Loss: {avg_loss:.4f} | Time: {elapsed:.1f}s')\n\nprint('\\nPhase 2 complete.')\nprint(f'Total loss trajectory: {losses_phase2[0]:.4f} → {losses_phase2[-1]:.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the full training loss curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(1, len(losses_phase2) + 1), losses_phase2, 'o-', color='#c084fc', linewidth=2, markersize=4)\n",
    "plt.axvline(x=5, color='#86efac', linestyle='--', alpha=0.5, label='End of Phase 1')\n",
    "plt.axhline(y=2.0, color='gray', linestyle='--', alpha=0.3, label='Random guess (MSE~2)')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Average Loss', fontsize=12)\n",
    "plt.title('Full training curve: 20 epochs', fontsize=13)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Notice: the loss decreases gradually, not the sharp drop you might expect.')\n",
    "print('The model is learning to denoise at 1000 different noise levels simultaneously.')\n",
    "print('This is a harder task than autoencoder reconstruction.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You filled in the diffusion-specific parts of the training loop and trained a real denoising model. The four fills were:\n",
    "\n",
    "1. **Sample timesteps** — `torch.randint` to get a random $t$ per image\n",
    "2. **Sample noise** — `torch.randn_like` to get the \"answer key\"\n",
    "3. **Create noisy images** — `q_sample()` to apply the forward process\n",
    "4. **Compute loss** — `F.mse_loss` between predicted and actual noise\n",
    "\n",
    "Everything else was the standard training loop from Series 2. Same heartbeat: forward → loss → backward → update.\n",
    "\n",
    "The loss curve shows gradual improvement. This is expected — the model must learn to denoise at 1,000 different noise levels with one set of weights. Each batch only covers a sparse sampling of those levels.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Does Low Loss Mean Good Generation?\n\nYour training loss has decreased steadily over 20 epochs. In classification, low loss meant high accuracy. In autoencoders, low loss meant good reconstructions. Does low diffusion loss guarantee good generations?\n\nNot necessarily. The training loss is an **average** across all 1,000 timesteps. The model can have decent average MSE while still making correlated errors at certain noise levels. And in sampling, errors **accumulate** across 1,000 sequential steps — a small per-step error compounds into a large deviation by the end.\n\nYou already saw a hint of this in the denoising diagnostic: your 5-epoch model had a decreasing loss, but its predictions at high noise levels were rough and blobby. The loss was better than random, but the model had not yet learned enough to produce clean results.\n\n**The takeaway:** Loss is a necessary but not sufficient signal. The real test is always the generated samples. Let's see what 20 epochs of training actually produces.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Sampling (Supported)\n",
    "\n",
    "This is the moment of truth. From *Sampling and Generation*, the DDPM sampling algorithm:\n",
    "\n",
    "1. **Sample** $x_T \\sim \\mathcal{N}(0, I)$ — start from pure noise\n",
    "2. **Loop** from $t = T-1$ down to $t = 0$:\n",
    "   - Predict the noise: $\\hat{\\epsilon} = \\text{model}(x_t, t)$\n",
    "   - Compute the denoised estimate (the reverse step formula):\n",
    "     $$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon} \\right) + \\sigma_t z$$\n",
    "   - where $z \\sim \\mathcal{N}(0, I)$ for $t > 1$, and $z = 0$ for $t = 1$ (the last step commits without noise)\n",
    "3. **Return** $x_0$\n",
    "\n",
    "The loop skeleton is provided. You fill in the **reverse step computation** inside the loop.\n",
    "\n",
    "**Task:** Fill in the two `# YOUR CODE HERE` markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef sample(model, n_samples, T, betas, alphas, alpha_bars, device):\n    \"\"\"Generate images using the DDPM sampling algorithm.\n\n    This is the algorithm from Sampling and Generation:\n    start from pure noise, iteratively denoise, return the generated image.\n\n    Args:\n        model: trained denoising network\n        n_samples: number of images to generate\n        T: total timesteps\n        betas: noise schedule [T]\n        alphas: 1 - betas [T]\n        alpha_bars: cumulative product of alphas [T]\n        device: cuda or cpu\n\n    Returns:\n        generated images [n_samples, 1, 28, 28]\n    \"\"\"\n    model.eval()\n\n    # Step 1: Start from pure noise\n    x = torch.randn(n_samples, 1, 28, 28, device=device)\n\n    # Precompute sigma_t = sqrt(beta_t) for the noise injection\n    # (Schedule tensors stay on CPU — scalar indexing below extracts Python floats\n    #  that broadcast with CUDA tensors automatically.)\n    sigmas = torch.sqrt(betas)\n\n    # Step 2: Loop from t = T-1 down to t = 0\n    for t_val in reversed(range(T)):\n        t_batch = torch.full((n_samples,), t_val, device=device, dtype=torch.long)\n\n        # Predict the noise at this timestep\n        epsilon_hat = model(x, t_batch)\n\n        # Get schedule values for this timestep\n        # (Scalar indexing returns Python floats — no device mismatch.)\n        alpha_t = alphas[t_val]\n        alpha_bar_t = alpha_bars[t_val]\n        beta_t = betas[t_val]\n        sigma_t = sigmas[t_val]\n\n        # The reverse step formula from Sampling and Generation:\n        # x_{t-1} = (1/sqrt(alpha_t)) * (x_t - (beta_t / sqrt(1 - alpha_bar_t)) * epsilon_hat) + sigma_t * z\n        #\n        # YOUR CODE HERE: compute the denoised estimate (the mean of the reverse step)\n        # Hint: this is one line implementing the formula above, minus the sigma_t * z part\n\n\n        # Add noise for all steps except the last (t=0)\n        # At t=0, z = 0 (the last step commits without noise)\n        # This is the t=1 special case from Sampling and Generation\n        # (Note: t_val=0 corresponds to the final step in our 0-indexed loop)\n        #\n        # YOUR CODE HERE: sample z and add sigma_t * z when t_val > 0\n        # Hint: sample z = torch.randn_like(x), then only add it if t_val > 0\n\n\n    # Step 3: Return the generated images\n    # Clamp to [-1, 1] for display\n    return x.clamp(-1, 1)\n\nprint('sample() defined. Fill in the YOUR CODE HERE markers before running.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\nThe two fills implement the reverse step formula from *Sampling and Generation*.\n\n**Reverse step mean:**\n```python\nx = (1.0 / torch.sqrt(alpha_t)) * (x - (beta_t / torch.sqrt(1.0 - alpha_bar_t)) * epsilon_hat)\n```\nThis is the formula you traced with numbers at t=500 in the lesson. The first term scales up the signal. The second term subtracts the model's estimated noise contribution.\n\n**Noise injection (with final-step special case):**\n```python\nif t_val > 0:\n    z = torch.randn_like(x)\n    x = x + sigma_t * z\n```\nFor all steps except the last, we add a small amount of fresh noise. This is the stochastic part — like the temperature in language model sampling. At the final step, $z = 0$: the last step commits to a specific image without adding noise.\n\n**Index convention note:** Our loop uses 0-indexed `t_val` (Python convention), so `t_val=0` is the final step. In the DDPM paper and in *Sampling and Generation*, this corresponds to $t=1$ (1-indexed). The condition `t_val > 0` is equivalent to the paper's \"$z = 0$ when $t = 1$.\"\n\nRemember the analogy from *Sampling and Generation*: \"The last step commits. Every step before it explores.\"\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Timing Experiment\n",
    "\n",
    "Before generating, make a prediction.\n",
    "\n",
    "**Think about this:** One training step takes one forward pass through the model (for one random timestep). How long did each training epoch take? Roughly how long is one forward pass?\n",
    "\n",
    "Sampling requires **T = 1,000 sequential forward passes per image.** If you want to generate 64 images, that is 64 × 1,000 = 64,000 forward passes.\n",
    "\n",
    "**Before running, predict:** How long will it take to generate 64 images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PREDICTION\n",
    "# Before running the next cell, write your prediction here:\n",
    "#\n",
    "# How long will it take to generate 64 images?\n",
    "# Your prediction: _____ seconds\n",
    "#\n",
    "# How does that compare to one training epoch?\n",
    "# Your reasoning: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: time a single training step for comparison\n",
    "model.train()\n",
    "sample_batch, _ = next(iter(dataloader))\n",
    "sample_batch = sample_batch.to(device)\n",
    "\n",
    "# Warm up the GPU\n",
    "for _ in range(5):\n",
    "    t_dummy = torch.randint(0, T, (sample_batch.shape[0],), device=device)\n",
    "    x_t_dummy, noise_dummy = q_sample(sample_batch, t_dummy, alpha_bars_device)\n",
    "    _ = model(x_t_dummy, t_dummy)\n",
    "\n",
    "# Time one training step\n",
    "torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "start = time.time()\n",
    "\n",
    "t_dummy = torch.randint(0, T, (sample_batch.shape[0],), device=device)\n",
    "x_t_dummy, noise_dummy = q_sample(sample_batch, t_dummy, alpha_bars_device)\n",
    "pred = model(x_t_dummy, t_dummy)\n",
    "loss = F.mse_loss(pred, noise_dummy)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "train_step_time = time.time() - start\n",
    "\n",
    "print(f'One training step: {train_step_time*1000:.1f} ms')\n",
    "print(f'  (1 forward pass + 1 backward pass for 1 random timestep)')\n",
    "print()\n",
    "print(f'Now generating 64 images...')\n",
    "print(f'  (64 images × 1000 timesteps = 64,000 sequential forward passes)')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 64 images and TIME it\n",
    "torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "start = time.time()\n",
    "\n",
    "generated = sample(model, 64, T, betas, alphas, alpha_bars, device)\n",
    "\n",
    "torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "sample_time = time.time() - start\n",
    "\n",
    "print(f'Generated 64 images in {sample_time:.1f} seconds.')\n",
    "print(f'That is {sample_time/64:.2f} seconds per image.')\n",
    "print(f'\\nCompare:')\n",
    "print(f'  One training step:       {train_step_time*1000:.1f} ms  (1 forward pass)')\n",
    "print(f'  Generating one image:    {sample_time/64*1000:.0f} ms  (1000 forward passes)')\n",
    "print(f'  Ratio:                   {(sample_time/64)/train_step_time:.0f}x slower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated images as an 8x8 grid\n",
    "def show_grid(images, nrow=8, title='Generated Images'):\n",
    "    \"\"\"Display a batch of images as a grid.\"\"\"\n",
    "    # images: [n, 1, 28, 28], range [-1, 1]\n",
    "    images = images.cpu()\n",
    "    # Rescale to [0, 1] for display\n",
    "    images = (images + 1) / 2\n",
    "    images = images.clamp(0, 1)\n",
    "\n",
    "    grid = torchvision.utils.make_grid(images, nrow=nrow, padding=2)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(grid.permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_grid(generated, title=f'Generated MNIST digits (20 epochs, {sample_time:.0f}s to generate)')\n",
    "\n",
    "print('Look at your generated digits:')\n",
    "print('- Are they recognizable? (They should be — if imperfect.)')\n",
    "print('- Are they varied? (Different digits in different positions.)')\n",
    "print('- Are they perfect? (They should NOT be — this is a tiny model on 28x28.)')\n",
    "print()\n",
    "print('These images never existed in the training set.')\n",
    "print('They were generated from pure Gaussian noise by a model you built from scratch.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cost of Pixel-Space Diffusion\n",
    "\n",
    "Let that sampling time sink in. This is the **deliberate pain** of this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate: how long would it take at scale?\n",
    "time_per_image = sample_time / 64\n",
    "\n",
    "print('=== The Cost of Pixel-Space DDPM ===')\n",
    "print()\n",
    "print(f'Your model: {sum(p.numel() for p in model.parameters()):,} parameters')\n",
    "print(f'Image size: 28×28 (784 pixels)')\n",
    "print(f'Timesteps:  {T}')\n",
    "print()\n",
    "print(f'Time to generate 1 image:      {time_per_image:.2f}s')\n",
    "print(f'Time to generate 64 images:    {sample_time:.1f}s')\n",
    "print(f'Time to generate 1000 images:  {time_per_image * 1000 / 60:.1f} minutes')\n",
    "print()\n",
    "print('Now imagine scaling up:')\n",
    "print(f'  Stable Diffusion: 512×512 image = {512*512//784:.0f}x more pixels')\n",
    "print(f'  Stable Diffusion: ~860M parameters vs your {sum(p.numel() for p in model.parameters())//1000}K')\n",
    "print(f'  Each forward pass would be MUCH slower.')\n",
    "print(f'  1000 steps × slower forward pass = minutes per image.')\n",
    "print()\n",
    "print('⚠ This slowness is NOT a bug in your code.')\n",
    "print('  It is a fundamental property of pixel-space DDPM.')\n",
    "print('  Every image requires T=1000 sequential forward passes.')\n",
    "print('  This is why latent diffusion was invented.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You implemented the DDPM sampling algorithm — the reverse process from *Sampling and Generation* — and generated real images from pure noise. The two fills were:\n",
    "\n",
    "1. **The reverse step formula** — compute the denoised estimate by subtracting the model's predicted noise contribution, scaled by the schedule parameters.\n",
    "2. **The noise injection** — add fresh noise at every step except the last. The last step ($t = 0$) commits without noise.\n",
    "\n",
    "You also experienced the computational cost firsthand. One training step is fast (one forward pass). One sampling run is ~1000x slower (1000 sequential forward passes). This is the fundamental bottleneck of pixel-space DDPM.\n",
    "\n",
    "The timing is not a failure of your implementation. It is a property of the algorithm. And it is exactly what motivates latent diffusion in Module 6.3.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Reflection and Bridge (Independent)\n",
    "\n",
    "You have now built two generative models in this series:\n",
    "1. A **VAE** (Module 6.1) that generates images with one decoder forward pass\n",
    "2. A **DDPM** (this lesson) that generates images with 1,000 iterative denoising steps\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "Think about these questions before reading the discussion below.\n",
    "\n",
    "**Question 1:** Compare your VAE samples from *Exploring Latent Spaces* to your diffusion samples. Which produced sharper, more detailed images? Which was faster?\n",
    "\n",
    "**Question 2:** The VAE generates in one forward pass (instant). Diffusion takes 1,000 steps (slow). Why does diffusion produce better quality despite using the same basic building blocks (conv layers, MSE loss, backprop)?\n",
    "\n",
    "**Question 3:** Is there a way to get the quality of diffusion with the speed of a VAE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR REFLECTION\n",
    "# Before reading the discussion below, write your answers here:\n",
    "#\n",
    "# Question 1 — Quality vs Speed:\n",
    "#   VAE quality: ...\n",
    "#   VAE speed: ...\n",
    "#   Diffusion quality: ...\n",
    "#   Diffusion speed: ...\n",
    "#\n",
    "# Question 2 — Why is diffusion better quality?\n",
    "#   Your reasoning: ...\n",
    "#\n",
    "# Question 3 — Can we get both?\n",
    "#   Your idea: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "**VAE vs Diffusion — a tale of two tradeoffs:**\n",
    "\n",
    "| | VAE | Diffusion (DDPM) |\n",
    "|---|---|---|\n",
    "| **Speed** | One decoder forward pass — instant | 1,000 sequential forward passes — slow |\n",
    "| **Quality** | Blurry, averaged features | Sharper, more detailed |\n",
    "| **Why** | Must compress everything through a bottleneck | Iterative refinement — each step only needs to make a small correction |\n",
    "| **Loss** | Reconstruction + KL divergence | Simple MSE on noise prediction |\n",
    "\n",
    "**Why does diffusion produce better quality?** The VAE must compress an image into a small latent vector and reconstruct everything in one shot — details that do not survive the bottleneck are lost. Diffusion breaks the problem into 1,000 tiny steps. Each step only needs to make a small correction. The cumulative effect of 1,000 small corrections produces more detail than one big reconstruction.\n",
    "\n",
    "**The cost:** Those 1,000 steps are sequential. You cannot parallelize them — step $t-1$ depends on step $t$. This is the fundamental bottleneck.\n",
    "\n",
    "**Can we get both quality AND speed?** Yes — and the insight is elegant:\n",
    "\n",
    "> *What if you ran diffusion in the VAE's latent space instead of pixel space?*\n",
    "\n",
    "The VAE's encoder compresses a 28×28 image into a small latent representation. The latent space is much smaller than pixel space. If you run the diffusion process in that compressed space:\n",
    "- Each forward pass through the denoising network is faster (smaller input)\n",
    "- The 1,000 steps happen in a compressed space\n",
    "- The VAE's decoder upsamples the result back to pixel space at the end\n",
    "\n",
    "This is **latent diffusion** — the core idea behind Stable Diffusion. That is Module 6.3.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Every formula from the module became a line of PyTorch code.** The closed-form formula became `q_sample()`. The 7-step training algorithm became `train_epoch()`. The reverse step formula became the core of `sample()`. No new theory — just translation.\n",
    "\n",
    "2. **A working diffusion model generates recognizable images from pure noise.** Even a minimal architecture (<1M parameters) trained on MNIST for 20 epochs produces varied, recognizable digits. Architecture sophistication improves quality; it does not gate whether diffusion works at all.\n",
    "\n",
    "3. **The training loop is the standard loop with diffusion-specific data preparation.** Sample a random timestep, sample noise, create the noisy image with `q_sample()`, predict the noise, compute MSE loss, backpropagate. The heartbeat has not changed since Series 2.\n",
    "\n",
    "4. **Sampling is 1,000x slower than training.** One training step: one forward pass. One sample: 1,000 sequential forward passes. This is a fundamental property of pixel-space DDPM, not a bug. You measured it yourself.\n",
    "\n",
    "5. **The sampling cost motivates latent diffusion.** Running diffusion in a compressed latent space (instead of pixel space) makes each step faster. This is the core idea behind Stable Diffusion — Module 6.3.\n",
    "\n",
    "**What you built:** A complete DDPM pipeline — forward process, denoising network, training loop, sampling algorithm — from scratch. Digits emerged from pure noise, generated by a model you built and trained yourself. That is the payoff of four lessons of theory.\n",
    "\n",
    "**What comes next:** Module 6.3 answers the question that the sampling wait raises: *what if you ran diffusion in a compressed latent space?*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}