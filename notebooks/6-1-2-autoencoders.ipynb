{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders: Build, Train, Explore\n",
    "\n",
    "**Module 6.1, Lesson 2** | CourseAI\n",
    "\n",
    "In this notebook you will:\n",
    "\n",
    "1. **Build a convolutional autoencoder** â€” encoder (CNN you already know) + decoder (ConvTranspose2d) with a 32-dimensional bottleneck\n",
    "2. **Train it on Fashion-MNIST** and visualize reconstructions â€” see what the bottleneck preserves vs loses\n",
    "3. **Experiment with bottleneck size** â€” compare 2D, 8D, and 32D latent spaces and observe the quality tradeoff\n",
    "4. **Visualize the 2D latent space** â€” color-code encoded images by digit class, observe clustering\n",
    "5. **Build a denoising autoencoder** â€” add noise to inputs, train the network to reconstruct clean images\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies, import libraries, and download Fashion-MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Download Fashion-MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # scales pixels to [0, 1]\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=128, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False\n",
    ")\n",
    "\n",
    "# Class names for visualization\n",
    "CLASS_NAMES = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples:     {len(test_dataset)}')\n",
    "print(f'Image shape:      {train_dataset[0][0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Shared Helpers\n",
    "\n",
    "Utility functions used across multiple exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_reconstructions(model, test_loader, n=8, title='Reconstructions'):\n",
    "    \"\"\"Show original images (top row) and reconstructions (bottom row).\"\"\"\n",
    "    model.eval()\n",
    "    images, _ = next(iter(test_loader))\n",
    "    images = images[:n].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        recon = model(images)\n",
    "\n",
    "    images_np = images.cpu().numpy()\n",
    "    recon_np = recon.cpu().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(2, n, figsize=(n * 1.5, 3))\n",
    "    for i in range(n):\n",
    "        axes[0, i].imshow(images_np[i].squeeze(), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_title('Original', fontsize=10)\n",
    "\n",
    "        axes[1, i].imshow(recon_np[i].squeeze(), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_title('Reconstructed', fontsize=10)\n",
    "\n",
    "    fig.suptitle(title, fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_autoencoder(model, train_loader, num_epochs=10, lr=1e-3):\n",
    "    \"\"\"Train an autoencoder with MSE reconstruction loss. Returns loss history.\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "\n",
    "        for images, _ in train_loader:  # labels ignored!\n",
    "            images = images.to(device)\n",
    "\n",
    "            recon = model(images)\n",
    "            loss = criterion(recon, images)  # target IS the input\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            total += images.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        history.append(epoch_loss)\n",
    "        print(f'  Epoch {epoch+1:2d}/{num_epochs}  Loss: {epoch_loss:.6f}')\n",
    "\n",
    "    return history\n",
    "\n",
    "print('Helpers loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Build a Simple Autoencoder [Guided]\n",
    "\n",
    "The autoencoder has an **hourglass** shape: the encoder compresses the input through a bottleneck, and the decoder reconstructs it.\n",
    "\n",
    "- **Encoder**: Conv2d layers (the CNN you already know) that shrink spatial dimensions and end at a small latent vector\n",
    "- **Bottleneck**: A 32-dimensional vector â€” the latent code\n",
    "- **Decoder**: Linear + Unflatten + ConvTranspose2d layers that expand back to 28x28\n",
    "\n",
    "The code below is complete. Read through it carefully before running.\n",
    "\n",
    "**Before running, predict:**\n",
    "- How many parameters does this model have? (Hint: the encoder is like a small CNN ending at 32 outputs, and the decoder mirrors it.)\n",
    "- What is the compression ratio? (784 input pixels down to 32 latent numbers)\n",
    "- Why does the decoder end with `Sigmoid` instead of `ReLU`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, bottleneck_size=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder: same CNN pattern you already know\n",
    "        # Spatial shrinks, channels grow â€” then flatten to the bottleneck\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),   # 1x28x28 -> 16x14x14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),  # 16x14x14 -> 32x7x7\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),                                # 32*7*7 = 1568\n",
    "            nn.Linear(32 * 7 * 7, bottleneck_size),      # 1568 -> bottleneck\n",
    "        )\n",
    "\n",
    "        # Decoder: reverse the encoder\n",
    "        # Linear -> unflatten back to spatial -> ConvTranspose2d to upsample\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_size, 32 * 7 * 7),      # bottleneck -> 1568\n",
    "            nn.Unflatten(1, (32, 7, 7)),                  # reshape to 32x7x7\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2,       # 32x7x7 -> 16x14x14\n",
    "                               padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2,        # 16x14x14 -> 1x28x28\n",
    "                               padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),  # pixel values in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)     # compress\n",
    "        recon = self.decoder(latent)  # reconstruct\n",
    "        return recon\n",
    "\n",
    "\n",
    "# Create the model and examine it\n",
    "model = Autoencoder(bottleneck_size=32).to(device)\n",
    "\n",
    "# Dimension check\n",
    "test_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f'Input shape:  {test_input.shape}')    # [1, 1, 28, 28]\n",
    "print(f'Output shape: {test_output.shape}')   # [1, 1, 28, 28] â€” same as input!\n",
    "print()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "encoder_params = sum(p.numel() for p in model.encoder.parameters())\n",
    "decoder_params = sum(p.numel() for p in model.decoder.parameters())\n",
    "print(f'Encoder params: {encoder_params:,}')\n",
    "print(f'Decoder params: {decoder_params:,}')\n",
    "print(f'Total params:   {total_params:,}')\n",
    "print()\n",
    "\n",
    "# Compression ratio\n",
    "input_size = 28 * 28  # 784 pixels\n",
    "bottleneck_size = 32\n",
    "print(f'Compression: {input_size} pixels -> {bottleneck_size} latent numbers')\n",
    "print(f'Ratio: {input_size / bottleneck_size:.1f}x compression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:**\n",
    "\n",
    "The model takes a 1x28x28 image and returns a 1x28x28 reconstruction. The hourglass shape is visible in the parameter counts â€” the encoder and decoder are roughly symmetric.\n",
    "\n",
    "The compression ratio is 24.5x: 784 pixels squeezed into 32 numbers. The network must decide what to keep. The `Sigmoid` at the end ensures output pixels are in [0, 1], matching the input range from `ToTensor()`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Train and Visualize Reconstructions [Guided]\n",
    "\n",
    "Now we train the autoencoder. The training loop is identical to every model you have trained, with one key difference: **the target IS the input**. Labels are ignored.\n",
    "\n",
    "```python\n",
    "loss = criterion(recon, images)  # not criterion(output, labels)!\n",
    "```\n",
    "\n",
    "The loss is MSE between input and reconstruction. It measures what the bottleneck fails to preserve.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Will the reconstructions be sharp or blurry? (32 numbers to represent 784 pixels...)\n",
    "- Will you be able to tell which clothing category each reconstruction belongs to?\n",
    "- What kind of detail do you expect to be lost first â€” overall shape, or fine texture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training autoencoder (bottleneck=32)...')\n",
    "print('=' * 45)\n",
    "history_32 = train_autoencoder(model, train_loader, num_epochs=15)\n",
    "print('=' * 45)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss curve\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(range(1, len(history_32) + 1), history_32, 'o-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Autoencoder Training Loss (bottleneck=32)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstructions\n",
    "show_reconstructions(model, test_loader, n=10, title='Autoencoder Reconstructions (bottleneck=32)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:**\n",
    "\n",
    "The reconstructions are recognizable but **blurry**. You can clearly tell a trouser from a sneaker from a dress â€” the overall shape survives the bottleneck. But fine details like texture, stitching patterns, and sharp edges are lost.\n",
    "\n",
    "This is the bottleneck at work: 32 numbers can capture the broad structure (silhouette, category, rough proportions) but cannot encode every pixel perfectly. The network learned to keep what matters most for reconstruction and discard the rest.\n",
    "\n",
    "Notice: the loss compares the reconstruction to the **input**, not to any label. The data is its own target. Labels are completely ignored (the `_` in `for images, _ in train_loader`).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Experiment with Bottleneck Size [Supported]\n",
    "\n",
    "The bottleneck size controls the **tradeoff between compression and reconstruction quality**:\n",
    "- Too small â†’ the network loses important information, reconstructions are very blurry\n",
    "- Too large â†’ the network can copy pixels without learning meaningful features (the overcomplete trap)\n",
    "\n",
    "Your task: train autoencoders with **2D** and **8D** bottlenecks, then compare all three (2D, 8D, 32D) side by side.\n",
    "\n",
    "**Think first:** With only 2 latent dimensions, what can possibly survive the compression? With 8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and train an autoencoder with bottleneck_size=2\n",
    "# 1. Create the model: Autoencoder(bottleneck_size=2).to(device)\n",
    "# 2. Train it: train_autoencoder(model_2d, train_loader, num_epochs=15)\n",
    "\n",
    "model_2d = None   # TODO: replace\n",
    "history_2d = None  # TODO: replace\n",
    "\n",
    "print()\n",
    "\n",
    "# TODO: Create and train an autoencoder with bottleneck_size=8\n",
    "# Same pattern as above\n",
    "\n",
    "model_8d = None   # TODO: replace\n",
    "history_8d = None  # TODO: replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The key insight: we use the exact same `Autoencoder` class â€” the only thing that changes is the `bottleneck_size` argument. The architecture adapts automatically because the Linear layers connecting to the bottleneck adjust their dimensions.\n",
    "\n",
    "```python\n",
    "# 2D bottleneck\n",
    "model_2d = Autoencoder(bottleneck_size=2).to(device)\n",
    "print('Training autoencoder (bottleneck=2)...')\n",
    "history_2d = train_autoencoder(model_2d, train_loader, num_epochs=15)\n",
    "\n",
    "print()\n",
    "\n",
    "# 8D bottleneck\n",
    "model_8d = Autoencoder(bottleneck_size=8).to(device)\n",
    "print('Training autoencoder (bottleneck=8)...')\n",
    "history_8d = train_autoencoder(model_8d, train_loader, num_epochs=15)\n",
    "```\n",
    "\n",
    "Nothing new â€” we are reusing the architecture from Exercise 1 with different bottleneck sizes.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare reconstructions across all three bottleneck sizes\n",
    "# Get a fixed batch of test images for fair comparison\n",
    "fixed_images, _ = next(iter(test_loader))\n",
    "fixed_images = fixed_images[:8].to(device)\n",
    "\n",
    "fig, axes = plt.subplots(4, 8, figsize=(12, 6))\n",
    "\n",
    "# Row 0: originals\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(fixed_images[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "axes[0, 0].set_ylabel('Original', fontsize=10, rotation=0, labelpad=60)\n",
    "\n",
    "# Row 1: bottleneck=2\n",
    "with torch.no_grad():\n",
    "    recon_2d = model_2d(fixed_images).cpu().numpy()\n",
    "for i in range(8):\n",
    "    axes[1, i].imshow(recon_2d[i].squeeze(), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "axes[1, 0].set_ylabel('2D', fontsize=10, rotation=0, labelpad=60)\n",
    "\n",
    "# Row 2: bottleneck=8\n",
    "with torch.no_grad():\n",
    "    recon_8d = model_8d(fixed_images).cpu().numpy()\n",
    "for i in range(8):\n",
    "    axes[2, i].imshow(recon_8d[i].squeeze(), cmap='gray')\n",
    "    axes[2, i].axis('off')\n",
    "axes[2, 0].set_ylabel('8D', fontsize=10, rotation=0, labelpad=60)\n",
    "\n",
    "# Row 3: bottleneck=32\n",
    "with torch.no_grad():\n",
    "    recon_32d = model(fixed_images).cpu().numpy()\n",
    "for i in range(8):\n",
    "    axes[3, i].imshow(recon_32d[i].squeeze(), cmap='gray')\n",
    "    axes[3, i].axis('off')\n",
    "axes[3, 0].set_ylabel('32D', fontsize=10, rotation=0, labelpad=60)\n",
    "\n",
    "fig.suptitle('Reconstruction Quality vs Bottleneck Size', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nFinal MSE loss by bottleneck size:')\n",
    "print(f'  2D:  {history_2d[-1]:.6f}')\n",
    "print(f'  8D:  {history_8d[-1]:.6f}')\n",
    "print(f'  32D: {history_32[-1]:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:**\n",
    "\n",
    "With **2 dimensions**: only the crudest shape survives â€” you might tell a trouser from a bag, but details are completely gone. Two numbers can only encode the broadest features.\n",
    "\n",
    "With **8 dimensions**: recognizable category and rough shape, but still quite blurry. The network has more room to encode distinguishing features.\n",
    "\n",
    "With **32 dimensions**: the best reconstruction of the three â€” outlines are clearer, proportions are more accurate. But it is still imperfect.\n",
    "\n",
    "This is the bottleneck tradeoff: fewer dimensions forces the network to learn what truly matters (better compression, worse reconstruction). More dimensions allows better reconstruction but less meaningful compression. And if the bottleneck is too large (>= 784), the network can just copy â€” learning nothing. That is the **overcomplete trap**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Visualize the 2D Latent Space [Supported]\n",
    "\n",
    "The 2D autoencoder has a special property: we can **plot the latent space directly** on a scatter plot. Each image encodes to a point (z1, z2) in 2D space.\n",
    "\n",
    "Your task: encode all test images through the 2D encoder and plot them, color-coded by clothing class. This reveals whether the autoencoder organizes similar items near each other.\n",
    "\n",
    "**Think first:** Do you expect clear clusters by class? Remember â€” the autoencoder was never told about labels. It only learned to reconstruct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all test images into the 2D latent space\n",
    "model_2d.eval()\n",
    "all_latents = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        # TODO: Use model_2d.encoder (not the full model) to get latent codes\n",
    "        # Hint: latent = model_2d.encoder(images)\n",
    "        latent = None  # TODO: replace\n",
    "\n",
    "        all_latents.append(latent.cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_latents = torch.cat(all_latents).numpy()  # shape: [10000, 2]\n",
    "all_labels = torch.cat(all_labels).numpy()     # shape: [10000]\n",
    "\n",
    "print(f'Latent vectors shape: {all_latents.shape}')\n",
    "print(f'Labels shape: {all_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The key insight: we use `model_2d.encoder` (just the encoder half), not `model_2d` (the full autoencoder). The full model returns the reconstruction; the encoder returns the latent code.\n",
    "\n",
    "```python\n",
    "latent = model_2d.encoder(images)\n",
    "```\n",
    "\n",
    "This gives us the 2D latent vector for each image â€” the compressed representation that the bottleneck learned.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a scatter plot of the 2D latent space, color-coded by class\n",
    "#\n",
    "# Steps:\n",
    "# 1. Create a figure: plt.figure(figsize=(10, 8))\n",
    "# 2. For each class (0-9), plot its points with a different color:\n",
    "#    mask = all_labels == class_idx\n",
    "#    plt.scatter(all_latents[mask, 0], all_latents[mask, 1],\n",
    "#                s=2, alpha=0.5, label=CLASS_NAMES[class_idx])\n",
    "# 3. Add legend, title, axis labels\n",
    "#\n",
    "# Hint: loop over range(10) for the 10 classes\n",
    "\n",
    "pass  # TODO: replace with your plotting code\n",
    "\n",
    "print('\\nNotice: the autoencoder was NEVER told about class labels.')\n",
    "print('Any clustering emerged purely from learning to reconstruct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The key insight: we loop over each class and plot its points separately so each class gets its own color and legend entry. The scatter plot reveals structure that the autoencoder discovered on its own.\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for class_idx in range(10):\n",
    "    mask = all_labels == class_idx\n",
    "    plt.scatter(\n",
    "        all_latents[mask, 0],\n",
    "        all_latents[mask, 1],\n",
    "        s=2, alpha=0.5,\n",
    "        label=CLASS_NAMES[class_idx]\n",
    "    )\n",
    "\n",
    "plt.legend(markerscale=5, fontsize=9)\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('2D Latent Space (colored by class)')\n",
    "plt.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nNotice: the autoencoder was NEVER told about class labels.')\n",
    "print('Any clustering emerged purely from learning to reconstruct.')\n",
    "```\n",
    "\n",
    "Common mistake: forgetting to set a small `s` (point size) and `alpha` (transparency). With 10,000 points, large opaque dots create an unreadable blob.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:**\n",
    "\n",
    "You should see partial clustering â€” items of the same class tend to group together, but the boundaries are messy and overlapping. Trousers are probably in one region, bags in another, but T-shirts and shirts overlap heavily (because they look similar).\n",
    "\n",
    "The remarkable thing: **the autoencoder was never told about labels**. It learned to organize similar-looking items near each other purely from the pressure to reconstruct. To compress a trouser into 2 numbers and get a trouser-shaped reconstruction, the network must encode \"this is trouser-shaped\" somewhere in those 2 numbers.\n",
    "\n",
    "But notice the **gaps**. There are regions of the 2D space where no training image was encoded. If you fed a random 2D point from one of these gaps to the decoder, you would get garbage â€” not a recognizable image. This is exactly why the autoencoder is **not a generative model**. The latent space has structure where data was encoded, but uncharted gaps between the clusters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Build a Denoising Autoencoder [Independent]\n",
    "\n",
    "A **denoising autoencoder** receives a noisy version of the input and must reconstruct the **clean** original. This forces the network to learn even more robust features â€” it cannot just memorize or copy, because the noise is different every time.\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "1. Write a function `add_noise(images, noise_factor=0.3)` that adds Gaussian noise to images and clamps the result to [0, 1]\n",
    "2. Build a `DenoisingAutoencoder` with bottleneck_size=32 (same architecture as `Autoencoder`)\n",
    "3. Write a training loop where:\n",
    "   - The **input** to the model is `noisy_images = add_noise(images)`\n",
    "   - The **target** for the loss is the **clean** `images`\n",
    "   - `loss = criterion(model(noisy_images), images)`\n",
    "4. Train for 15 epochs\n",
    "5. Visualize: show noisy input (top row), denoised reconstruction (middle row), clean original (bottom row)\n",
    "\n",
    "**Hint:** The architecture is identical to `Autoencoder`. The only change is in the training loop: you feed noisy images in but compute loss against clean images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here â€” build and train a denoising autoencoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The key insight: the architecture does not change at all. The only difference from a regular autoencoder is **what you feed in** and **what you compare against**. The model receives noisy images but the loss is computed against the clean originals. This forces the bottleneck to learn features that are robust to noise â€” it must figure out what the \"real\" image looks like underneath the noise.\n",
    "\n",
    "```python\n",
    "def add_noise(images, noise_factor=0.3):\n",
    "    \"\"\"Add Gaussian noise to images and clamp to [0, 1].\"\"\"\n",
    "    noise = torch.randn_like(images) * noise_factor\n",
    "    noisy = images + noise\n",
    "    return torch.clamp(noisy, 0.0, 1.0)\n",
    "\n",
    "\n",
    "# Same architecture â€” the denoising behavior comes from the training loop\n",
    "denoising_model = Autoencoder(bottleneck_size=32).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(denoising_model.parameters(), lr=1e-3)\n",
    "\n",
    "print('Training denoising autoencoder...')\n",
    "print('=' * 45)\n",
    "\n",
    "for epoch in range(15):\n",
    "    denoising_model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        noisy_images = add_noise(images)  # corrupt the input\n",
    "\n",
    "        recon = denoising_model(noisy_images)  # feed noisy\n",
    "        loss = criterion(recon, images)         # compare to CLEAN\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        total += images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    print(f'  Epoch {epoch+1:2d}/15  Loss: {epoch_loss:.6f}')\n",
    "\n",
    "print('=' * 45)\n",
    "print('Done!')\n",
    "\n",
    "# Visualize: noisy input â†’ denoised â†’ clean original\n",
    "denoising_model.eval()\n",
    "test_images, _ = next(iter(test_loader))\n",
    "test_images = test_images[:8].to(device)\n",
    "noisy_test = add_noise(test_images, noise_factor=0.3)\n",
    "\n",
    "with torch.no_grad():\n",
    "    denoised = denoising_model(noisy_test)\n",
    "\n",
    "fig, axes = plt.subplots(3, 8, figsize=(12, 5))\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(noisy_test[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    axes[1, i].imshow(denoised[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "    axes[2, i].imshow(test_images[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Noisy', fontsize=10, rotation=0, labelpad=50)\n",
    "axes[1, 0].set_ylabel('Denoised', fontsize=10, rotation=0, labelpad=50)\n",
    "axes[2, 0].set_ylabel('Original', fontsize=10, rotation=0, labelpad=50)\n",
    "\n",
    "fig.suptitle('Denoising Autoencoder Results', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Common mistake: computing loss against the noisy images instead of the clean originals. That would just train a regular autoencoder on noisy data. The denoising objective specifically requires `criterion(recon, clean_images)`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The autoencoder is an hourglass: compress through a bottleneck, then reconstruct.** The encoder is a CNN you already know. The decoder reverses it with ConvTranspose2d. The bottleneck forces the network to learn what matters about the input.\n",
    "\n",
    "2. **Reconstruction loss = MSE between input and output. The target IS the input.** No labels needed. The data supervises itself. The loss measures what the bottleneck fails to preserve.\n",
    "\n",
    "3. **Bottleneck size controls the compression-quality tradeoff.** Smaller bottleneck = more compression, blurrier reconstruction, but the latent code captures only what truly matters. Too large and the network just copies pixels (the overcomplete trap).\n",
    "\n",
    "4. **The latent space has structure even without labels.** Similar items cluster together because the network must encode similar-looking images to similar latent codes to reconstruct them well.\n",
    "\n",
    "5. **The autoencoder is NOT a generative model.** The latent space has gaps â€” random points produce garbage. Only points near real encoded images are meaningful. Making the latent space smooth enough to sample from is what the Variational Autoencoder does (next lesson)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}