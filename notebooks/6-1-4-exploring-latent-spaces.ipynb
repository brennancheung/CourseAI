{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Exploring Latent Spaces\n\nIn this notebook, you'll explore the latent space of a trained VAE on Fashion-MNIST.\n\n**What you'll do:**\n- Encode Fashion-MNIST items into the latent space and visualize the 2D manifold with t-SNE\n- Interpolate between two images in latent space and watch smooth, coherent transitions\n- Perform latent arithmetic — extract directions between encoded items and apply them to new items\n- Sample from different regions of the latent space and map which regions generate which categories\n- Design a targeted generation experiment to create specific clothing styles by navigating the latent space\n\n**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones — they reveal gaps in your mental model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: self-contained for Google Colab\n",
    "# No additional pip installs needed — torch, sklearn, matplotlib are all available by default\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VAE Definition + Quick Training\n",
    "# ============================================================\n",
    "# If you have a saved checkpoint from the VAE notebook, you can\n",
    "# load it instead. Otherwise, this trains a fresh VAE in ~2 min\n",
    "# on a GPU (or ~5 min on CPU).\n",
    "\n",
    "LATENT_DIM = 32\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=LATENT_DIM):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder: image -> (mu, logvar)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1),   # (32, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # (64, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),                                # (64*7*7 = 3136)\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(3136, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(3136, latent_dim)\n",
    "\n",
    "        # Decoder: z -> image\n",
    "        self.decoder_fc = nn.Linear(latent_dim, 3136)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),  # (32, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),   # (1, 28, 28)\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.decoder_fc(z)\n",
    "        h = h.view(-1, 64, 7, 7)\n",
    "        return self.decoder(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, beta=1.0):\n",
    "    recon = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon + beta * kl\n",
    "\n",
    "\n",
    "# Load Fashion-MNIST\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Category names for labeling\n",
    "CATEGORY_NAMES = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]\n",
    "\n",
    "# Train the VAE (or load a checkpoint)\n",
    "model = VAE(LATENT_DIM).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print('Training VAE...')\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch, _ in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, mu, logvar = model(batch)\n",
    "        loss = vae_loss(recon, batch, mu, logvar, beta=1.0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_dataset)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'  Epoch {epoch+1:2d}/15  loss: {avg_loss:.2f}')\n",
    "\n",
    "model.eval()\n",
    "print('Done! VAE is ready for exploration.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Shared Helpers\n",
    "# ============================================================\n",
    "\n",
    "def show_grid(images, nrow=5, title=None):\n",
    "    \"\"\"Display a batch of images as a grid.\"\"\"\n",
    "    n = images.shape[0]\n",
    "    ncol = nrow\n",
    "    nrow_actual = (n + ncol - 1) // ncol\n",
    "    fig, axes = plt.subplots(nrow_actual, ncol, figsize=(ncol * 1.5, nrow_actual * 1.5))\n",
    "    if nrow_actual == 1:\n",
    "        axes = axes[np.newaxis, :]\n",
    "    for i in range(nrow_actual * ncol):\n",
    "        r, c = divmod(i, ncol)\n",
    "        axes[r, c].axis('off')\n",
    "        if i < n:\n",
    "            axes[r, c].imshow(images[i].squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_interpolation_strip(images, labels=None, title=None):\n",
    "    \"\"\"Display a row of images as an interpolation strip.\"\"\"\n",
    "    n = len(images)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(n * 1.5, 2))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(images[i].squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "        if labels and i < len(labels):\n",
    "            ax.set_title(labels[i], fontsize=9)\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=13, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_items_by_category(dataset, category_idx, n=10):\n",
    "    \"\"\"Get n items from a specific category.\"\"\"\n",
    "    items = []\n",
    "    for img, label in dataset:\n",
    "        if label == category_idx:\n",
    "            items.append(img)\n",
    "            if len(items) >= n:\n",
    "                break\n",
    "    return torch.stack(items)\n",
    "\n",
    "\n",
    "def encode_to_mu(images):\n",
    "    \"\"\"Encode images and return just the mu (mean) vectors.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        mu, _ = model.encode(images.to(device))\n",
    "    return mu\n",
    "\n",
    "\n",
    "print('Helpers loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Encode and Visualize the Latent Space (Guided)\n",
    "\n",
    "The VAE compressed every 28x28 image (784 pixels) down to a 32-dimensional latent code. Similar items should encode to nearby points — T-shirts near T-shirts, sneakers near sneakers. But 32 dimensions are invisible. To see the structure, we project the latent codes down to 2D using **t-SNE** (t-distributed Stochastic Neighbor Embedding), which preserves neighborhood relationships: points that are close in 32D stay close in 2D.\n",
    "\n",
    "We will encode the entire test set (10,000 images), project to 2D, and color each point by its category label.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Will the categories form distinct clusters, or will everything be mixed together?\n",
    "- Which categories do you think will overlap? (Hint: think about which clothing items look similar.)\n",
    "- Will there be sharp boundaries between clusters, or smooth transitions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the entire test set\n",
    "all_z = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        mu, _ = model.encode(images.to(device))  # encode returns (mu, logvar) — use mu\n",
    "        all_z.append(mu.cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_z = torch.cat(all_z).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "print(f'Encoded {len(all_z)} images to {all_z.shape[1]}-dimensional latent codes')\n",
    "print(f'Now projecting to 2D with t-SNE (this takes ~30 seconds)...')\n",
    "\n",
    "# Project to 2D with t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "z_2d = tsne.fit_transform(all_z)\n",
    "\n",
    "# Plot colored by category\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "scatter = ax.scatter(z_2d[:, 0], z_2d[:, 1],\n",
    "                     c=all_labels, cmap='tab10', s=1, alpha=0.5)\n",
    "\n",
    "# Add a legend with category names\n",
    "handles = []\n",
    "for i, name in enumerate(CATEGORY_NAMES):\n",
    "    handles.append(plt.Line2D([0], [0], marker='o', color='w',\n",
    "                              markerfacecolor=plt.cm.tab10(i / 10), markersize=8, label=name))\n",
    "ax.legend(handles=handles, loc='upper right', fontsize=9, framealpha=0.8)\n",
    "\n",
    "ax.set_title('VAE Latent Space (t-SNE projection)', fontsize=14)\n",
    "ax.set_xlabel('t-SNE dim 1')\n",
    "ax.set_ylabel('t-SNE dim 2')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nLook for:')\n",
    "print('  - Clusters: similar items group together')\n",
    "print('  - Smooth transitions: related categories blur into each other')\n",
    "print('  - Overlap: where categories share features (e.g., pullover and coat)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:**\n",
    "\n",
    "The t-SNE plot reveals the structure that the KL regularizer created. Without KL (a plain autoencoder), you would see scattered points with no organization. With KL, the latent space is organized by similarity:\n",
    "\n",
    "- **Clusters** — T-shirts near T-shirts, sneakers near sneakers. The VAE learned that these items are similar and encoded them nearby.\n",
    "- **Smooth transitions** — Related categories blend together. Shoes blur into boots. Shirts blur into coats. There are no hard walls between categories.\n",
    "- **Overlap** — Where categories share visual features (pullover and coat, for instance), their regions overlap. The VAE cannot fully separate them because they genuinely look similar.\n",
    "\n",
    "This is the structure that makes interpolation and arithmetic possible. The space is not a random soup — it is a map of what the network learned about clothing.\n",
    "\n",
    "**Caveat:** t-SNE is a *visualization* tool, not ground truth. It distorts distances and can exaggerate gaps. If you see a cluster, those points really are nearby in 32D. If you see a gap, it might be real or might be t-SNE exaggerating. Different runs (or different `perplexity` values) produce different layouts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Interpolate Between Two Images (Guided)\n",
    "\n",
    "The latent space is smooth — the KL regularizer filled all the gaps. That means you can **walk** from one encoded image to another and see coherent intermediate images at every step. This is **latent interpolation**.\n",
    "\n",
    "The formula: encode image A to $z_A$, encode image B to $z_B$, then:\n",
    "\n",
    "$$z_t = (1-t) \\cdot z_A + t \\cdot z_B \\quad \\text{for } t \\in [0, 1]$$\n",
    "\n",
    "At $t=0$ you get image A. At $t=1$ you get image B. In between, you get plausible intermediate images — not ghostly overlays, but coherent garments morphing from one form to another.\n",
    "\n",
    "We will do two things:\n",
    "1. **Pixel-space interpolation** — average the raw pixel values. This produces a ghostly double exposure.\n",
    "2. **Latent-space interpolation** — average the latent codes, then decode. This produces coherent transitions.\n",
    "\n",
    "**Before running, predict:**\n",
    "- For pixel interpolation at $t=0.5$: what will you see? (Think: what does averaging two grayscale images look like?)\n",
    "- For latent interpolation at $t=0.5$ between a T-shirt and a trouser: will the intermediate image look like clothing, or like noise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one T-shirt and one trouser from the test set\n",
    "tshirt_images = get_items_by_category(test_dataset, 0, n=1)   # 0 = T-shirt/top\n",
    "trouser_images = get_items_by_category(test_dataset, 1, n=1)  # 1 = Trouser\n",
    "\n",
    "image_A = tshirt_images[0:1]   # shape: (1, 1, 28, 28)\n",
    "image_B = trouser_images[0:1]\n",
    "\n",
    "# Encode both images\n",
    "z_A = encode_to_mu(image_A)  # shape: (1, 32)\n",
    "z_B = encode_to_mu(image_B)\n",
    "\n",
    "# Create interpolation steps\n",
    "n_steps = 8\n",
    "t_values = torch.linspace(0, 1, n_steps)\n",
    "\n",
    "# --- Pixel-space interpolation (the naive approach) ---\n",
    "pixel_interp = []\n",
    "for t in t_values:\n",
    "    blended = (1 - t) * image_A + t * image_B\n",
    "    pixel_interp.append(blended.squeeze())\n",
    "\n",
    "# --- Latent-space interpolation (the VAE approach) ---\n",
    "latent_interp = []\n",
    "with torch.no_grad():\n",
    "    for t in t_values:\n",
    "        z_t = (1 - t) * z_A + t * z_B  # interpolate in latent space\n",
    "        decoded = model.decode(z_t)      # decode the interpolated code\n",
    "        latent_interp.append(decoded.squeeze().cpu())\n",
    "\n",
    "# Display both strips side by side\n",
    "t_labels = [f't={t:.2f}' for t in t_values]\n",
    "\n",
    "print('PIXEL-SPACE interpolation (averaging raw pixels):')\n",
    "show_interpolation_strip(pixel_interp, labels=t_labels, title='Pixel Interpolation: T-shirt to Trouser')\n",
    "\n",
    "print('LATENT-SPACE interpolation (averaging latent codes, then decoding):')\n",
    "show_interpolation_strip(latent_interp, labels=t_labels, title='Latent Interpolation: T-shirt to Trouser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:**\n",
    "\n",
    "The two strips tell completely different stories:\n",
    "\n",
    "- **Pixel interpolation** produced a ghostly double exposure. At $t=0.5$, both shapes are visible at once — transparent and overlapping. It looks like a photography accident, not a real garment. That is because pixel-space does not understand \"clothing.\" It just averages numbers.\n",
    "\n",
    "- **Latent interpolation** produced a coherent transition. At every step, there is ONE solid shape that smoothly morphs from a T-shirt into trousers. The intermediate images look like actual clothing items. That is because the VAE's latent space is organized — the decoder knows what to do with every point along the path.\n",
    "\n",
    "This is the key insight of continuous latent spaces: **interpolation is not blending two images.** It is asking the decoder, \"What image lives at this intermediate point in the space you organized?\" The answer is a coherent image because the KL regularizer filled all the gaps.\n",
    "\n",
    "Remember the city map analogy from the VAE lesson: the KL term built roads connecting all the buildings. Interpolation is literally walking those roads. Every location along the path is a real place — a plausible image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Latent Space Arithmetic (Supported)\n",
    "\n",
    "If the latent space captures meaningful structure, then the **direction** between two encoded items captures the **difference** between them. You can extract that direction and apply it to something else.\n",
    "\n",
    "The classic setup:\n",
    "- Encode an ankle boot and a sneaker\n",
    "- The vector $z(\\text{ankle boot}) - z(\\text{sneaker})$ captures roughly \"what makes a boot different from a sneaker\" — something about height or ankle coverage\n",
    "- Add that direction to a sandal: $z(\\text{sandal}) + [z(\\text{ankle boot}) - z(\\text{sneaker})]$\n",
    "- If the space is well-organized, the result should look like a higher or more boot-like sandal\n",
    "\n",
    "To reduce noise, we will average across multiple examples of each category rather than using single items. This gives us the \"average sneaker,\" \"average ankle boot,\" and \"average sandal\" in latent space.\n",
    "\n",
    "**Your task:** Fill in the TODO sections to compute the attribute direction and apply it.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "The arithmetic is just vector subtraction and addition on the latent codes (mu vectors). Compute the direction by subtracting one category's average from another's, then add that direction to a third category's average.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get multiple examples from each category and encode them\n",
    "n_samples = 20  # average over 20 examples per category for a cleaner signal\n",
    "\n",
    "sneaker_imgs = get_items_by_category(test_dataset, 7, n=n_samples)       # 7 = Sneaker\n",
    "ankle_boot_imgs = get_items_by_category(test_dataset, 9, n=n_samples)    # 9 = Ankle boot\n",
    "sandal_imgs = get_items_by_category(test_dataset, 5, n=n_samples)        # 5 = Sandal\n",
    "\n",
    "# Encode each batch and compute the average latent code\n",
    "z_sneaker_avg = encode_to_mu(sneaker_imgs).mean(dim=0, keepdim=True)       # shape: (1, 32)\n",
    "z_ankle_boot_avg = encode_to_mu(ankle_boot_imgs).mean(dim=0, keepdim=True)\n",
    "z_sandal_avg = encode_to_mu(sandal_imgs).mean(dim=0, keepdim=True)\n",
    "\n",
    "# ============================================================\n",
    "# TODO: Compute the \"boot-ness\" direction\n",
    "# What direction in latent space takes you from a sneaker to an ankle boot?\n",
    "# ============================================================\n",
    "boot_direction = # TODO: subtract the average sneaker code from the average ankle boot code\n",
    "\n",
    "# ============================================================\n",
    "# TODO: Apply the boot direction to the sandal\n",
    "# This should produce something more boot-like than a sandal\n",
    "# ============================================================\n",
    "z_result = # TODO: add the boot direction to the average sandal code\n",
    "\n",
    "# Decode everything to see the results\n",
    "with torch.no_grad():\n",
    "    img_sneaker = model.decode(z_sneaker_avg)\n",
    "    img_ankle_boot = model.decode(z_ankle_boot_avg)\n",
    "    img_sandal = model.decode(z_sandal_avg)\n",
    "    img_result = model.decode(z_result)\n",
    "\n",
    "# Display: sneaker, ankle boot, the direction, sandal, result\n",
    "fig, axes = plt.subplots(1, 5, figsize=(14, 3))\n",
    "titles = ['Avg Sneaker', 'Avg Ankle Boot', '\"Boot-ness\"\\ndirection', 'Avg Sandal', 'Sandal + Boot-ness']\n",
    "\n",
    "axes[0].imshow(img_sneaker.squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "axes[1].imshow(img_ankle_boot.squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "# For the \"direction\" panel, show the difference in decoded space (just for visualization)\n",
    "diff_image = (img_ankle_boot - img_sneaker).squeeze().cpu().numpy()\n",
    "axes[2].imshow(diff_image, cmap='RdBu', vmin=-0.5, vmax=0.5)\n",
    "\n",
    "axes[3].imshow(img_sandal.squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "axes[4].imshow(img_result.squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "for ax, title in zip(axes, titles):\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Add arithmetic symbols between panels\n",
    "fig.text(0.26, 0.5, '-', fontsize=24, ha='center', va='center', color='white')\n",
    "fig.text(0.445, 0.5, '=', fontsize=24, ha='center', va='center', color='white')\n",
    "fig.text(0.635, 0.5, '+', fontsize=24, ha='center', va='center', color='white')\n",
    "fig.text(0.82, 0.5, '=', fontsize=24, ha='center', va='center', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('The result should look somewhat boot-like — taller than a sandal,')\n",
    "print('possibly with more ankle coverage. It will be noisy — this is Fashion-MNIST,')\n",
    "print('not a face dataset with smooth attribute variation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The key insight is that **directions in latent space capture relationships**. Subtracting two category averages extracts what makes them different. Adding that direction to a third category transfers the difference.\n",
    "\n",
    "```python\n",
    "# The \"boot-ness\" direction: what changes when you go from sneaker to ankle boot\n",
    "boot_direction = z_ankle_boot_avg - z_sneaker_avg\n",
    "\n",
    "# Apply it to the sandal: sandal + boot-ness = boot-like sandal\n",
    "z_result = z_sandal_avg + boot_direction\n",
    "```\n",
    "\n",
    "The arithmetic is trivial — just vector subtraction and addition. The remarkable part is that it produces meaningful results: the decoded image should look like a sandal that has been made more boot-like (taller, more ankle coverage).\n",
    "\n",
    "The results are noisy because Fashion-MNIST has discrete categories with limited smooth attribute variation. On face datasets (CelebA), this same technique transfers attributes like \"wearing glasses\" or \"smiling\" much more cleanly. The concept is real; clean results require data with consistent, continuous variation.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What the result shows:**\n",
    "\n",
    "If the arithmetic worked, the result image should be directionally correct — something sandal-like but taller or with more coverage. It will not be perfectly clean. That noisiness is honest: most random directions in latent space do not correspond to interpretable features. Only specific learned directions encode meaningful attributes, and even those work best when the training data has consistent, continuous variation in that attribute. Fashion-MNIST's discrete categories make this noisier than the famous face-attribute examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Sample from Different Regions (Supported)\n",
    "\n",
    "The KL regularizer organized the latent space so that similar items cluster together. Different regions of the space generate different types of clothing. You can map this out by systematically sampling from different locations.\n",
    "\n",
    "**Your task:** Create a 2D grid of latent codes by varying the first two dimensions of z while keeping the rest at 0. Decode each point and display the resulting images. This gives you a \"map\" of what the decoder produces across the latent space.\n",
    "\n",
    "You will set dimensions 3 through 32 to zero (the mean of N(0,1)) and vary dimensions 1 and 2 across a grid from -3 to +3. Each grid cell is a decoded image from that location in latent space.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Create a meshgrid of values from -3 to +3 for two dimensions. For each (d1, d2) pair, build a latent vector that is all zeros except for those two dimensions. Decode each vector and place the image in the corresponding grid cell.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Create a grid of latent codes and decode them\n",
    "# ============================================================\n",
    "\n",
    "grid_size = 10  # 10x10 grid\n",
    "grid_range = torch.linspace(-3, 3, grid_size)\n",
    "\n",
    "# We will vary dimensions 0 and 1, keeping all others at 0\n",
    "# This gives a 2D \"slice\" through the 32-dimensional latent space\n",
    "\n",
    "decoded_grid = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for d2_val in reversed(grid_range):   # reversed so top of plot = positive d2\n",
    "        row_images = []\n",
    "        for d1_val in grid_range:\n",
    "            # TODO: Create a latent vector z of shape (1, LATENT_DIM)\n",
    "            # with all zeros, except dimension 0 = d1_val and dimension 1 = d2_val\n",
    "            z = # TODO\n",
    "\n",
    "            # TODO: Decode z to get an image\n",
    "            img = # TODO\n",
    "\n",
    "            row_images.append(img.squeeze().cpu().numpy())\n",
    "        decoded_grid.append(row_images)\n",
    "\n",
    "# Display as a single large image\n",
    "fig, axes = plt.subplots(grid_size, grid_size, figsize=(12, 12))\n",
    "for r in range(grid_size):\n",
    "    for c in range(grid_size):\n",
    "        axes[r, c].imshow(decoded_grid[r][c], cmap='gray', vmin=0, vmax=1)\n",
    "        axes[r, c].axis('off')\n",
    "\n",
    "# Label the axes\n",
    "fig.text(0.5, 0.02, 'Latent Dimension 1 (from -3 to +3)', ha='center', fontsize=13)\n",
    "fig.text(0.02, 0.5, 'Latent Dimension 2 (from -3 to +3)', va='center', rotation='vertical', fontsize=13)\n",
    "fig.suptitle('Decoded Images Across a 2D Slice of the Latent Space', fontsize=14, y=0.98)\n",
    "plt.tight_layout(rect=[0.03, 0.03, 1, 0.97])\n",
    "plt.show()\n",
    "\n",
    "print('Each cell is a decoded image from a different point in latent space.')\n",
    "print('Look for smooth transitions as you move in any direction.')\n",
    "print('Different regions produce different categories of clothing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The key insight is that each point in latent space decodes to a specific image. By sweeping two dimensions across a grid, you are exploring a 2D plane through the 32-dimensional space. Different regions of this plane produce different types of clothing, and the transitions between regions are smooth.\n",
    "\n",
    "```python\n",
    "# Create a zero vector and set dimensions 0 and 1\n",
    "z = torch.zeros(1, LATENT_DIM).to(device)\n",
    "z[0, 0] = d1_val\n",
    "z[0, 1] = d2_val\n",
    "\n",
    "# Decode\n",
    "img = model.decode(z)\n",
    "```\n",
    "\n",
    "Note that we are only seeing a thin slice of the full space. The other 30 dimensions are fixed at 0. Moving along those other dimensions would reveal more structure, more categories, and more variation. What you see here is just a 2D cross-section through a much richer space.\n",
    "\n",
    "Common things to notice:\n",
    "- Certain regions consistently produce the same category (e.g., a corner might always produce sneakers)\n",
    "- Transitions between regions are gradual, not abrupt\n",
    "- The center (0, 0) tends to produce something generic or average-looking, because that is the most densely populated region of N(0,1)\n",
    "- The edges (near -3 or +3) may produce odder or less recognizable items, because those are far from the training distribution's center\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Targeted Generation Experiment (Independent)\n",
    "\n",
    "You now have all the tools: encoding, decoding, interpolation, arithmetic, and region mapping. Use them to design an experiment that generates specific types of images by intentionally navigating the latent space.\n",
    "\n",
    "**Task:** Pick a clothing category you want to generate variations of (e.g., different styles of shoes, or variations of a T-shirt). Use any combination of the techniques you have practiced to:\n",
    "\n",
    "1. Find where that category lives in the latent space (encode several examples and look at their average latent code)\n",
    "2. Generate a grid of variations by perturbing the average code in different directions\n",
    "3. Display the results and describe what each direction of perturbation seems to control\n",
    "\n",
    "**Requirements:**\n",
    "- Generate at least 15 images that are variations of your chosen category\n",
    "- Use at least two different techniques (e.g., sampling near the category center + arithmetic from another category)\n",
    "- Include a brief written interpretation: what do the different directions seem to control?\n",
    "\n",
    "This is independent — write the code from scratch. No skeleton is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# YOUR EXPERIMENT\n",
    "# ============================================================\n",
    "# Design and implement your targeted generation experiment here.\n",
    "# Some ideas to get you started:\n",
    "#\n",
    "# Approach A: \"Neighborhood sampling\"\n",
    "#   - Encode many examples of one category\n",
    "#   - Compute the average latent code (the category center)\n",
    "#   - Sample nearby points: z_center + small_noise\n",
    "#   - Vary the noise magnitude to control how different the variations are\n",
    "#\n",
    "# Approach B: \"Dimension exploration\"\n",
    "#   - Start at a category center\n",
    "#   - Vary one latent dimension at a time while holding others fixed\n",
    "#   - See which dimensions control which visual features\n",
    "#\n",
    "# Approach C: \"Category blending\"\n",
    "#   - Pick two related categories (e.g., sneaker and ankle boot)\n",
    "#   - Use interpolation with different t values to generate a spectrum\n",
    "#   - Use arithmetic to transfer attributes from a third category\n",
    "#\n",
    "# Write your code below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution (one possible approach)</summary>\n",
    "\n",
    "There is no single correct answer — the point is to navigate the latent space intentionally and observe what happens. Here is one approach that combines neighborhood sampling with dimension exploration for sneaker variations:\n",
    "\n",
    "**Reasoning:** To generate variations of sneakers, we first find where sneakers live in latent space by averaging many encoded sneakers. Then we explore two things: (1) what happens when we add small random noise (variations within the category), and (2) what each latent dimension controls (by varying one dimension at a time).\n",
    "\n",
    "```python\n",
    "# Find the sneaker center in latent space\n",
    "sneaker_imgs = get_items_by_category(test_dataset, 7, n=50)\n",
    "z_sneaker_center = encode_to_mu(sneaker_imgs).mean(dim=0, keepdim=True)\n",
    "\n",
    "# --- Technique 1: Neighborhood sampling ---\n",
    "# Small perturbations around the center produce sneaker variations\n",
    "torch.manual_seed(42)\n",
    "noise_scale = 0.5  # small enough to stay in the sneaker region\n",
    "z_variations = z_sneaker_center + noise_scale * torch.randn(15, LATENT_DIM).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    variation_imgs = model.decode(z_variations)\n",
    "\n",
    "show_grid(variation_imgs, nrow=5, title='Sneaker Variations (neighborhood sampling, scale=0.5)')\n",
    "\n",
    "# --- Technique 2: Dimension exploration ---\n",
    "# Vary the top 5 most-variant dimensions one at a time\n",
    "# First, find which dimensions have the most variance across sneakers\n",
    "z_sneakers = encode_to_mu(sneaker_imgs)  # (50, 32)\n",
    "dim_variance = z_sneakers.var(dim=0)     # (32,)\n",
    "top_dims = dim_variance.argsort(descending=True)[:5].cpu().numpy()\n",
    "\n",
    "print(f'Top 5 highest-variance dimensions for sneakers: {top_dims}')\n",
    "print(f'(These are the dimensions that vary most across different sneakers)\\n')\n",
    "\n",
    "# For each top dimension, sweep from -2 to +2 around the sneaker center\n",
    "sweep_values = torch.linspace(-2, 2, 7)\n",
    "\n",
    "for dim_idx in top_dims[:3]:  # just top 3 for space\n",
    "    swept_images = []\n",
    "    with torch.no_grad():\n",
    "        for val in sweep_values:\n",
    "            z = z_sneaker_center.clone()\n",
    "            z[0, dim_idx] = val\n",
    "            img = model.decode(z)\n",
    "            swept_images.append(img.squeeze().cpu())\n",
    "    labels = [f'{v:.1f}' for v in sweep_values]\n",
    "    show_interpolation_strip(swept_images, labels=labels,\n",
    "                           title=f'Varying dimension {dim_idx}')\n",
    "\n",
    "print('Look at each strip. What visual feature does each dimension control?')\n",
    "print('Common findings: width, brightness, shape curvature, sole thickness, etc.')\n",
    "```\n",
    "\n",
    "The neighborhood sampling shows that adding small noise to the sneaker center produces plausible sneaker variations — different shapes, widths, and styles, all recognizably sneakers. The dimension exploration reveals which latent dimensions control which visual features, though the features are often not cleanly interpretable (a single dimension might mix several visual attributes).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **A trained VAE's latent space is a continuous, organized space where you can sample, interpolate, and do arithmetic.** The KL regularizer organized the space so that similar items cluster together, and every region is meaningful. This is the structure that makes generation possible.\n",
    "\n",
    "2. **Interpolation in latent space produces coherent transitions; pixel-space interpolation does not.** Pixel averaging creates ghostly overlays. Latent averaging creates plausible intermediate images because the decoder understands the space between encoded points. Interpolation is not blending — it is asking \"what lives at this point in the organized space?\"\n",
    "\n",
    "3. **Directions in latent space capture relationships between concepts.** Subtracting two encoded items gives a direction that represents their difference. Adding that direction to a third item transfers the difference. This works when the attribute has consistent, continuous variation in the training data.\n",
    "\n",
    "4. **The structure in the latent space reflects what the network learned about the data.** Similar items cluster together. The t-SNE visualization makes this structure visible. Different regions generate different categories, with smooth transitions between them.\n",
    "\n",
    "5. **VAE generation works but has a fundamental quality ceiling from the reconstruction-vs-KL tradeoff.** The blurriness is not a training failure — it is the price of a smooth, sampleable latent space. You cannot fix it with more epochs. Diffusion models take a fundamentally different approach to overcome this limitation.\n",
    "\n",
    "You learned to sample from a distribution and create things that have never existed. That is the core of generative AI. Everything from here forward is about doing it better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}