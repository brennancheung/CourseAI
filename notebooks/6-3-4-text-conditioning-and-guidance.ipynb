{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Conditioning & Guidance\n",
    "\n",
    "**Module 6.3, Lesson 4** | CourseAI\n",
    "\n",
    "In the lesson, you learned how cross-attention injects CLIP text embeddings into the U-Net, and how classifier-free guidance amplifies the text signal at inference time. Now you will build and explore both mechanisms hands-on.\n",
    "\n",
    "**What you will do:**\n",
    "- Modify a working self-attention implementation into cross-attention by changing where K and V come from\n",
    "- Visualize cross-attention weights as a heatmap showing different spatial locations attending to different text tokens\n",
    "- Implement the CFG formula and observe how guidance scale affects the magnitude of noise predictions\n",
    "- Generate images from a real text-conditioned diffusion model at different guidance scales and identify the quality/fidelity tradeoff\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "Everything builds on what you already know. Cross-attention is the same QKV formula from Module 4.2 — the only change is where K and V come from. CFG is one line of arithmetic. No new theory — just practice.\n",
    "\n",
    "**Estimated time:** 30–45 minutes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies and import everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "print('\\nSetup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Cross-Attention from Self-Attention [Guided]\n",
    "\n",
    "You built the self-attention formula across three lessons in Module 4.2:\n",
    "\n",
    "$$\\text{output} = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "In self-attention, Q, K, and V are all projected from the **same input**. Cross-attention changes one thing: **K and V come from a different input** (the text embeddings instead of the spatial features).\n",
    "\n",
    "Below is a working self-attention implementation. We will modify it to perform cross-attention by changing where K and V come from. The code demonstrates both side by side with dummy tensors: a 4×4 spatial feature map (16 locations, dimension 32) and 6 text token embeddings (dimension 32).\n",
    "\n",
    "**Before running, predict:**\n",
    "- In self-attention over 16 spatial locations, what is the shape of the attention weight matrix? (`16 × 16` — each location attends to every other location.)\n",
    "- After changing K and V to come from 6 text tokens instead, what shape will the attention weight matrix be? (No longer square — `16 × 6`, because 16 queries attend over 6 keys.)\n",
    "- Will the output shape change? (No — the output is always `(num_queries, d_v)`. The number of queries stays 16 and d_v stays 32.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Self-attention implementation ----\n",
    "\n",
    "d_model = 32   # embedding dimension\n",
    "H, W = 4, 4    # spatial feature map size\n",
    "n_spatial = H * W  # 16 spatial locations\n",
    "\n",
    "# Dummy spatial features: (16, 32) -- 16 locations, each with a 32-dim feature vector\n",
    "# In the real U-Net, these come from a residual block at 16x16 or 32x32 resolution.\n",
    "x_spatial = torch.randn(n_spatial, d_model)\n",
    "\n",
    "# Learned projection matrices\n",
    "W_Q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_K = nn.Linear(d_model, d_model, bias=False)\n",
    "W_V = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "def self_attention(x, W_Q, W_K, W_V):\n",
    "    \"\"\"Standard self-attention: Q, K, V all come from the same input x.\"\"\"\n",
    "    Q = W_Q(x)    # (n, d_model)\n",
    "    K = W_K(x)    # (n, d_model)  <-- same input x\n",
    "    V = W_V(x)    # (n, d_model)  <-- same input x\n",
    "    \n",
    "    d_k = Q.shape[-1]\n",
    "    attn_weights = torch.softmax(Q @ K.T / (d_k ** 0.5), dim=-1)\n",
    "    output = attn_weights @ V\n",
    "    return output, attn_weights\n",
    "\n",
    "# Run self-attention\n",
    "with torch.no_grad():\n",
    "    sa_output, sa_weights = self_attention(x_spatial, W_Q, W_K, W_V)\n",
    "\n",
    "print('=== Self-Attention ===')\n",
    "print(f'Input shape:             {x_spatial.shape}')      # (16, 32)\n",
    "print(f'Q shape:                 ({n_spatial}, {d_model})')\n",
    "print(f'K shape:                 ({n_spatial}, {d_model})  <-- from same input')\n",
    "print(f'V shape:                 ({n_spatial}, {d_model})  <-- from same input')\n",
    "print(f'Attention weights shape: {sa_weights.shape}')     # (16, 16) -- SQUARE\n",
    "print(f'Output shape:            {sa_output.shape}')      # (16, 32)\n",
    "print()\n",
    "print(f'The attention matrix is {sa_weights.shape[0]}×{sa_weights.shape[1]} -- SQUARE.')\n",
    "print(f'Each of {n_spatial} spatial locations attends to every other spatial location.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Cross-attention: the one-line change ----\n",
    "#\n",
    "# The ONLY difference: K and V are projected from the text embeddings,\n",
    "# not from the spatial features. Q still comes from spatial features.\n",
    "# The formula is IDENTICAL: output = softmax(QK^T / sqrt(d_k)) V\n",
    "\n",
    "T = 6  # number of text tokens (e.g., \"a cat sitting in a sunset\")\n",
    "\n",
    "# Dummy text embeddings: (6, 32) -- 6 tokens, each 32-dim\n",
    "# In the real U-Net, these come from CLIP's text encoder.\n",
    "x_text = torch.randn(T, d_model)\n",
    "\n",
    "def cross_attention(x_spatial, x_text, W_Q, W_K, W_V):\n",
    "    \"\"\"Cross-attention: Q from spatial features, K and V from text embeddings.\"\"\"\n",
    "    Q = W_Q(x_spatial)  # (n_spatial, d_model)\n",
    "    K = W_K(x_text)     # (T, d_model)  <-- DIFFERENT input!\n",
    "    V = W_V(x_text)     # (T, d_model)  <-- DIFFERENT input!\n",
    "    \n",
    "    d_k = Q.shape[-1]\n",
    "    attn_weights = torch.softmax(Q @ K.T / (d_k ** 0.5), dim=-1)\n",
    "    output = attn_weights @ V\n",
    "    return output, attn_weights\n",
    "\n",
    "# Run cross-attention\n",
    "with torch.no_grad():\n",
    "    ca_output, ca_weights = cross_attention(x_spatial, x_text, W_Q, W_K, W_V)\n",
    "\n",
    "print('=== Cross-Attention ===')\n",
    "print(f'Spatial input shape:     {x_spatial.shape}')      # (16, 32)\n",
    "print(f'Text input shape:        {x_text.shape}')         # (6, 32)\n",
    "print(f'Q shape:                 ({n_spatial}, {d_model})  <-- from spatial features')\n",
    "print(f'K shape:                 ({T}, {d_model})           <-- from TEXT')\n",
    "print(f'V shape:                 ({T}, {d_model})           <-- from TEXT')\n",
    "print(f'Attention weights shape: {ca_weights.shape}')     # (16, 6) -- NOT SQUARE\n",
    "print(f'Output shape:            {ca_output.shape}')      # (16, 32)\n",
    "print()\n",
    "print(f'The attention matrix is {ca_weights.shape[0]}×{ca_weights.shape[1]} -- RECTANGULAR.')\n",
    "print(f'Each of {n_spatial} spatial locations attends over {T} text tokens.')\n",
    "print(f'Different dimensions because Q and K come from different inputs.')\n",
    "print()\n",
    "print(f'Output shape is still ({n_spatial}, {d_model}) -- same as self-attention.')\n",
    "print(f'The number of queries determines the output size, not the number of keys.')\n",
    "print()\n",
    "print('--- Comparison ---')\n",
    "print(f'Self-attention weights: {sa_weights.shape}  (square: spatial × spatial)')\n",
    "print(f'Cross-attention weights: {ca_weights.shape}  (rectangular: spatial × text)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: each row of attention weights sums to 1 (softmax over text tokens)\n",
    "row_sums = ca_weights.sum(dim=-1)\n",
    "print('Row sums of cross-attention weights (should all be 1.0):')\n",
    "print(f'  Min: {row_sums.min():.6f}')\n",
    "print(f'  Max: {row_sums.max():.6f}')\n",
    "print()\n",
    "print('Each spatial location has its own probability distribution over text tokens.')\n",
    "print('This is what makes text conditioning SPATIALLY VARYING:')\n",
    "print('different locations can attend to different words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You modified self-attention into cross-attention with a **one-line change**: K and V now come from the text embeddings instead of the spatial features. Everything else — the dot-product scoring, the softmax, the weighted average — is identical.\n",
    "\n",
    "The key shape change:\n",
    "- **Self-attention:** `(16 × 16)` — square, because Q and K come from the same 16 spatial locations.\n",
    "- **Cross-attention:** `(16 × 6)` — rectangular, because 16 spatial queries attend over 6 text keys.\n",
    "\n",
    "The output shape stays `(16, 32)` in both cases — one enriched vector per spatial location. In self-attention, each location is enriched by other spatial locations. In cross-attention, each location is enriched by the text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Visualize Cross-Attention Weights [Guided]\n",
    "\n",
    "The lesson showed a table where different spatial regions (cat's face, sky, cat's body) attend to different text tokens with different weights. Now you will compute and visualize this yourself.\n",
    "\n",
    "We will use the cross-attention function from Exercise 1 with the same 4×4 spatial features and 6 text tokens. The resulting attention weight matrix has shape `(16, 6)` — 16 spatial locations, each with a distribution over 6 text tokens. We will visualize this as a heatmap.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Since these are random embeddings (not a trained model), will different spatial locations show different attention patterns? (Yes — different random query vectors will dot-product differently against the key vectors. The patterns will not be meaningful, but they will be different.)\n",
    "- Which text token will get the most attention on average across all spatial locations? (Cannot predict with random weights — it depends on the random initialization. But the mean attention per token will NOT be uniform, because the random queries will happen to align more with some keys than others.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the cross-attention weights computed in Exercise 1\n",
    "# ca_weights has shape (16, 6): 16 spatial locations x 6 text tokens\n",
    "\n",
    "# We will give the text tokens meaningful labels for the visualization\n",
    "text_token_labels = ['a', 'cat', 'sitting', 'in', 'a', 'sunset']\n",
    "\n",
    "# Create spatial position labels (row, col in 4x4 grid)\n",
    "spatial_labels = [f'({r},{c})' for r in range(H) for c in range(W)]\n",
    "\n",
    "# ---- Heatmap visualization ----\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), \n",
    "                                gridspec_kw={'width_ratios': [3, 1]})\n",
    "\n",
    "# Main heatmap\n",
    "weights_np = ca_weights.numpy()\n",
    "im = ax1.imshow(weights_np, cmap='magma', aspect='auto')\n",
    "\n",
    "# Annotate each cell\n",
    "for i in range(n_spatial):\n",
    "    for j in range(T):\n",
    "        val = weights_np[i, j]\n",
    "        color = 'black' if val > 0.25 else 'white'\n",
    "        ax1.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                fontsize=7, color=color)\n",
    "\n",
    "ax1.set_xticks(range(T))\n",
    "ax1.set_xticklabels(text_token_labels, fontsize=10)\n",
    "ax1.set_yticks(range(n_spatial))\n",
    "ax1.set_yticklabels(spatial_labels, fontsize=8)\n",
    "ax1.set_xlabel('Text Tokens', fontsize=11)\n",
    "ax1.set_ylabel('Spatial Location (row, col)', fontsize=11)\n",
    "ax1.set_title('Cross-Attention Weights: 16 spatial locations \\u00d7 6 text tokens', fontsize=12)\n",
    "plt.colorbar(im, ax=ax1, shrink=0.8)\n",
    "\n",
    "# Mean attention per text token (bar chart)\n",
    "mean_attn = weights_np.mean(axis=0)  # average across spatial locations\n",
    "bars = ax2.barh(range(T), mean_attn, color='#c084fc')\n",
    "ax2.set_yticks(range(T))\n",
    "ax2.set_yticklabels(text_token_labels, fontsize=10)\n",
    "ax2.set_xlabel('Mean Attention', fontsize=10)\n",
    "ax2.set_title('Avg Attention\\nper Token', fontsize=11)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# Annotate bars\n",
    "for i, v in enumerate(mean_attn):\n",
    "    ax2.text(v + 0.005, i, f'{v:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Observations:')\n",
    "print('  1. Each row sums to 1.0 (softmax) -- a probability distribution over text tokens.')\n",
    "print('  2. Different spatial locations have DIFFERENT attention patterns.')\n",
    "print('     This is what \"spatially varying conditioning\" means.')\n",
    "print('  3. With random weights, the patterns are not meaningful.')\n",
    "print('     In a trained model, spatial locations near the cat would attend')\n",
    "print('     strongly to \"cat\" and locations in the sky would attend to \"sunset.\"')\n",
    "print()\n",
    "\n",
    "# Which token gets the most and least attention on average?\n",
    "most_attended = text_token_labels[mean_attn.argmax()]\n",
    "least_attended = text_token_labels[mean_attn.argmin()]\n",
    "print(f'  Most attended token (on average):  \"{most_attended}\" ({mean_attn.max():.3f})')\n",
    "print(f'  Least attended token (on average): \"{least_attended}\" ({mean_attn.min():.3f})')\n",
    "print()\n",
    "print('  With random embeddings, this is not meaningful. In a real model, content')\n",
    "print('  words (\"cat\", \"sunset\") would receive more attention than function words (\"a\", \"in\").')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape and visualize as a 4x4 spatial grid for each text token\n",
    "# This shows WHERE in the spatial feature map each text token gets attention.\n",
    "\n",
    "fig, axes = plt.subplots(1, T, figsize=(15, 3))\n",
    "for j in range(T):\n",
    "    # Attention from all spatial locations to text token j\n",
    "    attn_for_token = weights_np[:, j].reshape(H, W)  # (4, 4)\n",
    "    im = axes[j].imshow(attn_for_token, cmap='magma', vmin=0, vmax=weights_np.max())\n",
    "    axes[j].set_title(f'\"{text_token_labels[j]}\"', fontsize=11)\n",
    "    axes[j].set_xticks([])\n",
    "    axes[j].set_yticks([])\n",
    "\n",
    "plt.suptitle('Cross-Attention: Where each text token is attended to in the 4\\u00d74 spatial grid',\n",
    "             fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Each subplot shows one text token\\'s attention across the 4\\u00d74 spatial grid.')\n",
    "print('Brighter = more attention from that spatial location to that text token.')\n",
    "print()\n",
    "print('In a trained model, you would see:')\n",
    "print('  - \"cat\" is bright in the region where the cat is')\n",
    "print('  - \"sunset\" is bright in the sky region')\n",
    "print('  - Function words (\"a\", \"in\") are dim everywhere')\n",
    "print()\n",
    "print('This is the spatially-varying nature of text conditioning:')\n",
    "print('each spatial location extracts different information from the text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You visualized the cross-attention weight matrix two ways:\n",
    "\n",
    "1. **Heatmap (16 rows × 6 columns):** Each row is one spatial location's distribution over text tokens. Different rows have different patterns — this is spatially-varying conditioning.\n",
    "2. **Spatial grid per token:** For each text token, you saw where in the 4×4 feature map it receives the most attention. In a trained model, content words like \"cat\" and \"sunset\" would light up in the corresponding spatial regions.\n",
    "\n",
    "The key insight: **cross-attention gives each spatial location its own text signal.** The cat region gets \"cat\" information. The sky gets \"sunset\" information. This is fundamentally different from timestep conditioning, which injects the same signal everywhere.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Implement Classifier-Free Guidance [Supported]\n",
    "\n",
    "CFG is a simple formula applied at inference time:\n",
    "\n",
    "$$\\epsilon_{\\text{cfg}} = \\epsilon_{\\text{uncond}} + w \\cdot (\\epsilon_{\\text{cond}} - \\epsilon_{\\text{uncond}})$$\n",
    "\n",
    "Where:\n",
    "- $\\epsilon_{\\text{uncond}}$ = model's prediction without text (null embedding)\n",
    "- $\\epsilon_{\\text{cond}}$ = model's prediction with text\n",
    "- $w$ = guidance scale (typically 7.5 for Stable Diffusion)\n",
    "\n",
    "The difference $(\\epsilon_{\\text{cond}} - \\epsilon_{\\text{uncond}})$ is the **text direction** — the effect the text has on the model's prediction. CFG amplifies this direction by $w$.\n",
    "\n",
    "Below is a dummy model that produces different noise predictions depending on whether it receives a text embedding or a null embedding. **Your task:** implement the `apply_cfg` function and plot how the L2 norm of the CFG prediction changes with guidance scale.\n",
    "\n",
    "**Hints:**\n",
    "- The CFG formula is one line: `noise_uncond + w * (noise_cond - noise_uncond)`\n",
    "- `torch.norm(tensor)` computes the L2 norm\n",
    "- Higher guidance scale should produce larger-magnitude predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dummy \"model\" that simulates conditional and unconditional noise predictions.\n",
    "# In a real diffusion model, this would be two forward passes through the U-Net:\n",
    "#   noise_uncond = unet(x_t, t, null_embedding)\n",
    "#   noise_cond   = unet(x_t, t, text_embedding)\n",
    "\n",
    "# Simulate noise predictions for a 4x4 spatial feature map (flattened to 16 dims)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Unconditional prediction: what the model predicts without any text\n",
    "noise_uncond = torch.randn(16) * 0.5  # moderate magnitude\n",
    "\n",
    "# Conditional prediction: slightly different -- the text nudges the prediction\n",
    "# In reality, the difference is small but meaningful.\n",
    "text_direction = torch.tensor([\n",
    "    0.08, -0.05, 0.12, -0.03, 0.07, -0.09, 0.04, 0.11,\n",
    "    -0.06, 0.10, -0.04, 0.08, -0.07, 0.05, 0.09, -0.06\n",
    "])\n",
    "noise_cond = noise_uncond + text_direction  # conditional = unconditional + text effect\n",
    "\n",
    "print('Noise predictions (first 6 dimensions):')\n",
    "print(f'  Unconditional: {noise_uncond[:6].tolist()}')\n",
    "print(f'  Conditional:   {noise_cond[:6].tolist()}')\n",
    "print(f'  Difference:    {text_direction[:6].tolist()}')\n",
    "print()\n",
    "print(f'The difference (text direction) is small: L2 norm = {text_direction.norm():.4f}')\n",
    "print(f'Unconditional prediction L2 norm: {noise_uncond.norm():.4f}')\n",
    "print(f'Conditional prediction L2 norm:   {noise_cond.norm():.4f}')\n",
    "print()\n",
    "print('The text only nudges the prediction slightly. CFG will amplify this nudge.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cfg(noise_uncond, noise_cond, guidance_scale):\n",
    "    \"\"\"Apply classifier-free guidance.\n",
    "    \n",
    "    Args:\n",
    "        noise_uncond: model's noise prediction without text conditioning\n",
    "        noise_cond: model's noise prediction with text conditioning\n",
    "        guidance_scale: the weight w that amplifies the text direction\n",
    "    \n",
    "    Returns:\n",
    "        The guided noise prediction.\n",
    "    \"\"\"\n",
    "    # TODO: Implement the CFG formula.\n",
    "    # noise_cfg = noise_uncond + w * (noise_cond - noise_uncond)\n",
    "    # Hint: it is literally one line.\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test at specific guidance scales\n",
    "test_scales = [0, 1, 3, 7.5, 15]\n",
    "\n",
    "print('CFG predictions at different guidance scales:')\n",
    "print('=' * 65)\n",
    "print(f'{\"w\":>6s}  {\"L2 Norm\":>10s}  First 4 dimensions')\n",
    "print('-' * 65)\n",
    "\n",
    "for w in test_scales:\n",
    "    result = apply_cfg(noise_uncond, noise_cond, w)\n",
    "    norm = result.norm().item()\n",
    "    dims = result[:4].tolist()\n",
    "    dims_str = ', '.join(f'{d:+.3f}' for d in dims)\n",
    "    print(f'{w:>6.1f}  {norm:>10.4f}  [{dims_str}]')\n",
    "\n",
    "print()\n",
    "print('Observations:')\n",
    "print('  w=0:   Unconditional prediction (text ignored entirely)')\n",
    "print('  w=1:   Conditional prediction (no amplification)')\n",
    "print('  w=7.5: Typical Stable Diffusion -- text effect amplified 7.5x')\n",
    "print('  w=15:  Aggressive -- text dominates, large magnitude prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Plot L2 norm of CFG prediction vs guidance scale.\n#\n# Create a range of guidance scales from 0 to 20 (use torch.linspace or np.linspace).\n# For each scale, apply CFG and compute the L2 norm of the result.\n# Plot guidance_scale (x-axis) vs L2_norm (y-axis).\n#\n# Structure is provided -- fill in the TODOs.\n\nguidance_scales = np.linspace(0, 20, 100)\nnorms = []\n\nfor w in guidance_scales:\n    # TODO: Apply CFG at this guidance scale and compute the L2 norm.\n    # Hint: result = apply_cfg(noise_uncond, noise_cond, w)\n    #        norms.append(result.norm().item())\n    pass\n\n# Plot (only if norms were computed -- fill in the loop above first)\nfig, ax = plt.subplots(figsize=(10, 5))\n\nif norms:\n    ax.plot(guidance_scales, norms, color='#c084fc', linewidth=2)\n\n    # Mark the specific guidance scales from the test above\n    for w in test_scales:\n        result = apply_cfg(noise_uncond, noise_cond, w)\n        if result is not None:\n            norm_val = result.norm().item()\n            ax.plot(w, norm_val, 'o', color='#f59e0b', markersize=8, zorder=5)\n            ax.annotate(f'w={w}', (w, norm_val), textcoords='offset points',\n                        xytext=(8, 8), fontsize=9, color='#f59e0b')\n\n    ax.set_xlabel('Guidance Scale (w)', fontsize=12)\n    ax.set_ylabel('L2 Norm of CFG Prediction', fontsize=12)\n    ax.set_title('CFG Prediction Magnitude vs Guidance Scale', fontsize=13)\n    ax.axvline(x=7.5, color='#3b82f6', linestyle='--', alpha=0.5, label='Typical SD default (w=7.5)')\n    ax.axvline(x=1.0, color='#22c55e', linestyle='--', alpha=0.5, label='No guidance (w=1)')\n    ax.legend(fontsize=10)\n    ax.grid(alpha=0.2)\n    plt.tight_layout()\n    plt.show()\n\n    print('The L2 norm increases roughly linearly with guidance scale.')\n    print('Higher w = more aggressive denoising in the text direction.')\n    print('At extreme scales, the model over-commits to the text signal,')\n    print('producing oversaturated, artifact-heavy images in practice.')\nelse:\n    plt.close(fig)\n    print('⚠ norms list is empty. Fill in the TODO loop above to compute')\n    print('  the L2 norm at each guidance scale, then re-run this cell.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The CFG formula is one line of vector arithmetic. The key insight: `(noise_cond - noise_uncond)` is the **text direction** — the vector that captures how the text changes the model's prediction. Multiplying by `w` amplifies this direction.\n",
    "\n",
    "**`apply_cfg` function:**\n",
    "```python\n",
    "def apply_cfg(noise_uncond, noise_cond, guidance_scale):\n",
    "    return noise_uncond + guidance_scale * (noise_cond - noise_uncond)\n",
    "```\n",
    "\n",
    "**Norm computation loop:**\n",
    "```python\n",
    "for w in guidance_scales:\n",
    "    result = apply_cfg(noise_uncond, noise_cond, w)\n",
    "    norms.append(result.norm().item())\n",
    "```\n",
    "\n",
    "At `w=0`, you get the unconditional prediction (text ignored). At `w=1`, you get the conditional prediction (no amplification). At `w > 1`, you **extrapolate** beyond the conditional prediction in the text direction. The L2 norm grows because the amplified text direction adds magnitude to the prediction vector.\n",
    "\n",
    "The equivalent form is: `noise_cfg = (1 - w) * noise_uncond + w * noise_cond`. At `w > 1`, the coefficient on `noise_uncond` is **negative** — you are actively subtracting the unconditional component and adding more than 100% of the conditional component. This is extrapolation, not interpolation.\n",
    "\n",
    "Common mistake: writing `noise_cond + w * (noise_cond - noise_uncond)` (starting from conditional instead of unconditional). This doubles the text effect at `w=1` instead of recovering the conditional prediction.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You implemented the CFG formula and observed:\n",
    "\n",
    "1. **The formula is one line** of vector arithmetic: `noise_uncond + w * (noise_cond - noise_uncond)`.\n",
    "2. **`w=0` recovers unconditional, `w=1` recovers conditional.** Values in between interpolate; values above 1 extrapolate.\n",
    "3. **Higher guidance scale = larger prediction magnitude.** The model commits more aggressively to the text direction. In practice, this means more vivid, text-faithful images — up to a point, after which it causes artifacts.\n",
    "\n",
    "The text direction `(noise_cond - noise_uncond)` is a small vector — the text only nudges the prediction. CFG amplifies this nudge by `w`. At `w=7.5`, a nudge of 0.08 becomes a push of 0.60. At `w=15`, it becomes 1.20. The model goes from \"gently considering the text\" to \"aggressively optimizing for the text.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 4: CFG with a Real Diffusion Model [Independent]\n\nNow for the payoff: use a real text-conditioned diffusion model to generate images at different guidance scales and see the quality/fidelity tradeoff with your own eyes.\n\n**Your task:**\n1. Load a small pretrained text-to-image diffusion pipeline\n2. Choose a text prompt (something with clear visual content, e.g., \"a lighthouse on a cliff at sunset\")\n3. Generate images at guidance scales: `w = 1, 3, 7.5, 12, 20`\n4. Display them side by side\n5. Observe: which guidance scale looks best? At what point do artifacts appear?\n\n**No skeleton code is provided.** Use the `diffusers` library, which was installed in the setup cell.\n\n**Key tips:**\n- `StableDiffusionPipeline.from_pretrained(\"nota-ai/bk-sdm-small\", torch_dtype=torch.float16)` loads a small distilled version of SD v1.5 (fast, low VRAM)\n- `.to(device)` moves it to GPU\n- `pipe(prompt, guidance_scale=w, num_inference_steps=30).images[0]` generates one image\n- Use the **same random seed** for each guidance scale (via `generator=torch.Generator(device).manual_seed(42)`) so the only variable is the guidance scale\n- **This requires a GPU.** If running on CPU, reduce `num_inference_steps` to 15 and expect slow generation.\n- For the full-size model, use `\"stable-diffusion-v1-5/stable-diffusion-v1-5\"` instead (requires more VRAM)\n\n**Reflection questions** (answer after generating):\n- At `w=1`, does the image match the prompt at all?\n- What is the sweet spot? Where do you see the best balance of quality and text fidelity?\n- At `w=20`, what specific artifacts do you notice? (oversaturation, distortion, repetitive patterns?)\n- Why does extreme guidance produce artifacts? (Think about what the formula does: it extrapolates far beyond the conditional prediction in the text direction, pushing the noise prediction into a region the model was never trained to operate in.)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#\n",
    "# Steps:\n",
    "# 1. Load the pipeline\n",
    "# 2. Choose a prompt\n",
    "# 3. Generate images at w = 1, 3, 7.5, 12, 20 (same seed each time)\n",
    "# 4. Display side by side with matplotlib\n",
    "# 5. Answer the reflection questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The experiment reveals the guidance scale tradeoff visually. The key insight: CFG is not \"free quality\" — it is a dial that trades diversity for text fidelity, with artifacts appearing at the extreme end.\n",
    "\n",
    "```python\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Load a small distilled model for speed (same architecture as SD v1.5, fewer parameters)\n",
    "# If you have enough VRAM, you can use \"stable-diffusion-v1-5/stable-diffusion-v1-5\" instead.\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"nota-ai/bk-sdm-small\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "prompt = \"a lighthouse on a cliff at sunset, oil painting\"\n",
    "guidance_scales = [1.0, 3.0, 7.5, 12.0, 20.0]\n",
    "images = []\n",
    "\n",
    "for w in guidance_scales:\n",
    "    # Same seed for every generation so the only variable is guidance scale\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    image = pipe(\n",
    "        prompt,\n",
    "        guidance_scale=w,\n",
    "        num_inference_steps=30,\n",
    "        generator=generator\n",
    "    ).images[0]\n",
    "    images.append(image)\n",
    "    print(f'  Generated at w={w}')\n",
    "\n",
    "# Display side by side\n",
    "fig, axes = plt.subplots(1, len(guidance_scales), figsize=(20, 4))\n",
    "for ax, img, w in zip(axes, images, guidance_scales):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'w = {w}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(f'\"{prompt}\" at different guidance scales', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Observations:')\n",
    "print('  w=1:   Diverse but may not follow the prompt well. Muted colors.')\n",
    "print('  w=3:   Starting to follow the prompt. Better composition.')\n",
    "print('  w=7.5: Strong text adherence + good image quality. The sweet spot.')\n",
    "print('  w=12:  Very saturated. Starting to see artifacts.')\n",
    "print('  w=20:  Oversaturated, distorted, artifact-heavy.')\n",
    "print()\n",
    "print('Why artifacts at high w?')\n",
    "print('  CFG extrapolates far beyond the conditional prediction.')\n",
    "print('  The noise prediction lands in a region the model was never')\n",
    "print('  trained to operate in. The denoising process breaks down.')\n",
    "```\n",
    "\n",
    "**If you do not have a GPU:** You can still run this with `num_inference_steps=10` on CPU, but it will be slow (~2-5 minutes per image). Alternatively, use the plot from Exercise 3 and the lesson's GradientCards to understand the tradeoff conceptually.\n",
    "\n",
    "**What to notice:** The sweet spot is typically around `w=7.5`. Below that, images are diverse but may not follow the prompt. Above that, the model over-optimizes for the text at the expense of image coherence. This is the tradeoff the lesson describes: CFG is a contrast slider, and too much contrast destroys the image.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Cross-attention is the same QKV formula as self-attention — the only change is where K and V come from.** Q comes from the U-Net's spatial features, K and V come from CLIP text embeddings. Same softmax, same weighted average, same \"learned lens\" pattern. The attention weight matrix changes from square (spatial × spatial) to rectangular (spatial × text).\n",
    "\n",
    "2. **Cross-attention creates spatially-varying text conditioning.** Each spatial location generates its own query and gets its own distribution over text tokens. The cat region attends to \"cat,\" the sky attends to \"sunset.\" This is fundamentally different from timestep conditioning, which injects the same signal everywhere.\n",
    "\n",
    "3. **CFG is one line of arithmetic that amplifies the text signal.** `noise_cfg = noise_uncond + w * (noise_cond - noise_uncond)`. The difference `(noise_cond - noise_uncond)` is the text direction — the effect the text has on the prediction. The guidance scale `w` controls how aggressively to follow the text.\n",
    "\n",
    "4. **The guidance scale is a tradeoff, not a free improvement.** Low `w` = diverse but unfaithful to text. Medium `w` (7.5) = the sweet spot. High `w` = oversaturated, distorted, artifact-heavy. The model is extrapolating beyond its training distribution.\n",
    "\n",
    "5. **The full conditioning pipeline is now clear.** Timestep via adaptive norm (global, every resolution) tells the network WHEN. Text via cross-attention (spatially varying, middle resolutions) tells it WHAT. CFG turns up the volume on the WHAT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}