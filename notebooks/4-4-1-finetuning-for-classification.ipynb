{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Finetuning for Classification\n",
        "\n",
        "**Module 4.4, Lesson 1** | CourseAI\n",
        "\n",
        "In this notebook you will:\n",
        "\n",
        "1. **Load pretrained GPT-2** via HuggingFace and examine its architecture — identify what to keep and what to replace\n",
        "2. **Freeze the backbone and add a classification head** — replace `lm_head` with `nn.Linear(768, num_classes)`\n",
        "3. **Prepare a text classification dataset** — tokenize SST-2 examples and create a DataLoader\n",
        "4. **Implement the finetuning training loop** — frozen backbone, trainable head, cross-entropy loss\n",
        "5. **Evaluate, unfreeze layers, and compare** — measure accuracy, try partial unfreezing with differential learning rates\n",
        "\n",
        "For each exercise, **PREDICT the output before running the cell.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run this cell to install dependencies and configure the environment. Use a **GPU runtime** in Colab — even frozen-backbone training benefits from GPU acceleration for the transformer forward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q transformers datasets tiktoken\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from transformers import GPT2LMHeadModel\n",
        "import tiktoken\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Reproducible results\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "if device.type == 'cuda':\n",
        "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
        "\n",
        "# Nice plots\n",
        "plt.style.use('dark_background')\n",
        "plt.rcParams['figure.figsize'] = [10, 4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 1: Load GPT-2 and Examine the Architecture [Guided]\n",
        "\n",
        "The first step of transfer learning is always the same: load the pretrained model and understand what you have. In the CNN transfer learning lesson, you loaded a pretrained ResNet and identified `model.fc` as the part to replace. Here you will do the same with GPT-2.\n",
        "\n",
        "**Before running, predict:**\n",
        "- GPT-2 has a `lm_head` that projects to vocabulary size. What shape is `lm_head.weight`?\n",
        "- How many total parameters does GPT-2 have?\n",
        "- What is the hidden dimension (the size of each token's representation)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load pretrained GPT-2 from HuggingFace\n",
        "hf_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "hf_model.eval()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GPT-2 Architecture Overview\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# The model has two main parts:\n",
        "# 1. transformer — the backbone (embeddings + 12 transformer blocks + layer norm)\n",
        "# 2. lm_head — the output projection to vocabulary\n",
        "print(\"\\nTop-level modules:\")\n",
        "for name, _ in hf_model.named_children():\n",
        "    print(f\"  {name}\")\n",
        "\n",
        "# lm_head shape: projects from hidden_dim to vocab_size\n",
        "print(f\"\\nlm_head weight shape: {hf_model.lm_head.weight.shape}\")\n",
        "print(f\"  -> Projects from {hf_model.lm_head.weight.shape[1]} (hidden dim) \"\n",
        "      f\"to {hf_model.lm_head.weight.shape[0]} (vocab size)\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in hf_model.parameters())\n",
        "backbone_params = sum(p.numel() for p in hf_model.transformer.parameters())\n",
        "lm_head_params = sum(p.numel() for p in hf_model.lm_head.parameters())\n",
        "\n",
        "print(f\"\\nParameter counts:\")\n",
        "print(f\"  Total:     {total_params:>12,}\")\n",
        "print(f\"  Backbone:  {backbone_params:>12,}\")\n",
        "print(f\"  lm_head:   {lm_head_params:>12,}\")\n",
        "\n",
        "# The hidden dimension — this is what our classification head will take as input\n",
        "n_embd = hf_model.config.n_embd\n",
        "n_layer = hf_model.config.n_layer\n",
        "print(f\"\\nHidden dimension (n_embd): {n_embd}\")\n",
        "print(f\"Number of transformer blocks: {n_layer}\")\n",
        "\n",
        "# Verify weight tying: lm_head shares weights with token embeddings\n",
        "print(f\"\\nWeight tying check:\")\n",
        "print(f\"  lm_head.weight data_ptr:  {hf_model.lm_head.weight.data_ptr()}\")\n",
        "print(f\"  wte.weight data_ptr:      {hf_model.transformer.wte.weight.data_ptr()}\")\n",
        "print(f\"  Same tensor? {hf_model.lm_head.weight.data_ptr() == hf_model.transformer.wte.weight.data_ptr()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What happened\n",
        "\n",
        "GPT-2 has two main parts: `transformer` (the backbone) and `lm_head` (the output projection). The backbone extracts language features from tokens — 12 transformer blocks that turn input tokens into 768-dimensional hidden states. The `lm_head` projects those 768-dimensional hidden states to 50,257 vocabulary logits for next-token prediction.\n",
        "\n",
        "For classification, we will **keep the backbone** (it is our text feature extractor) and **replace `lm_head`** with a new linear layer that projects to the number of classes. This is exactly the same strategy as replacing `model.fc` in ResNet.\n",
        "\n",
        "Notice the weight tying: `lm_head` shares its weight tensor with the token embedding (`wte`). When we replace `lm_head`, we break that tie — the embedding stays (we still need it to embed input tokens), but the output projection now maps to class labels.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Freeze the Backbone and Add a Classification Head [Guided]\n",
        "\n",
        "Now we build the classification model. The pattern is identical to CNN transfer learning:\n",
        "1. Keep the pretrained backbone\n",
        "2. Freeze it (`requires_grad = False`)\n",
        "3. Add a new classification head\n",
        "\n",
        "The one genuinely new question: **which hidden state represents the whole sequence?** In a CNN, global average pooling collapses the spatial feature map into one vector. In a causal transformer, we take the **last token's** hidden state — it is the only position that has attended to all previous tokens (because of causal masking).\n",
        "\n",
        "**Before running, predict:**\n",
        "- How many parameters will the classification head have for binary classification (2 classes)?\n",
        "- What fraction of total model parameters will be trainable?\n",
        "- What shape will `last_hidden` have for a batch of 4 sequences, each 20 tokens long?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class GPT2ForClassification(nn.Module):\n",
        "    def __init__(self, hf_model, num_classes):\n",
        "        super().__init__()\n",
        "        # Keep the pretrained transformer backbone\n",
        "        self.transformer = hf_model.transformer\n",
        "\n",
        "        # Replace lm_head with a classification head\n",
        "        # 768 = GPT-2's hidden dimension (n_embd)\n",
        "        self.classifier = nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # 1. Run through the transformer backbone\n",
        "        outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        # hidden_states shape: (batch, seq_len, 768)\n",
        "\n",
        "        # 2. Take the LAST token's hidden state\n",
        "        # Causal masking means the last token has attended to ALL previous\n",
        "        # tokens — it is the only position with full sequence context.\n",
        "        if attention_mask is not None:\n",
        "            # Find the actual last token (not padding) for each sequence\n",
        "            # attention_mask is 1 for real tokens, 0 for padding\n",
        "            seq_lengths = attention_mask.sum(dim=1) - 1  # 0-indexed\n",
        "            last_hidden = hidden_states[\n",
        "                torch.arange(hidden_states.size(0), device=hidden_states.device),\n",
        "                seq_lengths\n",
        "            ]\n",
        "        else:\n",
        "            last_hidden = hidden_states[:, -1, :]\n",
        "        # last_hidden shape: (batch, 768)\n",
        "\n",
        "        # 3. Classify\n",
        "        logits = self.classifier(last_hidden)\n",
        "        # logits shape: (batch, num_classes)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Create the classification model\n",
        "num_classes = 2  # Sentiment: positive / negative\n",
        "model = GPT2ForClassification(hf_model, num_classes).to(device)\n",
        "\n",
        "# Freeze the entire transformer backbone\n",
        "for param in model.transformer.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "frozen_params = total_params - trainable_params\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GPT2ForClassification\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total parameters:     {total_params:>12,}\")\n",
        "print(f\"Frozen (backbone):    {frozen_params:>12,}\")\n",
        "print(f\"Trainable (head):     {trainable_params:>12,}\")\n",
        "print(f\"Trainable fraction:   {trainable_params / total_params:.4%}\")\n",
        "\n",
        "# Verify the classification head dimensions\n",
        "print(f\"\\nClassifier weight shape: {model.classifier.weight.shape}\")\n",
        "print(f\"Classifier bias shape:   {model.classifier.bias.shape}\")\n",
        "print(f\"Head params: {768 * num_classes} (weight) + {num_classes} (bias) = {768 * num_classes + num_classes}\")\n",
        "\n",
        "# Quick forward pass to verify shapes\n",
        "test_ids = torch.randint(0, 50257, (4, 20)).to(device)\n",
        "with torch.no_grad():\n",
        "    test_logits = model(test_ids)\n",
        "\n",
        "print(f\"\\nForward pass test:\")\n",
        "print(f\"  Input shape:  {test_ids.shape}  (batch=4, seq_len=20)\")\n",
        "print(f\"  Output shape: {test_logits.shape}  (batch=4, num_classes=2)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What happened\n",
        "\n",
        "The classification head is tiny: 768 x 2 + 2 = 1,538 trainable parameters out of ~124 million total. You are training about 0.001% of the model. The backbone is a **general text feature extractor** — the pretrained transformer blocks convert token sequences into rich 768-dimensional representations. The classification head learns to map those representations to class predictions.\n",
        "\n",
        "The key architectural decision: we take `hidden_states[:, -1, :]` — the **last token's** hidden state. Because of causal masking, the last position is the only one that has attended to all previous tokens. Using any earlier position would throw away information. This is the direct consequence of the causal attention pattern you studied earlier.\n",
        "\n",
        "When sequences have padding, we use the attention mask to find the actual last real token for each sequence, rather than blindly taking position -1 (which would be a padding token).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Prepare the SST-2 Dataset [Supported]\n",
        "\n",
        "We need a text classification dataset. SST-2 (Stanford Sentiment Treebank) is a standard benchmark: movie review sentences labeled as positive (1) or negative (0).\n",
        "\n",
        "Your job: tokenize the sentences with tiktoken (the same BPE tokenizer GPT-2 uses), pad/truncate to a fixed length, and create DataLoaders.\n",
        "\n",
        "The setup is the same input pipeline as generation — same tokenizer, same token IDs. The only difference is that each example also has a label.\n",
        "\n",
        "<details>\n",
        "<summary>Hint: tokenization</summary>\n",
        "\n",
        "Use `enc.encode(text)` to get token IDs. Truncate to `max_length` if longer. Pad with a pad token (we will use 50256, the `<|endoftext|>` token) if shorter. Build an attention mask: 1 for real tokens, 0 for padding.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load SST-2 dataset\n",
        "sst2 = load_dataset('glue', 'sst2')\n",
        "print(f\"Train examples: {len(sst2['train'])}\")\n",
        "print(f\"Validation examples: {len(sst2['validation'])}\")\n",
        "print(f\"\\nSample: {sst2['train'][0]}\")\n",
        "\n",
        "# We will use a subset for speed — 2000 train, 500 validation\n",
        "train_data = sst2['train'].shuffle(seed=42).select(range(2000))\n",
        "val_data = sst2['validation']\n",
        "\n",
        "# Tokenizer — same tiktoken encoder GPT-2 uses\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "PAD_TOKEN = enc.encode('<|endoftext|>')[0]  # 50256\n",
        "MAX_LENGTH = 64\n",
        "\n",
        "print(f\"\\nPad token ID: {PAD_TOKEN}\")\n",
        "print(f\"Max sequence length: {MAX_LENGTH}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class SST2Dataset(Dataset):\n",
        "    def __init__(self, hf_dataset, tokenizer, max_length):\n",
        "        self.input_ids = []\n",
        "        self.attention_masks = []\n",
        "        self.labels = []\n",
        "\n",
        "        for example in hf_dataset:\n",
        "            text = example['sentence']\n",
        "            label = example['label']\n",
        "\n",
        "            # TODO: Tokenize the text\n",
        "            # 1. Encode with tiktoken: token_ids = tokenizer.encode(text)\n",
        "            # 2. Truncate to max_length if longer: token_ids = token_ids[:max_length]\n",
        "            # 3. Create attention_mask: [1] * len(token_ids) + [0] * padding_needed\n",
        "            # 4. Pad token_ids to max_length with PAD_TOKEN\n",
        "\n",
        "\n",
        "            self.input_ids.append(torch.tensor(token_ids, dtype=torch.long))\n",
        "            self.attention_masks.append(torch.tensor(attention_mask, dtype=torch.long))\n",
        "            self.labels.append(label)\n",
        "\n",
        "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attention_masks[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# TODO: Create train_dataset and val_dataset\n",
        "# train_dataset = SST2Dataset(train_data, enc, MAX_LENGTH)\n",
        "# val_dataset = SST2Dataset(val_data, enc, MAX_LENGTH)\n",
        "\n",
        "\n",
        "# TODO: Create DataLoaders\n",
        "# train_loader: batch_size=32, shuffle=True\n",
        "# val_loader: batch_size=32, shuffle=False\n",
        "\n",
        "\n",
        "# Verify\n",
        "sample_ids, sample_mask, sample_label = train_dataset[0]\n",
        "print(f\"Sample input_ids shape: {sample_ids.shape}\")\n",
        "print(f\"Sample attention_mask:  {sample_mask.shape}\")\n",
        "print(f\"Sample label: {sample_label}\")\n",
        "print(f\"Real tokens: {sample_mask.sum().item()}, padding: {(sample_mask == 0).sum().item()}\")\n",
        "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
        "print(f\"Val batches:   {len(val_loader)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "The key insight: this is the **same tokenization pipeline** you used for text generation. Same tiktoken encoder, same BPE token IDs. The only addition is padding (so batches have uniform length) and an attention mask (so the model knows which tokens are real).\n",
        "\n",
        "```python\n",
        "            # Tokenize\n",
        "            token_ids = tokenizer.encode(text)\n",
        "            # Truncate\n",
        "            token_ids = token_ids[:max_length]\n",
        "            # Create attention mask before padding\n",
        "            padding_needed = max_length - len(token_ids)\n",
        "            attention_mask = [1] * len(token_ids) + [0] * padding_needed\n",
        "            # Pad\n",
        "            token_ids = token_ids + [PAD_TOKEN] * padding_needed\n",
        "```\n",
        "\n",
        "Then create datasets and loaders:\n",
        "\n",
        "```python\n",
        "train_dataset = SST2Dataset(train_data, enc, MAX_LENGTH)\n",
        "val_dataset = SST2Dataset(val_data, enc, MAX_LENGTH)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "```\n",
        "\n",
        "Common mistake: forgetting to create the attention mask. Without it, the model would treat padding tokens as real content, and `hidden_states[:, -1, :]` would always be a padding token's representation — useless for classification.\n",
        "\n",
        "</details>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Implement the Finetuning Training Loop [Supported]\n",
        "\n",
        "The training loop follows the same heartbeat as every loop you have written: forward, loss, backward, step. The differences from pretraining are surface-level:\n",
        "- Loss compares against **class labels** (not next-token targets)\n",
        "- Optimizer updates only **classifier parameters** (not the full model)\n",
        "- `nn.CrossEntropyLoss` with 2 classes instead of 50,257\n",
        "\n",
        "Fill in the TODOs to complete the training loop and evaluation function.\n",
        "\n",
        "<details>\n",
        "<summary>Hint: what to pass to the optimizer</summary>\n",
        "\n",
        "Only the classification head parameters need gradients: `model.classifier.parameters()`. The backbone is frozen, so passing `model.parameters()` to the optimizer would work but waste memory tracking frozen params. Be explicit.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "    \"\"\"Evaluate classification accuracy on a dataset.\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, labels in data_loader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # TODO: Forward pass and count correct predictions\n",
        "            # 1. Get logits from model (pass both input_ids and attention_mask)\n",
        "            # 2. Get predicted class: _, predicted = torch.max(logits, dim=1)\n",
        "            # 3. Update correct and total counts\n",
        "\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "# TODO: Set up optimizer — only train the classifier head parameters\n",
        "# optimizer = torch.optim.AdamW(???, lr=1e-3)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 5\n",
        "\n",
        "# Training history\n",
        "history = {'train_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "\n",
        "print(f\"Training for {num_epochs} epochs (frozen backbone, trainable head)\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.classifier.parameters()):,}\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for input_ids, attention_mask, labels in train_loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # TODO: Complete the training step\n",
        "        # 1. Forward pass: logits = model(input_ids, attention_mask)\n",
        "        # 2. Compute loss: loss = criterion(logits, labels)\n",
        "        # 3. Backward pass: optimizer.zero_grad(), loss.backward(), optimizer.step()\n",
        "\n",
        "\n",
        "        # Track metrics\n",
        "        running_loss += loss.item() * input_ids.size(0)\n",
        "        _, predicted = torch.max(logits, dim=1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    val_acc = evaluate(model, val_loader, device)\n",
        "\n",
        "    history['train_loss'].append(epoch_loss)\n",
        "    history['train_acc'].append(epoch_acc)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}  \"\n",
        "          f\"Train Loss: {epoch_loss:.4f}  \"\n",
        "          f\"Train Acc: {epoch_acc:.1%}  \"\n",
        "          f\"Val Acc: {val_acc:.1%}\")\n",
        "\n",
        "print(\"=\" * 65)\n",
        "print(f\"\\nFinal validation accuracy (frozen backbone): {history['val_acc'][-1]:.1%}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "The training loop is the same heartbeat — forward, loss, backward, step. The only difference from pretraining: the optimizer only receives the classifier head parameters, and loss is against class labels.\n",
        "\n",
        "Evaluate function:\n",
        "```python\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            _, predicted = torch.max(logits, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "```\n",
        "\n",
        "Optimizer:\n",
        "```python\n",
        "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=1e-3)\n",
        "```\n",
        "\n",
        "Training step:\n",
        "```python\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "```\n",
        "\n",
        "Notice we pass `model.classifier.parameters()` to the optimizer, not `model.parameters()`. The backbone is frozen, so its parameters would be ignored anyway, but being explicit makes the intent clear and is slightly more memory efficient.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot training curves\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "epochs_range = range(1, num_epochs + 1)\n",
        "\n",
        "ax1.plot(epochs_range, history['train_loss'], 'o-', linewidth=2)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training Loss (Frozen Backbone)')\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "ax2.plot(epochs_range, history['train_acc'], 'o-', linewidth=2, label='Train')\n",
        "ax2.plot(epochs_range, history['val_acc'], 's-', linewidth=2, label='Val')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Accuracy (Frozen Backbone)')\n",
        "ax2.legend()\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 5: Unfreeze Layers, Compare, and Explore [Independent]\n",
        "\n",
        "The frozen backbone gave you a baseline accuracy. Now explore what happens when you partially unfreeze the model.\n",
        "\n",
        "Your tasks:\n",
        "\n",
        "1. **Record the frozen-backbone accuracy** from Exercise 4.\n",
        "2. **Create a fresh model** (reload weights — important so the comparison is fair).\n",
        "3. **Unfreeze the last 2 transformer blocks** while keeping the rest frozen.\n",
        "4. **Use differential learning rates**: lower LR for the unfrozen backbone layers (1e-5), higher LR for the classification head (1e-3). This is the same strategy you used with ResNet.\n",
        "5. **Train for 3 epochs** and compare validation accuracy.\n",
        "6. **Generate text** with both models (frozen and partially-unfrozen) to check for catastrophic forgetting. Use a prompt like `\"The movie was\"` and generate 20 tokens.\n",
        "\n",
        "Think about:\n",
        "- Does unfreezing improve accuracy? By how much?\n",
        "- Does the partially-unfrozen model still generate coherent text?\n",
        "- What is the tradeoff between frozen and unfrozen training?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your code here.\n",
        "#\n",
        "# Suggested structure:\n",
        "#\n",
        "# 1. Store the frozen accuracy:\n",
        "#    frozen_acc = history['val_acc'][-1]\n",
        "#\n",
        "# 2. Create a fresh model:\n",
        "#    hf_model_fresh = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "#    model_unfrozen = GPT2ForClassification(hf_model_fresh, num_classes=2).to(device)\n",
        "#\n",
        "# 3. Freeze everything first, then unfreeze last 2 blocks:\n",
        "#    for param in model_unfrozen.transformer.parameters():\n",
        "#        param.requires_grad = False\n",
        "#    for block_idx in [10, 11]:  # last 2 of 12 blocks (0-indexed)\n",
        "#        for param in model_unfrozen.transformer.h[block_idx].parameters():\n",
        "#            param.requires_grad = True\n",
        "#\n",
        "# 4. Set up optimizer with differential LR (parameter groups):\n",
        "#    optimizer = torch.optim.AdamW([\n",
        "#        {'params': model_unfrozen.transformer.h[10].parameters(), 'lr': 1e-5},\n",
        "#        {'params': model_unfrozen.transformer.h[11].parameters(), 'lr': 1e-5},\n",
        "#        {'params': model_unfrozen.classifier.parameters(), 'lr': 1e-3},\n",
        "#    ])\n",
        "#\n",
        "# 5. Train for 3 epochs (same loop structure as Exercise 4)\n",
        "#\n",
        "# 6. Compare accuracies\n",
        "#\n",
        "# 7. Generate text with both models to check for catastrophic forgetting:\n",
        "#    def generate_text(transformer_backbone, prompt, max_new_tokens=20):\n",
        "#        \"\"\"Generate text using just the transformer backbone.\"\"\"\n",
        "#        input_ids = torch.tensor([enc.encode(prompt)]).to(device)\n",
        "#        with torch.no_grad():\n",
        "#            for _ in range(max_new_tokens):\n",
        "#                outputs = transformer_backbone(input_ids)\n",
        "#                # Use the last hidden state projected through wte transpose\n",
        "#                # (since we removed lm_head, we approximate by using the\n",
        "#                # embedding matrix transposed as a projection)\n",
        "#                hidden = outputs.last_hidden_state[:, -1, :]\n",
        "#                logits = hidden @ transformer_backbone.wte.weight.T\n",
        "#                next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "#                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "#        return enc.decode(input_ids[0].tolist())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "The key insight is that partial unfreezing trades safety for potential accuracy. With a frozen backbone, you cannot overfit the backbone — there is zero risk of catastrophic forgetting because the backbone weights never change. With partial unfreezing, you allow the last few layers to adapt their representations to your task, which can help, but you need a lower learning rate (differential LR) to avoid destroying the pretrained features.\n",
        "\n",
        "```python\n",
        "# 1. Record frozen accuracy\n",
        "frozen_acc = history['val_acc'][-1]\n",
        "\n",
        "# 2. Fresh model\n",
        "hf_model_fresh = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model_unfrozen = GPT2ForClassification(hf_model_fresh, num_classes=2).to(device)\n",
        "\n",
        "# 3. Freeze all, then unfreeze last 2 blocks\n",
        "for param in model_unfrozen.transformer.parameters():\n",
        "    param.requires_grad = False\n",
        "for block_idx in [10, 11]:\n",
        "    for param in model_unfrozen.transformer.h[block_idx].parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "unfrozen_trainable = sum(p.numel() for p in model_unfrozen.parameters() if p.requires_grad)\n",
        "print(f\"Trainable parameters (unfrozen): {unfrozen_trainable:,}\")\n",
        "\n",
        "# 4. Differential learning rates\n",
        "optimizer_unfrozen = torch.optim.AdamW([\n",
        "    {'params': model_unfrozen.transformer.h[10].parameters(), 'lr': 1e-5},\n",
        "    {'params': model_unfrozen.transformer.h[11].parameters(), 'lr': 1e-5},\n",
        "    {'params': model_unfrozen.classifier.parameters(), 'lr': 1e-3},\n",
        "])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "unfrozen_history = {'train_loss': [], 'val_acc': []}\n",
        "\n",
        "# 5. Train\n",
        "for epoch in range(3):\n",
        "    model_unfrozen.train()\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "\n",
        "    for input_ids, attention_mask, labels in train_loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits = model_unfrozen(input_ids, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        optimizer_unfrozen.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_unfrozen.step()\n",
        "\n",
        "        running_loss += loss.item() * input_ids.size(0)\n",
        "        total += input_ids.size(0)\n",
        "\n",
        "    val_acc = evaluate(model_unfrozen, val_loader, device)\n",
        "    unfrozen_history['train_loss'].append(running_loss / total)\n",
        "    unfrozen_history['val_acc'].append(val_acc)\n",
        "    print(f\"Epoch {epoch+1}/3  Loss: {running_loss/total:.4f}  Val Acc: {val_acc:.1%}\")\n",
        "\n",
        "# 6. Compare\n",
        "unfrozen_acc = unfrozen_history['val_acc'][-1]\n",
        "print(f\"\\n{'='*40}\")\n",
        "print(f\"Frozen backbone:  {frozen_acc:.1%}\")\n",
        "print(f\"Unfrozen (last 2): {unfrozen_acc:.1%}\")\n",
        "print(f\"Difference: {unfrozen_acc - frozen_acc:+.1%}\")\n",
        "\n",
        "# 7. Generate text to check catastrophic forgetting\n",
        "def generate_text(transformer_backbone, prompt, max_new_tokens=20):\n",
        "    transformer_backbone.eval()\n",
        "    input_ids = torch.tensor([enc.encode(prompt)]).to(device)\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            outputs = transformer_backbone(input_ids)\n",
        "            hidden = outputs.last_hidden_state[:, -1, :]\n",
        "            logits = hidden @ transformer_backbone.wte.weight.T\n",
        "            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "    return enc.decode(input_ids[0].tolist())\n",
        "\n",
        "prompt = \"The movie was\"\n",
        "print(f\"\\nGeneration check (prompt: '{prompt}')\")\n",
        "print(f\"  Frozen model:   {generate_text(model.transformer, prompt)}\")\n",
        "print(f\"  Unfrozen model: {generate_text(model_unfrozen.transformer, prompt)}\")\n",
        "```\n",
        "\n",
        "You should observe that the frozen model generates text identically to vanilla GPT-2 (no forgetting at all), while the partially-unfrozen model may show slight differences but should still be largely coherent. With only 2 blocks unfrozen and a low LR, catastrophic forgetting is minimal. If you unfroze all blocks with a high LR, the text generation would degrade noticeably.\n",
        "\n",
        "</details>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **A pretrained transformer is a text feature extractor.** Add a classification head, freeze the backbone, train the head. The same transfer learning pattern as CNNs — only the feature extractor changed.\n",
        "\n",
        "2. **Use the last token's hidden state as the sequence representation.** Causal masking means the last position has attended to all previous tokens — it is the only position with full sequence context. The architecture dictates this choice.\n",
        "\n",
        "3. **The classification head is tiny.** For binary classification: 768 x 2 + 2 = 1,538 trainable parameters out of ~124 million total. You are training about 0.001% of the model.\n",
        "\n",
        "4. **The training loop is the same heartbeat.** Forward, loss, backward, step — same structure as every loop since Series 1. The only differences: loss is against class labels, optimizer updates only head parameters.\n",
        "\n",
        "5. **Start frozen, unfreeze if needed.** Frozen backbone is safe (no forgetting, no overfitting), fast, and often good enough. Partial unfreezing with differential learning rates is the middle ground — same strategy as CNN transfer learning."
      ]
    }
  ]
}