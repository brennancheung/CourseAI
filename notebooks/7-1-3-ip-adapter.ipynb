{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IP-Adapter\n",
    "\n",
    "**Module 7.1, Lesson 3** | CourseAI\n",
    "\n",
    "You know the architectureâ€”decoupled cross-attention adds a parallel K/V pathway for CLIP image embeddings alongside the existing text K/V projections. This notebook is where the architecture becomes a tool. Reference images, scale tuning, text-image coexistence, and composability with ControlNet.\n",
    "\n",
    "**What you will do:**\n",
    "- Load IP-Adapter and generate with a reference image, comparing output with and without image conditioning (scale=0 vs scale=0.6)\n",
    "- Sweep the IP-Adapter scale parameter across five values and observe the transition from text-dominant to image-dominant\n",
    "- Test text-image coexistence by pairing the same reference image with three different text prompts\n",
    "- Combine IP-Adapter with ControlNetâ€”reference image for visual style, edge map for spatial structure\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "Every concept in this notebook comes from the lesson. Decoupled cross-attention, the scale parameter as a volume knob, text-image coexistence via parallel K/V paths, and composability with ControlNet. No new theoryâ€”just hands-on practice with real models.\n",
    "\n",
    "**Estimated time:** 40â€“60 minutes (model downloads may take several minutes on first run).\n",
    "\n",
    "**VRAM requirements:** This notebook is designed for a T4 GPU (16 GB). It carefully manages GPU memory by clearing pipelines between heavy operations. Follow the cleanup cells between exercises.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies, import everything, and configure the environment.\n",
    "\n",
    "**Important:** Set the runtime to GPU before running. In Colab: Runtime â†’ Change runtime type â†’ T4 GPU.\n",
    "\n",
    "The first run will download model weights (~5 GB for SD v1.5 + IP-Adapter weights + ControlNet checkpoint). Subsequent runs use cached weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate safetensors controlnet_aux opencv-python-headless\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from PIL import Image\n",
    "from diffusers import (\n",
    "    StableDiffusionPipeline,\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    ControlNetModel,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('WARNING: No GPU detected. This notebook requires a GPU for image generation.')\n",
    "    print('In Colab: Runtime â†’ Change runtime type â†’ T4 GPU')\n",
    "\n",
    "print('\\nSetup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Helpers\n",
    "\n",
    "Utility functions used across multiple exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sample_image(url, filename=\"sample.jpg\"):\n",
    "    \"\"\"Download an image from a URL and return it as a PIL Image.\"\"\"\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    return Image.open(filename).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def show_images(images, titles, figsize=None, suptitle=None):\n",
    "    \"\"\"Display a row of images with titles.\"\"\"\n",
    "    n = len(images)\n",
    "    if figsize is None:\n",
    "        figsize = (5 * n, 5)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle, fontsize=13, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def cleanup_pipeline(pipe):\n",
    "    \"\"\"Delete a pipeline and free GPU memory.\"\"\"\n",
    "    del pipe\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"GPU memory after cleanup: {allocated:.2f} GB allocated\")\n",
    "\n",
    "\n",
    "def make_generator(seed):\n",
    "    \"\"\"Create a torch Generator with the given seed for reproducible results.\"\"\"\n",
    "    return torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "\n",
    "# Download a reference image for IP-Adapter exercises.\n",
    "# This is a photograph of a golden retrieverâ€”a subject with distinctive\n",
    "# visual identity (coat color, fur texture, facial features) that is\n",
    "# easy to verify in generated outputs.\n",
    "REFERENCE_URL = \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_diner.png\"\n",
    "reference_image = download_sample_image(REFERENCE_URL, \"reference.jpg\")\n",
    "reference_image = reference_image.resize((512, 512))\n",
    "\n",
    "show_images([reference_image], [\"Reference Image (512x512)\"])\n",
    "print(\"This reference image will be used throughout the notebook.\")\n",
    "print(\"IP-Adapter will extract its visual character via CLIP image embeddings.\")\n",
    "print(\"The image never enters the denoising loopâ€”it enters via cross-attention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Load IP-Adapter and Compare With/Without `[Guided]`\n",
    "\n",
    "From the lesson: IP-Adapter adds a parallel K/V pathway for CLIP image embeddings in cross-attention. The existing text K/V path is **completely untouched**. Setting the IP-Adapter scale to 0 should produce output identical to vanilla SD with text only.\n",
    "\n",
    "We will load IP-Adapter, generate with the same prompt and seed at two scale values:\n",
    "- **scale=0.0:** Image branch contributes nothing (`text_out + 0 Ã— image_out = text_out`)\n",
    "- **scale=0.6:** Image branch provides moderate visual influence\n",
    "\n",
    "This directly tests the lesson's core claim: the text path is untouched, and the image path is purely additive.\n",
    "\n",
    "**Before running, predict:**\n",
    "- At scale=0.0, will the reference image have any effect on the output?\n",
    "- At scale=0.6, what aspects of the reference image will transfer? (color palette? subject identity? exact pixel layout?)\n",
    "- Will the text prompt still control the scene composition at scale=0.6?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load SD v1.5 pipeline with IP-Adapter\n",
    "#\n",
    "# IP-Adapter is loaded as an add-on to an existing SD pipeline.\n",
    "# The h94/IP-Adapter repository provides pre-trained adapter weights\n",
    "# for SD v1.5 that work with any text prompt.\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    ").to(device)\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Load the IP-Adapter weights into the pipeline.\n",
    "# This adds the trainable W_K_image and W_V_image projections at every\n",
    "# cross-attention layer in the U-Net. The frozen text K/V path is untouched.\n",
    "pipe.load_ip_adapter(\n",
    "    \"h94/IP-Adapter\",\n",
    "    subfolder=\"models\",\n",
    "    weight_name=\"ip-adapter_sd15.bin\",\n",
    ")\n",
    "\n",
    "print(\"SD v1.5 pipeline loaded with IP-Adapter.\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate at scale=0.0 (image branch disabled) and scale=0.6 (moderate influence)\n",
    "\n",
    "prompt = \"a painting of a dog in a garden, beautiful, detailed\"\n",
    "seed = 42\n",
    "num_steps = 20\n",
    "\n",
    "# Scale=0.0: image_out is multiplied by 0, so the reference image has no effect.\n",
    "# output = text_out + 0 Ã— image_out = text_out\n",
    "pipe.set_ip_adapter_scale(0.0)\n",
    "generator = make_generator(seed)\n",
    "img_scale_0 = pipe(\n",
    "    prompt,\n",
    "    ip_adapter_image=reference_image,\n",
    "    num_inference_steps=num_steps,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "print(\"Generated at scale=0.0 (no image influence).\")\n",
    "\n",
    "# Scale=0.6: moderate image influence.\n",
    "# output = text_out + 0.6 Ã— image_out\n",
    "pipe.set_ip_adapter_scale(0.6)\n",
    "generator = make_generator(seed)\n",
    "img_scale_06 = pipe(\n",
    "    prompt,\n",
    "    ip_adapter_image=reference_image,\n",
    "    num_inference_steps=num_steps,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "print(\"Generated at scale=0.6 (moderate image influence).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compare the results\n",
    "\n",
    "show_images(\n",
    "    [reference_image, img_scale_0, img_scale_06],\n",
    "    [\n",
    "        \"Reference Image\\n(CLIP encodes this)\",\n",
    "        \"Scale=0.0\\n(image branch disabled)\",\n",
    "        \"Scale=0.6\\n(moderate image influence)\",\n",
    "    ],\n",
    "    suptitle=\"Same prompt, same seedâ€”IP-Adapter scale controls image influence\",\n",
    ")\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Scale=0.0: The reference image has NO effect. This is vanilla SD with text only.\")\n",
    "print(\"  The model generates a generic dog in a garden based purely on the text prompt.\")\n",
    "print(\"\")\n",
    "print(\"- Scale=0.6: The reference image's visual character transfers. The generated dog\")\n",
    "print(\"  takes on qualities from the referenceâ€”color palette, visual style, mood.\")\n",
    "print(\"  But the text prompt STILL controls the scene (garden setting, composition).\")\n",
    "print(\"\")\n",
    "print(\"Key insight: IP-Adapter is purely additive. At scale=0, the image branch\")\n",
    "print(\"contributes nothing. The text path is untouched at every scale value.\")\n",
    "print(\"This is decoupled cross-attention in action.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You loaded IP-Adapter into an SD v1.5 pipeline and generated with the same prompt and seed at two scale values:\n",
    "\n",
    "- **Scale=0.0:** The image branch contributes nothing (`text_out + 0 Ã— image_out = text_out`). The output is identical to vanilla SD with text only. The reference image is encoded by CLIP but its influence is zeroed out.\n",
    "- **Scale=0.6:** The reference image's visual character transfersâ€”color palette, visual style, mood. But the text prompt still controls the scene composition. The garden setting, the painting style, the dog's pose all come from the text. The reference image adds its visual identity on top.\n",
    "\n",
    "This confirms the lesson's core claim: the text K/V path is completely untouched. IP-Adapter adds a new information source via a parallel K/V pathway. It is addition, not replacement. Same principle as ControlNet at conditioning_scale=0â€”the adapter contributes nothing, and the frozen model is unchanged.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Scale Parameter Sweep `[Guided]`\n",
    "\n",
    "From the lesson: the scale parameter is a **volume knob for image influence**, directly paralleling ControlNet's conditioning scale. At scale=0.0, the image is silent. At scale=1.0, the image is at full volume. The transition from text-dominant to image-dominant should be smooth and progressive.\n",
    "\n",
    "We will generate with the same reference image and prompt at five scale values: 0.0, 0.3, 0.5, 0.7, and 1.0. Watch how the visual character of the reference image gradually emerges.\n",
    "\n",
    "**Before running, predict:**\n",
    "- At what scale value will you first notice the reference image's influence?\n",
    "- At scale=1.0, will the text prompt still have any visible effect?\n",
    "- Is the transition gradual or sudden? Does image influence \"snap on\" at a threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate at five scale values using the same pipeline from Exercise 1\n",
    "\n",
    "scales = [0.0, 0.3, 0.5, 0.7, 1.0]\n",
    "prompt = \"a painting of a dog in a garden, beautiful, detailed\"\n",
    "seed = 42\n",
    "num_steps = 20\n",
    "\n",
    "sweep_results = {}\n",
    "for scale in scales:\n",
    "    pipe.set_ip_adapter_scale(scale)\n",
    "    generator = make_generator(seed)\n",
    "    result = pipe(\n",
    "        prompt,\n",
    "        ip_adapter_image=reference_image,\n",
    "        num_inference_steps=num_steps,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    sweep_results[scale] = result\n",
    "    print(f\"Generated at scale={scale}\")\n",
    "\n",
    "print(f\"\\nGenerated {len(sweep_results)} images across IP-Adapter scales.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the scale sweep as a comparison grid\n",
    "\n",
    "fig, axes = plt.subplots(1, len(scales) + 1, figsize=(4 * (len(scales) + 1), 4))\n",
    "\n",
    "# First column: the reference image\n",
    "axes[0].imshow(reference_image)\n",
    "axes[0].set_title(\"Reference Image\\n(input to CLIP)\", fontsize=10)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# One column per scale value\n",
    "scale_labels = {\n",
    "    0.0: \"0.0\\n(text only)\",\n",
    "    0.3: \"0.3\\n(subtle influence)\",\n",
    "    0.5: \"0.5\\n(balanced)\",\n",
    "    0.7: \"0.7\\n(image-leaning)\",\n",
    "    1.0: \"1.0\\n(strong image)\",\n",
    "}\n",
    "\n",
    "for i, scale in enumerate(scales):\n",
    "    axes[i + 1].imshow(sweep_results[scale])\n",
    "    axes[i + 1].set_title(f\"Scale {scale_labels[scale]}\", fontsize=10)\n",
    "    axes[i + 1].axis('off')\n",
    "\n",
    "plt.suptitle(\n",
    "    \"IP-Adapter Scale Sweep: same reference, same prompt, varying image influence\",\n",
    "    fontsize=13, y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The volume knob for image influence:\")\n",
    "print(\"  Scale 0.0: Pure text conditioning. Generic dog in a garden.\")\n",
    "print(\"  Scale 0.3: Subtle shift. Color palette begins to warm toward the reference.\")\n",
    "print(\"  Scale 0.5: Clear influence. Visual character from the reference is visible.\")\n",
    "print(\"  Scale 0.7: Strong influence. The reference's visual identity dominates.\")\n",
    "print(\"  Scale 1.0: Full image conditioning. Very strong reference image influence.\")\n",
    "print(\"\")\n",
    "print(\"The transition is gradual, not sudden. This is the same 'volume knob' pattern\")\n",
    "print(\"from ControlNet's conditioning_scaleâ€”a continuous dial, not an on/off switch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You swept the IP-Adapter scale from 0.0 to 1.0 and observed the gradual transition from text-dominant to image-dominant generation:\n",
    "\n",
    "- **Scale 0.0:** Pure text conditioning. The model generates a generic dog in a garden. The reference image has zero influence.\n",
    "- **Scale 0.3:** Subtle shift. The color palette begins to warm, visual details start to echo the reference. You might need to look carefully to notice the influence.\n",
    "- **Scale 0.5:** Clearly visible influence. The generated dog takes on visual qualities from the referenceâ€”color, texture, mood. Text still controls the scene.\n",
    "- **Scale 0.7:** Strong image influence. The reference's visual identity is prominent. The text prompt contributes compositional structure but the visual character is clearly from the reference.\n",
    "- **Scale 1.0:** Full image conditioning. The reference image's visual character dominates. The text prompt still provides some scene guidance, but the visual identity is strongly controlled by the reference.\n",
    "\n",
    "The transition is gradual because the scale parameter is a linear multiplier on the image attention output: `output = text_out + scale Ã— image_out`. There is no threshold or discontinuity. This is the same \"volume knob\" pattern from ControlNet's conditioning scaleâ€”familiar control, new conditioning dimension.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Text-Image Coexistence `[Supported]`\n",
    "\n",
    "From the lesson: IP-Adapter is **addition, not replacement**. The text K/V path is untouched. Different text prompts with the same reference image should produce different outputsâ€”the image provides visual character while text controls content and composition.\n",
    "\n",
    "This exercise directly tests whether you understand the decoupling. If IP-Adapter replaced the text prompt, all three outputs below would look identical. They should not.\n",
    "\n",
    "Your task: generate with the same reference image and three different text prompts. The pipeline is already loaded. You write the generation loop.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Will all three outputs look the same (because the reference image dominates)?\n",
    "- Will all three outputs look completely different (because the text prompt dominates)?\n",
    "- Or something in betweenâ€”and if so, what aspect does the image control vs the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three different text prompts, same reference image, same scale.\n",
    "#\n",
    "# The prompts describe different scenes, styles, and compositions.\n",
    "# If IP-Adapter is truly decoupled (addition, not replacement),\n",
    "# the text should control the scene while the reference image\n",
    "# provides visual character.\n",
    "\n",
    "prompts = [\n",
    "    \"a painting of a dog in a garden, beautiful, detailed\",\n",
    "    \"a dog running on a beach at sunset, photorealistic\",\n",
    "    \"a dog sitting in a snowy forest, winter landscape, serene\",\n",
    "]\n",
    "\n",
    "seed = 42\n",
    "num_steps = 20\n",
    "scale = 0.6  # Moderate image influenceâ€”enough to see the reference's effect\n",
    "\n",
    "pipe.set_ip_adapter_scale(scale)\n",
    "\n",
    "coexistence_results = {}  # Maps prompt -> generated PIL Image\n",
    "\n",
    "for p in prompts:\n",
    "    # TODO: Generate an image using the pipeline with this prompt.\n",
    "    #\n",
    "    # Use: pipe(p, ip_adapter_image=reference_image,\n",
    "    #          num_inference_steps=num_steps, generator=make_generator(seed))\n",
    "    #\n",
    "    # Store the result (.images[0]) in coexistence_results[p]\n",
    "    pass\n",
    "\n",
    "print(f\"Generated {len(coexistence_results)} images with different prompts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "\n",
    "if not coexistence_results:\n",
    "    print(\"coexistence_results is emptyâ€”go back and fill in the TODO.\")\n",
    "    print(\"You need to generate an image for each prompt and store it in coexistence_results[p].\")\n",
    "    print(\"Check the solution below if you get stuck.\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "    # First column: reference image\n",
    "    axes[0].imshow(reference_image)\n",
    "    axes[0].set_title(\"Reference Image\\n(same for all three)\", fontsize=10)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # One column per prompt\n",
    "    short_labels = [\n",
    "        \"\\\"...dog in a garden\\\"\",\n",
    "        \"\\\"...dog on a beach\\\"\",\n",
    "        \"\\\"...dog in snowy forest\\\"\",\n",
    "    ]\n",
    "\n",
    "    for i, (p, label) in enumerate(zip(prompts, short_labels)):\n",
    "        axes[i + 1].imshow(coexistence_results[p])\n",
    "        axes[i + 1].set_title(f\"Prompt: {label}\", fontsize=10)\n",
    "        axes[i + 1].axis('off')\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"Same reference image, scale={scale}â€”three different text prompts\",\n",
    "        fontsize=13, y=1.02,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Observations:\")\n",
    "    print(\"- The three outputs are clearly DIFFERENT. Text controls the scene:\")\n",
    "    print(\"  garden vs beach vs snowy forest. Composition varies per prompt.\")\n",
    "    print(\"\")\n",
    "    print(\"- But the visual character is SHARED across all three. The reference\")\n",
    "    print(\"  image's influence (color palette, visual style, mood) carries through.\")\n",
    "    print(\"\")\n",
    "    print(\"This is decoupled cross-attention in action:\")\n",
    "    print(\"  Text K/V path â†’ controls WHAT (scene, composition, subject)\")\n",
    "    print(\"  Image K/V path â†’ controls WHAT-IT-LOOKS-LIKE (visual character)\")\n",
    "    print(\"  Both run in parallel. Neither replaces the other.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the pipeline before Exercise 4\n",
    "cleanup_pipeline(pipe)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Pipeline cleaned up. Ready for Exercise 4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The key insight is that the generation call is identical for each promptâ€”only the text string changes. The IP-Adapter scale and reference image stay fixed. If IP-Adapter replaced text conditioning, all three outputs would be identical regardless of the prompt. The fact that they differ proves decoupling.\n",
    "\n",
    "```python\n",
    "for p in prompts:\n",
    "    generator = make_generator(seed)\n",
    "    result = pipe(\n",
    "        p,\n",
    "        ip_adapter_image=reference_image,\n",
    "        num_inference_steps=num_steps,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    coexistence_results[p] = result\n",
    "```\n",
    "\n",
    "**Why the outputs differ:** The text K/V path (`W_K_text`, `W_V_text`) processes each prompt independently. Different prompts produce different K_text and V_text tensors, which produce different text attention outputs. The image K/V path (`W_K_image`, `W_V_image`) produces the same image attention output for all three (same reference image). The final output is `text_out + scale Ã— image_out`â€”different text_out values, same image_out. Hence: different scenes, shared visual character.\n",
    "\n",
    "**Common mistake:** Forgetting to recreate the generator for each prompt. Without resetting the seed, each generation starts from different random noise, making it harder to isolate the effect of the text prompt from random variation.\n",
    "\n",
    "</details>\n",
    "\n",
    "### What Just Happened\n",
    "\n",
    "You generated three images with the same reference image but three different text prompts:\n",
    "\n",
    "- **Dog in a garden:** Garden setting, painterly styleâ€”the reference image's visual character blends with the garden scene.\n",
    "- **Dog on a beach at sunset:** Beach and sunset compositionâ€”completely different scene, but the reference image's visual identity carries through.\n",
    "- **Dog in a snowy forest:** Winter landscapeâ€”yet again different content, yet the visual character from the reference is consistent.\n",
    "\n",
    "The outputs are clearly different because the text K/V path is untouched. Each prompt produces different K_text and V_text tensors, creating different text attention outputs. The image K/V path adds the same image influence to all three. This is the practical proof of the lesson's central claim: **image prompting is addition, not replacement.** The two K/V paths run in parallel, and the text prompt retains full control over scene content and composition.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Combine IP-Adapter with ControlNet `[Independent]`\n",
    "\n",
    "From the lesson: IP-Adapter provides **WHAT-IT-LOOKS-LIKE** (visual identity via decoupled cross-attention). ControlNet provides **WHERE** (spatial structure via additive encoder features). Text provides **WHAT** (semantic content via the original cross-attention). All three are additive and composableâ€”they target different parts of the U-Net.\n",
    "\n",
    "**Your task:** Combine IP-Adapter with ControlNet in a single generation.\n",
    "\n",
    "1. Download a **structure source image** (different from the reference image) and extract Canny edges from it\n",
    "2. Load a pipeline with both IP-Adapter and ControlNet\n",
    "3. Generate with:\n",
    "   - The **reference image** providing visual style/identity (via IP-Adapter)\n",
    "   - The **edge map** providing spatial structure (via ControlNet)\n",
    "   - A **text prompt** providing semantic content\n",
    "4. Display: structure source, edge map, reference image, and generated output\n",
    "5. Interpret the results: which aspects came from which conditioning source?\n",
    "\n",
    "No scaffolding. You decide the prompt, the scales, and how to interpret the results.\n",
    "\n",
    "**Hints:**\n",
    "- Use `StableDiffusionControlNetPipeline` with a ControlNet, then call `pipe.load_ip_adapter(...)` to add IP-Adapter on top\n",
    "- The ControlNet conditioning image goes in `image=edge_map`\n",
    "- The IP-Adapter reference image goes in `ip_adapter_image=reference_image`\n",
    "- Use `pipe.set_ip_adapter_scale(...)` for IP-Adapter scale and `controlnet_conditioning_scale=...` for ControlNet scale\n",
    "- Start with moderate scales (IP-Adapter ~0.5, ControlNet ~0.7) and adjust\n",
    "\n",
    "**Available structure source images:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure source image options (for extracting edges)\n",
    "# These are DIFFERENT from the IP-Adapter reference imageâ€”that is the point.\n",
    "# The reference image provides visual STYLE. The structure source provides spatial LAYOUT.\n",
    "\n",
    "structure_options = {\n",
    "    \"vermeer\": \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\",\n",
    "    \"architecture\": \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_input.png\",\n",
    "}\n",
    "\n",
    "# Show what is available\n",
    "option_images = []\n",
    "option_titles = []\n",
    "for name, url in structure_options.items():\n",
    "    try:\n",
    "        img = download_sample_image(url, f\"{name}.jpg\").resize((512, 512))\n",
    "        option_images.append(img)\n",
    "        option_titles.append(name)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not download {name}: {e}\")\n",
    "\n",
    "show_images(\n",
    "    [reference_image] + option_images,\n",
    "    [\"Reference Image\\n(IP-Adapter: visual style)\"] + [f\"{t}\\n(ControlNet: spatial structure)\" for t in option_titles],\n",
    "    suptitle=\"Reference image for STYLE (left) vs structure source options for LAYOUT (right)\",\n",
    ")\n",
    "\n",
    "print(\"Pick a structure source image. You will extract edges from it (ControlNet)\")\n",
    "print(\"while using the reference image for visual style (IP-Adapter).\")\n",
    "print(\"The two images serve different conditioning roles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your IP-Adapter + ControlNet composition code goes here.\n",
    "#\n",
    "# Workflow:\n",
    "# 1. Choose a structure source image and extract Canny edges\n",
    "# 2. Load ControlNet (Canny) + SD pipeline\n",
    "# 3. Load IP-Adapter into the pipeline (pipe.load_ip_adapter(...))\n",
    "# 4. Set IP-Adapter scale (pipe.set_ip_adapter_scale(...))\n",
    "# 5. Generate with:\n",
    "#    - image=edge_map (ControlNet spatial conditioning)\n",
    "#    - ip_adapter_image=reference_image (IP-Adapter visual conditioning)\n",
    "#    - controlnet_conditioning_scale=... (ControlNet strength)\n",
    "#    - A text prompt of your choice\n",
    "# 6. Display: structure source, edge map, reference image, and generated output\n",
    "#\n",
    "# Remember:\n",
    "# - Use torch.float16 for all models\n",
    "# - Canny thresholds around (100, 200) work well for clean edges\n",
    "# - Start with IP-Adapter scale ~0.5, ControlNet scale ~0.7\n",
    "# - Use make_generator(seed) for reproducible results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The core insight is that IP-Adapter and ControlNet target different parts of the U-Net and are fully composable. ControlNet adds structural features at the skip connections (WHERE). IP-Adapter adds image features via decoupled cross-attention K/V projections (WHAT-IT-LOOKS-LIKE). Text provides semantic content via the original cross-attention K/V (WHAT). All three are additive.\n",
    "\n",
    "```python\n",
    "# 1. Choose structure source and extract edges\n",
    "structure_url = structure_options[\"vermeer\"]\n",
    "structure_image = download_sample_image(structure_url, \"structure.jpg\").resize((512, 512))\n",
    "\n",
    "edges = cv2.Canny(np.array(structure_image), 100, 200)\n",
    "edge_map = Image.fromarray(np.stack([edges] * 3, axis=-1))\n",
    "\n",
    "# 2. Load ControlNet + SD pipeline\n",
    "controlnet_canny = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet_canny,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    ").to(device)\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# 3. Load IP-Adapter on top\n",
    "pipe.load_ip_adapter(\n",
    "    \"h94/IP-Adapter\",\n",
    "    subfolder=\"models\",\n",
    "    weight_name=\"ip-adapter_sd15.bin\",\n",
    ")\n",
    "\n",
    "# 4. Set scales\n",
    "pipe.set_ip_adapter_scale(0.5)\n",
    "\n",
    "# 5. Generate\n",
    "generator = make_generator(42)\n",
    "result = pipe(\n",
    "    \"a beautiful portrait, detailed, masterpiece\",\n",
    "    image=edge_map,\n",
    "    ip_adapter_image=reference_image,\n",
    "    num_inference_steps=20,\n",
    "    generator=generator,\n",
    "    controlnet_conditioning_scale=0.7,\n",
    ").images[0]\n",
    "\n",
    "# 6. Display\n",
    "show_images(\n",
    "    [structure_image, edge_map, reference_image, result],\n",
    "    [\n",
    "        \"Structure Source\\n(layout comes from here)\",\n",
    "        \"Canny Edges\\n(ControlNet: WHERE)\",\n",
    "        \"Reference Image\\n(IP-Adapter: WHAT-IT-LOOKS-LIKE)\",\n",
    "        \"Generated\\n(all three combined)\",\n",
    "    ],\n",
    "    suptitle=\"ControlNet (structure) + IP-Adapter (style) + text (content)\",\n",
    ")\n",
    "\n",
    "print(\"Three conditioning channels in one generation:\")\n",
    "print(\"  ControlNet (edges): spatial structure/layout from the structure source\")\n",
    "print(\"  IP-Adapter (reference): visual style/mood from the reference image\")\n",
    "print(\"  Text: semantic content and rendering style\")\n",
    "\n",
    "# Clean up\n",
    "cleanup_pipeline(pipe)\n",
    "del controlnet_canny\n",
    "```\n",
    "\n",
    "**Key decisions:**\n",
    "- IP-Adapter at 0.5 (moderate visual influenceâ€”we want to see the reference's character without overpowering)\n",
    "- ControlNet at 0.7 (strong structural adherenceâ€”the edges should clearly control composition)\n",
    "- The structure source image and reference image are intentionally DIFFERENT. The structure provides WHERE. The reference provides WHAT-IT-LOOKS-LIKE. If they were the same image, you would not be able to tell which conditioning channel contributed what.\n",
    "\n",
    "**Common mistake:** Using the same image for both IP-Adapter and ControlNet. This defeats the purpose of the exerciseâ€”you cannot observe composability if both signals come from the same source.\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **IP-Adapter is purely additive.** At scale=0, the image branch contributes nothing and the output is identical to vanilla SD. The text K/V path is completely untouched at every scale value. Remove IP-Adapter and the frozen model is bit-for-bit identical.\n",
    "\n",
    "2. **The scale parameter is a volume knob for image influence.** The transition from text-dominant (0.0) to image-dominant (1.0) is gradual and continuous. Same \"volume knob\" pattern as ControlNet's conditioning scaleâ€”familiar control, new conditioning dimension.\n",
    "\n",
    "3. **Image prompting is addition, not replacement.** The same reference image with three different text prompts produces three different outputs. Text controls content and composition (WHAT). The reference image controls visual character (WHAT-IT-LOOKS-LIKE). Both K/V paths run in parallel.\n",
    "\n",
    "4. **IP-Adapter and ControlNet are composable.** ControlNet provides spatial structure (WHERE) via additive encoder features. IP-Adapter provides visual identity (WHAT-IT-LOOKS-LIKE) via decoupled cross-attention K/V. Text provides semantic content (WHAT). All three are additive, target different parts of the U-Net, and work together in a single generation.\n",
    "\n",
    "5. **The practical workflow is: choose reference image â†’ set scale â†’ compose with other conditioning â†’ iterate.** The tools are the same every time; the creative decisions are what change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}