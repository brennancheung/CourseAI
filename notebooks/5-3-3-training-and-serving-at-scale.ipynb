{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Training & Serving at Scale\n\nIn this notebook, you'll calculate, simulate, and reason about the engineering that gets large models trained and served. No multi-GPU hardware needed—everything runs as calculation and simulation on a single CPU.\n\n**What you'll do:**\n- Build a training memory calculator that computes per-GPU requirements for different parallelism strategies and ZeRO stages, then see why a 70B model cannot train on a single GPU\n- Simulate speculative decoding with a draft-then-verify loop: measure how acceptance rate changes with draft length K, and see why the speedup comes from parallel verification\n- Simulate a continuous batching inference server: compare GPU utilization between static batching (wait for all to finish) and continuous batching (fill completed slots immediately)\n- Build a parallelism strategy advisor: given a model size, GPU count, and GPU memory, recommend the right combination of data, tensor, and pipeline parallelism\n\n**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones—they reveal gaps in your mental model."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Setup -- self-contained for Google Colab\n",
    "# No extra pip installs needed.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "# Reproducible results\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "print('Setup complete.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Helpers\n",
    "\n",
    "Formatting utilities used across multiple exercises."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def format_bytes(b: float) -> str:\n",
    "    \"\"\"Human-readable byte string.\"\"\"\n",
    "    if b >= 1e12:\n",
    "        return f'{b / 1e12:.1f} TB'\n",
    "    if b >= 1e9:\n",
    "        return f'{b / 1e9:.1f} GB'\n",
    "    if b >= 1e6:\n",
    "        return f'{b / 1e6:.1f} MB'\n",
    "    return f'{b / 1e3:.1f} KB'\n",
    "\n",
    "\n",
    "def format_number(n: float) -> str:\n",
    "    \"\"\"Human-readable large number.\"\"\"\n",
    "    if n >= 1e12:\n",
    "        return f'{n / 1e12:.1f}T'\n",
    "    if n >= 1e9:\n",
    "        return f'{n / 1e9:.1f}B'\n",
    "    if n >= 1e6:\n",
    "        return f'{n / 1e6:.0f}M'\n",
    "    return f'{n / 1e3:.0f}K'\n",
    "\n",
    "\n",
    "print('Helpers loaded.')\n",
    "print(f'  format_bytes(84e9) = {format_bytes(84e9)}')\n",
    "print(f'  format_number(70e9) = {format_number(70e9)}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Training Memory Calculator (Guided)\n",
    "\n",
    "You already know the training memory breakdown from the LoRA & Quantization lesson: mixed-precision Adam training requires ~12 bytes per parameter (2B weights + 2B gradients + 4B momentum + 4B variance). Optimizer states alone account for two-thirds of training memory.\n",
    "\n",
    "In this exercise, you'll see a complete memory calculator that computes per-GPU requirements across:\n",
    "- **No parallelism** (single GPU)\n",
    "- **Data parallelism** (full model replicated on each GPU)\n",
    "- **ZeRO Stage 1** (shard optimizer states across GPUs)\n",
    "- **ZeRO Stage 2** (shard optimizer states + gradients)\n",
    "- **ZeRO Stage 3** (shard everything)\n",
    "\n",
    "You'll apply it to three models: GPT-2 (124M), LLaMA 7B, and LLaMA 70B.\n",
    "\n",
    "**Before running, predict:**\n",
    "- For a 7B model with mixed-precision Adam, what is the total training memory? (Hint: ~12 bytes/param)\n",
    "- With ZeRO Stage 1 on 8 GPUs, the optimizer states are sharded 8 ways. Optimizer states are ~8 bytes/param. What is the per-GPU optimizer memory? What is the total per-GPU memory?\n",
    "- Will ZeRO Stage 1 alone make a 70B model fit on 8 A100 GPUs (80 GB each)?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Training Memory Calculator ---\n",
    "#\n",
    "# Mixed-precision Adam breakdown (per parameter):\n",
    "#   - bf16 weights:    2 bytes (forward/backward)\n",
    "#   - bf16 gradients:  2 bytes\n",
    "#   - fp32 momentum:   4 bytes (Adam state)\n",
    "#   - fp32 variance:   4 bytes (Adam state)\n",
    "#   Total: 12 bytes/param\n",
    "#   Optimizer states: 8 bytes/param (2/3 of total)\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    name: str\n",
    "    num_params: float  # number of parameters\n",
    "\n",
    "\n",
    "BYTES_PER_PARAM_WEIGHTS = 2     # bf16\n",
    "BYTES_PER_PARAM_GRADIENTS = 2   # bf16\n",
    "BYTES_PER_PARAM_MOMENTUM = 4    # fp32 Adam\n",
    "BYTES_PER_PARAM_VARIANCE = 4    # fp32 Adam\n",
    "BYTES_PER_PARAM_OPTIMIZER = BYTES_PER_PARAM_MOMENTUM + BYTES_PER_PARAM_VARIANCE  # 8\n",
    "BYTES_PER_PARAM_TOTAL = (BYTES_PER_PARAM_WEIGHTS + BYTES_PER_PARAM_GRADIENTS\n",
    "                         + BYTES_PER_PARAM_OPTIMIZER)  # 12\n",
    "\n",
    "\n",
    "def training_memory(\n",
    "    num_params: float,\n",
    "    num_gpus: int = 1,\n",
    "    zero_stage: int = 0,  # 0 = no ZeRO, 1/2/3 = ZeRO stages\n",
    ") -> dict:\n",
    "    \"\"\"Compute per-GPU training memory for a given model and parallelism config.\n",
    "\n",
    "    ZeRO stages:\n",
    "      Stage 0 (data parallelism): full model replicated on each GPU\n",
    "      Stage 1: shard optimizer states across GPUs\n",
    "      Stage 2: shard optimizer states + gradients\n",
    "      Stage 3: shard everything (weights + gradients + optimizer states)\n",
    "    \"\"\"\n",
    "    # What each GPU stores depends on ZeRO stage\n",
    "    weights_per_gpu = num_params * BYTES_PER_PARAM_WEIGHTS\n",
    "    gradients_per_gpu = num_params * BYTES_PER_PARAM_GRADIENTS\n",
    "    optimizer_per_gpu = num_params * BYTES_PER_PARAM_OPTIMIZER\n",
    "\n",
    "    # ZeRO Stage 1: shard optimizer states\n",
    "    if zero_stage >= 1:\n",
    "        optimizer_per_gpu = optimizer_per_gpu / num_gpus\n",
    "\n",
    "    # ZeRO Stage 2: also shard gradients\n",
    "    if zero_stage >= 2:\n",
    "        gradients_per_gpu = gradients_per_gpu / num_gpus\n",
    "\n",
    "    # ZeRO Stage 3: also shard weights\n",
    "    if zero_stage >= 3:\n",
    "        weights_per_gpu = weights_per_gpu / num_gpus\n",
    "\n",
    "    total_per_gpu = weights_per_gpu + gradients_per_gpu + optimizer_per_gpu\n",
    "\n",
    "    return {\n",
    "        'weights_per_gpu': weights_per_gpu,\n",
    "        'gradients_per_gpu': gradients_per_gpu,\n",
    "        'optimizer_per_gpu': optimizer_per_gpu,\n",
    "        'total_per_gpu': total_per_gpu,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Apply to three models ---\n",
    "\n",
    "models = [\n",
    "    ModelConfig('GPT-2 (124M)', 124e6),\n",
    "    ModelConfig('LLaMA 7B', 7e9),\n",
    "    ModelConfig('LLaMA 70B', 70e9),\n",
    "]\n",
    "\n",
    "A100_MEMORY_GB = 80\n",
    "A100_MEMORY_BYTES = A100_MEMORY_GB * 1e9\n",
    "\n",
    "print('=== Single GPU Training Memory ===')\n",
    "print(f'{\"Model\":<20} {\"Weights\":>10} {\"Grads\":>10} {\"Optimizer\":>10} {\"Total\":>10} {\"Fits A100?\":>12}')\n",
    "print('-' * 75)\n",
    "for model in models:\n",
    "    mem = training_memory(model.num_params, num_gpus=1, zero_stage=0)\n",
    "    fits = mem['total_per_gpu'] <= A100_MEMORY_BYTES\n",
    "    print(f'{model.name:<20} '\n",
    "          f'{format_bytes(mem[\"weights_per_gpu\"]):>10} '\n",
    "          f'{format_bytes(mem[\"gradients_per_gpu\"]):>10} '\n",
    "          f'{format_bytes(mem[\"optimizer_per_gpu\"]):>10} '\n",
    "          f'{format_bytes(mem[\"total_per_gpu\"]):>10} '\n",
    "          f'{\"YES\" if fits else \"NO\":>12}')\n",
    "\n",
    "print()\n",
    "print(f'A100 GPU memory: {A100_MEMORY_GB} GB')\n",
    "print(f'70B model needs {format_bytes(70e9 * BYTES_PER_PARAM_TOTAL)} -- '\n",
    "      f'{70e9 * BYTES_PER_PARAM_TOTAL / A100_MEMORY_BYTES:.0f}x more than a single A100.')\n",
    "print(f'The model does not fit. Not \"it\\'s slow\" -- it is physically impossible to begin.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- ZeRO stages across GPU counts ---\n",
    "# For each model, compute per-GPU memory with ZeRO stages 0-3 on 8 GPUs.\n",
    "\n",
    "NUM_GPUS = 8\n",
    "\n",
    "print(f'=== Per-GPU Memory with {NUM_GPUS} GPUs ===')\n",
    "print()\n",
    "\n",
    "for model in models:\n",
    "    print(f'--- {model.name} ({format_number(model.num_params)} params) ---')\n",
    "    print(f'{\"ZeRO Stage\":<14} {\"Weights\":>10} {\"Grads\":>10} {\"Optimizer\":>10} '\n",
    "          f'{\"Total/GPU\":>10} {\"Fits A100?\":>12}')\n",
    "    print('-' * 70)\n",
    "\n",
    "    stage_labels = {\n",
    "        0: 'No ZeRO (DP)',\n",
    "        1: 'Stage 1',\n",
    "        2: 'Stage 2',\n",
    "        3: 'Stage 3',\n",
    "    }\n",
    "\n",
    "    for stage in [0, 1, 2, 3]:\n",
    "        mem = training_memory(model.num_params, num_gpus=NUM_GPUS, zero_stage=stage)\n",
    "        fits = mem['total_per_gpu'] <= A100_MEMORY_BYTES\n",
    "        print(f'{stage_labels[stage]:<14} '\n",
    "              f'{format_bytes(mem[\"weights_per_gpu\"]):>10} '\n",
    "              f'{format_bytes(mem[\"gradients_per_gpu\"]):>10} '\n",
    "              f'{format_bytes(mem[\"optimizer_per_gpu\"]):>10} '\n",
    "              f'{format_bytes(mem[\"total_per_gpu\"]):>10} '\n",
    "              f'{\"YES\" if fits else \"NO\":>12}')\n",
    "    print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Visualize: per-GPU memory by ZeRO stage for the 70B model ---\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "stages = ['No ZeRO\\n(Data Par.)', 'ZeRO\\nStage 1', 'ZeRO\\nStage 2', 'ZeRO\\nStage 3']\n",
    "weights_vals = []\n",
    "grads_vals = []\n",
    "opt_vals = []\n",
    "\n",
    "model_70b = models[2]\n",
    "for stage in [0, 1, 2, 3]:\n",
    "    mem = training_memory(model_70b.num_params, num_gpus=NUM_GPUS, zero_stage=stage)\n",
    "    weights_vals.append(mem['weights_per_gpu'] / 1e9)\n",
    "    grads_vals.append(mem['gradients_per_gpu'] / 1e9)\n",
    "    opt_vals.append(mem['optimizer_per_gpu'] / 1e9)\n",
    "\n",
    "x = np.arange(len(stages))\n",
    "bar_width = 0.5\n",
    "\n",
    "ax.bar(x, weights_vals, bar_width, label='Weights (bf16)', color='#60a5fa', alpha=0.8)\n",
    "ax.bar(x, grads_vals, bar_width, bottom=weights_vals, label='Gradients (bf16)', color='#34d399', alpha=0.8)\n",
    "ax.bar(x, opt_vals, bar_width,\n",
    "       bottom=[w + g for w, g in zip(weights_vals, grads_vals)],\n",
    "       label='Optimizer States (fp32)', color='#f59e0b', alpha=0.8)\n",
    "\n",
    "# A100 capacity line\n",
    "ax.axhline(y=A100_MEMORY_GB, color='#f87171', linestyle='--', linewidth=2, label=f'A100 capacity ({A100_MEMORY_GB} GB)')\n",
    "\n",
    "# Add total labels on top of each bar\n",
    "for i, (w, g, o) in enumerate(zip(weights_vals, grads_vals, opt_vals)):\n",
    "    total = w + g + o\n",
    "    color = '#f87171' if total > A100_MEMORY_GB else '#34d399'\n",
    "    ax.text(i, total + 8, f'{total:.0f} GB', ha='center', va='bottom',\n",
    "            fontsize=10, fontweight='bold', color=color)\n",
    "\n",
    "ax.set_ylabel('Per-GPU Memory (GB)', fontsize=12)\n",
    "ax.set_title(f'LLaMA 70B Training Memory: Per-GPU on {NUM_GPUS} A100 GPUs',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(stages, fontsize=10)\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_ylim(0, max(weights_vals[0] + grads_vals[0] + opt_vals[0], A100_MEMORY_GB) * 1.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nKey observations:')\n",
    "print('1. Without ZeRO, data parallelism requires the FULL model on each GPU -- no memory savings.')\n",
    "print('2. ZeRO Stage 1 shards optimizer states (the orange bar) -- the biggest component.')\n",
    "print('   Optimizer states drop from 560 GB to 70 GB per GPU. But total is still ~210 GB.')\n",
    "print('3. ZeRO Stage 3 shards everything -- total per GPU drops to ~105 GB.')\n",
    "print('   Still above 80 GB! Even ZeRO Stage 3 alone does not make 70B fit on 8 A100s.')\n",
    "print('4. You need ZeRO PLUS tensor/pipeline parallelism to actually train 70B models.')\n",
    "print('   This is why frontier model training combines multiple strategies.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**What just happened:** The training memory breakdown makes the problem visceral. A 70B model needs 840 GB for training—over 10x what a single A100 provides. Data parallelism does not help because it *replicates* the full model. ZeRO Stage 1 targets the biggest component (optimizer states at 560 GB) and shards them across GPUs, but even with 8-way sharding, per-GPU memory is ~210 GB. Even ZeRO Stage 3 (sharding everything) gives ~105 GB per GPU—still above the 80 GB limit.\n\nThis is why frontier model training combines ZeRO with tensor parallelism (split weight matrices within layers) and pipeline parallelism (split layers across GPUs). No single strategy is sufficient. The choice of strategy is determined by which bottleneck dominates—memory, compute, or communication.\n\n---\n\n## Exercise 2: Speculative Decoding Simulator (Supported)\n\nRemember the `generate()` loop from building nanoGPT: each token requires one forward pass through the entire model. For a 70B model, that means one forward pass per token, sequentially. Speculative decoding modifies this loop: a small, fast *draft model* generates K candidate tokens, then the large *target model* verifies all K in a single forward pass.\n\nThe key insight from the lesson: the speed does not come from the small model being fast. It comes from the large model verifying multiple tokens *in parallel* in one forward pass, instead of generating them one at a time.\n\nIn this exercise, you'll simulate speculative decoding with:\n- A draft model that generates candidate tokens (with some probability of matching the target)\n- A target model that verifies all candidates in one pass\n- Acceptance following the \"reject from first disagreement\" rule\n\nYou'll measure acceptance rate at different draft lengths K and see the speedup.\n\n**Before running, predict:**\n- If the draft model has a 70% per-token match rate and drafts K=5 tokens, what is the probability that all 5 are accepted? (Hint: independent events)\n- As K increases from 1 to 8, does the average number of accepted tokens per round increase, decrease, or plateau?\n- If the draft model takes 10ms per token and the target model takes 50ms per forward pass (regardless of how many tokens it verifies), what draft length K maximizes tokens per second?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Speculative Decoding Simulator ---\n",
    "#\n",
    "# We simulate the draft-then-verify loop WITHOUT actual language models.\n",
    "# Instead, we model:\n",
    "#   - Draft model: generates tokens that match the target with probability `match_prob`\n",
    "#   - Target model: verifies all K draft tokens in one \"forward pass\"\n",
    "#   - Acceptance rule: accept from the start until the first disagreement\n",
    "#\n",
    "# This captures the core mechanism: the large model verifies in parallel,\n",
    "# and the speedup depends on the acceptance rate.\n",
    "\n",
    "def simulate_speculative_round(\n",
    "    k: int,\n",
    "    match_prob: float,\n",
    ") -> int:\n",
    "    \"\"\"Simulate one round of speculative decoding.\n",
    "\n",
    "    The draft model produces K tokens. Each matches the target model's\n",
    "    output with probability `match_prob` (independently).\n",
    "\n",
    "    Returns the number of accepted tokens (0 to K).\n",
    "    Acceptance rule: accept from start until first disagreement.\n",
    "    Even if token 4 matches but token 3 doesn't, we stop at token 3.\n",
    "    \"\"\"\n",
    "    # TODO: Simulate K token matches. For each draft position (0 to K-1):\n",
    "    #   1. Generate a random number with np.random.random()\n",
    "    #   2. If it's < match_prob, the token matches (accept and continue)\n",
    "    #   3. If it's >= match_prob, the token doesn't match (reject and stop)\n",
    "    #   Return the count of consecutive accepted tokens from the start.\n",
    "    #\n",
    "    # Note: even when we reject at position i, the target model has already\n",
    "    # computed its own token for position i in the same forward pass.\n",
    "    # So we always get at least 1 new token per round (the resampled one).\n",
    "    # But for simplicity, we return the count of ACCEPTED draft tokens here.\n",
    "\n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "\n",
    "def simulate_speculative_decoding(\n",
    "    total_tokens: int,\n",
    "    k: int,\n",
    "    match_prob: float,\n",
    "    draft_time_ms: float,\n",
    "    target_time_ms: float,\n",
    "    num_trials: int = 200,\n",
    ") -> dict:\n",
    "    \"\"\"Simulate speculative decoding over many rounds and measure performance.\n",
    "\n",
    "    Args:\n",
    "        total_tokens: total tokens to generate\n",
    "        k: number of draft tokens per round\n",
    "        match_prob: probability each draft token matches target\n",
    "        draft_time_ms: time for draft model to generate one token\n",
    "        target_time_ms: time for target model's forward pass (constant, regardless of K)\n",
    "        num_trials: number of simulation runs to average over\n",
    "\n",
    "    Returns:\n",
    "        dict with average metrics across trials\n",
    "    \"\"\"\n",
    "    all_rounds = []\n",
    "    all_accepted = []\n",
    "    all_times = []\n",
    "\n",
    "    for _ in range(num_trials):\n",
    "        tokens_generated = 0\n",
    "        num_rounds = 0\n",
    "        total_time = 0.0\n",
    "        accepted_counts = []\n",
    "\n",
    "        while tokens_generated < total_tokens:\n",
    "            # TODO: Simulate one speculative decoding round:\n",
    "            #   1. Draft phase: draft model generates K tokens\n",
    "            #      Time cost: k * draft_time_ms\n",
    "            #   2. Verify phase: target model verifies all K in one pass\n",
    "            #      Time cost: target_time_ms (constant!)\n",
    "            #   3. Count accepted tokens from simulate_speculative_round(k, match_prob)\n",
    "            #   4. Total new tokens this round = accepted + 1\n",
    "            #      (the +1 is the target model's own token at the rejection point)\n",
    "            #   5. Update tokens_generated, num_rounds, total_time, accepted_counts\n",
    "\n",
    "            pass  # Replace with your implementation\n",
    "\n",
    "        all_rounds.append(num_rounds)\n",
    "        all_accepted.append(np.mean(accepted_counts))\n",
    "        all_times.append(total_time)\n",
    "\n",
    "    # Baseline: target model generating tokens one at a time (no speculation)\n",
    "    baseline_time = total_tokens * target_time_ms\n",
    "\n",
    "    return {\n",
    "        'k': k,\n",
    "        'match_prob': match_prob,\n",
    "        'avg_rounds': np.mean(all_rounds),\n",
    "        'avg_accepted_per_round': np.mean(all_accepted),\n",
    "        'avg_tokens_per_round': np.mean(all_accepted) + 1,\n",
    "        'avg_time_ms': np.mean(all_times),\n",
    "        'baseline_time_ms': baseline_time,\n",
    "        'speedup': baseline_time / np.mean(all_times),\n",
    "    }\n",
    "\n",
    "\n",
    "# Quick test\n",
    "test_accepted = simulate_speculative_round(5, 0.7)\n",
    "print(f'Test round (K=5, match_prob=0.7): {test_accepted} tokens accepted')\n",
    "print('(Run a few times -- you should see values from 0 to 5, averaging around 2-3)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\nThe key insight is the acceptance rule: we accept consecutive tokens from the start until the first disagreement. This means acceptance follows a geometric distribution—each additional token has the same independent probability of matching.\n\n```python\ndef simulate_speculative_round(k: int, match_prob: float) -> int:\n    accepted = 0\n    for _ in range(k):\n        if np.random.random() < match_prob:\n            accepted += 1\n        else:\n            break\n    return accepted\n```\n\nFor the simulation loop:\n\n```python\nwhile tokens_generated < total_tokens:\n    # Draft phase: K tokens, each takes draft_time_ms\n    draft_cost = k * draft_time_ms\n    # Verify phase: one forward pass, constant cost\n    verify_cost = target_time_ms\n    # Count accepted\n    accepted = simulate_speculative_round(k, match_prob)\n    # Total new tokens = accepted drafts + 1 resampled token\n    new_tokens = accepted + 1\n\n    tokens_generated += new_tokens\n    num_rounds += 1\n    total_time += draft_cost + verify_cost\n    accepted_counts.append(accepted)\n```\n\nThe `+1` is crucial: even when the target model rejects at position i, it has already computed its own token for that position during the same forward pass. So every round produces at least 1 token, and up to K+1 tokens if all drafts are accepted (K accepted + 1 bonus token from the target model going one position further—though for simplicity we cap at the K accepted + 1 resampled).\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper: Working Speculative Decoding Simulator\n",
    "\n",
    "**Run the cell below** to get working implementations for the analysis. If your implementation above works, this redefines the same functions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Reference implementation ---\n",
    "\n",
    "def simulate_speculative_round(k: int, match_prob: float) -> int:\n",
    "    \"\"\"Simulate one speculative decoding round. Returns accepted count.\"\"\"\n",
    "    accepted = 0\n",
    "    for _ in range(k):\n",
    "        if np.random.random() < match_prob:\n",
    "            accepted += 1\n",
    "        else:\n",
    "            break\n",
    "    return accepted\n",
    "\n",
    "\n",
    "def simulate_speculative_decoding(\n",
    "    total_tokens: int,\n",
    "    k: int,\n",
    "    match_prob: float,\n",
    "    draft_time_ms: float,\n",
    "    target_time_ms: float,\n",
    "    num_trials: int = 200,\n",
    ") -> dict:\n",
    "    \"\"\"Simulate speculative decoding and measure performance.\"\"\"\n",
    "    all_rounds = []\n",
    "    all_accepted = []\n",
    "    all_times = []\n",
    "\n",
    "    for _ in range(num_trials):\n",
    "        tokens_generated = 0\n",
    "        num_rounds = 0\n",
    "        total_time = 0.0\n",
    "        accepted_counts = []\n",
    "\n",
    "        while tokens_generated < total_tokens:\n",
    "            draft_cost = k * draft_time_ms\n",
    "            verify_cost = target_time_ms\n",
    "            accepted = simulate_speculative_round(k, match_prob)\n",
    "            new_tokens = accepted + 1\n",
    "\n",
    "            tokens_generated += new_tokens\n",
    "            num_rounds += 1\n",
    "            total_time += draft_cost + verify_cost\n",
    "            accepted_counts.append(accepted)\n",
    "\n",
    "        all_rounds.append(num_rounds)\n",
    "        all_accepted.append(np.mean(accepted_counts))\n",
    "        all_times.append(total_time)\n",
    "\n",
    "    baseline_time = total_tokens * target_time_ms\n",
    "\n",
    "    return {\n",
    "        'k': k,\n",
    "        'match_prob': match_prob,\n",
    "        'avg_rounds': np.mean(all_rounds),\n",
    "        'avg_accepted_per_round': np.mean(all_accepted),\n",
    "        'avg_tokens_per_round': np.mean(all_accepted) + 1,\n",
    "        'avg_time_ms': np.mean(all_times),\n",
    "        'baseline_time_ms': baseline_time,\n",
    "        'speedup': baseline_time / np.mean(all_times),\n",
    "    }\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "print('Reference speculative decoding simulator loaded.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Sweep draft length K from 1 to 8 ---\n",
    "#\n",
    "# Timing model (realistic for 70B target, 7B draft):\n",
    "#   Draft model: 10 ms per token (small, fast)\n",
    "#   Target model: 50 ms per forward pass (large, but constant regardless of K)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "TOTAL_TOKENS = 100\n",
    "MATCH_PROB = 0.70\n",
    "DRAFT_TIME_MS = 10.0\n",
    "TARGET_TIME_MS = 50.0\n",
    "\n",
    "k_values = list(range(1, 9))\n",
    "results = []\n",
    "\n",
    "print(f'=== Speculative Decoding: {TOTAL_TOKENS} tokens, match_prob={MATCH_PROB} ===')\n",
    "print(f'Draft model: {DRAFT_TIME_MS} ms/token, Target model: {TARGET_TIME_MS} ms/pass')\n",
    "print()\n",
    "print(f'{\"K\":>3} {\"Acc/Round\":>10} {\"Tok/Round\":>10} {\"Rounds\":>8} '\n",
    "      f'{\"Time (ms)\":>10} {\"Baseline\":>10} {\"Speedup\":>8}')\n",
    "print('-' * 65)\n",
    "\n",
    "for k in k_values:\n",
    "    r = simulate_speculative_decoding(\n",
    "        total_tokens=TOTAL_TOKENS,\n",
    "        k=k,\n",
    "        match_prob=MATCH_PROB,\n",
    "        draft_time_ms=DRAFT_TIME_MS,\n",
    "        target_time_ms=TARGET_TIME_MS,\n",
    "    )\n",
    "    results.append(r)\n",
    "    print(f'{k:>3} {r[\"avg_accepted_per_round\"]:>10.2f} {r[\"avg_tokens_per_round\"]:>10.2f} '\n",
    "          f'{r[\"avg_rounds\"]:>8.1f} {r[\"avg_time_ms\"]:>10.0f} '\n",
    "          f'{r[\"baseline_time_ms\"]:>10.0f} {r[\"speedup\"]:>7.2f}x')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Visualize: speedup and tokens per round vs K ---\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "speedups = [r['speedup'] for r in results]\n",
    "toks_per_round = [r['avg_tokens_per_round'] for r in results]\n",
    "accepted_per_round = [r['avg_accepted_per_round'] for r in results]\n",
    "\n",
    "# Left plot: speedup vs K\n",
    "ax1.plot(k_values, speedups, color='#34d399', linewidth=2, marker='o', markersize=6)\n",
    "best_k = k_values[np.argmax(speedups)]\n",
    "best_speedup = max(speedups)\n",
    "ax1.axvline(x=best_k, color='#f59e0b', linestyle='--', alpha=0.7,\n",
    "            label=f'Optimal K={best_k} ({best_speedup:.2f}x)')\n",
    "ax1.axhline(y=1.0, color='#f87171', linestyle='--', alpha=0.5, label='No speculation (1x)')\n",
    "ax1.set_xlabel('Draft Length K', fontsize=12)\n",
    "ax1.set_ylabel('Speedup vs Baseline', fontsize=12)\n",
    "ax1.set_title('Speculative Decoding Speedup', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.set_xticks(k_values)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "# Right plot: accepted tokens per round vs K\n",
    "ax2.bar(k_values, accepted_per_round, color='#60a5fa', alpha=0.7, label='Accepted drafts')\n",
    "ax2.bar(k_values, [1] * len(k_values), bottom=accepted_per_round,\n",
    "        color='#a78bfa', alpha=0.7, label='Resampled token (+1)')\n",
    "# Theoretical max line\n",
    "ax2.plot(k_values, [k + 1 for k in k_values], color='#f87171', linestyle='--',\n",
    "         alpha=0.5, label='Theoretical max (all accepted)')\n",
    "ax2.set_xlabel('Draft Length K', fontsize=12)\n",
    "ax2.set_ylabel('Tokens per Round', fontsize=12)\n",
    "ax2.set_title('Tokens Generated per Speculative Round', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.set_xticks(k_values)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nOptimal draft length: K={best_k} giving {best_speedup:.2f}x speedup.')\n",
    "print(f'\\nWhy does speedup eventually decrease with larger K?')\n",
    "print(f'  - More draft tokens = more draft time (K * {DRAFT_TIME_MS} ms)')\n",
    "print(f'  - But acceptance drops geometrically: P(all K match) = {MATCH_PROB}^K')\n",
    "print(f'  - At K=8: P(all match) = {MATCH_PROB**8:.3f} -- unlikely to get all 8.')\n",
    "print(f'  - The draft overhead grows linearly but accepted tokens plateau.')\n",
    "print(f'  - Optimal K balances draft cost against expected accepted tokens.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- How match probability affects optimal K ---\n",
    "#\n",
    "# A better draft model (higher match_prob) allows longer drafts.\n",
    "# A worse draft model should use shorter drafts.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "match_probs = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "colors = ['#f87171', '#fb923c', '#f59e0b', '#34d399', '#60a5fa']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for match_prob, color in zip(match_probs, colors):\n",
    "    speedups_mp = []\n",
    "    for k in k_values:\n",
    "        r = simulate_speculative_decoding(\n",
    "            total_tokens=TOTAL_TOKENS, k=k, match_prob=match_prob,\n",
    "            draft_time_ms=DRAFT_TIME_MS, target_time_ms=TARGET_TIME_MS,\n",
    "        )\n",
    "        speedups_mp.append(r['speedup'])\n",
    "    ax.plot(k_values, speedups_mp, color=color, linewidth=2, marker='o',\n",
    "            markersize=5, label=f'match_prob={match_prob}')\n",
    "\n",
    "ax.axhline(y=1.0, color='#334155', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Draft Length K', fontsize=12)\n",
    "ax.set_ylabel('Speedup vs Baseline', fontsize=12)\n",
    "ax.set_title('Speculative Decoding: Match Probability vs Optimal K',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.set_xticks(k_values)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nKey insight: the optimal K depends on the draft model quality.')\n",
    "print('  - High match probability (good draft model) -> longer drafts are worthwhile')\n",
    "print('  - Low match probability (weak draft model) -> short drafts or no speculation')\n",
    "print('  - Below ~50% match rate, speculation barely helps (draft overhead dominates)')\n",
    "print('\\nThe speed comes from the large model verifying in PARALLEL, not from')\n",
    "print('the small model being fast. A perfect draft model (match_prob=1.0) would')\n",
    "print('give speedup = target_time / (K * draft_time + target_time) * K.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**What just happened:** Speculative decoding trades draft model compute for parallel verification by the target model. The sweet spot depends on two factors: (1) how well the draft model approximates the target (match probability), and (2) the time ratio between draft and target forward passes.\n\nAt 70% match probability, the optimal draft length is around K=3-4. Longer drafts waste time because acceptance drops geometrically—`P(all K match) = 0.7^K`. At K=8, the probability of accepting all 8 is only 5.7%, so most of those draft tokens are wasted effort.\n\nThe critical misconception: the speedup does NOT come from the draft model being fast. It comes from the target model verifying K tokens in a single forward pass instead of generating them one at a time. The target model's forward pass costs roughly the same whether it processes 1 token or 5 (compute-bound matmul).\n\n---\n\n## Exercise 3: Continuous Batching Simulator (Supported)\n\nStatic batching during inference works like batched training: start N requests together, run until ALL finish. The problem: requests have different lengths. Short requests finish early but their batch slots sit idle, consuming memory and producing nothing, until the longest request completes.\n\nContinuous batching fixes this: when a request completes, its slot is immediately filled with the next request from the queue. Like a restaurant waitlist—as a table opens, the next party is seated immediately.\n\nIn this exercise, you'll simulate both strategies on a queue of 50 requests with realistic length distributions and compare GPU utilization.\n\n**Before running, predict:**\n- With static batching (batch_size=8) and requests varying from 10 to 300 tokens, what happens to GPU utilization as short requests finish?\n- Will continuous batching have higher or lower average latency per request than static batching?\n- If the longest request in a static batch takes 300 tokens and the shortest takes 10, what fraction of compute is wasted?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Continuous Batching Simulator ---\n",
    "#\n",
    "# We simulate an inference server processing a queue of requests.\n",
    "# Each request has a target length (number of tokens to generate).\n",
    "# One \"step\" generates one token for every active slot in the batch.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 50 requests with realistic length distribution\n",
    "NUM_REQUESTS = 50\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Lengths: mean 80, std 60, clipped to [10, 300]\n",
    "request_lengths = np.clip(\n",
    "    np.random.normal(loc=80, scale=60, size=NUM_REQUESTS).astype(int),\n",
    "    10, 300\n",
    ")\n",
    "\n",
    "print(f'Generated {NUM_REQUESTS} requests.')\n",
    "print(f'  Length range: {request_lengths.min()} to {request_lengths.max()} tokens')\n",
    "print(f'  Mean length: {request_lengths.mean():.0f} tokens')\n",
    "print(f'  Batch size: {BATCH_SIZE}')\n",
    "print()\n",
    "\n",
    "# Show first 10 request lengths\n",
    "print(f'First 10 request lengths: {request_lengths[:10].tolist()}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Static Batching ---\n",
    "#\n",
    "# Process requests in fixed batches. Each batch waits for ALL requests\n",
    "# to complete before the next batch starts.\n",
    "\n",
    "def simulate_static_batching(\n",
    "    request_lengths: np.ndarray,\n",
    "    batch_size: int,\n",
    ") -> dict:\n",
    "    \"\"\"Simulate static batching.\n",
    "\n",
    "    Returns:\n",
    "        dict with total_steps, utilization history, and per-request latencies.\n",
    "    \"\"\"\n",
    "    n = len(request_lengths)\n",
    "    utilization_history = []  # fraction of batch slots doing useful work at each step\n",
    "    request_latencies = []    # total time each request waits (from batch start to batch end)\n",
    "    total_steps = 0\n",
    "\n",
    "    idx = 0  # next request to assign\n",
    "    while idx < n:\n",
    "        # Fill the batch\n",
    "        batch_end = min(idx + batch_size, n)\n",
    "        batch_lengths = request_lengths[idx:batch_end]\n",
    "        batch_actual_size = len(batch_lengths)\n",
    "        max_len = batch_lengths.max()\n",
    "\n",
    "        # Run the batch for max_len steps\n",
    "        for step in range(max_len):\n",
    "            # How many requests are still active at this step?\n",
    "            active = int(np.sum(batch_lengths > step))\n",
    "            utilization_history.append(active / batch_size)\n",
    "\n",
    "        total_steps += max_len\n",
    "\n",
    "        # All requests in this batch have latency = max_len\n",
    "        # (they all wait until the longest finishes)\n",
    "        for length in batch_lengths:\n",
    "            request_latencies.append(max_len)\n",
    "\n",
    "        idx = batch_end\n",
    "\n",
    "    return {\n",
    "        'total_steps': total_steps,\n",
    "        'utilization_history': utilization_history,\n",
    "        'request_latencies': request_latencies,\n",
    "        'avg_utilization': np.mean(utilization_history),\n",
    "        'avg_latency': np.mean(request_latencies),\n",
    "    }\n",
    "\n",
    "\n",
    "static_result = simulate_static_batching(request_lengths, BATCH_SIZE)\n",
    "print(f'=== Static Batching ===')\n",
    "print(f'Total steps: {static_result[\"total_steps\"]}')\n",
    "print(f'Average utilization: {static_result[\"avg_utilization\"]:.1%}')\n",
    "print(f'Average latency per request: {static_result[\"avg_latency\"]:.0f} steps')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Continuous Batching ---\n",
    "#\n",
    "# When a request completes, its slot is immediately filled from the queue.\n",
    "# The batch stays full (or as full as possible) at all times.\n",
    "\n",
    "def simulate_continuous_batching(\n",
    "    request_lengths: np.ndarray,\n",
    "    batch_size: int,\n",
    ") -> dict:\n",
    "    \"\"\"Simulate continuous batching.\n",
    "\n",
    "    Each slot independently tracks its request's remaining tokens.\n",
    "    When a request completes, the slot is filled from the queue.\n",
    "\n",
    "    Returns:\n",
    "        dict with total_steps, utilization history, and per-request latencies.\n",
    "    \"\"\"\n",
    "    n = len(request_lengths)\n",
    "    utilization_history = []\n",
    "    # Track when each request starts and finishes for latency calculation\n",
    "    request_start_step = [0] * n\n",
    "    request_end_step = [0] * n\n",
    "\n",
    "    # Initialize slots: each slot has (request_index, remaining_tokens)\n",
    "    # Fill initial batch from the queue\n",
    "    queue_idx = 0  # next request to pull from queue\n",
    "    slots = []     # list of (request_index, remaining_tokens)\n",
    "\n",
    "    # TODO: Fill initial batch slots from the queue.\n",
    "    # For each slot up to batch_size (or until queue is empty):\n",
    "    #   1. Assign request queue_idx to this slot\n",
    "    #   2. Record request_start_step[queue_idx] = 0 (starts at step 0)\n",
    "    #   3. Append (queue_idx, request_lengths[queue_idx]) to slots\n",
    "    #   4. Increment queue_idx\n",
    "\n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "    step = 0\n",
    "    while len(slots) > 0:\n",
    "        # Record utilization for this step\n",
    "        utilization_history.append(len(slots) / batch_size)\n",
    "\n",
    "        # TODO: Process one step and handle completions:\n",
    "        # 1. Decrement remaining tokens for each slot\n",
    "        # 2. Find completed slots (remaining <= 0)\n",
    "        # 3. For completed requests: record request_end_step\n",
    "        # 4. Replace completed slots with new requests from the queue\n",
    "        #    (record their request_start_step)\n",
    "        # 5. Remove any empty slots (no more requests to fill)\n",
    "        #\n",
    "        # Hint: process the step by creating a new slots list:\n",
    "        #   new_slots = []\n",
    "        #   for (req_idx, remaining) in slots:\n",
    "        #       remaining -= 1\n",
    "        #       if remaining <= 0:   # completed\n",
    "        #           request_end_step[req_idx] = step + 1\n",
    "        #           if queue_idx < n:  # fill slot from queue\n",
    "        #               request_start_step[queue_idx] = step + 1\n",
    "        #               new_slots.append((queue_idx, request_lengths[queue_idx]))\n",
    "        #               queue_idx += 1\n",
    "        #       else:  # still running\n",
    "        #           new_slots.append((req_idx, remaining))\n",
    "        #   slots = new_slots\n",
    "\n",
    "        pass  # Replace with your implementation\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    request_latencies = [\n",
    "        request_end_step[i] - request_start_step[i]\n",
    "        for i in range(n)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        'total_steps': step,\n",
    "        'utilization_history': utilization_history,\n",
    "        'request_latencies': request_latencies,\n",
    "        'avg_utilization': np.mean(utilization_history),\n",
    "        'avg_latency': np.mean(request_latencies),\n",
    "    }\n",
    "\n",
    "\n",
    "continuous_result = simulate_continuous_batching(request_lengths, BATCH_SIZE)\n",
    "print(f'=== Continuous Batching ===')\n",
    "print(f'Total steps: {continuous_result[\"total_steps\"]}')\n",
    "print(f'Average utilization: {continuous_result[\"avg_utilization\"]:.1%}')\n",
    "print(f'Average latency per request: {continuous_result[\"avg_latency\"]:.0f} steps')\n",
    "print()\n",
    "print(f'=== Comparison ===')\n",
    "print(f'{\"Metric\":<25} {\"Static\":>10} {\"Continuous\":>12}')\n",
    "print('-' * 50)\n",
    "print(f'{\"Total steps\":<25} {static_result[\"total_steps\"]:>10} {continuous_result[\"total_steps\"]:>12}')\n",
    "print(f'{\"Avg utilization\":<25} {static_result[\"avg_utilization\"]:>9.1%} {continuous_result[\"avg_utilization\"]:>11.1%}')\n",
    "print(f'{\"Avg latency (steps)\":<25} {static_result[\"avg_latency\"]:>10.0f} {continuous_result[\"avg_latency\"]:>12.0f}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\nThe key insight is that continuous batching treats each batch slot independently. When a request completes, its slot is immediately available for the next request from the queue—like a restaurant seating the next party as soon as a table opens.\n\n```python\n# Fill initial batch\nfor _ in range(min(batch_size, n)):\n    request_start_step[queue_idx] = 0\n    slots.append((queue_idx, request_lengths[queue_idx]))\n    queue_idx += 1\n\n# Process one step\nnew_slots = []\nfor (req_idx, remaining) in slots:\n    remaining -= 1\n    if remaining <= 0:\n        request_end_step[req_idx] = step + 1\n        if queue_idx < n:\n            request_start_step[queue_idx] = step + 1\n            new_slots.append((queue_idx, request_lengths[queue_idx]))\n            queue_idx += 1\n    else:\n        new_slots.append((req_idx, remaining))\nslots = new_slots\n```\n\nThe improvement is in *slot utilization*, not batch size. Static batching has 8 slots but many sit idle after their request finishes. Continuous batching keeps slots filled, so GPU utilization stays near 100% until the queue is nearly empty.\n\nNote that continuous batching does not change the latency for any individual request—each request still takes exactly as many steps as its length. The improvement is in *throughput*: total steps to serve all requests decreases because GPU cycles are not wasted on empty slots.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper: Working Continuous Batching Simulator\n",
    "\n",
    "**Run the cell below** to get working implementations for the visualization. If your implementation above works, this redefines the same function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Reference implementation ---\n",
    "\n",
    "def simulate_continuous_batching(\n",
    "    request_lengths: np.ndarray,\n",
    "    batch_size: int,\n",
    ") -> dict:\n",
    "    \"\"\"Simulate continuous batching.\"\"\"\n",
    "    n = len(request_lengths)\n",
    "    utilization_history = []\n",
    "    request_start_step = [0] * n\n",
    "    request_end_step = [0] * n\n",
    "\n",
    "    queue_idx = 0\n",
    "    slots = []\n",
    "\n",
    "    for _ in range(min(batch_size, n)):\n",
    "        request_start_step[queue_idx] = 0\n",
    "        slots.append((queue_idx, int(request_lengths[queue_idx])))\n",
    "        queue_idx += 1\n",
    "\n",
    "    step = 0\n",
    "    while len(slots) > 0:\n",
    "        utilization_history.append(len(slots) / batch_size)\n",
    "\n",
    "        new_slots = []\n",
    "        for (req_idx, remaining) in slots:\n",
    "            remaining -= 1\n",
    "            if remaining <= 0:\n",
    "                request_end_step[req_idx] = step + 1\n",
    "                if queue_idx < n:\n",
    "                    request_start_step[queue_idx] = step + 1\n",
    "                    new_slots.append((queue_idx, int(request_lengths[queue_idx])))\n",
    "                    queue_idx += 1\n",
    "            else:\n",
    "                new_slots.append((req_idx, remaining))\n",
    "        slots = new_slots\n",
    "        step += 1\n",
    "\n",
    "    request_latencies = [\n",
    "        request_end_step[i] - request_start_step[i]\n",
    "        for i in range(n)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        'total_steps': step,\n",
    "        'utilization_history': utilization_history,\n",
    "        'request_latencies': request_latencies,\n",
    "        'avg_utilization': np.mean(utilization_history),\n",
    "        'avg_latency': np.mean(request_latencies),\n",
    "    }\n",
    "\n",
    "\n",
    "# Recompute with reference implementation\n",
    "static_result = simulate_static_batching(request_lengths, BATCH_SIZE)\n",
    "continuous_result = simulate_continuous_batching(request_lengths, BATCH_SIZE)\n",
    "\n",
    "print(f'Reference continuous batching loaded.')\n",
    "print(f'Static total steps: {static_result[\"total_steps\"]}, '\n",
    "      f'utilization: {static_result[\"avg_utilization\"]:.1%}')\n",
    "print(f'Continuous total steps: {continuous_result[\"total_steps\"]}, '\n",
    "      f'utilization: {continuous_result[\"avg_utilization\"]:.1%}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Visualize: utilization over time for both strategies ---\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=False)\n",
    "\n",
    "# Top: Static batching utilization\n",
    "static_util = static_result['utilization_history']\n",
    "ax1.fill_between(range(len(static_util)), static_util, alpha=0.3, color='#f87171')\n",
    "ax1.plot(range(len(static_util)), static_util, color='#f87171', linewidth=1)\n",
    "ax1.axhline(y=static_result['avg_utilization'], color='#f87171', linestyle='--',\n",
    "            alpha=0.7, label=f'Average: {static_result[\"avg_utilization\"]:.1%}')\n",
    "ax1.set_ylabel('GPU Utilization', fontsize=11)\n",
    "ax1.set_title('Static Batching: Utilization Over Time', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylim(0, 1.1)\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "# Bottom: Continuous batching utilization\n",
    "cont_util = continuous_result['utilization_history']\n",
    "ax2.fill_between(range(len(cont_util)), cont_util, alpha=0.3, color='#34d399')\n",
    "ax2.plot(range(len(cont_util)), cont_util, color='#34d399', linewidth=1)\n",
    "ax2.axhline(y=continuous_result['avg_utilization'], color='#34d399', linestyle='--',\n",
    "            alpha=0.7, label=f'Average: {continuous_result[\"avg_utilization\"]:.1%}')\n",
    "ax2.set_xlabel('Step', fontsize=11)\n",
    "ax2.set_ylabel('GPU Utilization', fontsize=11)\n",
    "ax2.set_title('Continuous Batching: Utilization Over Time', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylim(0, 1.1)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Static batching: utilization drops in a sawtooth pattern as short requests finish.')\n",
    "print('Each batch starts at 100% and decays. GPU is doing wasted work on empty slots.')\n",
    "print()\n",
    "print('Continuous batching: utilization stays near 100% for most of the run.')\n",
    "print('It only drops at the very end when the queue empties and remaining requests trickle out.')\n",
    "print()\n",
    "print(f'Total steps -- Static: {static_result[\"total_steps\"]}, '\n",
    "      f'Continuous: {continuous_result[\"total_steps\"]}')\n",
    "print(f'Throughput improvement: '\n",
    "      f'{static_result[\"total_steps\"] / continuous_result[\"total_steps\"]:.2f}x fewer steps '\n",
    "      f'to serve all {NUM_REQUESTS} requests.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Wasted compute analysis ---\n",
    "\n",
    "# In static batching, compute useful tokens and wasted tokens per batch\n",
    "total_useful_tokens = int(request_lengths.sum())\n",
    "\n",
    "# Static: total tokens generated = total steps * batch_size\n",
    "# (each step generates for batch_size slots, whether useful or not)\n",
    "static_total_tokens = sum(\n",
    "    int(request_lengths[i:min(i + BATCH_SIZE, len(request_lengths))].max()) * BATCH_SIZE\n",
    "    for i in range(0, len(request_lengths), BATCH_SIZE)\n",
    ")\n",
    "static_wasted = static_total_tokens - total_useful_tokens\n",
    "\n",
    "# Continuous: useful work = total useful tokens, wasted only at tail end\n",
    "continuous_total_tokens = sum(\n",
    "    int(len(slots_at_step) )\n",
    "    for slots_at_step in [[1]] * continuous_result['total_steps']  # approximate\n",
    ")\n",
    "# Better calculation: sum of utilization * batch_size at each step\n",
    "continuous_total_work = sum(\n",
    "    u * BATCH_SIZE for u in continuous_result['utilization_history']\n",
    ")\n",
    "continuous_wasted = continuous_total_work - total_useful_tokens\n",
    "\n",
    "print(f'=== Compute Waste Analysis ===')\n",
    "print(f'Total useful tokens to generate: {total_useful_tokens}')\n",
    "print()\n",
    "print(f'Static batching:')\n",
    "print(f'  Total GPU-token-steps: {static_total_tokens}')\n",
    "print(f'  Wasted: {static_wasted} ({static_wasted / static_total_tokens:.1%})')\n",
    "print(f'  These are forward passes that produce nothing -- the GPU computes')\n",
    "print(f'  attention, runs the FFN, generates a token... and throws it away.')\n",
    "print()\n",
    "print(f'Continuous batching:')\n",
    "print(f'  Total GPU-token-steps: {continuous_total_work:.0f}')\n",
    "print(f'  Wasted: {continuous_wasted:.0f} ({continuous_wasted / continuous_total_work:.1%})')\n",
    "print(f'  Minimal waste -- only at the tail end when the queue empties.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**What just happened:** Static batching wastes compute because short requests finish early but their batch slots remain occupied. The utilization graph shows the sawtooth pattern: each batch starts at 100% and drops as requests complete. The batch cannot move on until the longest request finishes.\n\nContinuous batching eliminates this waste by treating each slot independently. When a request completes at step 20, its slot is immediately filled with the next request from the queue. The GPU stays busy producing useful tokens instead of empty ones.\n\nThe improvement is in *slot utilization*, not batch size. Both strategies use the same batch size (8). The difference is what happens to completed slots. Static batching leaves them empty. Continuous batching fills them.\n\n---\n\n## Exercise 4: Parallelism Strategy Advisor (Independent)\n\nThroughout this lesson, you learned three parallelism strategies—data, tensor, and pipeline—plus ZeRO optimizer sharding. Each addresses a different bottleneck. The choice depends on whether the model fits on one GPU, how many GPUs you have, and how much memory is available.\n\n**Your task:** Build a function `recommend_parallelism(config)` that takes a model configuration and hardware setup, then recommends a parallelism strategy.\n\nThe function should:\n1. Compute total training memory (12 bytes/param for mixed-precision Adam)\n2. Determine if data parallelism alone works (full model fits on one GPU)\n3. Check if ZeRO stages reduce per-GPU memory enough\n4. If not, recommend tensor and/or pipeline parallelism\n5. Output a clear recommendation with reasoning\n\nTest on these five configurations:\n- **(a)** GPT-2 (124M params), 1 GPU, 80 GB\n- **(b)** GPT-2 (124M params), 4 GPUs, 80 GB each\n- **(c)** LLaMA 7B, 4 GPUs, 80 GB each\n- **(d)** LLaMA 70B, 8 GPUs, 80 GB each\n- **(e)** Hypothetical 175B model, 64 GPUs, 80 GB each\n\n**Decision logic to implement:**\n- Training memory per param = 12 bytes (bf16 weights + bf16 grads + fp32 Adam)\n- If total training memory fits on one GPU: data parallelism (replicate model, split data)\n- If not but ZeRO Stage 3 on all GPUs makes it fit: ZeRO + data parallelism\n- If not: need model parallelism. Compute minimum tensor/pipeline parallelism degree.\n  - Tensor parallelism degree = minimum GPUs to split each layer so it fits in memory\n  - Pipeline parallelism degree = remaining GPUs for layer splitting\n  - Data parallelism degree = any remaining GPUs\n\n**No skeleton is provided.** Design the function and output format yourself.\n\n**Before running, predict:**\n- GPT-2 (124M) needs ~1.5 GB for training. What strategy for 1 GPU? For 4 GPUs?\n- LLaMA 70B needs 840 GB. With 8 GPUs and ZeRO Stage 3, per-GPU is ~105 GB. Does it fit?\n- The 175B model needs ~2.1 TB. How many GPUs would ZeRO Stage 3 alone require?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Your parallelism strategy advisor here.\n",
    "#\n",
    "# Suggested approach:\n",
    "#\n",
    "# 1. Define a config (dataclass or dict) with:\n",
    "#    name, num_params, num_gpus, gpu_memory_gb\n",
    "#\n",
    "# 2. Write recommend_parallelism(config) that:\n",
    "#    a. Computes total training memory = num_params * 12 bytes\n",
    "#    b. Checks if single-GPU works (total <= gpu_memory)\n",
    "#    c. Checks ZeRO stages (shard optimizer, gradients, weights)\n",
    "#    d. If ZeRO alone doesn't suffice, compute model parallelism needs\n",
    "#    e. Returns a recommendation with reasoning\n",
    "#\n",
    "# 3. Test on configurations (a) through (e)\n",
    "#\n",
    "# 4. Print clear, formatted output showing:\n",
    "#    - Model size and total training memory\n",
    "#    - Hardware (GPUs, memory per GPU)\n",
    "#    - Recommendation: which strategies and why\n",
    "#    - Per-GPU memory after applying the recommendation\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The key insight is that parallelism strategy selection is a decision tree driven by one question: does the model's training memory fit on the available GPUs? Each strategy addresses a different bottleneck: data parallelism scales throughput when the model fits, ZeRO reduces per-GPU memory by sharding, and tensor/pipeline parallelism split the model itself when even sharding is not enough.\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class HardwareConfig:\n",
    "    name: str\n",
    "    num_params: float\n",
    "    num_gpus: int\n",
    "    gpu_memory_gb: float\n",
    "\n",
    "\n",
    "def recommend_parallelism(cfg: HardwareConfig) -> None:\n",
    "    \"\"\"Recommend parallelism strategy and print analysis.\"\"\"\n",
    "    BYTES_PER_PARAM = 12  # bf16 weights + bf16 grads + fp32 Adam\n",
    "    BYTES_PER_PARAM_WEIGHTS = 2\n",
    "    BYTES_PER_PARAM_GRADS = 2\n",
    "    BYTES_PER_PARAM_OPT = 8\n",
    "\n",
    "    total_memory = cfg.num_params * BYTES_PER_PARAM\n",
    "    gpu_memory_bytes = cfg.gpu_memory_gb * 1e9\n",
    "\n",
    "    print(f'=== {cfg.name} ===')\n",
    "    print(f'  Parameters: {format_number(cfg.num_params)}')\n",
    "    print(f'  Training memory: {format_bytes(total_memory)}')\n",
    "    print(f'  Hardware: {cfg.num_gpus} GPU(s) x {cfg.gpu_memory_gb} GB')\n",
    "    print(f'  Total GPU memory: {format_bytes(cfg.num_gpus * gpu_memory_bytes)}')\n",
    "    print()\n",
    "\n",
    "    # Check 1: Does the model fit on a single GPU?\n",
    "    if total_memory <= gpu_memory_bytes:\n",
    "        print(f'  Strategy: DATA PARALLELISM')\n",
    "        print(f'  Reason: Full model ({format_bytes(total_memory)}) fits on one GPU '\n",
    "              f'({cfg.gpu_memory_gb} GB).')\n",
    "        if cfg.num_gpus > 1:\n",
    "            print(f'  Benefit: {cfg.num_gpus}x throughput from {cfg.num_gpus}-way data parallelism.')\n",
    "        print(f'  Per-GPU memory: {format_bytes(total_memory)}')\n",
    "        print()\n",
    "        return\n",
    "\n",
    "    # Check 2: Does ZeRO help enough?\n",
    "    for stage, stage_name in [(1, 'Stage 1'), (2, 'Stage 2'), (3, 'Stage 3')]:\n",
    "        w = cfg.num_params * BYTES_PER_PARAM_WEIGHTS\n",
    "        g = cfg.num_params * BYTES_PER_PARAM_GRADS\n",
    "        o = cfg.num_params * BYTES_PER_PARAM_OPT\n",
    "        if stage >= 1:\n",
    "            o = o / cfg.num_gpus\n",
    "        if stage >= 2:\n",
    "            g = g / cfg.num_gpus\n",
    "        if stage >= 3:\n",
    "            w = w / cfg.num_gpus\n",
    "        per_gpu = w + g + o\n",
    "\n",
    "        if per_gpu <= gpu_memory_bytes:\n",
    "            print(f'  Strategy: ZeRO {stage_name} + DATA PARALLELISM')\n",
    "            print(f'  Reason: Full model does not fit on one GPU '\n",
    "                  f'({format_bytes(total_memory)} > {cfg.gpu_memory_gb} GB).')\n",
    "            print(f'  ZeRO {stage_name} shards '\n",
    "                  f'{\"optimizer states\" if stage == 1 else \"optimizer + gradients\" if stage == 2 else \"everything\"} '\n",
    "                  f'across {cfg.num_gpus} GPUs.')\n",
    "            print(f'  Per-GPU memory: {format_bytes(per_gpu)} '\n",
    "                  f'(weights: {format_bytes(w)}, grads: {format_bytes(g)}, opt: {format_bytes(o)})')\n",
    "            print()\n",
    "            return\n",
    "\n",
    "    # Check 3: Need model parallelism\n",
    "    # Even ZeRO Stage 3 is not enough. Need to split the model.\n",
    "    # Find minimum tensor parallelism degree so per-GPU fits.\n",
    "    # With tensor parallelism degree T and ZeRO Stage 3 on all GPUs:\n",
    "    #   Each GPU holds 1/T of the weights, 1/T of the gradients,\n",
    "    #   and 1/num_gpus of the optimizer states.\n",
    "    #   But tensor parallelism also means each GPU only handles 1/T of compute.\n",
    "    # For memory estimation with combined strategies:\n",
    "    #   Tensor-parallel degree T, pipeline-parallel degree P,\n",
    "    #   data-parallel degree D, where T * P * D = num_gpus.\n",
    "    #   Each GPU stores 1/(T*P) of weights+grads, 1/(T*P*D) of optimizer via ZeRO.\n",
    "\n",
    "    best_config = None\n",
    "    for tp in [1, 2, 4, 8]:\n",
    "        for pp in [1, 2, 4, 8, 16]:\n",
    "            if tp * pp > cfg.num_gpus:\n",
    "                continue\n",
    "            dp = cfg.num_gpus // (tp * pp)\n",
    "            if dp < 1:\n",
    "                continue\n",
    "            if tp * pp * dp != cfg.num_gpus:\n",
    "                continue\n",
    "\n",
    "            # Memory per GPU with this configuration + ZeRO Stage 3:\n",
    "            model_shard = 1.0 / (tp * pp)  # fraction of model per GPU\n",
    "            w = cfg.num_params * model_shard * BYTES_PER_PARAM_WEIGHTS\n",
    "            g = cfg.num_params * model_shard * BYTES_PER_PARAM_GRADS\n",
    "            # ZeRO Stage 3 shards optimizer across data-parallel replicas\n",
    "            o = cfg.num_params * model_shard * BYTES_PER_PARAM_OPT / dp\n",
    "            per_gpu = w + g + o\n",
    "\n",
    "            if per_gpu <= gpu_memory_bytes:\n",
    "                if best_config is None or tp + pp < best_config[0] + best_config[1]:\n",
    "                    best_config = (tp, pp, dp, per_gpu, w, g, o)\n",
    "\n",
    "    if best_config is None:\n",
    "        print(f'  Strategy: INSUFFICIENT RESOURCES')\n",
    "        print(f'  Reason: {cfg.num_gpus} GPUs with {cfg.gpu_memory_gb} GB each '\n",
    "              f'cannot accommodate {format_bytes(total_memory)} training memory.')\n",
    "        print(f'  Need more GPUs or GPUs with more memory.')\n",
    "        print()\n",
    "        return\n",
    "\n",
    "    tp, pp, dp, per_gpu, w, g, o = best_config\n",
    "    strategies = []\n",
    "    if tp > 1:\n",
    "        strategies.append(f'Tensor Parallelism (degree {tp})')\n",
    "    if pp > 1:\n",
    "        strategies.append(f'Pipeline Parallelism (degree {pp})')\n",
    "    if dp > 1:\n",
    "        strategies.append(f'Data Parallelism (degree {dp})')\n",
    "    strategies.append('ZeRO Stage 3')\n",
    "\n",
    "    print(f'  Strategy: {\" + \".join(strategies)}')\n",
    "    print(f'  Reason: Model does not fit even with ZeRO alone. '\n",
    "          f'Need to split the model across GPUs.')\n",
    "    print(f'  Layout: {tp}x tensor x {pp}x pipeline x {dp}x data = '\n",
    "          f'{tp * pp * dp} GPUs')\n",
    "    print(f'  Per-GPU memory: {format_bytes(per_gpu)} '\n",
    "          f'(weights: {format_bytes(w)}, grads: {format_bytes(g)}, opt: {format_bytes(o)})')\n",
    "    print()\n",
    "\n",
    "\n",
    "# --- Test on five configurations ---\n",
    "\n",
    "configs = [\n",
    "    HardwareConfig('(a) GPT-2, 1 GPU', 124e6, 1, 80),\n",
    "    HardwareConfig('(b) GPT-2, 4 GPUs', 124e6, 4, 80),\n",
    "    HardwareConfig('(c) LLaMA 7B, 4 GPUs', 7e9, 4, 80),\n",
    "    HardwareConfig('(d) LLaMA 70B, 8 GPUs', 70e9, 8, 80),\n",
    "    HardwareConfig('(e) 175B Model, 64 GPUs', 175e9, 64, 80),\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "    recommend_parallelism(cfg)\n",
    "\n",
    "print('=== Summary ===')\n",
    "print('The choice of parallelism strategy is determined by the bottleneck:')\n",
    "print('  - Model fits on 1 GPU? -> Data parallelism (scale throughput)')\n",
    "print('  - Model too large? -> ZeRO shards optimizer/gradient memory')\n",
    "print('  - Still too large? -> Tensor + pipeline parallelism split the model')\n",
    "print('  - Communication overhead shapes every decision.')\n",
    "```\n",
    "\n",
    "**Design choices:**\n",
    "- The advisor follows a clear priority: data parallelism first (simplest), then ZeRO (reduces memory without splitting the model), then model parallelism (most complex, highest communication overhead).\n",
    "- For model parallelism, we search over tensor and pipeline degrees to find the minimum split that fits in memory. Tensor parallelism is preferred for fewer GPUs (lower communication overhead within a node) and pipeline parallelism for more GPUs (lower frequency communication between stages).\n",
    "- The `T * P * D = num_gpus` constraint ensures all GPUs are utilized.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **Training memory is dominated by optimizer states (2/3 of total).** A 70B model needs ~840 GB for mixed-precision Adam training. ZeRO targets the biggest cost first by sharding optimizer states across GPUs, but even full sharding (Stage 3) may not be enough for the largest models.\n\n2. **Three parallelism strategies address three different bottlenecks.** Data parallelism splits data (throughput scaling when the model fits). Tensor parallelism splits layers (when individual layers are too large). Pipeline parallelism splits blocks across GPUs (when the model has too many layers). Frontier models combine all three.\n\n3. **Speculative decoding turns sequential generation into parallel verification.** The speedup comes from the target model verifying K draft tokens in one forward pass, not from the draft model being fast. Optimal draft length K depends on the match probability—diminishing returns at larger K because acceptance drops geometrically.\n\n4. **Continuous batching eliminates wasted compute from static batching.** By filling completed slots immediately from a request queue, GPU utilization stays near 100% instead of decaying as short requests finish. The improvement is in slot utilization, not batch size.\n\n5. **Communication overhead is the constraint that shapes every parallelism decision.** Moving data between GPUs is orders of magnitude slower than computing on it. Every strategy in this lesson is a different answer to the same question: how do you distribute work across devices when communication is expensive?"
  }
 ]
}