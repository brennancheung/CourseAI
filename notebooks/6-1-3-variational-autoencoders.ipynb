{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "In this notebook, you'll convert a regular autoencoder into a VAE and see the generative payoff.\n",
    "\n",
    "**What you'll do:**\n",
    "- Implement the reparameterization trick and verify gradients flow through sampling\n",
    "- Convert an autoencoder to a VAE by changing the encoder to output mu + log_var\n",
    "- Implement the VAE loss function (reconstruction + KL divergence) and train the model\n",
    "- Compare the VAE's latent space to the autoencoder's â€” see how KL regularization fills the gaps\n",
    "- Generate new Fashion-MNIST items by sampling from the latent space\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones â€” they reveal gaps in your mental model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup â€” use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Download Fashion-MNIST\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "CLASS_NAMES = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f\"Training set: {len(train_dataset)} images\")\n",
    "print(f\"Test set: {len(test_dataset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Helpers\n",
    "\n",
    "A regular autoencoder for comparison. This is the same architecture from the Autoencoders notebook â€” encoder compresses to a single point in latent space, decoder reconstructs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 2  # Using 2D so we can visualize the latent space\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"Regular autoencoder â€” encodes each image to a single point.\"\"\"\n",
    "    def __init__(self, latent_dim=LATENT_DIM):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 7 * 7, latent_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32 * 7 * 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (32, 7, 7)),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "def train_autoencoder(model, train_loader, num_epochs=10, lr=1e-3):\n",
    "    \"\"\"Train a regular autoencoder with MSE reconstruction loss.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            recon = model(images)\n",
    "            loss = F.mse_loss(recon, images, reduction='sum') / images.size(0)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    return losses\n",
    "\n",
    "\n",
    "# Train the regular autoencoder (we'll compare to the VAE later)\n",
    "print(\"Training regular autoencoder...\")\n",
    "ae_model = Autoencoder(LATENT_DIM).to(device)\n",
    "ae_losses = train_autoencoder(ae_model, train_loader, num_epochs=10)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: The Reparameterization Trick (Guided)\n",
    "\n",
    "The VAE encodes each image to a distribution (mu + sigma), then **samples** z from that distribution during training. But sampling is random â€” how do gradients flow backward through randomness?\n",
    "\n",
    "The trick: instead of sampling z directly from N(mu, sigma^2), we sample epsilon from N(0, 1) and compute:\n",
    "\n",
    "$$z = \\mu + \\sigma \\cdot \\epsilon$$\n",
    "\n",
    "Now z is a deterministic function of mu and sigma (the learnable parts). The randomness is isolated in epsilon, which is just input noise. Gradients flow through mu and sigma normally.\n",
    "\n",
    "**Before running, predict:** If we sample z directly from N(mu, sigma^2), will `mu.grad` be None or have a value? What about with the reparameterization trick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt 1: Sample z directly (the naive way)\n",
    "mu = torch.tensor([1.0, -0.5], requires_grad=True)\n",
    "logvar = torch.tensor([0.0, 0.0], requires_grad=True)  # log(1.0) = 0, so sigma = 1\n",
    "\n",
    "# This creates a Normal distribution and samples from it\n",
    "# But torch.distributions sampling breaks the gradient path\n",
    "sigma = torch.exp(0.5 * logvar)\n",
    "dist = torch.distributions.Normal(mu, sigma)\n",
    "z_direct = dist.sample()  # No gradient connection!\n",
    "\n",
    "# Try to backprop through it\n",
    "loss_direct = z_direct.sum()\n",
    "loss_direct.backward()\n",
    "\n",
    "print(\"=== Direct sampling (naive) ===\")\n",
    "print(f\"z = {z_direct.detach().numpy()}\")\n",
    "print(f\"mu.grad = {mu.grad}\")\n",
    "print(f\"Gradient flows to mu? {mu.grad is not None and mu.grad.abs().sum() > 0}\")\n",
    "print()\n",
    "\n",
    "# Reset gradients\n",
    "mu = torch.tensor([1.0, -0.5], requires_grad=True)\n",
    "logvar = torch.tensor([0.0, 0.0], requires_grad=True)\n",
    "\n",
    "# Attempt 2: Reparameterization trick\n",
    "sigma = torch.exp(0.5 * logvar)\n",
    "eps = torch.randn_like(sigma)           # Sample noise (no learnable params)\n",
    "z_reparam = mu + sigma * eps            # Deterministic function of mu, sigma\n",
    "\n",
    "loss_reparam = z_reparam.sum()\n",
    "loss_reparam.backward()\n",
    "\n",
    "print(\"=== Reparameterization trick ===\")\n",
    "print(f\"z = {z_reparam.detach().numpy()}\")\n",
    "print(f\"mu.grad = {mu.grad}\")\n",
    "print(f\"logvar.grad = {logvar.grad}\")\n",
    "print(f\"Gradient flows to mu? {mu.grad is not None and mu.grad.abs().sum() > 0}\")\n",
    "print(f\"Gradient flows to logvar? {logvar.grad is not None and logvar.grad.abs().sum() > 0}\")\n",
    "print()\n",
    "print(\"The trick: isolate randomness in eps (not learnable),\")\n",
    "print(\"compute z = mu + sigma * eps (deterministic in mu, sigma).\")\n",
    "print(\"Gradients flow through mu and sigma normally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:**\n",
    "\n",
    "- Direct sampling (`dist.sample()`) creates a value that has no gradient connection to the distribution parameters. Calling `.backward()` gives `mu.grad = None` â€” the gradient path is broken.\n",
    "- The reparameterization trick reformulates sampling as `z = mu + sigma * eps`. Since `eps` is just a constant (from PyTorch's perspective), `z` is a differentiable function of `mu` and `sigma`. Gradients flow.\n",
    "\n",
    "This is the clever engineering trick that makes VAEs trainable. Without it, we couldn't backpropagate through the sampling step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Convert the Autoencoder to a VAE (Guided)\n",
    "\n",
    "Now we convert the autoencoder into a VAE. There are exactly **three changes**:\n",
    "\n",
    "1. The encoder outputs `mu` and `logvar` instead of a single point `z`\n",
    "2. The `reparameterize()` method samples `z` from the distribution\n",
    "3. `forward()` returns `recon, mu, logvar` (the loss function needs mu and logvar)\n",
    "\n",
    "Everything else â€” the conv layers, the decoder â€” is identical to the autoencoder.\n",
    "\n",
    "**Before running, predict:** The autoencoder's encoder ends with `nn.Linear(32*7*7, latent_dim)` â€” one linear layer producing one vector. What does the VAE's encoder need instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=LATENT_DIM):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder: same CNN backbone as the autoencoder\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # CHANGE 1: Two output heads instead of one\n",
    "        # The autoencoder had: nn.Linear(32*7*7, latent_dim) -> one point\n",
    "        # The VAE has: two separate linear layers -> mu and logvar\n",
    "        self.fc_mu = nn.Linear(32 * 7 * 7, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(32 * 7 * 7, latent_dim)\n",
    "\n",
    "        # Decoder: identical to the autoencoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32 * 7 * 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (32, 7, 7)),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    # CHANGE 2: Reparameterization trick (from Exercise 1)\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)    # sigma = exp(0.5 * log(sigma^2))\n",
    "        eps = torch.randn_like(std)       # epsilon ~ N(0, 1)\n",
    "        return mu + std * eps             # z = mu + sigma * epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode to distribution parameters\n",
    "        h = self.encoder_conv(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "\n",
    "        # Sample z using reparameterization trick\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Decode\n",
    "        recon = self.decoder(z)\n",
    "\n",
    "        # CHANGE 3: Return mu and logvar too (the loss needs them)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode a latent vector without encoding first. For generation.\"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "# Verify the architecture\n",
    "vae_model = VAE(LATENT_DIM).to(device)\n",
    "\n",
    "# Test with a single batch\n",
    "test_images, _ = next(iter(test_loader))\n",
    "test_images = test_images.to(device)\n",
    "recon, mu, logvar = vae_model(test_images)\n",
    "\n",
    "print(f\"Input shape:   {test_images.shape}\")\n",
    "print(f\"Recon shape:   {recon.shape}\")\n",
    "print(f\"mu shape:      {mu.shape}\")\n",
    "print(f\"logvar shape:  {logvar.shape}\")\n",
    "print()\n",
    "print(f\"mu range:      [{mu.min().item():.3f}, {mu.max().item():.3f}]\")\n",
    "print(f\"logvar range:  [{logvar.min().item():.3f}, {logvar.max().item():.3f}]\")\n",
    "print()\n",
    "print(\"The encoder now outputs a cloud (mu + logvar) for each image,\")\n",
    "print(\"not a single point. Each image is described by a distribution,\")\n",
    "print(\"not a location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:**\n",
    "\n",
    "The autoencoder's encoder ended with one linear layer: `image -> z` (a point). The VAE's encoder ends with **two** linear layers: `image -> mu` and `image -> logvar` (a distribution). The reparameterization trick then samples a `z` from that distribution.\n",
    "\n",
    "Notice that `mu` and `logvar` each have shape `(batch_size, 2)` â€” each image gets its own 2D distribution. This is **per-image**, not one global distribution for the whole dataset. Each image becomes its own cloud in latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: VAE Loss and Training (Supported)\n",
    "\n",
    "The VAE loss has two terms that **compete**:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{VAE}} = \\underbrace{\\mathcal{L}_{\\text{recon}}}_{\\text{reconstruction}} + \\underbrace{\\text{KL}(\\,q(z|x)\\;\\|\\;\\mathcal{N}(0,1)\\,)}_{\\text{regularization}}$$\n",
    "\n",
    "- **Reconstruction loss** wants sharp, specialized latent codes (same as the autoencoder)\n",
    "- **KL divergence** wants organized, overlapping distributions near the center\n",
    "\n",
    "The KL formula for Gaussian distributions is:\n",
    "\n",
    "$$\\text{KL} = -\\frac{1}{2}\\sum_{j=1}^{d}\\left(1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\right)$$\n",
    "\n",
    "In code: `kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())`\n",
    "\n",
    "**Your task:** Fill in the TODO sections to implement the VAE loss function and training loop.\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The key insight is that the two loss terms serve different purposes. Reconstruction loss ensures the model can faithfully reproduce inputs (same as an autoencoder). KL divergence acts as a regularizer on the latent space shape â€” it penalizes means far from zero (don't hide in a corner) and penalizes small variances (don't collapse clouds to points). Together, they create a smooth, sampleable latent space.\n",
    "\n",
    "```python\n",
    "# TODO 1: KL divergence\n",
    "kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "# TODO 2: Total loss\n",
    "loss = recon_loss + kl_loss\n",
    "```\n",
    "\n",
    "We use `reduction='sum'` for reconstruction loss so it matches the scale of the KL term, which is also a sum. Both are then normalized by batch size for stable training.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, train_loader, num_epochs=10, lr=1e-3):\n",
    "    \"\"\"Train a VAE with reconstruction + KL divergence loss.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    history = {'total': [], 'recon': [], 'kl': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_total, epoch_recon, epoch_kl = 0, 0, 0\n",
    "\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            recon, mu, logvar = model(images)\n",
    "\n",
    "            # Reconstruction loss (same as autoencoder)\n",
    "            recon_loss = F.mse_loss(recon, images, reduction='sum')\n",
    "\n",
    "            # TODO 1: KL divergence loss\n",
    "            # Formula: -0.5 * sum(1 + logvar - mu^2 - exp(logvar))\n",
    "            # This measures how far each image's distribution is from N(0, 1)\n",
    "            kl_loss = ...  # YOUR CODE HERE (one line)\n",
    "\n",
    "            # TODO 2: Total loss = reconstruction + KL\n",
    "            # Normalize by batch size for stable training\n",
    "            loss = ...  # YOUR CODE HERE (one line)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            epoch_total += loss.item() * batch_size\n",
    "            epoch_recon += recon_loss.item()\n",
    "            epoch_kl += kl_loss.item()\n",
    "\n",
    "        n = len(train_loader.dataset)\n",
    "        history['total'].append(epoch_total / n)\n",
    "        history['recon'].append(epoch_recon / n)\n",
    "        history['kl'].append(epoch_kl / n)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{num_epochs} | \"\n",
    "                  f\"Total: {history['total'][-1]:.4f} | \"\n",
    "                  f\"Recon: {history['recon'][-1]:.4f} | \"\n",
    "                  f\"KL: {history['kl'][-1]:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# Train the VAE\n",
    "print(\"Training VAE...\")\n",
    "vae_model = VAE(LATENT_DIM).to(device)\n",
    "vae_history = train_vae(vae_model, train_loader, num_epochs=20)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training curves â€” watch the two losses compete\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(vae_history['total'], linewidth=2)\n",
    "axes[0].set_title('Total Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(vae_history['recon'], linewidth=2, color='#4a9eff', label='Reconstruction')\n",
    "axes[1].set_title('Reconstruction Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "axes[2].plot(vae_history['kl'], linewidth=2, color='#ff6b6b', label='KL Divergence')\n",
    "axes[2].set_title('KL Divergence')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Reconstruction loss decreases (model learns to reconstruct).\")\n",
    "print(\"KL divergence may increase early on as the encoder starts forming\")\n",
    "print(\"meaningful distributions, then stabilizes. The two losses compete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:**\n",
    "\n",
    "You trained a VAE with the two-term loss. The reconstruction loss pulls the model toward sharp, specialized latent codes (just like an autoencoder). The KL term pulls the other way â€” it penalizes means far from zero and variances that are too small. The tension between these two forces creates a latent space that is both useful (can reconstruct) and organized (can sample from).\n",
    "\n",
    "Watch the KL curve: it typically rises at first (the encoder starts producing non-trivial distributions) then settles. If KL stays near zero, the model is collapsing to an autoencoder. If KL dominates, everything will look blurry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Compare Latent Spaces (Supported)\n",
    "\n",
    "The whole point of KL regularization is to fill the gaps in the latent space. Let's see if it worked. We'll encode the test set with both models and plot their 2D latent spaces side by side.\n",
    "\n",
    "**Your task:** Fill in the TODO to extract the VAE's latent codes (mu values) from the test set. Then compare the two latent spaces.\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "For the VAE, the encoder returns `(recon, mu, logvar)`. We want just `mu` â€” the center of each image's cloud. We collect these across all batches, just like we collect `z` from the autoencoder.\n",
    "\n",
    "```python\n",
    "# TODO: Get VAE latent codes\n",
    "recon, mu, logvar = vae_model(images)\n",
    "vae_codes.append(mu.detach().cpu())\n",
    "```\n",
    "\n",
    "We use `mu` (not a sample from the distribution) because `mu` is the center of each cloud â€” it gives the clearest picture of the latent space structure. Sampling would add noise.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the entire test set with both models\n",
    "ae_model.eval()\n",
    "vae_model.eval()\n",
    "\n",
    "ae_codes = []\n",
    "vae_codes = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Autoencoder: encoder gives us z directly\n",
    "        ae_z = ae_model.encoder(images)\n",
    "        ae_codes.append(ae_z.cpu())\n",
    "\n",
    "        # TODO: Get VAE latent codes (the mu values)\n",
    "        # Hint: vae_model returns (recon, mu, logvar)\n",
    "        # We want mu â€” the center of each image's cloud\n",
    "        ...  # YOUR CODE HERE (2 lines: call vae_model, append mu)\n",
    "\n",
    "        all_labels.append(labels)\n",
    "\n",
    "ae_codes = torch.cat(ae_codes).numpy()\n",
    "vae_codes = torch.cat(vae_codes).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "print(f\"Encoded {len(all_labels)} test images.\")\n",
    "print(f\"AE codes shape: {ae_codes.shape}\")\n",
    "print(f\"VAE codes shape: {vae_codes.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both latent spaces side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    mask = all_labels == i\n",
    "    axes[0].scatter(ae_codes[mask, 0], ae_codes[mask, 1],\n",
    "                    c=[colors[i]], s=1, alpha=0.5, label=CLASS_NAMES[i])\n",
    "    axes[1].scatter(vae_codes[mask, 0], vae_codes[mask, 1],\n",
    "                    c=[colors[i]], s=1, alpha=0.5, label=CLASS_NAMES[i])\n",
    "\n",
    "axes[0].set_title('Autoencoder Latent Space', fontsize=14)\n",
    "axes[0].set_xlabel('z[0]')\n",
    "axes[0].set_ylabel('z[1]')\n",
    "axes[0].grid(alpha=0.2)\n",
    "\n",
    "axes[1].set_title('VAE Latent Space (mu)', fontsize=14)\n",
    "axes[1].set_xlabel('z[0]')\n",
    "axes[1].set_ylabel('z[1]')\n",
    "axes[1].grid(alpha=0.2)\n",
    "axes[1].legend(loc='upper right', fontsize=8, markerscale=5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics to quantify the difference\n",
    "print(f\"\\nAutoencoder latent range: [{ae_codes.min():.2f}, {ae_codes.max():.2f}]\")\n",
    "print(f\"VAE latent range:         [{vae_codes.min():.2f}, {vae_codes.max():.2f}]\")\n",
    "print(f\"\\nAE codes std:  {ae_codes.std():.3f}\")\n",
    "print(f\"VAE codes std: {vae_codes.std():.3f}\")\n",
    "print(f\"\\nNotice how the VAE latent space is more compact and centered.\")\n",
    "print(f\"KL regularization pulled everything toward N(0, 1) â€” no far-flung corners.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode random points from both latent spaces to see the gap problem\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "\n",
    "# Sample random z from N(0, 1) â€” the space we want to be able to sample from\n",
    "torch.manual_seed(42)\n",
    "z_random = torch.randn(8, LATENT_DIM).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ae_generated = ae_model.decoder(z_random).cpu()\n",
    "    vae_generated = vae_model.decode(z_random).cpu()\n",
    "\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(ae_generated[i, 0], cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(vae_generated[i, 0], cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('AE', fontsize=12, rotation=0, labelpad=25)\n",
    "axes[1, 0].set_ylabel('VAE', fontsize=12, rotation=0, labelpad=25)\n",
    "\n",
    "fig.suptitle('Random z ~ N(0,1) decoded by each model', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top row: Autoencoder. Random z lands in gaps â†’ garbage.\")\n",
    "print(\"Bottom row: VAE. Random z lands in meaningful space â†’ recognizable items.\")\n",
    "print(\"\\nSame decoder architecture. Same random z. Different training objective.\")\n",
    "print(\"The KL regularizer is what makes the difference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:**\n",
    "\n",
    "The latent space comparison shows exactly what KL regularization does:\n",
    "\n",
    "- **Autoencoder:** Scattered points with arbitrary range. Clusters may be far apart with empty gaps between them. Random z from N(0,1) likely falls in a gap â€” the decoder produces garbage.\n",
    "- **VAE:** Compact, centered distributions near the origin. The clouds overlap. Random z from N(0,1) lands in meaningful territory â€” the decoder produces recognizable items.\n",
    "\n",
    "KL divergence is the regularizer that pulled everything toward N(0,1). It penalizes means far from zero (\"don't hide in a corner\") and variances too small (\"don't collapse clouds to points\"). The result: a smooth, continuous latent space with no gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Generate New Digits (Independent)\n",
    "\n",
    "You have a trained VAE with a smooth, sampleable latent space. Now use it to **generate** new Fashion-MNIST items that never existed in the training set.\n",
    "\n",
    "**Your task:**\n",
    "1. Sample a grid of z vectors from N(0, 1) â€” try a range spanning [-3, 3] on each axis\n",
    "2. Decode each z vector into an image\n",
    "3. Display the results as a grid that shows how the latent space varies smoothly\n",
    "\n",
    "This is the generative payoff: sample any point from N(0, 1), decode it, get a plausible image. The autoencoder cannot do this. The VAE can, because KL regularization made the entire latent space meaningful.\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The key insight is that the VAE's latent space is smooth and continuous â€” you can create a grid of evenly spaced z values and decode each one. As you move through the grid, the decoded images should change smoothly. This is exactly the \"roads between buildings\" that the autoencoder lacked.\n",
    "\n",
    "```python\n",
    "# Create a grid of z values spanning the latent space\n",
    "n = 15  # 15x15 grid\n",
    "z1 = torch.linspace(-3, 3, n)\n",
    "z2 = torch.linspace(-3, 3, n)\n",
    "\n",
    "# Build all z vectors\n",
    "grid_z = torch.zeros(n * n, LATENT_DIM)\n",
    "idx = 0\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        grid_z[idx, 0] = z1[j]\n",
    "        grid_z[idx, 1] = z2[n - 1 - i]  # flip so y increases upward\n",
    "        idx += 1\n",
    "\n",
    "# Decode all at once\n",
    "vae_model.eval()\n",
    "with torch.no_grad():\n",
    "    generated = vae_model.decode(grid_z.to(device)).cpu()\n",
    "\n",
    "# Stitch into one big image\n",
    "canvas = np.zeros((n * 28, n * 28))\n",
    "idx = 0\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = generated[idx, 0].numpy()\n",
    "        idx += 1\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(canvas, cmap='gray')\n",
    "plt.title('VAE Latent Space Grid: z sampled from [-3, 3]', fontsize=14)\n",
    "plt.xlabel('z[0]')\n",
    "plt.ylabel('z[1]')\n",
    "plt.xticks(np.linspace(0, n*28, 5), [f'{v:.1f}' for v in np.linspace(-3, 3, 5)])\n",
    "plt.yticks(np.linspace(0, n*28, 5), [f'{v:.1f}' for v in np.linspace(3, -3, 5)])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Notice how the images change smoothly as you move across the grid. Similar items cluster together and blend into each other at the boundaries. There are no gaps â€” every point in the latent space decodes to something plausible.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Create a grid of z values (e.g., 15x15) spanning [-3, 3] on each axis\n",
    "# 2. Decode each z with vae_model.decode(z.to(device))\n",
    "# 3. Display as a grid image\n",
    "#\n",
    "# This is the generative payoff â€” the autoencoder can't do this,\n",
    "# but the VAE can because every point in the latent space is meaningful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The reparameterization trick** lets gradients flow through sampling: sample epsilon from N(0,1), compute z = mu + sigma * epsilon. The randomness is in epsilon (not learnable), so gradients flow through mu and sigma normally.\n",
    "\n",
    "2. **The VAE encodes to a distribution (mu + logvar), not a point.** Each image becomes a cloud in latent space. Nearby images have overlapping clouds, filling the gaps that made autoencoder generation fail.\n",
    "\n",
    "3. **KL divergence is a regularizer on the latent space shape.** Two intuitions: don't hide codes in a corner (penalizes large means), and don't make clouds so small they're basically points (penalizes small variance). Same principle as L2 on weights â€” constraints force better representations.\n",
    "\n",
    "4. **The VAE loss = reconstruction + KL, and the two terms compete.** Reconstruction wants sharp, specialized codes. KL wants organized, overlapping distributions. VAE reconstructions are blurrier than autoencoder reconstructions â€” that's the price of a smooth, sampleable latent space.\n",
    "\n",
    "5. **The result: a smooth latent space you can sample from.** Sample any point from N(0,1), decode it, and you get a plausible image. The autoencoder's failure is fixed â€” you have built your first true generative model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}