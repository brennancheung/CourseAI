{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tokenization: Building BPE from Scratch\n\nIn this notebook you'll:\n- Build a character-level tokenizer and see why sequences get too long\n- Build a word-level tokenizer and see the unknown-word problem\n- Implement BPE's core operations: count pairs, merge pairs, train the merge table\n- Encode new text using your learned merge table\n- Decode token IDs back to text losslessly\n\n**For each exercise, PREDICT the output before running the cell.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from collections import Counter\n\n# Reproducible results\nimport random\nrandom.seed(42)\n\n# A small corpus to work with\ncorpus = [\n    \"low low low low low\",\n    \"lower lower newest newest\",\n    \"the cat sat on the mat\",\n    \"the dog sat on the log\",\n    \"the newest cat is the lowest\",\n]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 1: Character-Level Tokenization (Guided)\n\nThe simplest tokenizer: split text into individual characters. Each character gets an ID.\n\n**Before running, predict:** How many tokens will \"The cat sat on the mat\" produce? (Count carefully â€” spaces are characters too.) What about the vocabulary size for our corpus?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def char_tokenize(text: str) -> list[str]:\n    \"\"\"Split text into individual characters.\"\"\"\n    return list(text)\n\n\ndef build_char_vocab(texts: list[str]) -> dict[str, int]:\n    \"\"\"Build a vocabulary mapping each unique character to an integer ID.\"\"\"\n    chars = set()\n    for text in texts:\n        chars.update(list(text))\n    return {ch: i for i, ch in enumerate(sorted(chars))}\n\n\n# Test it\ntest_text = \"The cat sat on the mat\"\nchar_tokens = char_tokenize(test_text)\nchar_vocab = build_char_vocab(corpus)\n\nprint(f\"Text: '{test_text}'\")\nprint(f\"Tokens: {char_tokens}\")\nprint(f\"Number of tokens: {len(char_tokens)}\")\nprint(f\"Vocabulary size: {len(char_vocab)}\")\nprint(f\"Vocabulary: {char_vocab}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Result:** \"The cat sat on the mat\" = 22 tokens (every character including spaces). The vocabulary is tiny (~20 characters for this corpus), but sequences are long. The model would need to learn that T-h-e means \"the\" from scratch."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 2: Word-Level Tokenization (Supported)\n\nThe other extreme: split on spaces. Each word gets an ID.\n\n**Task:** Implement `word_tokenize` and `build_word_vocab` by filling in the TODOs.\n\n**Then try:** What happens if you tokenize \"ChatGPT is amazing\" with a vocabulary built from our corpus?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Split text on whitespace.\"\"\"\n",
    "    return ____  # FILL IN: one line\n",
    "\n",
    "\n",
    "def build_word_vocab(texts: list[str]) -> dict[str, int]:\n",
    "    \"\"\"Build a vocabulary mapping each unique word to an integer ID.\"\"\"\n",
    "    words = set()\n",
    "    for text in texts:\n",
    "        words.update(____) # FILL IN: add all words from this text\n",
    "    vocab = {word: i for i, word in enumerate(sorted(words))}\n",
    "    vocab[\"[UNK]\"] = len(vocab)  # Unknown token\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# Test it\n",
    "word_tokens = word_tokenize(test_text)\n",
    "word_vocab = build_word_vocab(corpus)\n",
    "\n",
    "print(f\"Text: '{test_text}'\")\n",
    "print(f\"Tokens: {word_tokens}\")\n",
    "print(f\"Number of tokens: {len(word_tokens)}\")\n",
    "print(f\"Vocabulary size: {len(word_vocab)}\")\n",
    "print(f\"\\nVocabulary: {word_vocab}\")\n",
    "\n",
    "# Now try a sentence with unknown words\n",
    "novel_text = \"ChatGPT is amazing\"\n",
    "novel_tokens = word_tokenize(novel_text)\n",
    "encoded = [word_vocab.get(w, word_vocab[\"[UNK]\"]) for w in novel_tokens]\n",
    "print(f\"\\nNovel text: '{novel_text}'\")\n",
    "print(f\"Tokens: {novel_tokens}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Notice: 'ChatGPT' and 'amazing' both map to [UNK] = {word_vocab['[UNK]']}\")\n",
    "print(f\"The model cannot distinguish between them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe simplest word tokenizer just splits on whitespace. The vocabulary builder collects all unique words from the corpus.\n\n```python\ndef word_tokenize(text: str) -> list[str]:\n    return text.split()\n\ndef build_word_vocab(texts: list[str]) -> dict[str, int]:\n    words = set()\n    for text in texts:\n        words.update(text.split())\n    vocab = {word: i for i, word in enumerate(sorted(words))}\n    vocab[\"[UNK]\"] = len(vocab)\n    return vocab\n```\n\n**Key insight:** \"ChatGPT\" and \"amazing\" both become `[UNK]`. The model sees them as identical â€” all information about unknown words is destroyed.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the approaches\n",
    "\n",
    "Run this cell to see the tradeoff side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(f\"{'Character-level':>20} | {'Word-level':>20}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Vocab size:':>20} {len(char_vocab):>4} | {'Vocab size:':>20} {len(word_vocab):>4}\")\n",
    "print(f\"{'Tokens for test:':>20} {len(char_tokens):>4} | {'Tokens for test:':>20} {len(word_tokens):>4}\")\n",
    "print(f\"{'OOV words:':>20} {'None':>4} | {'OOV words:':>20} {'Many':>4}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nWe need something in between...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 3: BPE â€” Count Adjacent Pairs (Supported)\n\nBPE starts with characters and iteratively merges the most common adjacent pair.\n\nFirst, we need a function to count all adjacent pairs in a token sequence.\n\n**Task:** Implement `get_pair_counts` that takes a list of tokens and returns a Counter of adjacent pairs.\n\n**Example:** `[\"l\", \"o\", \"w\", \"l\", \"o\", \"w\"]` should return `{(\"l\", \"o\"): 2, (\"o\", \"w\"): 2, (\"w\", \"l\"): 1}`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_counts(tokens: list[str]) -> Counter:\n",
    "    \"\"\"Count all adjacent token pairs.\n",
    "    \n",
    "    Args:\n",
    "        tokens: A list of string tokens\n",
    "    \n",
    "    Returns:\n",
    "        Counter mapping (token_a, token_b) tuples to their frequency\n",
    "    \"\"\"\n",
    "    pairs = Counter()\n",
    "    for i in range(len(tokens) - 1):\n",
    "        pair = ____  # FILL IN: create a tuple of adjacent tokens\n",
    "        pairs[pair] += 1\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# Test it\n",
    "test_tokens = list(\"low low\")\n",
    "print(f\"Tokens: {test_tokens}\")\n",
    "print(f\"Pair counts: {get_pair_counts(test_tokens)}\")\n",
    "print()\n",
    "\n",
    "# Try with the full corpus (joined)\n",
    "full_text = \" \".join(corpus)\n",
    "full_tokens = list(full_text)\n",
    "all_pairs = get_pair_counts(full_tokens)\n",
    "print(f\"Top 10 pairs in corpus:\")\n",
    "for pair, count in all_pairs.most_common(10):\n",
    "    print(f\"  {pair} : {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\nWe iterate through the token list, looking at each adjacent pair `(tokens[i], tokens[i+1])`. A tuple of two tokens forms the key, and we count how many times each pair appears.\n\n```python\ndef get_pair_counts(tokens: list[str]) -> Counter:\n    pairs = Counter()\n    for i in range(len(tokens) - 1):\n        pair = (tokens[i], tokens[i + 1])\n        pairs[pair] += 1\n    return pairs\n```\n\n**Key insight:** The most frequent pair will be the first merge. BPE is greedy â€” it always merges the most common pair first.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 4: BPE â€” Merge a Pair (Supported)\n\nOnce we've identified the most frequent pair, we need to merge all occurrences.\n\n**Task:** Implement `merge_pair` that:\n1. Scans through the token list\n2. Wherever it finds `pair[0]` followed by `pair[1]`, replaces both with `pair[0] + pair[1]`\n3. Returns the new token list\n\n**Example:** `merge_pair([\"l\", \"o\", \"w\"], (\"l\", \"o\"))` should return `[\"lo\", \"w\"]`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(tokens: list[str], pair: tuple[str, str]) -> list[str]:\n",
    "    \"\"\"Replace all occurrences of a pair with a merged token.\n",
    "    \n",
    "    Args:\n",
    "        tokens: Current token list\n",
    "        pair: The (token_a, token_b) pair to merge\n",
    "    \n",
    "    Returns:\n",
    "        New token list with all occurrences of the pair merged\n",
    "    \"\"\"\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Check if current position matches the pair\n",
    "        if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n",
    "            new_tokens.append(____)  # FILL IN: the merged token\n",
    "            i += ____               # FILL IN: skip both tokens\n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    return new_tokens\n",
    "\n",
    "\n",
    "# Test it\n",
    "test = list(\"low low low\")\n",
    "print(f\"Before: {test}\")\n",
    "\n",
    "merged = merge_pair(test, (\"l\", \"o\"))\n",
    "print(f\"After merging ('l', 'o'): {merged}\")\n",
    "\n",
    "merged2 = merge_pair(merged, (\"lo\", \"w\"))\n",
    "print(f\"After merging ('lo', 'w'): {merged2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe key is using a while loop (not for) because when we merge a pair, we skip ahead by 2 instead of 1. A for loop with `range` would visit every index, but after a merge we need to jump past the second token.\n\n```python\ndef merge_pair(tokens: list[str], pair: tuple[str, str]) -> list[str]:\n    new_tokens = []\n    i = 0\n    while i < len(tokens):\n        if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n            new_tokens.append(pair[0] + pair[1])  # Concatenate the pair\n            i += 2                                  # Skip both tokens\n        else:\n            new_tokens.append(tokens[i])\n            i += 1\n    return new_tokens\n```\n\n**Note:** This is a simple linear scan. Production tokenizers use more efficient data structures, but the logic is identical.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 5: BPE â€” The Training Loop (Supported)\n\nNow combine `get_pair_counts` and `merge_pair` into the full BPE training loop.\n\n**Task:** Implement `train_bpe` that:\n1. Starts with character-level tokens\n2. Repeats `num_merges` times:\n   - Count all pairs\n   - Find the most frequent pair\n   - If the most frequent pair appears fewer than 2 times, stop\n   - Merge that pair in the token list\n   - Record the merge\n3. Returns the final tokens and the list of merges\n\n**Hints:**\n- Use `Counter.most_common(1)` to get the most frequent pair\n- The merge list is important â€” it's what we'll use for encoding new text later"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(\n",
    "    text: str, num_merges: int = 20\n",
    ") -> tuple[list[str], list[tuple[str, str]]]:\n",
    "    \"\"\"Train BPE on a text corpus.\n",
    "    \n",
    "    Args:\n",
    "        text: The training text\n",
    "        num_merges: Maximum number of merges to perform\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (final_tokens, merge_list)\n",
    "        - final_tokens: the text after all merges\n",
    "        - merge_list: ordered list of (pair_a, pair_b) merges\n",
    "    \"\"\"\n",
    "    # TODO: Start with character-level tokens\n",
    "    # tokens = ...\n",
    "    # merges = []\n",
    "    \n",
    "    # TODO: For each merge step:\n",
    "    #   1. Count pairs\n",
    "    #   2. Find most frequent\n",
    "    #   3. If count < 2, stop\n",
    "    #   4. Merge the pair\n",
    "    #   5. Record the merge\n",
    "    #   6. (Optional) Print progress\n",
    "    \n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "\n",
    "# Train on our corpus\n",
    "full_text = \" \".join(corpus)\n",
    "final_tokens, merges = train_bpe(full_text, num_merges=20)\n",
    "\n",
    "print(f\"Original length (chars): {len(list(full_text))}\")\n",
    "print(f\"Final length (tokens):   {len(final_tokens)}\")\n",
    "print(f\"Compression: {1 - len(final_tokens)/len(list(full_text)):.1%}\")\n",
    "print(f\"\\nMerges learned ({len(merges)}):\")\n",
    "for i, merge in enumerate(merges):\n",
    "    print(f\"  {i+1}. '{merge[0]}' + '{merge[1]}' -> '{merge[0] + merge[1]}'\")\n",
    "print(f\"\\nFinal tokens: {final_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe training loop is just repeated application of the two primitives: count pairs, merge the best one. The key insight is that we stop when no pair appears more than once â€” further merges would only create tokens seen once, which wastes vocabulary slots.\n\n```python\ndef train_bpe(\n    text: str, num_merges: int = 20\n) -> tuple[list[str], list[tuple[str, str]]]:\n    tokens = list(text)\n    merges = []\n\n    for step in range(num_merges):\n        pair_counts = get_pair_counts(tokens)\n        if not pair_counts:\n            break\n        \n        best_pair, count = pair_counts.most_common(1)[0]\n        if count < 2:\n            break\n        \n        tokens = merge_pair(tokens, best_pair)\n        merges.append(best_pair)\n        print(f\"  Step {step+1}: merge '{best_pair[0]}' + '{best_pair[1]}' -> '{best_pair[0] + best_pair[1]}' (count: {count}, tokens: {len(tokens)})\")\n\n    return tokens, merges\n```\n\n**Key insight:** Watch the order of merges. The most frequent characters merge first (spaces, common letters), then common bigrams, then common words. Linguistic structure emerges from pure frequency statistics.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 6: BPE â€” Encode New Text (Supported)\n\nTraining learns a merge table. Encoding applies those merges (in order!) to new text.\n\n**Task:** Implement `encode` that:\n1. Starts with character-level tokens of the input text\n2. Applies each merge from the merge list, in order\n3. Converts tokens to integer IDs using a vocabulary\n\n**Important:** Merges must be applied in the same order they were learned. This ensures deterministic encoding.\n\n**Hints:**\n- You already have `merge_pair` â€” just apply each learned merge in sequence\n- Build the vocabulary from all unique tokens seen during training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bpe_vocab(text: str, merges: list[tuple[str, str]]) -> dict[str, int]:\n",
    "    \"\"\"Build vocabulary from base characters + all merge results.\"\"\"\n",
    "    vocab = set(text)  # All base characters\n",
    "    for pair in merges:\n",
    "        vocab.add(pair[0] + pair[1])  # Add each merged token\n",
    "    return {token: i for i, token in enumerate(sorted(vocab, key=lambda t: (len(t), t)))}\n",
    "\n",
    "\n",
    "def encode(text: str, merges: list[tuple[str, str]], vocab: dict[str, int]) -> list[int]:\n",
    "    \"\"\"Encode text into token IDs using learned BPE merges.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to encode\n",
    "        merges: Ordered list of learned merges\n",
    "        vocab: Token-to-ID mapping\n",
    "    \n",
    "    Returns:\n",
    "        List of integer token IDs\n",
    "    \"\"\"\n",
    "    # TODO: Start with character-level tokens\n",
    "    # tokens = ...\n",
    "    \n",
    "    # TODO: Apply each merge in order\n",
    "    # for merge in merges:\n",
    "    #     tokens = ...\n",
    "    \n",
    "    # TODO: Convert tokens to IDs\n",
    "    # ids = ...\n",
    "    \n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_bpe_vocab(full_text, merges)\n",
    "print(f\"Vocabulary ({len(vocab)} tokens):\")\n",
    "for token, idx in vocab.items():\n",
    "    display = token.replace(' ', '\\u2423')\n",
    "    print(f\"  {idx:3d} -> '{display}'\")\n",
    "\n",
    "# Encode some text\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "test_sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the lowest\",\n",
    "    \"newer\",\n",
    "    \"ChatGPT\",\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    tokens_list = list(sentence)\n",
    "    for merge in merges:\n",
    "        tokens_list = merge_pair(tokens_list, merge)\n",
    "    ids = [vocab.get(t, -1) for t in tokens_list]\n",
    "    print(f\"\\n'{sentence}'\")\n",
    "    print(f\"  Tokens: {tokens_list}\")\n",
    "    print(f\"  IDs:    {ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\nEncoding is the reverse of training: start from characters, then replay the exact same merges in order. The merge order matters because earlier merges create tokens that later merges depend on.\n\n```python\ndef encode(text: str, merges: list[tuple[str, str]], vocab: dict[str, int]) -> list[int]:\n    tokens = list(text)\n    for merge in merges:\n        tokens = merge_pair(tokens, merge)\n    ids = [vocab.get(t, -1) for t in tokens]\n    return ids\n```\n\n**Key insight:** Encoding is deterministic. Same text + same merge table = same token IDs, every time. The merge order is critical â€” applying merges in a different order could produce different results.\n\n**Notice:** \"ChatGPT\" gets split into characters since none of its character pairs were in our tiny training corpus. A real tokenizer trained on internet text would handle it better. But it doesn't become `[UNK]` â€” it degrades gracefully to character-level.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 7: BPE â€” Decode Back to Text (Supported)\n\nDecoding is simple: map IDs back to tokens, then concatenate.\n\n**Task:** Implement `decode` that converts a list of integer IDs back to text."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids: list[int], vocab: dict[str, int]) -> str:\n",
    "    \"\"\"Decode token IDs back to text.\n",
    "    \n",
    "    Args:\n",
    "        ids: List of integer token IDs\n",
    "        vocab: Token-to-ID mapping (same one used for encoding)\n",
    "    \n",
    "    Returns:\n",
    "        Reconstructed text string\n",
    "    \"\"\"\n",
    "    # TODO: Build reverse vocabulary (ID -> token)\n",
    "    # id_to_token = ...\n",
    "    \n",
    "    # TODO: Map each ID to its token and join\n",
    "    # text = ...\n",
    "    \n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "\n",
    "# Test roundtrip: encode then decode\n",
    "for sentence in test_sentences:\n",
    "    ids = encode(sentence, merges, vocab)\n",
    "    reconstructed = decode(ids, vocab)\n",
    "    matches = \"OK\" if reconstructed == sentence else \"MISMATCH\"\n",
    "    print(f\"[{matches}] '{sentence}' -> {ids} -> '{reconstructed}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\nDecoding just reverses the vocabulary mapping and concatenates. Because BPE tokens are subword pieces that fit together exactly, simple string concatenation reconstructs the original text perfectly.\n\n```python\ndef decode(ids: list[int], vocab: dict[str, int]) -> str:\n    id_to_token = {i: token for token, i in vocab.items()}\n    tokens = [id_to_token[i] for i in ids]\n    return \"\".join(tokens)\n```\n\n**Key insight:** Decoding is lossless. `decode(encode(text)) == text` always. This is a requirement for any tokenizer â€” you must be able to reconstruct the original text perfectly.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 8: Put It All Together (Independent)\n\nYou now have all the pieces. Let's use them to explore how BPE behaves on different inputs.\n\n**Tasks:**\n1. Train BPE on a larger corpus (provided below) with 40 merges\n2. Encode several test sentences and examine the token boundaries\n3. Compare token counts for English vs. completely novel words\n4. Try encoding code and see how it differs from prose"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A somewhat larger corpus for more interesting merges\n",
    "larger_corpus = \"\"\"\n",
    "the cat sat on the mat the cat sat on the mat the cat is the best cat\n",
    "the dog sat on the log the dog sat on the log the dog is the best dog\n",
    "the lowest price is the newest offer the lowest price is the newest offer\n",
    "lower lower lower lower newest newest newest newest\n",
    "the best thing is the newest thing the best thing is the newest thing\n",
    "this is a test this is a test this is a test this is a test\n",
    "learning is the best way to understand learning is the best way to understand\n",
    "the transformer model is a newer model the transformer model is a newer model\n",
    "\"\"\".strip()\n",
    "\n",
    "# TODO: Train BPE with more merges\n",
    "# final_tokens, merges = train_bpe(larger_corpus, num_merges=40)\n",
    "# vocab = build_bpe_vocab(larger_corpus, merges)\n",
    "\n",
    "# TODO: Encode these test sentences and examine the results\n",
    "# test_cases = [\n",
    "#     \"the cat sat on the mat\",         # Familiar sentence\n",
    "#     \"the newest transformer model\",    # Mix of common and less common\n",
    "#     \"xyzzy abcdef\",                    # Completely novel words\n",
    "#     \"print('hello world')\",            # Code\n",
    "#     \"123 + 456 = 579\",                 # Arithmetic\n",
    "# ]\n",
    "\n",
    "# for text in test_cases:\n",
    "#     ids = encode(text, merges, vocab)\n",
    "#     # Apply merges to see token boundaries\n",
    "#     tokens = list(text)\n",
    "#     for merge in merges:\n",
    "#         tokens = merge_pair(tokens, merge)\n",
    "#     print(f\"'{text}'\")\n",
    "#     print(f\"  -> {tokens}  ({len(tokens)} tokens)\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe key insight is that BPE performance depends on training data. Frequent substrings in the training corpus get merged into single tokens, so familiar text compresses well. Novel words (like \"xyzzy\") stay at character-level because those character pairs were never frequent enough to merge.\n\n```python\n# Train BPE with more merges\nfinal_tokens, merges = train_bpe(larger_corpus, num_merges=40)\nvocab = build_bpe_vocab(larger_corpus, merges)\n\n# Encode test cases\ntest_cases = [\n    \"the cat sat on the mat\",         # Familiar sentence\n    \"the newest transformer model\",    # Mix of common and less common\n    \"xyzzy abcdef\",                    # Completely novel words\n    \"print('hello world')\",            # Code\n    \"123 + 456 = 579\",                 # Arithmetic\n]\n\nfor text in test_cases:\n    ids = encode(text, merges, vocab)\n    # Apply merges to see token boundaries\n    tokens = list(text)\n    for merge in merges:\n        tokens = merge_pair(tokens, merge)\n    print(f\"'{text}'\")\n    print(f\"  -> {tokens}  ({len(tokens)} tokens)\")\n    print()\n```\n\n**What to notice:** Familiar phrases like \"the cat\" compress into few tokens. Novel words like \"xyzzy\" stay as individual characters. Code and numbers typically tokenize poorly because they weren't frequent in training data. This is why real tokenizers train on diverse corpora.\n\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **Character-level** tokenization gives tiny vocabulary and no OOV, but very long sequences. The model must learn spelling from scratch.\n2. **Word-level** tokenization gives compact sequences, but unknown words are invisible (`[UNK]`).\n3. **BPE (subword)** is the middle ground. Common words stay whole, rare words split into recognizable pieces. It degrades gracefully â€” unknown words become characters, not `[UNK]`.\n4. **The merge table IS the tokenizer.** Training = count pairs + merge the most frequent, repeat. Encoding = replay merges in order. Decoding = map IDs to tokens and concatenate.\n5. **Encoding is deterministic.** Same text + same merge table = same output, every time. The merge order is critical.\n\nYour five functions (`get_pair_counts`, `merge_pair`, `train_bpe`, `encode`, `decode`) implement the core algorithm behind GPT's tokenizer in ~50 lines of Python."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}