{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Embeddings & Positional Encoding\n\n**Module 4.1, Lesson 3** — Language Modeling Fundamentals\n\nIn this notebook you'll:\n- Create `nn.Embedding` and verify it's just matrix indexing\n- Prove the one-hot equivalence (one-hot x W = row selection)\n- Compute cosine similarity between token pairs to see what training produces\n- Implement sinusoidal positional encoding from the formula\n- Combine embeddings + positional encoding into the model input\n- (Stretch) Explore pretrained GPT-2 embeddings\n\n**For each exercise, PREDICT the output before running the cell.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Reproducible results\ntorch.manual_seed(42)\n\n# Nice plots\nplt.style.use('dark_background')\nplt.rcParams['figure.figsize'] = [10, 4]\n\nprint(f\"PyTorch version: {torch.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Exercise 1: nn.Embedding Basics (Guided)\n\nCreate an embedding layer and verify that calling it with a token ID returns the same vector as indexing the weight matrix directly.\n\n**Before running, predict:** If you create `nn.Embedding(10, 4)`, what shape will the weight matrix be? If you look up token ID 3, what shape will the result be?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small embedding: 10 tokens, 4 dimensions\n",
    "vocab_size = 10\n",
    "embed_dim = 4\n",
    "embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "print(f\"Embedding weight shape: {embedding.weight.shape}\")\n",
    "print(f\"Weight matrix:\\n{embedding.weight.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up token ID 3 two ways:\n",
    "token_id = 3\n",
    "\n",
    "# Way 1: Call the embedding layer with a tensor\n",
    "via_call = embedding(torch.tensor([token_id]))\n",
    "\n",
    "# Way 2: Index the weight matrix directly\n",
    "via_index = embedding.weight[token_id]\n",
    "\n",
    "print(f\"Via embedding call: {via_call}\")\n",
    "print(f\"Via weight index:   {via_index}\")\n",
    "print(f\"\\nAre they equal? {torch.allclose(via_call.squeeze(), via_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that embeddings are learnable parameters\n",
    "print(f\"requires_grad: {embedding.weight.requires_grad}\")\n",
    "print(f\"\\nNumber of parameters: {embedding.weight.numel()}\")\n",
    "print(f\"That's {vocab_size} tokens × {embed_dim} dimensions = {vocab_size * embed_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn:** What would `nn.Embedding(50000, 768)` have for parameter count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "# Calculate: vocab_size * embed_dim\n",
    "big_params = 50000 * 768\n",
    "print(f\"Parameters: {big_params:,}\")  # 38,400,000 — 38.4 million just for embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Exercise 2: One-Hot Equivalence Proof (Guided)\n\nShow that multiplying a one-hot vector by the embedding weight matrix gives the same result as `nn.Embedding` lookup.\n\n**Before running, predict:** If you multiply a one-hot vector of shape `[10]` by a weight matrix of shape `[10, 4]`, what shape is the result? Will it match the embedding lookup?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot vector for token ID 3\n",
    "token_id = 3\n",
    "one_hot = F.one_hot(torch.tensor(token_id), num_classes=vocab_size).float()\n",
    "\n",
    "print(f\"One-hot vector: {one_hot}\")\n",
    "print(f\"Shape: {one_hot.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply one-hot by the weight matrix\n",
    "# one_hot shape: [10], weight shape: [10, 4]\n",
    "# Result shape: [4] — one row of the weight matrix\n",
    "via_matmul = one_hot @ embedding.weight\n",
    "\n",
    "# Compare with direct lookup\n",
    "via_lookup = embedding(torch.tensor([token_id])).squeeze()\n",
    "\n",
    "print(f\"Via one-hot @ weight: {via_matmul}\")\n",
    "print(f\"Via embedding lookup: {via_lookup}\")\n",
    "print(f\"\\nAre they equal? {torch.allclose(via_matmul, via_lookup)}\")\n",
    "print(\"\\n✓ one-hot × W = row selection. nn.Embedding skips the sparse vector.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify for ALL tokens at once\n",
    "all_ids = torch.arange(vocab_size)\n",
    "all_one_hot = F.one_hot(all_ids, num_classes=vocab_size).float()\n",
    "\n",
    "via_matmul_all = all_one_hot @ embedding.weight\n",
    "via_lookup_all = embedding(all_ids)\n",
    "\n",
    "print(f\"All equal? {torch.allclose(via_matmul_all, via_lookup_all)}\")\n",
    "print(f\"\\nOne-hot identity × W = W. The embedding IS the weight matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Exercise 3: Cosine Similarity Between Token Pairs (Guided)\n\nAt initialization, embeddings are random — no meaningful similarity. But let's see what cosine similarity looks like, and then we'll look at pretrained embeddings.\n\n**Before running, predict:** For random 64-dimensional vectors, will cosine similarity between any two tokens be close to 0, close to 1, or random?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a: torch.Tensor, b: torch.Tensor) -> float:\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return (F.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0))).item()\n",
    "\n",
    "# Random embeddings: similarity is ~0 (random directions)\n",
    "random_emb = nn.Embedding(5, 64)  # larger dim for more meaningful cosine sim\n",
    "tokens = {\"cat\": 0, \"dog\": 1, \"bird\": 2, \"the\": 3, \"seven\": 4}\n",
    "\n",
    "print(\"=== Random (untrained) embeddings ===\")\n",
    "for name_a, id_a in tokens.items():\n",
    "    for name_b, id_b in tokens.items():\n",
    "        if id_a < id_b:\n",
    "            sim = cosine_similarity(\n",
    "                random_emb.weight[id_a],\n",
    "                random_emb.weight[id_b]\n",
    "            )\n",
    "            print(f\"  cos({name_a}, {name_b}) = {sim:.3f}\")\n",
    "\n",
    "print(\"\\nAll roughly ~0. Random vectors in high dimensions are nearly orthogonal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's MANUALLY set embeddings to show what training produces.\n",
    "# After training, similar tokens should have similar vectors.\n",
    "\n",
    "trained_emb = nn.Embedding(5, 4)\n",
    "with torch.no_grad():\n",
    "    trained_emb.weight[0] = torch.tensor([0.8, 0.3, -0.2, 0.5])   # \"cat\" - animal\n",
    "    trained_emb.weight[1] = torch.tensor([0.7, 0.4, -0.1, 0.6])   # \"dog\" - animal (similar to cat!)\n",
    "    trained_emb.weight[2] = torch.tensor([0.6, 0.5, -0.3, 0.4])   # \"bird\" - animal (close too)\n",
    "    trained_emb.weight[3] = torch.tensor([-0.1, -0.8, 0.9, -0.3]) # \"the\" - function word (different!)\n",
    "    trained_emb.weight[4] = torch.tensor([0.2, -0.6, 0.1, 0.8])   # \"seven\" - number (different!)\n",
    "\n",
    "print(\"=== Trained (simulated) embeddings ===\")\n",
    "for name_a, id_a in tokens.items():\n",
    "    for name_b, id_b in tokens.items():\n",
    "        if id_a < id_b:\n",
    "            sim = cosine_similarity(\n",
    "                trained_emb.weight[id_a],\n",
    "                trained_emb.weight[id_b]\n",
    "            )\n",
    "            print(f\"  cos({name_a}, {name_b}) = {sim:.3f}\")\n",
    "\n",
    "print(\"\\nAnimals cluster together (high similarity). Function words are distant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight:** At initialization, all pairs have ~0 similarity (random). After training, similar tokens have high cosine similarity. Training shapes the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Exercise 4: Sinusoidal Positional Encoding (Guided)\n\nImplement the sinusoidal positional encoding from the formula:\n\n$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n\n**Before running, predict:** Position 0 uses `sin(0)` and `cos(0)` for its first two dimensions. What values are those?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_positional_encoding(max_len: int, d_model: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate sinusoidal positional encoding matrix.\n",
    "    \n",
    "    Args:\n",
    "        max_len: Maximum sequence length\n",
    "        d_model: Embedding dimension\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [max_len, d_model]\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]\n",
    "    \n",
    "    # Compute the division term: 10000^(2i/d_model)\n",
    "    div_term = torch.exp(\n",
    "        torch.arange(0, d_model, 2, dtype=torch.float) * (-np.log(10000.0) / d_model)\n",
    "    )  # [d_model/2]\n",
    "    \n",
    "    # Even dimensions: sin\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    # Odd dimensions: cos\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# Generate for 20 positions, 64 dimensions\n",
    "pe = sinusoidal_positional_encoding(20, 64)\n",
    "print(f\"Shape: {pe.shape}\")\n",
    "print(f\"\\nPosition 0, first 8 dims: {pe[0, :8].tolist()}\")\n",
    "print(f\"Position 1, first 8 dims: {pe[1, :8].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize as a heatmap\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.imshow(pe.numpy(), cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Value')\n",
    "plt.xlabel('Encoding Dimension')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Sinusoidal Positional Encoding')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: slow-changing waves (coarse position)\")\n",
    "print(\"Right: fast-changing waves (fine position)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify key properties:\n",
    "\n",
    "# 1. Each position has a unique encoding\n",
    "for i in range(20):\n",
    "    for j in range(i + 1, 20):\n",
    "        assert not torch.allclose(pe[i], pe[j], atol=1e-6), f\"Positions {i} and {j} are the same!\"\n",
    "print(\"✓ All 20 positions have unique encodings\")\n",
    "\n",
    "# 2. Nearby positions have similar encodings\n",
    "sim_adjacent = cosine_similarity(pe[0], pe[1])\n",
    "sim_distant = cosine_similarity(pe[0], pe[19])\n",
    "print(f\"\\n✓ Nearby positions are more similar:\")\n",
    "print(f\"  cos(pos 0, pos 1)  = {sim_adjacent:.3f}\")\n",
    "print(f\"  cos(pos 0, pos 19) = {sim_distant:.3f}\")\n",
    "\n",
    "# 3. Works for any sequence length\n",
    "pe_long = sinusoidal_positional_encoding(10000, 64)\n",
    "print(f\"\\n✓ Can generate for any length: shape {pe_long.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Exercise 5: Combining Embeddings + Positional Encoding (Guided)\n\nPut it all together: take a sentence, look up token embeddings, add positional encoding.\n\n**Before running, predict:** If token embeddings have shape `[5, 64]` and positional encodings have shape `[5, 64]`, what shape is the model input? What operation combines them?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the full input pipeline\n",
    "vocab_size = 100\n",
    "embed_dim = 64\n",
    "max_seq_len = 128\n",
    "\n",
    "# Create both embedding tables\n",
    "token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "# For sinusoidal: no learnable parameters, just compute it\n",
    "pe_matrix = sinusoidal_positional_encoding(max_seq_len, embed_dim)\n",
    "\n",
    "# Simulate a tokenized sentence: [23, 45, 12, 67, 89]\n",
    "token_ids = torch.tensor([23, 45, 12, 67, 89])\n",
    "seq_len = len(token_ids)\n",
    "\n",
    "# Step 1: Look up token embeddings\n",
    "tok_emb = token_embedding(token_ids)  # [5, 64]\n",
    "print(f\"Token embeddings shape: {tok_emb.shape}\")\n",
    "\n",
    "# Step 2: Get positional encodings for positions 0..4\n",
    "pos_enc = pe_matrix[:seq_len]  # [5, 64]\n",
    "print(f\"Positional encoding shape: {pos_enc.shape}\")\n",
    "\n",
    "# Step 3: Add them together\n",
    "model_input = tok_emb + pos_enc  # [5, 64]\n",
    "print(f\"Model input shape: {model_input.shape}\")\n",
    "print(f\"\\n✓ The tensor that enters the transformer: {seq_len} tokens × {embed_dim} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SAME sentence with LEARNED positional encoding:\n",
    "pos_embedding = nn.Embedding(max_seq_len, embed_dim)  # learnable\n",
    "positions = torch.arange(seq_len)  # [0, 1, 2, 3, 4]\n",
    "\n",
    "model_input_learned = token_embedding(token_ids) + pos_embedding(positions)\n",
    "print(f\"Model input (learned PE) shape: {model_input_learned.shape}\")\n",
    "print(f\"\\nSame shape, different approach. Learned PE has {pos_embedding.weight.numel():,} extra parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the bag-of-words problem:\n",
    "# Same tokens, different order — without position, they're identical\n",
    "\n",
    "sentence_a = torch.tensor([10, 20, 30])  # \"dog bites man\"\n",
    "sentence_b = torch.tensor([30, 20, 10])  # \"man bites dog\"\n",
    "\n",
    "# Without positional encoding\n",
    "emb_a = token_embedding(sentence_a)\n",
    "emb_b = token_embedding(sentence_b)\n",
    "\n",
    "# Sum embeddings (a simple aggregation): identical!\n",
    "print(\"WITHOUT positional encoding:\")\n",
    "print(f\"  Sum of embeddings equal? {torch.allclose(emb_a.sum(dim=0), emb_b.sum(dim=0))}\")\n",
    "\n",
    "# WITH positional encoding: different!\n",
    "pe_3 = pe_matrix[:3]\n",
    "input_a = emb_a + pe_3\n",
    "input_b = emb_b + pe_3\n",
    "\n",
    "print(f\"\\nWITH positional encoding:\")\n",
    "print(f\"  Sum of inputs equal? {torch.allclose(input_a.sum(dim=0), input_b.sum(dim=0))}\")\n",
    "print(f\"\\n✓ Position breaks the symmetry. The model can now distinguish order.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Exercise 6 (Stretch): Explore Pretrained GPT-2 Embeddings (Guided)\n\nLoad the actual GPT-2 token embeddings and explore semantic similarity.\n\n**Before running, predict:** Will semantically similar words (like \"cat\" and \"dog\") have higher cosine similarity than unrelated words (like \"cat\" and \"seven\") in trained embeddings?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers if needed\n",
    "try:\n",
    "    from transformers import GPT2Tokenizer, GPT2Model\n",
    "except ImportError:\n",
    "    !pip install transformers -q\n",
    "    from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Extract the token embedding matrix\n",
    "wte = model.wte.weight.detach()  # [50257, 768]\n",
    "print(f\"GPT-2 embedding matrix shape: {wte.shape}\")\n",
    "print(f\"Vocabulary size: {wte.shape[0]}\")\n",
    "print(f\"Embedding dimension: {wte.shape[1]}\")\n",
    "print(f\"Total parameters: {wte.numel():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_embedding(word: str) -> torch.Tensor:\n",
    "    \"\"\"Get the embedding for a single token (word must be a single token).\"\"\"\n",
    "    ids = tokenizer.encode(word)\n",
    "    # Take the first token if multiple\n",
    "    return wte[ids[0]]\n",
    "\n",
    "def token_similarity(word_a: str, word_b: str) -> float:\n",
    "    \"\"\"Cosine similarity between two tokens' embeddings.\"\"\"\n",
    "    emb_a = get_token_embedding(word_a)\n",
    "    emb_b = get_token_embedding(word_b)\n",
    "    return cosine_similarity(emb_a, emb_b)\n",
    "\n",
    "# Explore semantic similarities\n",
    "pairs = [\n",
    "    (\" cat\", \" dog\"),\n",
    "    (\" cat\", \" the\"),\n",
    "    (\" king\", \" queen\"),\n",
    "    (\" man\", \" woman\"),\n",
    "    (\" happy\", \" sad\"),\n",
    "    (\" happy\", \" seven\"),\n",
    "    (\" France\", \" Germany\"),\n",
    "    (\" France\", \" banana\"),\n",
    "]\n",
    "\n",
    "print(\"GPT-2 Token Embedding Similarities:\")\n",
    "print(\"=\" * 45)\n",
    "for word_a, word_b in pairs:\n",
    "    sim = token_similarity(word_a, word_b)\n",
    "    bar = \"█\" * int(abs(sim) * 20)\n",
    "    print(f\"  cos({word_a.strip():>8}, {word_b.strip():<8}) = {sim:+.3f}  {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_tokens(word: str, k: int = 10) -> None:\n",
    "    \"\"\"Find the k nearest tokens to a given word in embedding space.\"\"\"\n",
    "    emb = get_token_embedding(word).unsqueeze(0)  # [1, 768]\n",
    "    # Cosine similarity with all tokens\n",
    "    sims = F.cosine_similarity(emb, wte)  # [50257]\n",
    "    \n",
    "    # Get top-k (skip the first one — it's the token itself)\n",
    "    top_k = torch.topk(sims, k + 1)\n",
    "    \n",
    "    print(f\"\\nNearest neighbors to '{word.strip()}':\")\n",
    "    for i in range(1, k + 1):\n",
    "        idx = top_k.indices[i].item()\n",
    "        sim = top_k.values[i].item()\n",
    "        token_str = tokenizer.decode([idx])\n",
    "        print(f\"  {i}. '{token_str}' (sim={sim:.3f})\")\n",
    "\n",
    "find_nearest_tokens(\" cat\")\n",
    "find_nearest_tokens(\" king\")\n",
    "find_nearest_tokens(\" Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Select interesting tokens\n",
    "token_groups = {\n",
    "    'animals': [' cat', ' dog', ' bird', ' fish', ' horse', ' bear'],\n",
    "    'numbers': [' one', ' two', ' three', ' four', ' five', ' six'],\n",
    "    'countries': [' France', ' Germany', ' Japan', ' China', ' India', ' Brazil'],\n",
    "    'colors': [' red', ' blue', ' green', ' yellow', ' black', ' white'],\n",
    "}\n",
    "\n",
    "colors_map = {'animals': 'green', 'numbers': 'orange', 'countries': 'blue', 'colors': 'red'}\n",
    "\n",
    "# Collect embeddings\n",
    "all_tokens = []\n",
    "all_embeddings = []\n",
    "all_groups = []\n",
    "\n",
    "for group, tokens in token_groups.items():\n",
    "    for token in tokens:\n",
    "        all_tokens.append(token.strip())\n",
    "        all_embeddings.append(get_token_embedding(token).numpy())\n",
    "        all_groups.append(group)\n",
    "\n",
    "embeddings_matrix = np.stack(all_embeddings)\n",
    "\n",
    "# PCA to 2D\n",
    "pca = PCA(n_components=2)\n",
    "coords = pca.fit_transform(embeddings_matrix)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "for group in token_groups:\n",
    "    mask = [g == group for g in all_groups]\n",
    "    group_coords = coords[mask]\n",
    "    group_tokens = [t for t, m in zip(all_tokens, mask) if m]\n",
    "    plt.scatter(group_coords[:, 0], group_coords[:, 1], \n",
    "                label=group, c=colors_map[group], s=100, alpha=0.7)\n",
    "    for j, token in enumerate(group_tokens):\n",
    "        plt.annotate(token, (group_coords[j, 0], group_coords[j, 1]),\n",
    "                     fontsize=9, ha='center', va='bottom', \n",
    "                     textcoords='offset points', xytext=(0, 5))\n",
    "\n",
    "plt.legend()\n",
    "plt.title('GPT-2 Token Embeddings (PCA 2D Projection)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Similar tokens cluster together — this is what training produces!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Key Takeaways\n\n1. **`nn.Embedding` is a learnable lookup table.** It stores a weight matrix of shape `[vocab_size, embed_dim]` and indexing by token ID returns one row. Equivalent to one-hot x matrix, without the sparse vector.\n2. **Cosine similarity reveals structure.** Random embeddings have ~0 similarity between all pairs. After training, semantically similar tokens cluster together in embedding space.\n3. **Sinusoidal positional encoding gives each position a unique, smooth encoding** using multi-frequency waves. Low dimensions change slowly (coarse position), high dimensions change fast (fine position).\n4. **Token embedding + positional encoding = the model's input.** Without position, the model sees a bag of words and cannot distinguish \"dog bites man\" from \"man bites dog.\"\n5. **Everything downstream operates on these vectors.** Attention, transformer blocks, the whole model — it all starts from this `[seq_len, embed_dim]` tensor."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}