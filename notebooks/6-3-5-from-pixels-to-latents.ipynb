{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Pixels to Latents\n",
    "\n",
    "**Module 6.3, Lesson 5** | CourseAI\n",
    "\n",
    "In the lesson, you learned that latent diffusion runs the **same diffusion algorithm** you implemented in Module 6.2, operating on the VAE's compressed 64x64x4 latent tensors instead of 512x512x3 pixel images. Now you will explore this hands-on with Stable Diffusion's actual components.\n",
    "\n",
    "**What you will do:**\n",
    "- Load Stable Diffusion's pre-trained VAE and encode/decode real images to see the 48x compression in action\n",
    "- Visualize the 4 latent channels as heatmaps and interpolate between encoded images in latent space\n",
    "- Compute the compression ratio and estimate the computational cost savings that make SD practical\n",
    "- Trace the full encode-diffuse-decode pipeline step by step using real SD components\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "This is a CONSOLIDATE notebook. No new algorithms, no new math. Every concept here is something you already know from Modules 6.1, 6.2, and 6.3. The exercises verify that you can connect the pieces and trace data through the full pipeline.\n",
    "\n",
    "**Estimated time:** 30-45 minutes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies and import everything."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q diffusers transformers accelerate\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB')\n",
    "\n",
    "print('\\nSetup complete.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Helpers\n",
    "\n",
    "Utilities for loading images and converting between PIL images and tensors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_image_from_url(url, size=512):\n",
    "    \"\"\"Download an image from a URL and resize to size x size.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    img = img.resize((size, size), Image.LANCZOS)\n",
    "    return img\n",
    "\n",
    "\n",
    "def pil_to_tensor(img):\n",
    "    \"\"\"Convert a PIL image to a tensor in the format the VAE expects.\n",
    "    \n",
    "    SD's VAE expects: (batch, 3, H, W) with values in [-1, 1].\n",
    "    PIL images are (H, W, 3) with values in [0, 255].\n",
    "    \"\"\"\n",
    "    arr = np.array(img).astype(np.float32) / 255.0  # [0, 1]\n",
    "    arr = arr * 2.0 - 1.0  # [-1, 1]\n",
    "    tensor = torch.from_numpy(arr).permute(2, 0, 1).unsqueeze(0)  # (1, 3, H, W)\n",
    "    return tensor.to(device)\n",
    "\n",
    "\n",
    "def tensor_to_pil(tensor):\n",
    "    \"\"\"Convert a VAE output tensor back to a PIL image.\n",
    "    \n",
    "    VAE output: (batch, 3, H, W) with values roughly in [-1, 1].\n",
    "    \"\"\"\n",
    "    img = tensor.detach().cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "    img = (img + 1.0) / 2.0  # [-1, 1] -> [0, 1]\n",
    "    img = np.clip(img, 0.0, 1.0)\n",
    "    return Image.fromarray((img * 255).astype(np.uint8))\n",
    "\n",
    "\n",
    "# Load sample images -- diverse subjects for comparing latent representations\n",
    "image_urls = {\n",
    "    'cat': 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Cat_November_2010-1a.jpg/1200px-Cat_November_2010-1a.jpg',\n",
    "    'landscape': 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Kluane_National_Park_-_June_2015.jpg/1280px-Kluane_National_Park_-_June_2015.jpg',\n",
    "    'building': 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a1/Neues_Rathaus_Hannover_2013.jpg/1024px-Neues_Rathaus_Hannover_2013.jpg',\n",
    "}\n",
    "\n",
    "images = {}\n",
    "for name, url in image_urls.items():\n",
    "    try:\n",
    "        images[name] = load_image_from_url(url)\n",
    "        print(f'Loaded {name}: {images[name].size}')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to load {name}: {e}')\n",
    "        # Fallback: generate a simple test pattern\n",
    "        arr = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)\n",
    "        images[name] = Image.fromarray(arr)\n",
    "        print(f'  Using random test pattern for {name}')\n",
    "\n",
    "print(f'\\nLoaded {len(images)} images, all resized to 512x512.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Explore SD's VAE Encoder and Decoder [Guided]\n",
    "\n",
    "In Module 6.1, you built a VAE that compressed 28x28x1 Fashion-MNIST images (784 values) into a 32-dimensional latent code. Stable Diffusion's VAE does the same thing at a much larger scale: it compresses 512x512x3 images (786,432 values) into 64x64x4 latent tensors (16,384 values).\n",
    "\n",
    "The lesson called the VAE a **translator between two languages**: pixel language (what you see) and latent language (what the diffusion model speaks). Now you will load this translator and use it.\n",
    "\n",
    "**Before running, predict:**\n",
    "- What shape will the latent tensor be after encoding a 512x512x3 image? (Think: the lesson said 64x64x4. With a batch dimension, that is `(1, 4, 64, 64)`.)\n",
    "- How many values is that? And what is the compression ratio compared to the input? (16,384 values vs 786,432 values = 48x compression.)\n",
    "- Will the decoded image look identical to the original, or slightly different? (Slightly different -- the VAE is lossy compression. But SD's VAE uses perceptual + adversarial loss, so it should be much sharper than your Module 6.1 toy VAE.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from diffusers import AutoencoderKL\n",
    "\n",
    "# Load Stable Diffusion v1.5's VAE\n",
    "# This is the exact same VAE used in the full SD pipeline.\n",
    "# It was trained separately on image reconstruction, then FROZEN.\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n",
    "    subfolder='vae',\n",
    "    torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,\n",
    ")\n",
    "vae = vae.to(device)\n",
    "vae.eval()  # Inference mode -- no training, no gradients\n",
    "\n",
    "print(f'VAE loaded on {device}')\n",
    "print(f'Parameters: {sum(p.numel() for p in vae.parameters()) / 1e6:.1f}M')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Encode and decode the cat image\n",
    "cat_img = images['cat']\n",
    "cat_tensor = pil_to_tensor(cat_img)\n",
    "\n",
    "print('=== Encoding ===')\n",
    "print(f'Input shape: {cat_tensor.shape}')   # (1, 3, 512, 512)\n",
    "print(f'Input values: [{cat_tensor.min():.1f}, {cat_tensor.max():.1f}]')\n",
    "print(f'Input size: {cat_tensor.shape[1]} x {cat_tensor.shape[2]} x {cat_tensor.shape[3]}')\n",
    "print(f'Input values count: {cat_tensor[0].numel():,}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    # The VAE encoder outputs a distribution (mean + logvar), just like your Module 6.1 VAE.\n",
    "    # .latent_dist gives us the distribution; .sample() applies the reparameterization trick.\n",
    "    latent_dist = vae.encode(cat_tensor).latent_dist\n",
    "    latent = latent_dist.sample()\n",
    "    \n",
    "    # SD applies a scaling factor to the latents (keeps them in a nice range for diffusion)\n",
    "    latent = latent * vae.config.scaling_factor\n",
    "\n",
    "print(f'\\nLatent shape: {latent.shape}')    # (1, 4, 64, 64)\n",
    "print(f'Latent values: [{latent.min():.2f}, {latent.max():.2f}]')\n",
    "print(f'Latent size: {latent.shape[1]} x {latent.shape[2]} x {latent.shape[3]}')\n",
    "print(f'Latent values count: {latent[0].numel():,}')\n",
    "\n",
    "# Compression ratio\n",
    "input_values = cat_tensor[0].numel()\n",
    "latent_values = latent[0].numel()\n",
    "ratio = input_values / latent_values\n",
    "print(f'\\nCompression ratio: {input_values:,} / {latent_values:,} = {ratio:.1f}x')\n",
    "print(f'The VAE compressed the image by {ratio:.0f}x.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Decode: latent -> pixel space\n",
    "print('=== Decoding ===')\n",
    "with torch.no_grad():\n",
    "    # Undo the scaling factor before decoding\n",
    "    decoded = vae.decode(latent / vae.config.scaling_factor).sample\n",
    "\n",
    "print(f'Decoded shape: {decoded.shape}')  # (1, 3, 512, 512)\n",
    "print(f'Decoded values: [{decoded.min():.2f}, {decoded.max():.2f}]')\n",
    "\n",
    "reconstructed_img = tensor_to_pil(decoded)\n",
    "\n",
    "# Compare original vs reconstruction\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(cat_img)\n",
    "axes[0].set_title('Original (512x512x3)', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Show the latent as a placeholder (just channel 0)\n",
    "latent_vis = latent[0, 0].float().cpu().numpy()\n",
    "axes[1].imshow(latent_vis, cmap='magma')\n",
    "axes[1].set_title('Latent channel 0 (64x64)', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(reconstructed_img)\n",
    "axes[2].set_title('Reconstructed (512x512x3)', fontsize=12)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle('SD VAE: Encode -> Latent (48x smaller) -> Decode', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute reconstruction quality (MSE and PSNR)\n",
    "mse = F.mse_loss(decoded.float(), cat_tensor.float()).item()\n",
    "psnr = 10 * np.log10(4.0 / mse)  # input range is [-1,1] so max range is 2, max^2 = 4\n",
    "print(f'\\nReconstruction MSE: {mse:.6f}')\n",
    "print(f'Reconstruction PSNR: {psnr:.1f} dB')\n",
    "print()\n",
    "print('Observations:')\n",
    "print('  1. The reconstruction is very close to the original.')\n",
    "print('     SD\\'s VAE uses perceptual + adversarial loss, NOT just MSE,')\n",
    "print('     producing much sharper results than your Module 6.1 toy VAE.')\n",
    "print('  2. The latent is 64x64x4 -- NOT a small image. It is an abstract')\n",
    "print('     learned representation. Channel 0 shown above has structure,')\n",
    "print('     but it is NOT a downscaled photo.')\n",
    "print('  3. The 48x compression is dramatic: 786,432 values -> 16,384 values.')\n",
    "print('     The diffusion model will process these tiny tensors at every step.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You loaded the exact VAE used in Stable Diffusion and saw the encode-decode pipeline in action:\n",
    "\n",
    "1. **Encode:** 512x512x3 image (786,432 values) -> 64x64x4 latent (16,384 values). A **48x compression**.\n",
    "2. **Decode:** 64x64x4 latent -> 512x512x3 reconstructed image. Nearly identical to the original.\n",
    "\n",
    "The reconstruction quality is dramatically better than your Module 6.1 toy VAE. SD's VAE preserves edges, textures, and fine detail because it uses perceptual and adversarial losses instead of plain MSE. The VAE is a **high-fidelity translator** -- good enough that the diffusion model can work entirely in latent space without quality concerns.\n",
    "\n",
    "The latent tensor is an abstract representation, not a small image. You cannot interpret it visually as a photo. The decoder is essential -- it translates from the latent language back to pixels.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Visualize the Latent Space [Guided]\n",
    "\n",
    "The latent tensor has 4 channels. What do these channels capture? In your Module 6.1 VAE, the 32-dimensional bottleneck was too abstract to interpret individually. SD's latent channels are spatial (64x64), so we can visualize them as heatmaps.\n",
    "\n",
    "We will also interpolate between two encoded images in latent space -- a direct callback to your **Exploring Latent Spaces** experience in Module 6.1. There, you interpolated between a T-shirt and a sneaker and got coherent intermediates. The same property holds here, at much higher resolution.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Will the 4 latent channels look the same or different from each other? (Different -- each channel captures different aspects of the image, similar to how CNN feature maps at different channels capture different patterns.)\n",
    "- When you interpolate between two encoded images in latent space and decode the intermediates, will you get coherent intermediate images or garbage? (Coherent intermediates -- the VAE's KL regularization organized the latent space. You proved this in Module 6.1. \"Clouds, not points.\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Encode all three images\n",
    "latents = {}\n",
    "for name, img in images.items():\n",
    "    tensor = pil_to_tensor(img)\n",
    "    with torch.no_grad():\n",
    "        dist = vae.encode(tensor).latent_dist\n",
    "        z = dist.sample() * vae.config.scaling_factor\n",
    "    latents[name] = z\n",
    "    print(f'{name}: encoded to {z.shape}')\n",
    "\n",
    "# Visualize the 4 channels for each image\n",
    "fig, axes = plt.subplots(len(images), 5, figsize=(18, 3.5 * len(images)))\n",
    "\n",
    "for row, (name, img) in enumerate(images.items()):\n",
    "    # Original image\n",
    "    axes[row, 0].imshow(img)\n",
    "    axes[row, 0].set_title(f'{name} (original)', fontsize=10)\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    # 4 latent channels\n",
    "    z = latents[name]\n",
    "    for ch in range(4):\n",
    "        channel_data = z[0, ch].float().cpu().numpy()\n",
    "        axes[row, ch + 1].imshow(channel_data, cmap='magma')\n",
    "        axes[row, ch + 1].set_title(f'Channel {ch}', fontsize=10)\n",
    "        axes[row, ch + 1].axis('off')\n",
    "\n",
    "plt.suptitle('Latent channels for different images (64x64 each)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Observations:')\n",
    "print('  1. Different channels capture different aspects of the image.')\n",
    "print('     Some channels emphasize edges, others capture color or brightness.')\n",
    "print('  2. The channels are NOT RGB. They are abstract learned features.')\n",
    "print('     The 4-channel representation encodes perceptual content,')\n",
    "print('     not raw color information.')\n",
    "print('  3. The spatial structure of the original image is visible in the')\n",
    "print('     channels -- the cat\\'s shape, the mountain\\'s outline, the')\n",
    "print('     building\\'s structure. The VAE preserves WHERE things are.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Interpolate between two images in latent space\n",
    "# This is the same experiment from Module 6.1 (Exploring Latent Spaces),\n",
    "# now with SD's VAE at 512x512 resolution.\n",
    "\n",
    "names = list(images.keys())\n",
    "name_a, name_b = names[0], names[1]  # e.g., cat and landscape\n",
    "z_a = latents[name_a]\n",
    "z_b = latents[name_b]\n",
    "\n",
    "# Linear interpolation in latent space\n",
    "n_steps = 7\n",
    "alphas = np.linspace(0, 1, n_steps)\n",
    "\n",
    "interpolated_images = []\n",
    "for alpha in alphas:\n",
    "    z_interp = (1 - alpha) * z_a + alpha * z_b\n",
    "    with torch.no_grad():\n",
    "        decoded = vae.decode(z_interp / vae.config.scaling_factor).sample\n",
    "    interpolated_images.append(tensor_to_pil(decoded))\n",
    "\n",
    "# Display the interpolation\n",
    "fig, axes = plt.subplots(1, n_steps, figsize=(20, 3.5))\n",
    "for i, (img, alpha) in enumerate(zip(interpolated_images, alphas)):\n",
    "    axes[i].imshow(img)\n",
    "    if i == 0:\n",
    "        axes[i].set_title(f'{name_a}\\n(alpha=0)', fontsize=9)\n",
    "    elif i == n_steps - 1:\n",
    "        axes[i].set_title(f'{name_b}\\n(alpha=1)', fontsize=9)\n",
    "    else:\n",
    "        axes[i].set_title(f'alpha={alpha:.2f}', fontsize=9)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f'Latent space interpolation: {name_a} -> {name_b}', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Interpolation: {name_a} -> {name_b} in {n_steps} steps')\n",
    "print()\n",
    "print('Observations:')\n",
    "print('  1. The intermediates are COHERENT images, not ghostly overlaps.')\n",
    "print('     This is latent-space interpolation, not pixel-space averaging.')\n",
    "print('  2. This is the same property you saw in Module 6.1 with Fashion-MNIST:')\n",
    "print('     the VAE\\'s KL regularization organized the latent space so that')\n",
    "print('     every point decodes to something meaningful.')\n",
    "print('  3. This continuity is what makes latent diffusion POSSIBLE.')\n",
    "print('     The denoising trajectory passes through latent space, and at every')\n",
    "print('     step, the current point must decode to something coherent.')\n",
    "print('     The \"roads\" in latent space (from Module 6.1) are what the')\n",
    "print('     diffusion model walks along during denoising.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You explored SD's latent space in two ways:\n",
    "\n",
    "1. **Channel visualization:** The 4 latent channels capture different aspects of the image -- edges, brightness, color structure. They are abstract learned features, NOT RGB channels or a downscaled thumbnail. The spatial structure is preserved (you can see shapes), but the representation is qualitatively different from pixels.\n",
    "\n",
    "2. **Latent interpolation:** Linear interpolation between two encoded images produces coherent intermediate images, not ghostly overlaps. This is the same property you proved in Module 6.1 -- the VAE's KL regularization organizes the latent space so every point decodes to something meaningful.\n",
    "\n",
    "This continuity is the foundation for latent diffusion. The denoising trajectory is a path through latent space from pure noise (z_T) to a clean latent (z_0). At every point along that path, the latent must be decodable into something coherent. The organized latent space guarantees this. **The VAE built the roads; diffusion walks them.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Compute the Compression Ratio and Cost Savings [Supported]\n",
    "\n",
    "The lesson showed that the VAE compresses 512x512x3 to 64x64x4 -- a 48x reduction. But how does this translate to actual computational savings during diffusion?\n",
    "\n",
    "Convolution cost scales roughly with spatial area (H x W). The U-Net runs at every denoising step, so the savings multiply by the number of steps. Your task: compute the concrete numbers and connect them to your capstone timing experience.\n",
    "\n",
    "**Hints:**\n",
    "- Pixel-space spatial area: 512 x 512 = 262,144\n",
    "- Latent-space spatial area: 64 x 64 = 4,096\n",
    "- `spatial_ratio = pixel_area / latent_area`\n",
    "- For the capstone comparison: your 28x28 MNIST model's spatial area was 784"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---- Part 1: Value count and compression ratio ----\n",
    "\n",
    "# TODO: Compute the number of values in a pixel-space image and a latent tensor.\n",
    "# Pixel-space: 512 * 512 * 3\n",
    "# Latent-space: 64 * 64 * 4\n",
    "pixel_values = None   # TODO: replace with the computation\n",
    "latent_values = None  # TODO: replace with the computation\n",
    "compression_ratio = None  # TODO: pixel_values / latent_values\n",
    "\n",
    "print('=== Value Count ===')\n",
    "if pixel_values is not None:\n",
    "    print(f'Pixel-space:  512 x 512 x 3 = {pixel_values:>10,} values')\n",
    "    print(f'Latent-space:  64 x  64 x 4 = {latent_values:>10,} values')\n",
    "    print(f'Compression ratio:             {compression_ratio:.1f}x')\n",
    "else:\n",
    "    print('Fill in the TODOs above to compute the compression ratio.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---- Part 2: Spatial area and convolution cost ----\n",
    "# Convolution cost scales with spatial area (H x W).\n",
    "# The U-Net processes feature maps at this spatial resolution.\n",
    "\n",
    "# TODO: Compute the spatial area for pixel-space and latent-space.\n",
    "pixel_spatial_area = None    # TODO: 512 * 512\n",
    "latent_spatial_area = None   # TODO: 64 * 64\n",
    "spatial_ratio = None         # TODO: pixel_spatial_area / latent_spatial_area\n",
    "\n",
    "print('=== Spatial Area (drives convolution cost) ===')\n",
    "if pixel_spatial_area is not None:\n",
    "    print(f'Pixel-space:  512 x 512 = {pixel_spatial_area:>10,} spatial positions')\n",
    "    print(f'Latent-space:  64 x  64 = {latent_spatial_area:>10,} spatial positions')\n",
    "    print(f'Spatial area ratio:        {spatial_ratio:.0f}x fewer spatial positions')\n",
    "    print()\n",
    "    print(f'The U-Net processes {spatial_ratio:.0f}x fewer spatial positions per step.')\n",
    "    print(f'Over 50 denoising steps, that is {spatial_ratio:.0f} x 50 = {spatial_ratio * 50:.0f}x')\n",
    "    print(f'fewer total spatial computations.')\n",
    "else:\n",
    "    print('Fill in the TODOs above.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---- Part 3: Connect to your capstone timing ----\n",
    "# In Build a Diffusion Model (6.2.5), you timed pixel-space diffusion on 28x28 MNIST.\n",
    "# How does 512x512 pixel-space compare? And how does 64x64 latent-space compare?\n",
    "\n",
    "capstone_spatial_area = 28 * 28  # your MNIST images were 28x28 = 784 spatial positions\n",
    "\n",
    "# TODO: Compute how many times larger 512x512 is compared to 28x28\n",
    "# Hint: pixel_spatial_area / capstone_spatial_area\n",
    "pixel_vs_capstone = None  # TODO\n",
    "\n",
    "# TODO: Compute how many times larger 64x64 is compared to 28x28\n",
    "# Hint: latent_spatial_area / capstone_spatial_area\n",
    "latent_vs_capstone = None  # TODO\n",
    "\n",
    "print('=== Comparison to Your Module 6.2 Capstone ===')\n",
    "if pixel_vs_capstone is not None and latent_vs_capstone is not None:\n",
    "    print(f'Your capstone: 28 x 28 = {capstone_spatial_area:,} spatial positions')\n",
    "    print()\n",
    "    print(f'512x512 pixel-space: {pixel_vs_capstone:.0f}x larger than your capstone')\n",
    "    print(f'  If your capstone took T minutes, pixel-space SD would take ~{pixel_vs_capstone:.0f}T minutes.')\n",
    "    print(f'  For T=2 minutes: ~{pixel_vs_capstone * 2:.0f} minutes ({pixel_vs_capstone * 2 / 60:.0f} hours). Impractical.')\n",
    "    print()\n",
    "    print(f'64x64 latent-space: {latent_vs_capstone:.1f}x larger than your capstone')\n",
    "    print(f'  If your capstone took T minutes, latent-space SD would take ~{latent_vs_capstone:.1f}T minutes.')\n",
    "    print(f'  For T=2 minutes: ~{latent_vs_capstone * 2:.1f} minutes. Practical!')\n",
    "    print()\n",
    "    print(f'The difference between \"impractical\" ({pixel_vs_capstone:.0f}x your capstone)')\n",
    "    print(f'and \"runs on a consumer GPU\" ({latent_vs_capstone:.1f}x your capstone)')\n",
    "    print(f'is the VAE compression.')\n",
    "else:\n",
    "    print('Fill in the TODOs above.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The key insight: the compression ratio is about **value count** (48x), but the computational savings for the U-Net are about **spatial area** (64x). Both are dramatic, and both make latent diffusion practical.\n",
    "\n",
    "**Part 1: Value count**\n",
    "```python\n",
    "pixel_values = 512 * 512 * 3       # 786,432\n",
    "latent_values = 64 * 64 * 4        # 16,384\n",
    "compression_ratio = pixel_values / latent_values  # 48.0x\n",
    "```\n",
    "\n",
    "**Part 2: Spatial area**\n",
    "```python\n",
    "pixel_spatial_area = 512 * 512      # 262,144\n",
    "latent_spatial_area = 64 * 64       # 4,096\n",
    "spatial_ratio = pixel_spatial_area / latent_spatial_area  # 64.0x\n",
    "```\n",
    "\n",
    "Note: the spatial ratio (64x) is different from the compression ratio (48x) because pixels have 3 channels while latents have 4 channels. The spatial ratio is what matters for convolution cost, since convolution scales with H x W.\n",
    "\n",
    "**Part 3: Capstone comparison**\n",
    "```python\n",
    "pixel_vs_capstone = pixel_spatial_area / capstone_spatial_area    # ~335x\n",
    "latent_vs_capstone = latent_spatial_area / capstone_spatial_area  # ~5.2x\n",
    "```\n",
    "\n",
    "If your 28x28 capstone took 2 minutes, pixel-space diffusion at 512x512 would take ~670 minutes (11 hours). Latent-space diffusion at 64x64 would take ~10 minutes. That is the difference between \"impractical\" and \"runs on a consumer GPU.\"\n",
    "\n",
    "Common mistake: confusing the compression ratio (48x, based on total values) with the spatial ratio (64x, based on H x W). For convolution cost, spatial area is what matters.\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 4: Trace the Full Pipeline [Independent]\n\nYou now know every component of Stable Diffusion. In this exercise, you will manually execute the encode-diffuse-decode pipeline step by step, using real pre-trained SD components. No `pipe()` convenience function -- you will call each component yourself.\n\n**Your task:**\n\n1. **Encode a text prompt** using CLIP's text encoder (the lesson calls this the \"WHAT\")\n2. **Encode an image** with the VAE encoder to get the starting latent z_0\n3. **Add noise** at a specific timestep using the diffusion scheduler (the same forward process formula from Module 6.2)\n4. **Run one denoising step** with the U-Net, providing the noisy latent, timestep, and text embeddings\n5. **Decode** the result with the VAE decoder to get a pixel image\n\nAfter completing the pipeline, answer the reflection questions:\n- Which step is the \"encode\" (VAE compression)?\n- Which step is the \"diffuse\" (the part that is identical to Module 6.2)?\n- Which step is the \"decode\" (translation back to pixels)?\n- What would happen if you skipped step 2 and started from pure random noise instead? (That is unconditional generation -- the normal SD sampling process.)\n\n**Key tips:**\n- All SD v1.5 components live under the same model ID with different subfolders -- look at how the VAE was loaded in Exercise 1 for the pattern\n- The `diffusers` and `transformers` libraries provide the U-Net, scheduler, tokenizer, and text encoder -- check their docs for the right classes\n- The scheduler has methods for both the forward process (adding noise) and reverse process (one denoising step)\n- Use `torch.float16` on GPU for memory efficiency; wrap inference in `torch.no_grad()`\n- If you get stuck, the full solution is in the collapsible block below the code cell"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# YOUR CODE HERE\n#\n# Execute the 5 conceptual steps from the markdown above:\n#   1. Encode a text prompt into embeddings (CLIP tokenizer + text encoder)\n#   2. Encode an image into the latent space (VAE encoder, already loaded)\n#   3. Add noise at a specific timestep (diffusion scheduler)\n#   4. Run one U-Net denoising step (provide noisy latent, timestep, text embeddings)\n#   5. Decode the result back to pixel space (VAE decoder)\n#\n# Refer to the diffusers and transformers library docs, or look at how the\n# VAE was loaded in Exercise 1 for patterns.\n# The full solution is in the <details> block below if you get stuck.\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\nThe key insight: you are manually executing the same pipeline that `StableDiffusionPipeline` automates. Each step maps to a concept from a specific module:\n\n- **Text encoding (Step 4):** Module 6.3, Lessons 3-4 (CLIP + cross-attention)\n- **VAE encoding (Step 5):** Module 6.1 (VAE encoder-decoder)\n- **Adding noise (Step 6):** Module 6.2 (forward process, closed-form formula)\n- **U-Net denoising (Step 7):** Module 6.2 (noise prediction) + Module 6.3 (U-Net architecture, conditioning)\n- **VAE decoding (Step 9):** Module 6.1 (decoder translates latent -> pixel)\n\nThe diffusion part (steps 6-8) is **identical** to what you implemented in Module 6.2. The only differences: the tensors are 64x64x4 instead of 28x28x1, and the U-Net receives text embeddings via cross-attention.\n\n**Note on dtype:** On GPU with float16, ensure your input tensors match the model's dtype. The `.to(dtype)` cast on `cat_tensor` below is important -- without it, you may get a dtype mismatch between your float32 image tensor and the float16 VAE/U-Net. On CPU (float32 throughout), this is not an issue.\n\n```python\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import UNet2DConditionModel, DDPMScheduler\n\nmodel_id = 'stable-diffusion-v1-5/stable-diffusion-v1-5'\ndtype = torch.float16 if device.type == 'cuda' else torch.float32\n\n# Step 1: Load text encoder and tokenizer\ntokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder='tokenizer')\ntext_encoder = CLIPTextModel.from_pretrained(\n    model_id, subfolder='text_encoder', torch_dtype=dtype\n).to(device)\n\n# Step 2: Load U-Net\nunet = UNet2DConditionModel.from_pretrained(\n    model_id, subfolder='unet', torch_dtype=dtype\n).to(device)\n\n# Step 3: Load scheduler\nscheduler = DDPMScheduler.from_pretrained(model_id, subfolder='scheduler')\n\n# Step 4: Encode text prompt\nprompt = \"a cat sitting in a garden\"\ntokens = tokenizer(\n    prompt, return_tensors='pt', padding='max_length',\n    max_length=77, truncation=True\n)\nwith torch.no_grad():\n    text_embeddings = text_encoder(\n        tokens.input_ids.to(device)\n    ).last_hidden_state  # (1, 77, 768)\n\nprint(f'Text embeddings shape: {text_embeddings.shape}')\nprint(f'  77 token positions, each with a 768-dim embedding.')\nprint(f'  This is what cross-attention uses as K and V.')\n\n# Step 5: Encode image with VAE\ncat_tensor = pil_to_tensor(images['cat']).to(dtype)\nwith torch.no_grad():\n    z_0 = vae.encode(cat_tensor).latent_dist.sample() * vae.config.scaling_factor\n\nprint(f'\\nLatent z_0 shape: {z_0.shape}')  # (1, 4, 64, 64)\nprint(f'  This is the ENCODED image in latent space.')\n\n# Step 6: Add noise at timestep 500\nnoise = torch.randn_like(z_0)\ntimestep = torch.tensor([500], device=device)\nscheduler.set_timesteps(1000)\nz_t = scheduler.add_noise(z_0, noise, timestep)\n\nprint(f'\\nNoisy latent z_t shape: {z_t.shape}')\nprint(f'  Same formula from Module 6.2:')\nprint(f'  z_t = sqrt(alpha_bar_t) * z_0 + sqrt(1-alpha_bar_t) * noise')\n\n# Step 7: One U-Net denoising step\nwith torch.no_grad():\n    noise_pred = unet(\n        z_t, timestep,\n        encoder_hidden_states=text_embeddings\n    ).sample\n\nprint(f'\\nPredicted noise shape: {noise_pred.shape}')  # (1, 4, 64, 64)\nprint(f'  The U-Net predicts the noise to subtract.')\nprint(f'  Same as Module 6.2 -- the model predicts epsilon.')\n\n# Step 8: Reverse step\nstep_output = scheduler.step(noise_pred, timestep.item(), z_t)\nz_denoised = step_output.prev_sample\n\nprint(f'\\nDenoised latent shape: {z_denoised.shape}')\nprint(f'  One step closer to z_0. In full sampling, you would')\nprint(f'  repeat this for all timesteps from T down to 0.')\n\n# Step 9: Decode with VAE\nwith torch.no_grad():\n    decoded_noisy = vae.decode(z_t / vae.config.scaling_factor).sample\n    decoded_denoised = vae.decode(z_denoised / vae.config.scaling_factor).sample\n\n# Step 10: Display\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\naxes[0].imshow(images['cat'])\naxes[0].set_title('Original image', fontsize=11)\naxes[0].axis('off')\n\naxes[1].imshow(tensor_to_pil(decoded_noisy))\naxes[1].set_title(f'Noisy (t=500, decoded)', fontsize=11)\naxes[1].axis('off')\n\naxes[2].imshow(tensor_to_pil(decoded_denoised))\naxes[2].set_title('After one denoise step (decoded)', fontsize=11)\naxes[2].axis('off')\n\nplt.suptitle('Encode -> Add Noise -> Denoise (1 step) -> Decode', fontsize=13)\nplt.tight_layout()\nplt.show()\n\nprint('\\n=== Pipeline Summary ===')\nprint(f'1. Text encoding (CLIP):    prompt -> {text_embeddings.shape}')\nprint(f'2. Image encoding (VAE):    {cat_tensor.shape} -> {z_0.shape}')\nprint(f'3. Add noise (forward):     {z_0.shape} -> {z_t.shape} at t=500')\nprint(f'4. Denoise (U-Net):         {z_t.shape} -> noise pred {noise_pred.shape}')\nprint(f'5. Reverse step:            {z_t.shape} -> {z_denoised.shape}')\nprint(f'6. Decode (VAE):            {z_denoised.shape} -> {decoded_denoised.shape}')\nprint()\nprint('Which parts are identical to Module 6.2?')\nprint('  Steps 3, 4, 5: adding noise, predicting noise, reverse step.')\nprint('  The ALGORITHM is the same. The tensors are 64x64x4 instead of 28x28x1.')\nprint('  The U-Net also receives text embeddings (cross-attention from Module 6.3).')\nprint()\nprint('What is NEW compared to Module 6.2?')\nprint('  Steps 1, 2, 6: text encoding, VAE encoding, VAE decoding.')\nprint('  These are the \"bookends\" -- the VAE translates between pixel and latent space.')\nprint('  The text encoder provides the WHAT signal via cross-attention.')\n```\n\nNote: one denoising step at t=500 will NOT produce a clean image. In full SD sampling, you would loop from t=T down to t=0 (typically 50 steps with DDIM). The point here is to trace the pipeline, not to generate a finished image.\n\nIf you started from pure random noise (skipping step 5), that is exactly what normal SD generation does: sample z_T from N(0,I), then denoise for 50 steps, then decode. The image encoding step is only used for image-to-image generation or for exercises like this one.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **SD's VAE compresses 512x512x3 images to 64x64x4 latents -- a 48x reduction in values, 64x reduction in spatial area.** The reconstruction quality is high because SD's VAE uses perceptual + adversarial losses, not plain MSE. The VAE is a high-fidelity translator between pixel space and latent space.\n",
    "\n",
    "2. **The latent space is organized and continuous, just like your Module 6.1 VAE.** Interpolation produces coherent intermediates. Every point decodes to something meaningful. This is what makes latent diffusion possible -- the denoising trajectory stays in meaningful territory at every step. The VAE built the roads; diffusion walks them.\n",
    "\n",
    "3. **The computational savings are dramatic and concrete.** 64x fewer spatial positions per denoising step. Your 28x28 capstone was ~5x smaller than latent space (manageable) but ~335x smaller than pixel space at 512x512 (impractical). The VAE compression is the difference between \"runs on a consumer GPU\" and \"requires a data center.\"\n",
    "\n",
    "4. **The full pipeline has clear stages: encode (CLIP + VAE) -> diffuse (identical to Module 6.2) -> decode (VAE).** Every formula from Module 6.2 applies unchanged. The noise schedule, the forward process, the reverse step, the training loss -- all the same. Only the tensor dimensions and the addition of text conditioning via cross-attention are different.\n",
    "\n",
    "5. **That is Stable Diffusion.** VAE for compression, diffusion for generation, U-Net for multi-scale denoising, sinusoidal embeddings for timestep, cross-attention for text, CFG for text amplification, latent space for speed. Every piece solves a specific problem. You understand all of them."
   ]
  }
 ]
}