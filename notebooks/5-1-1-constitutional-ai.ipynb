{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constitutional AI\n",
    "\n",
    "In this notebook, you'll experience the constitutional AI mechanism firsthand by running the critique-and-revision loop with real LLM API calls.\n",
    "\n",
    "**What you'll do:**\n",
    "- Write a constitutional principle and use it to generate an AI critique of a model response\n",
    "- Generate a revised response from a critique and create a preference pair (the training data that CAI produces at scale)\n",
    "- Test a deliberately bad principle and observe how critique quality degrades\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones â€” they reveal gaps in your mental model.\n",
    "\n",
    "**Important:** These exercises demonstrate the critique-and-revision *mechanism* that constitutional AI uses to generate training data. We are doing manually what CAI does at scale. The mechanism is the same; the scale is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup â€” self-contained for Google Colab\n!pip install -q openai\n\nimport os\nimport textwrap\nfrom openai import OpenAI\n\n# --- API Key Setup ---\n# Option 1: Set your API key as an environment variable (recommended)\n#   In Colab: go to the key icon in the left sidebar, add OPENAI_API_KEY\n# Option 2: Paste it directly (less secure, don't commit this)\n#   os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n# You can also use any OpenAI-compatible API (e.g., local Ollama, Together AI)\n# by changing the base_url:\n#   client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n\nclient = OpenAI()\n\n# Use a small, cheap model for the exercises\nMODEL = \"gpt-4o-mini\"\n\n\ndef call_llm(system_prompt: str, user_prompt: str, temperature: float = 0.3) -> str:\n    \"\"\"Call the LLM with a system prompt and user prompt. Returns the response text.\"\"\"\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ],\n        temperature=temperature,\n        max_tokens=500,\n    )\n    return response.choices[0].message.content\n\n\ndef print_wrapped(text: str, width: int = 80, prefix: str = \"\"):\n    \"\"\"Print text with word wrapping for readability.\"\"\"\n    for line in text.split(\"\\n\"):\n        wrapped = textwrap.fill(line, width=width, initial_indent=prefix, subsequent_indent=prefix)\n        print(wrapped)\n\n\n# --- Shared utilities used across exercises ---\n\nCRITIQUE_SYSTEM_PROMPT = (\n    \"You are an AI assistant tasked with evaluating responses according to a \"\n    \"specific principle. Your job is to critique the response by identifying \"\n    \"specific ways it violates or could better satisfy the principle. \"\n    \"Be concrete and specific in your critique.\"\n)\n\nREVISION_SYSTEM_PROMPT = (\n    \"You are an AI assistant tasked with revising a response to better satisfy \"\n    \"a constitutional principle. You will be given the original response and a \"\n    \"critique identifying specific problems. Rewrite the response to address \"\n    \"those problems while remaining helpful.\"\n)\n\n\ndef generate_critique(prompt: str, response: str, principle: str) -> str:\n    \"\"\"Generate a critique of a response using a constitutional principle.\n\n    This is the CRITIQUE step of the critique-and-revision loop.\n    In real constitutional AI, this runs thousands of times to generate training data.\n    \"\"\"\n    user_msg = (\n        f\"User prompt: {prompt}\\n\\n\"\n        f\"Model response: {response}\\n\\n\"\n        f\"Constitutional principle: {principle}\\n\\n\"\n        f\"Critique this response according to the principle above. \"\n        f\"Identify specific problems and explain how the response \"\n        f\"violates or could better satisfy the principle.\"\n    )\n    return call_llm(CRITIQUE_SYSTEM_PROMPT, user_msg)\n\n\ndef generate_revision(prompt: str, response: str, critique: str, principle: str) -> str:\n    \"\"\"Generate a revised response that addresses the critique.\n\n    This is the REVISION step of the critique-and-revision loop.\n    The revised response becomes SFT training data in real constitutional AI.\n    \"\"\"\n    user_msg = (\n        f\"User prompt: {prompt}\\n\\n\"\n        f\"Original response: {response}\\n\\n\"\n        f\"Critique (based on principle '{principle}'): {critique}\\n\\n\"\n        f\"Rewrite the response to address the problems identified in the critique. \"\n        f\"The revised response should satisfy the constitutional principle \"\n        f\"while remaining helpful and informative.\"\n    )\n    return call_llm(REVISION_SYSTEM_PROMPT, user_msg)\n\n\n# Quick test to verify the API is working\ntest = call_llm(\"You are a helpful assistant.\", \"Say 'API connection successful' and nothing else.\")\nprint(test)\nprint(f\"\\nUsing model: {MODEL}\")\nprint(\"Setup complete.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Principle-Guided Critique (Guided)\n",
    "\n",
    "In constitutional AI, the first step of the critique-and-revision loop is to take a model response and have an AI critique it *in light of a specific principle*. The principle steers the critique â€” different principles produce different critiques of the same response.\n",
    "\n",
    "We'll demonstrate this with a concrete scenario: a model gives a potentially problematic response, and we apply two different constitutional principles to critique it. The same response, two different lenses.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Given the principle \"Choose the response that better acknowledges uncertainty,\" what will the critique focus on?\n",
    "- Given the principle \"Choose the response that is less likely to cause physical harm,\" will the critique be different? How?\n",
    "- Will one principle produce a more actionable critique than the other for this specific response?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# The scenario: a user asks for medical advice and the model responds confidently\nPROMPT = \"I've had a persistent headache for three days. What should I do?\"\n\nINITIAL_RESPONSE = (\n    \"You should take 800mg of ibuprofen every 6 hours. If that doesn't work, \"\n    \"try alternating with acetaminophen. Make sure you're drinking plenty of water \"\n    \"and getting enough sleep. The headache should resolve within a day or two.\"\n)\n\n# Two constitutional principles that evaluate the same response differently\nPRINCIPLE_A = (\n    \"Choose the response that better acknowledges uncertainty and the limits \"\n    \"of the model's knowledge, especially regarding medical topics.\"\n)\n\nPRINCIPLE_B = (\n    \"Choose the response that is less likely to cause physical harm to the user, \"\n    \"particularly by avoiding specific dosage recommendations without knowing \"\n    \"the user's medical history.\"\n)\n\n\n# --- Generate critiques with both principles ---\n# generate_critique() is defined in the setup cell above\n\nprint(\"SCENARIO\")\nprint(\"=\" * 70)\nprint(f\"Prompt: {PROMPT}\")\nprint(f\"\\nInitial response:\")\nprint_wrapped(INITIAL_RESPONSE, prefix=\"  \")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"CRITIQUE A: Acknowledging Uncertainty\")\nprint(\"=\" * 70)\nprint(f\"Principle: {PRINCIPLE_A}\")\nprint()\ncritique_a = generate_critique(PROMPT, INITIAL_RESPONSE, PRINCIPLE_A)\nprint(\"Critique:\")\nprint_wrapped(critique_a, prefix=\"  \")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"CRITIQUE B: Avoiding Physical Harm\")\nprint(\"=\" * 70)\nprint(f\"Principle: {PRINCIPLE_B}\")\nprint()\ncritique_b = generate_critique(PROMPT, INITIAL_RESPONSE, PRINCIPLE_B)\nprint(\"Critique:\")\nprint_wrapped(critique_b, prefix=\"  \")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just observed:** The same response critiqued through two different principles produces two different critiques. Principle A focuses on the *epistemic* problem (the model is too confident about medical advice). Principle B focuses on the *safety* problem (specific dosage recommendations without medical history could cause harm).\n",
    "\n",
    "This is the core insight: **principles steer the feedback**. The critique is not generic â€” it evaluates the response along the specific dimension defined by the principle. In constitutional AI, different principles from the constitution are applied to different prompt-response pairs, generating training data that covers multiple dimensions of alignment simultaneously.\n",
    "\n",
    "Notice that this is a *data generation* process, not an inference-time behavior. The model that gets trained on this data will internalize these quality signals and produce better responses directly â€” without any critique step at inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 2: Revision and Preference Pairs (Supported)\n\nThe critique is only half of the critique-and-revision loop. The second half: use the critique to generate a *revised* response, then pair the original and revised responses as a preference pair. The revised response is preferred over the original. This preference pair is the training data that constitutional AI generates at scale.\n\nYou'll implement the revision step and construct the preference pair. The result is exactly the kind of data that would be used to train a reward model (RLAIF) or directly for SFT.\n\nFill in the TODOs below. Each TODO is 1-3 lines.\n\n**Note:** A working `generate_revision()` is provided in the setup cell. Here you'll build your own version (`my_generate_revision`) to understand the prompt construction.\n\n<details>\n<summary>Hint</summary>\n\nThe revision step asks the model to rewrite its response to address the specific problems identified in the critique. The key is that the revision prompt includes both the original response AND the critique, so the model knows exactly what to fix.\n\n```python\ndef my_generate_revision(prompt, response, critique, principle):\n    user_msg = (\n        f\"User prompt: {prompt}\\n\\n\"\n        f\"Original response: {response}\\n\\n\"\n        f\"Critique (based on principle '{principle}'): {critique}\\n\\n\"\n        f\"Rewrite the response to address the problems identified in the critique. \"\n        f\"The revised response should satisfy the constitutional principle \"\n        f\"while remaining helpful and informative.\"\n    )\n    return call_llm(REVISION_SYSTEM_PROMPT, user_msg)\n```\n\nThe preference pair is simply: (prompt, original_response, revised_response, preference=\"revised\"). This is the same format as human preference data in RLHF â€” just generated by AI instead of human annotators.\n\nCommon mistake: thinking the revision needs to be perfect. It just needs to be *better* than the original along the dimension defined by the principle. The preference is relative (revised > original), not absolute.\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the scenario and critique from Exercise 1.\n",
    "# Each exercise is independent, so we define the inputs here directly.\n",
    "\n",
    "PROMPT_EX2 = \"I've had a persistent headache for three days. What should I do?\"\n",
    "\n",
    "INITIAL_RESPONSE_EX2 = (\n",
    "    \"You should take 800mg of ibuprofen every 6 hours. If that doesn't work, \"\n",
    "    \"try alternating with acetaminophen. Make sure you're drinking plenty of water \"\n",
    "    \"and getting enough sleep. The headache should resolve within a day or two.\"\n",
    ")\n",
    "\n",
    "PRINCIPLE_EX2 = (\n",
    "    \"Choose the response that better acknowledges uncertainty and the limits \"\n",
    "    \"of the model's knowledge, especially regarding medical topics.\"\n",
    ")\n",
    "\n",
    "# Generate a critique (same as Exercise 1, repeated here for independence)\n",
    "critique_ex2 = generate_critique(PROMPT_EX2, INITIAL_RESPONSE_EX2, PRINCIPLE_EX2)\n",
    "\n",
    "print(\"CRITIQUE (for reference):\")\n",
    "print_wrapped(critique_ex2, prefix=\"  \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# The revision step: rewrite the response to address the critique\n# REVISION_SYSTEM_PROMPT and generate_revision() are defined in the setup cell.\n# Here you'll implement your OWN version to understand the prompt construction.\n\n\ndef my_generate_revision(prompt: str, response: str, critique: str, principle: str) -> str:\n    \"\"\"Your implementation of the revision step.\n    \n    This is the REVISION step of the critique-and-revision loop.\n    The revised response becomes SFT training data in real constitutional AI.\n    \"\"\"\n    # TODO: Construct the user message that includes:\n    #   - The original user prompt\n    #   - The original model response\n    #   - The critique (with the principle noted)\n    #   - An instruction to rewrite the response addressing the critique\n    # Follow the pattern from generate_critique() in the setup cell.\n    # YOUR CODE HERE (1 block, ~6-8 lines for the f-string)\n    user_msg = \"\"\n    \n    return call_llm(REVISION_SYSTEM_PROMPT, user_msg)\n\n\n# Generate the revised response using YOUR implementation\nrevised_response = my_generate_revision(\n    PROMPT_EX2, INITIAL_RESPONSE_EX2, critique_ex2, PRINCIPLE_EX2\n)\n\nprint(\"REVISED RESPONSE:\")\nprint_wrapped(revised_response, prefix=\"  \")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Construct the preference pair ---\n",
    "# This is the training data format. In RLHF, a human creates this.\n",
    "# In constitutional AI (RLAIF), an AI creates this from principles.\n",
    "\n",
    "# TODO: Create a preference_pair dictionary with these keys:\n",
    "#   \"prompt\" -> the user's original prompt (PROMPT_EX2)\n",
    "#   \"response_a\" -> the initial response (INITIAL_RESPONSE_EX2)\n",
    "#   \"response_b\" -> the revised response (revised_response)\n",
    "#   \"preferred\" -> \"b\" (the revised response is preferred)\n",
    "#   \"principle\" -> the principle used (PRINCIPLE_EX2)\n",
    "# YOUR CODE HERE (1 dictionary)\n",
    "preference_pair = {}\n",
    "\n",
    "\n",
    "# Display the preference pair as training data\n",
    "print(\"PREFERENCE PAIR (Constitutional AI Training Data)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nPrompt:\")\n",
    "print_wrapped(preference_pair.get(\"prompt\", \"[not set]\"), prefix=\"  \")\n",
    "print(f\"\\nResponse A (original):\")\n",
    "print_wrapped(preference_pair.get(\"response_a\", \"[not set]\"), prefix=\"  \")\n",
    "print(f\"\\nResponse B (revised):\")\n",
    "print_wrapped(preference_pair.get(\"response_b\", \"[not set]\"), prefix=\"  \")\n",
    "print(f\"\\nPreferred: Response {preference_pair.get('preferred', '[not set]').upper()}\")\n",
    "print(f\"\\nPrinciple used:\")\n",
    "print_wrapped(preference_pair.get(\"principle\", \"[not set]\"), prefix=\"  \")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"This is EXACTLY the same format as human preference data in RLHF.\")\n",
    "print(\"The only difference: an AI applying a principle created this pair,\")\n",
    "print(\"not a human annotator applying intuition.\")\n",
    "print(\"\\nIn real constitutional AI, this process runs thousands of times\")\n",
    "print(\"across different prompts and principles to generate training data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just built:** The complete critique-and-revision loop for one prompt. The output is a preference pair â€” the same data format used in RLHF, but generated by AI instead of human annotators.\n",
    "\n",
    "The revised response should be meaningfully better along the dimension defined by the principle (acknowledging uncertainty in medical advice). It doesn't need to be perfect â€” it needs to be *better than the original*. The preference is relative (revised > original), not absolute. This is the same insight from RLHF: relative comparisons are more reliable than absolute quality scores.\n",
    "\n",
    "We just did manually what constitutional AI does at scale. The mechanism is identical: principle â†’ critique â†’ revision â†’ preference pair. At scale, this generates millions of preference pairs in hours, compared to the months of human annotation required for InstructGPT's ~33K comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: When Principles Fail (Supported)\n",
    "\n",
    "Constitutional AI is only as good as its constitution. The lesson covered three failure modes: vague principles, conflicting principles, and missing principles. This exercise makes the first failure mode concrete: a deliberately vague principle produces a vague, unhelpful critique.\n",
    "\n",
    "You'll compare a *good* principle against a *bad* (vague) principle on the same response, and observe how the critique quality degrades. This demonstrates the lesson's key insight: **the alignment challenge shifts from \"enough annotators\" to \"right principles.\"**\n",
    "\n",
    "Fill in the TODOs below. Each TODO is 1-3 lines.\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The vague principle should be something like \"Choose the response that is better\" or \"Choose the more helpful response.\" These principles fail because they don't specify *what dimension* of quality to evaluate. The AI critique ends up being generic and non-actionable.\n",
    "\n",
    "```python\n",
    "VAGUE_PRINCIPLE = (\n",
    "    \"Choose the response that is better.\"\n",
    ")\n",
    "```\n",
    "\n",
    "The key observation: with a vague principle, the critique tends to be generic (\"the response could be improved\") rather than identifying specific, actionable problems. This means the revision won't be meaningfully better, and the preference pair won't teach the model anything useful.\n",
    "\n",
    "A good principle is *specific enough to discriminate*. \"Acknowledges uncertainty in medical topics\" is specific â€” it tells the critique exactly what to look for. \"Is better\" gives no direction.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scenario: a model gives investment advice with false certainty\n",
    "PROMPT_EX3 = \"Should I invest my savings in cryptocurrency?\"\n",
    "\n",
    "INITIAL_RESPONSE_EX3 = (\n",
    "    \"Yes, cryptocurrency is the future of finance. You should invest at least \"\n",
    "    \"50% of your savings in Bitcoin and Ethereum. The market always recovers \"\n",
    "    \"from dips, so there's very little risk if you hold long-term. Many experts \"\n",
    "    \"predict Bitcoin will reach $500,000 within five years.\"\n",
    ")\n",
    "\n",
    "# A GOOD principle: specific, actionable, targets a clear dimension\n",
    "GOOD_PRINCIPLE = (\n",
    "    \"Choose the response that better distinguishes between established facts \"\n",
    "    \"and speculative claims, and that avoids presenting predictions as certainties.\"\n",
    ")\n",
    "\n",
    "# TODO: Write a VAGUE principle that is too broad to be useful.\n",
    "# It should be something generic like \"be better\" or \"be more helpful\"\n",
    "# that gives the critique no specific direction.\n",
    "# YOUR CODE HERE (1 line)\n",
    "VAGUE_PRINCIPLE = \"\"\n",
    "\n",
    "\n",
    "# --- Generate critiques with both principles ---\n",
    "\n",
    "print(\"SCENARIO\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Prompt: {PROMPT_EX3}\")\n",
    "print(f\"\\nInitial response:\")\n",
    "print_wrapped(INITIAL_RESPONSE_EX3, prefix=\"  \")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GOOD PRINCIPLE (specific, actionable)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Principle: {GOOD_PRINCIPLE}\")\n",
    "print()\n",
    "critique_good = generate_critique(PROMPT_EX3, INITIAL_RESPONSE_EX3, GOOD_PRINCIPLE)\n",
    "print(\"Critique:\")\n",
    "print_wrapped(critique_good, prefix=\"  \")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VAGUE PRINCIPLE (too broad to discriminate)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Principle: {VAGUE_PRINCIPLE}\")\n",
    "print()\n",
    "critique_vague = generate_critique(PROMPT_EX3, INITIAL_RESPONSE_EX3, VAGUE_PRINCIPLE)\n",
    "print(\"Critique:\")\n",
    "print_wrapped(critique_vague, prefix=\"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare the critiques ---\n",
    "# Let's also generate revisions from both critiques to see the downstream effect\n",
    "\n",
    "revision_good = generate_revision(\n",
    "    PROMPT_EX3, INITIAL_RESPONSE_EX3, critique_good, GOOD_PRINCIPLE\n",
    ")\n",
    "revision_vague = generate_revision(\n",
    "    PROMPT_EX3, INITIAL_RESPONSE_EX3, critique_vague, VAGUE_PRINCIPLE\n",
    ")\n",
    "\n",
    "print(\"REVISION FROM GOOD PRINCIPLE:\")\n",
    "print(\"=\" * 70)\n",
    "print_wrapped(revision_good, prefix=\"  \")\n",
    "\n",
    "print(\"\\nREVISION FROM VAGUE PRINCIPLE:\")\n",
    "print(\"=\" * 70)\n",
    "print_wrapped(revision_vague, prefix=\"  \")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nGood principle critique: identifies SPECIFIC problems (speculative claims\")\n",
    "print(\"presented as facts, made-up price predictions, risk minimization).\")\n",
    "print(\"The revision addresses each problem concretely.\")\n",
    "print(\"\\nVague principle critique: generic feedback ('could be better', 'more helpful').\")\n",
    "print(\"The revision may improve surface quality but misses the core issues.\")\n",
    "print(\"\\nThis is the lesson's key insight: the CONSTITUTION QUALITY determines\")\n",
    "print(\"the ALIGNMENT QUALITY. Constitutional AI shifts the challenge from\")\n",
    "print(\"'find enough annotators' to 'design the right principles.'\")\n",
    "print(\"The difficulty doesn't disappear. It moves.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just observed:** Principle quality directly determines critique quality, which determines revision quality, which determines the quality of the training data. A vague principle like \"be better\" gives the AI critique no specific direction, producing generic feedback that fails to identify the actual problems in the response.\n",
    "\n",
    "This is the \"editor with blind spots\" pattern from Series 4 â€” but now the blind spots are in the *constitution*, not in the annotator pool. A vague principle is a blind spot: the AI critique cannot see problems that the principle does not name.\n",
    "\n",
    "In real constitutional AI, this means the design of the principle set is the critical engineering challenge. The principles must be:\n",
    "- **Specific enough** to discriminate between good and bad responses\n",
    "- **Comprehensive enough** to cover the dimensions of quality that matter\n",
    "- **Non-conflicting** (or with clear priority ordering when they do conflict)\n",
    "\n",
    "The principles can be iterated, version-controlled, and audited â€” unlike the implicit judgment of thousands of anonymous annotators. But they require careful thought about edge cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Principles steer the critique.** Different constitutional principles produce different critiques of the same response. The principle defines *what dimension* of quality to evaluate, giving the AI feedback specific direction that human annotators apply implicitly.\n",
    "\n",
    "2. **Critique-and-revision generates training data, not inference-time behavior.** The output of the loop is a preference pair (revised > original) â€” the same format as RLHF data. The model trained on this data produces good responses directly, with no critique step at inference time.\n",
    "\n",
    "3. **The mechanism is the same at any scale.** We ran the critique-and-revision loop manually on single examples. Constitutional AI runs it thousands of times across different prompts and principles. The mechanism is identical; the scale produces enough training data to align a model.\n",
    "\n",
    "4. **Principle quality determines alignment quality.** Vague or poorly-designed principles produce vague critiques and weak revisions. The alignment challenge shifts from \"find enough human annotators\" to \"design the right principles.\" The editor gets a style guide â€” but only if the style guide is well-written.\n",
    "\n",
    "5. **The blind spots move, they don't disappear.** In RLHF, blind spots live in the annotator pool. In constitutional AI, blind spots live in the constitution. The pattern is the same: the quality of the training data source determines the quality of alignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}