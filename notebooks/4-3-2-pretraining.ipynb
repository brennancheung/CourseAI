{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining on Real Text\n",
    "\n",
    "In this notebook, you'll train a GPT model from scratch on Shakespeare and watch gibberish transform into recognizable English.\n",
    "\n",
    "**What you'll do:**\n",
    "- Load and tokenize a text dataset, create training batches with the input/target offset\n",
    "- Run a forward pass through the GPT model and verify the initial loss matches theory\n",
    "- Implement the training loop with learning rate scheduling and gradient clipping\n",
    "- Add evaluation, loss tracking, and periodic text generation\n",
    "- Train the model, diagnose issues, and iterate on hyperparameters\n",
    "\n",
    "**Predict-first methodology:** For each exercise, PREDICT the output before running the cell. Wrong predictions are more valuable than correct ones\u2014they reveal gaps in your mental model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: self-contained for Google Colab\n",
    "!pip install -q tiktoken\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from itertools import cycle\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared: GPT Model from Building nanoGPT\n",
    "\n",
    "This is the complete GPT model you built in the previous lesson. We include it here so the notebook is self-contained. You already understand every line\u2014this is not new content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GPT Model (from Building nanoGPT) ---\n",
    "# You built this in the previous lesson. Included here for self-contained Colab use.\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 50257     # GPT-2 BPE vocabulary\n",
    "    block_size: int = 256       # Context length for training (smaller than 1024 for speed)\n",
    "    n_layer: int = 6            # Fewer layers for training on Colab\n",
    "    n_head: int = 6             # Number of attention heads\n",
    "    n_embd: int = 384           # Embedding dimension\n",
    "    dropout: float = 0.1        # Dropout rate\n",
    "    bias: bool = False          # Use bias in Linear layers?\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention with batched computation.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                 .view(1, 1, config.block_size, config.block_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        head_size = C // self.n_head\n",
    "        q = q.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
    "        scale = head_size ** -0.5\n",
    "        scores = q @ k.transpose(-2, -1) * scale\n",
    "        scores = scores.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        weights = self.attn_dropout(weights)\n",
    "        out = weights @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.c_proj(out)\n",
    "        out = self.resid_dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu   = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: MHA + FFN with residual connections and layer norm.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.ffn  = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.ffn(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"Complete GPT language model.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.block_size, \\\n",
    "            f\"Sequence length {T} exceeds block_size {self.config.block_size}\"\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos = torch.arange(0, T, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1)\n",
    "            )\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        \"\"\"Generate tokens autoregressively.\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "# Quick verification\n",
    "config = GPTConfig()\n",
    "n_params = sum(p.numel() for p in GPT(config).parameters())\n",
    "print(f'Model parameters: {n_params:,}')\n",
    "print(f'Config: {config.n_layer} layers, {config.n_head} heads, {config.n_embd} dim')\n",
    "print(f'Context length: {config.block_size}')\n",
    "print('Model definition loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on model size:** The lesson discusses GPT-2 small (124M params, 12 layers, 768 dim). For Colab training on TinyShakespeare, we use a smaller config (6 layers, 384 dim, ~10M params). The architecture and training loop are identical\u2014only the scale differs. This lets you train to convergence in minutes rather than hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Load and Tokenize Text, Create Training Batches\n",
    "\n",
    "**Type: GUIDED** \u2014 All code is provided. Your job is to predict, then verify.\n",
    "\n",
    "The first step in training a language model: turn raw text into token IDs, then slice them into context windows with the input/target offset.\n",
    "\n",
    "Remember the key insight from the lesson: for a chunk of tokens, the input is `tokens[i:i+T]` and the target is `tokens[i+1:i+T+1]`\u2014shifted by one position. Every position in the input predicts the token that follows it.\n",
    "\n",
    "**Before running, predict:**\n",
    "1. TinyShakespeare is about 1MB of text. Roughly how many BPE tokens will that produce? (Hint: BPE averages ~4 characters per token for English.)\n",
    "2. With a `block_size` of 256 and ~300K tokens, roughly how many training examples will the Dataset have?\n",
    "3. For a single training example of length T=256, how many next-token predictions does the model make in one forward pass?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download TinyShakespeare ---\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "if not os.path.exists('shakespeare.txt'):\n",
    "    urllib.request.urlretrieve(url, 'shakespeare.txt')\n",
    "    print('Downloaded shakespeare.txt')\n",
    "\n",
    "text = open('shakespeare.txt', 'r').read()\n",
    "print(f'Text length: {len(text):,} characters')\n",
    "print(f'First 200 characters:\\n{text[:200]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tokenize the entire corpus ---\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = enc.encode(text)\n",
    "print(f'Total tokens: {len(tokens):,}')\n",
    "print(f'Characters per token (avg): {len(text) / len(tokens):.1f}')\n",
    "\n",
    "# Show what tokenization looks like\n",
    "sample = text[:50]\n",
    "sample_tokens = enc.encode(sample)\n",
    "print(f'\\nSample text: \"{sample}\"')\n",
    "print(f'Token IDs: {sample_tokens}')\n",
    "print(f'Decoded tokens: {[enc.decode([t]) for t in sample_tokens]}')\n",
    "\n",
    "data = torch.tensor(tokens, dtype=torch.long)\n",
    "print(f'\\nData tensor shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create the Dataset with input/target offset ---\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx   : idx + self.block_size]      # input\n",
    "        y = self.data[idx+1 : idx + self.block_size + 1]  # target (shifted by 1)\n",
    "        return x, y\n",
    "\n",
    "# Split into train/val (90/10)\n",
    "block_size = config.block_size  # 256\n",
    "n = int(0.9 * len(data))\n",
    "train_dataset = TextDataset(data[:n], block_size)\n",
    "val_dataset   = TextDataset(data[n:], block_size)\n",
    "\n",
    "print(f'Train examples: {len(train_dataset):,}')\n",
    "print(f'Val examples:   {len(val_dataset):,}')\n",
    "\n",
    "# Verify the input/target offset\n",
    "x_sample, y_sample = train_dataset[0]\n",
    "print(f'\\nInput shape:  {x_sample.shape}  (block_size = {block_size})')\n",
    "print(f'Target shape: {y_sample.shape}')\n",
    "print(f'\\nInput tokens (first 10):  {x_sample[:10].tolist()}')\n",
    "print(f'Target tokens (first 10): {y_sample[:10].tolist()}')\n",
    "print(f'\\nNotice: target[i] == input[i+1]? {(y_sample[:-1] == x_sample[1:]).all().item()}')\n",
    "print('The target is the input shifted by one position. Every position predicts the next token.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create DataLoaders ---\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verify batch shapes\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(f'Batch input shape:  {x_batch.shape}  (batch_size, block_size)')\n",
    "print(f'Batch target shape: {y_batch.shape}')\n",
    "print(f'\\nEach batch = {batch_size} sequences x {block_size} positions = {batch_size * block_size:,} next-token predictions')\n",
    "print(f'Total batches per epoch: {len(train_loader):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:**\n",
    "\n",
    "1. The full Shakespeare text was tokenized into a single sequence of ~300K BPE tokens.\n",
    "2. The `TextDataset` slices this into overlapping context windows. Each window is 256 tokens. The input is `tokens[i:i+256]`, the target is `tokens[i+1:i+257]`\u2014shifted by one position.\n",
    "3. Each position in a training example is an independent next-token prediction. One sequence of length 256 produces 256 predictions in a single forward pass. This is why training is efficient.\n",
    "4. The DataLoader creates batches of 16 sequences. Each batch produces `16 * 256 = 4,096` next-token predictions.\n",
    "\n",
    "The Dataset/DataLoader pattern is the same one from your CNN work. The only new idea is the one-position offset between input and target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: First Forward Pass \u2014 Verify the Initial Loss\n",
    "\n",
    "**Type: GUIDED** \u2014 All code is provided. Predict before running.\n",
    "\n",
    "Before training, you need to verify the pipeline works. The sanity check: if the untrained model assigns equal probability to all 50,257 tokens, the cross-entropy loss should be $\\ln(50257) \\approx 10.82$.\n",
    "\n",
    "**Before running, predict:**\n",
    "1. What should the initial loss be? (Calculate: $-\\ln(1/V)$ where $V = 50{,}257$)\n",
    "2. The logits have shape `(B, T, V)`. What reshape is needed for `nn.CrossEntropyLoss`?\n",
    "3. After the forward pass, will the generated text be English or gibberish?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create model and run first forward pass ---\n",
    "torch.manual_seed(42)\n",
    "model = GPT(config)\n",
    "model = model.to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameters: {n_params:,}')\n",
    "\n",
    "# Get one batch\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "\n",
    "print(f'\\nLogits shape: {logits.shape}   (B, T, vocab_size)')\n",
    "print(f'Initial loss: {loss.item():.4f}')\n",
    "print(f'Expected loss (random): ln(50257) = {math.log(50257):.4f}')\n",
    "print(f'Difference: {abs(loss.item() - math.log(50257)):.4f}')\n",
    "\n",
    "# Is the loss close to ln(V)?\n",
    "is_close = abs(loss.item() - math.log(50257)) < 0.5\n",
    "print(f'\\nInitial loss close to ln(V)? {is_close}')\n",
    "if is_close:\n",
    "    print('The model is correctly initialized: near-uniform predictions over the vocabulary.')\n",
    "    print('The pipeline is wired correctly. Ready to train.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate from the untrained model ---\n",
    "prompt = enc.encode(\"ROMEO:\")\n",
    "idx = torch.tensor([prompt], device=device)\n",
    "\n",
    "model.eval()\n",
    "output = model.generate(idx, max_new_tokens=100, temperature=1.0)\n",
    "print('Generated text from UNTRAINED model:')\n",
    "print('-' * 50)\n",
    "print(enc.decode(output[0].tolist()))\n",
    "print('-' * 50)\n",
    "print('\\nRandom gibberish. Every weight is random. The architecture works')\n",
    "print('but the model has never seen any text. Training will change this.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:**\n",
    "\n",
    "The initial loss is close to $\\ln(50257) \\approx 10.82$, which confirms the model is assigning roughly equal probability to all tokens. This is the \"parameter count as architecture verification\" pattern\u2014one number confirms the entire pipeline (tokenization, dataset, model, loss) is wired correctly.\n",
    "\n",
    "The generated text is complete gibberish because every weight is random. The model produces valid token IDs through a working autoregressive loop, but the tokens are meaningless. Training will change this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Implement the Training Loop with LR Scheduling\n",
    "\n",
    "**Type: SUPPORTED** \u2014 Skeleton provided with TODO markers. You fill in the key parts.\n",
    "\n",
    "The training loop is the same heartbeat from Series 2: forward, loss, backward, step. Three things are new:\n",
    "- **Learning rate scheduling** (warmup + cosine decay)\n",
    "- **Gradient clipping** (one line, essential safety net)\n",
    "- **Manual LR update** via `param_group['lr']`\n",
    "\n",
    "The `get_lr` function implements the warmup + cosine decay schedule from the lesson. The warmup phase linearly ramps from `min_lr` to `max_lr` over the first `warmup_steps`. The cosine decay phase gradually decreases from `max_lr` to `min_lr` over the remaining steps.\n",
    "\n",
    "<details>\n",
    "<summary>\ud83d\udca1 Solution</summary>\n",
    "\n",
    "The key insight is that the LR schedule has two phases, and we check which phase we're in based on the current step:\n",
    "\n",
    "- **Warmup (step < warmup_steps):** Linear interpolation from `min_lr` to `max_lr`. The fraction `step / warmup_steps` goes from 0 to 1.\n",
    "- **Cosine decay (step >= warmup_steps):** The `progress` variable tracks how far through the decay phase we are (0 to 1). The cosine formula `0.5 * (1 + cos(pi * progress))` smoothly decays from 1 to 0.\n",
    "\n",
    "```python\n",
    "# get_lr function:\n",
    "def get_lr(step, warmup_steps, max_steps, max_lr, min_lr):\n",
    "    if step < warmup_steps:\n",
    "        return min_lr + (max_lr - min_lr) * (step / warmup_steps)\n",
    "    progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    return min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "# In the training loop:\n",
    "# Update LR:\n",
    "lr = get_lr(step, warmup_steps, max_steps, max_lr=6e-4, min_lr=6e-5)\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr\n",
    "\n",
    "# Forward:\n",
    "logits, loss = model(x, y)\n",
    "\n",
    "# Backward + clip + step:\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "Common mistake: putting `clip_grad_norm_` before `loss.backward()`. The gradients must exist before you can clip them.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Learning rate schedule ---\n",
    "\n",
    "def get_lr(step, warmup_steps, max_steps, max_lr, min_lr):\n",
    "    \"\"\"Cosine learning rate schedule with linear warmup.\"\"\"\n",
    "    # TODO: Implement the warmup phase\n",
    "    # If step < warmup_steps, linearly ramp from min_lr to max_lr\n",
    "    # Hint: the fraction (step / warmup_steps) goes from 0 to 1\n",
    "\n",
    "    # TODO: Implement the cosine decay phase\n",
    "    # 1. Compute progress: how far through the decay phase (0 to 1)\n",
    "    # 2. Use: min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * progress))\n",
    "    pass\n",
    "\n",
    "\n",
    "# Hyperparameters (from nanoGPT — known-good values)\n",
    "max_lr = 6e-4\n",
    "min_lr = 6e-5       # 10% of peak\n",
    "warmup_steps = 100\n",
    "max_steps = 3000    # Enough steps to see real learning on Colab\n",
    "\n",
    "# Visualize the schedule to verify your implementation\n",
    "lrs = [get_lr(s, warmup_steps, max_steps, max_lr, min_lr) for s in range(max_steps)]\n",
    "plt.plot(lrs, linewidth=2)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('LR Schedule: Warmup + Cosine Decay')\n",
    "plt.axvline(x=warmup_steps, color='gray', linestyle='--', alpha=0.5, label=f'Warmup ends (step {warmup_steps})')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f'LR at step 0: {lrs[0]:.2e} (should be close to min_lr = {min_lr:.2e})')\n",
    "print(f'LR at step {warmup_steps}: {lrs[warmup_steps]:.2e} (should be max_lr = {max_lr:.2e})')\n",
    "print(f'LR at step {max_steps-1}: {lrs[-1]:.2e} (should be close to min_lr = {min_lr:.2e})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- The complete training loop ---\n",
    "\n",
    "# Fresh model\n",
    "torch.manual_seed(42)\n",
    "model = GPT(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# AdamW optimizer — the transformer default\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=max_lr,\n",
    "    betas=(0.9, 0.95), weight_decay=0.1\n",
    ")\n",
    "\n",
    "# Create an infinite iterator that cycles through the dataset\n",
    "train_iter = iter(cycle(train_loader))\n",
    "\n",
    "# Track losses for plotting\n",
    "train_losses = []\n",
    "step_list = []\n",
    "\n",
    "print(f'Training for {max_steps} steps on {device}...')\n",
    "print(f'Batch size: {batch_size}, Block size: {block_size}')\n",
    "print(f'Predictions per step: {batch_size * block_size:,}')\n",
    "print()\n",
    "\n",
    "for step in range(max_steps):\n",
    "    model.train()\n",
    "\n",
    "    # TODO: Update learning rate for this step\n",
    "    # 1. Call get_lr to get the learning rate for this step\n",
    "    # 2. Set it on the optimizer: for param_group in optimizer.param_groups:\n",
    "    #        param_group['lr'] = lr\n",
    "\n",
    "    # Get batch\n",
    "    x, y = next(train_iter)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    # TODO: Forward pass — get logits and loss from the model\n",
    "    # Hint: logits, loss = model(x, y)\n",
    "\n",
    "    # TODO: Backward pass + gradient clipping + optimizer step\n",
    "    # 1. optimizer.zero_grad()\n",
    "    # 2. loss.backward()\n",
    "    # 3. clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    # 4. optimizer.step()\n",
    "\n",
    "    # Track losses\n",
    "    train_losses.append(loss.item())\n",
    "    step_list.append(step)\n",
    "\n",
    "    # Log every 100 steps\n",
    "    if step % 100 == 0:\n",
    "        print(f'step {step:5d} | loss {loss.item():.4f} | lr {lr:.2e}')\n",
    "\n",
    "print(f'\\nTraining complete.')\n",
    "print(f'Final loss: {train_losses[-1]:.4f}')\n",
    "print(f'Initial loss was: {train_losses[0]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot the training loss ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Raw loss\n",
    "axes[0].plot(step_list, train_losses, alpha=0.3, linewidth=0.5, color='#ff6b6b')\n",
    "# Smoothed loss (running average)\n",
    "window = 50\n",
    "smoothed = [sum(train_losses[max(0,i-window):i+1]) / min(i+1, window) for i in range(len(train_losses))]\n",
    "axes[0].plot(step_list, smoothed, linewidth=2, color='#ff6b6b', label='Smoothed')\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# LR schedule overlaid\n",
    "axes[1].plot(step_list, lrs[:len(step_list)], linewidth=2, color='#4ecdc4')\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('The raw loss is jagged because some batches are harder than others.')\n",
    "print('The smoothed line shows the real trend. The loss should drop fast initially,')\n",
    "print('then slow down\u2014the easy wins (common words, basic grammar) come first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just built:**\n",
    "\n",
    "The same training loop heartbeat from Series 2 (forward, loss, backward, step), plus three new instruments:\n",
    "1. **`get_lr()`** \u2014 Dynamic Goldilocks. Gentle warmup for fragile random weights, aggressive learning in the middle, careful convergence at the end.\n",
    "2. **`clip_grad_norm_()`** \u2014 One-line safety net. Bounds the gradient magnitude so one bad batch cannot undo hundreds of steps of progress.\n",
    "3. **Manual LR update** via `param_group['lr']` \u2014 Transparent. You see exactly what the learning rate is at every step.\n",
    "\n",
    "Notice the loss curve: jagged (batch-to-batch variance is normal for language models) but trending downward. The smoothed line reveals the real signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Add Evaluation and Text Generation During Training\n",
    "\n",
    "**Type: SUPPORTED** \u2014 Skeleton provided with TODO markers.\n",
    "\n",
    "Loss numbers tell you training is working. Generated text shows you *what* the model has learned. In this exercise, you'll add:\n",
    "- Validation loss evaluation (the scissors pattern from Series 1)\n",
    "- Periodic text generation to watch the model learn\n",
    "\n",
    "<details>\n",
    "<summary>\ud83d\udca1 Solution</summary>\n",
    "\n",
    "The evaluation loop is the same pattern from Series 2: `model.eval()`, `torch.no_grad()`, iterate over val batches, average the loss, then back to `model.train()`.\n",
    "\n",
    "```python\n",
    "# Evaluation:\n",
    "model.eval()\n",
    "val_loss_total = 0.0\n",
    "val_iter = iter(val_loader)\n",
    "with torch.no_grad():\n",
    "    for _ in range(val_steps):\n",
    "        xv, yv = next(val_iter)\n",
    "        xv, yv = xv.to(device), yv.to(device)\n",
    "        _, vloss = model(xv, yv)\n",
    "        val_loss_total += vloss.item()\n",
    "avg_val_loss = val_loss_total / val_steps\n",
    "\n",
    "# Generation:\n",
    "prompt_tokens = enc.encode(\"ROMEO:\")\n",
    "idx = torch.tensor([prompt_tokens], device=device)\n",
    "output = model.generate(idx, max_new_tokens=100, temperature=0.8)\n",
    "generated_text = enc.decode(output[0].tolist())\n",
    "```\n",
    "\n",
    "Key points:\n",
    "- Use `model.eval()` before evaluation and generation (disables dropout)\n",
    "- Use `torch.no_grad()` for evaluation (saves memory)\n",
    "- Return to `model.train()` after\n",
    "- Temperature 0.8 produces more focused text than 1.0\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training loop with evaluation and generation ---\n",
    "\n",
    "# Fresh model (start over to see the full progression)\n",
    "torch.manual_seed(42)\n",
    "model = GPT(config)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=max_lr,\n",
    "    betas=(0.9, 0.95), weight_decay=0.1\n",
    ")\n",
    "\n",
    "train_iter = iter(cycle(train_loader))\n",
    "\n",
    "# Tracking\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_steps_list = []\n",
    "generated_samples = []  # (step, loss, text) tuples\n",
    "\n",
    "# Evaluation settings\n",
    "eval_interval = 500      # Evaluate every N steps\n",
    "generate_interval = 500  # Generate text every N steps\n",
    "val_steps = 20           # Number of val batches to average over\n",
    "\n",
    "print(f'Training for {max_steps} steps with evaluation every {eval_interval} steps...')\n",
    "print()\n",
    "\n",
    "for step in range(max_steps):\n",
    "    model.train()\n",
    "\n",
    "    # Update learning rate\n",
    "    lr = get_lr(step, warmup_steps, max_steps, max_lr, min_lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # Get batch and train\n",
    "    x, y = next(train_iter)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # --- Evaluation (every eval_interval steps) ---\n",
    "    if step % eval_interval == 0:\n",
    "        # TODO: Evaluate on validation set\n",
    "        # 1. Set model.eval()\n",
    "        # 2. Initialize val_loss_total = 0.0\n",
    "        # 3. Create a fresh val iterator: val_iter = iter(val_loader)\n",
    "        # 4. In a torch.no_grad() block, loop val_steps times:\n",
    "        #    - Get a batch: xv, yv = next(val_iter)\n",
    "        #    - Move to device\n",
    "        #    - Forward pass: _, vloss = model(xv, yv)\n",
    "        #    - Accumulate: val_loss_total += vloss.item()\n",
    "        # 5. Compute avg_val_loss = val_loss_total / val_steps\n",
    "        avg_val_loss = 0.0  # Replace with your computation\n",
    "\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_steps_list.append(step)\n",
    "        print(f'step {step:5d} | train loss {loss.item():.4f} | val loss {avg_val_loss:.4f} | lr {lr:.2e}')\n",
    "\n",
    "    # --- Text generation (every generate_interval steps) ---\n",
    "    if step % generate_interval == 0:\n",
    "        model.eval()\n",
    "        # TODO: Generate text from the prompt \"ROMEO:\"\n",
    "        # 1. Encode the prompt: prompt_tokens = enc.encode(\"ROMEO:\")\n",
    "        # 2. Create tensor: idx = torch.tensor([prompt_tokens], device=device)\n",
    "        # 3. Generate: output = model.generate(idx, max_new_tokens=100, temperature=0.8)\n",
    "        # 4. Decode: generated_text = enc.decode(output[0].tolist())\n",
    "        generated_text = \"\"  # Replace with your generation\n",
    "\n",
    "        generated_samples.append((step, loss.item(), generated_text))\n",
    "        print(f'\\n--- Step {step} (loss={loss.item():.2f}) ---')\n",
    "        print(generated_text[:200])  # First 200 chars\n",
    "        print()\n",
    "\n",
    "# Final generation\n",
    "model.eval()\n",
    "prompt_tokens = enc.encode(\"ROMEO:\")\n",
    "idx = torch.tensor([prompt_tokens], device=device)\n",
    "output = model.generate(idx, max_new_tokens=200, temperature=0.8)\n",
    "print('\\n' + '=' * 60)\n",
    "print(f'FINAL (step {max_steps}, loss={train_losses[-1]:.2f}):')\n",
    "print('=' * 60)\n",
    "print(enc.decode(output[0].tolist()))\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot training and validation loss ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Smoothed train loss + val loss\n",
    "window = 50\n",
    "smoothed_train = [sum(train_losses[max(0,i-window):i+1]) / min(i+1, window) for i in range(len(train_losses))]\n",
    "axes[0].plot(range(len(train_losses)), smoothed_train, linewidth=2, color='#ff6b6b', label='Train (smoothed)', alpha=0.8)\n",
    "axes[0].plot(val_steps_list, val_losses, 'o-', linewidth=2, color='#4ecdc4', label='Validation', markersize=6)\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Train vs Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Zoom into last half\n",
    "mid = len(train_losses) // 2\n",
    "axes[1].plot(range(mid, len(train_losses)), smoothed_train[mid:], linewidth=2, color='#ff6b6b', label='Train (smoothed)', alpha=0.8)\n",
    "val_mask = [i for i, s in enumerate(val_steps_list) if s >= mid]\n",
    "axes[1].plot([val_steps_list[i] for i in val_mask], [val_losses[i] for i in val_mask],\n",
    "             'o-', linewidth=2, color='#4ecdc4', label='Validation', markersize=6)\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Train vs Validation Loss (Zoomed)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Watch for the scissors pattern: when train loss keeps dropping but val loss')\n",
    "print('starts rising, the model is memorizing rather than learning.')\n",
    "print(f'With ~300K tokens and ~{n_params//1_000_000}M params, this will happen eventually.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just built:**\n",
    "\n",
    "A complete training pipeline with monitoring:\n",
    "- **Validation loss** lets you detect overfitting (the scissors pattern from Series 1).\n",
    "- **Generated text** shows qualitative progress. The biggest leap is from gibberish to real words (loss ~10 to ~4). Later improvements from ~3 to ~2 are subtle. The relationship between loss and text quality is logarithmic, not linear.\n",
    "- **Loss smoothing** reveals the real trend underneath batch-to-batch noise. Language model loss is inherently noisier than MNIST because some batches contain predictable sequences and others contain rare words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Train, Diagnose, and Iterate\n",
    "\n",
    "**Type: INDEPENDENT** \u2014 Problem specification only. You write the code.\n",
    "\n",
    "You now have all the tools. Your task:\n",
    "\n",
    "1. **Experiment with hyperparameters.** Pick ONE thing to change and observe the effect:\n",
    "   - What happens if you remove LR warmup entirely (set `warmup_steps = 0`)?\n",
    "   - What happens if you use a constant LR of `6e-4` instead of the schedule?\n",
    "   - What happens if you remove gradient clipping?\n",
    "   - What happens if you increase `max_lr` to `3e-3` (5x higher)?\n",
    "\n",
    "2. **Compare the loss curves.** Plot your experimental run against the baseline from Exercise 4. Which is better? Why?\n",
    "\n",
    "3. **Diagnose and explain.** For each experiment, explain what happened in terms of the lesson's mental models:\n",
    "   - Did the warmup prevent early instability?\n",
    "   - Did the cosine decay help with convergence?\n",
    "   - Did gradient clipping prevent a loss spike?\n",
    "\n",
    "**Deliverable:** At least ONE experiment with a comparison plot and a written explanation of what you observed.\n",
    "\n",
    "<details>\n",
    "<summary>\ud83d\udca1 Solution</summary>\n",
    "\n",
    "The key insight is that each component of the training recipe (warmup, cosine decay, gradient clipping) solves a specific problem. The experiment reveals which problem:\n",
    "\n",
    "- **No warmup:** Random transformer weights are fragile. Without warmup, the first few steps can push the model into a bad region. You may see a loss spike in the first 100 steps, or the loss may never recover.\n",
    "- **Constant LR = 6e-4:** At this scale, a constant LR often works but converges to a worse final loss than the cosine schedule. The schedule gives better final performance because it takes smaller steps as the model approaches a good solution.\n",
    "- **No gradient clipping:** On TinyShakespeare at this model size, you may not see a difference\u2014clipping is a safety net that activates rarely. Try a higher LR (3e-3) without clipping to see it matter.\n",
    "- **max_lr = 3e-3:** Too aggressive. Likely destabilizes training. With clipping, it might survive but learn poorly. Without clipping, it may produce NaN loss.\n",
    "\n",
    "```python\n",
    "# Example: compare baseline vs no-warmup\n",
    "def train_experiment(warmup_steps_exp, max_lr_exp, use_clipping, label):\n",
    "    torch.manual_seed(42)\n",
    "    model_exp = GPT(config).to(device)\n",
    "    opt_exp = torch.optim.AdamW(model_exp.parameters(), lr=max_lr_exp,\n",
    "                                 betas=(0.9, 0.95), weight_decay=0.1)\n",
    "    exp_iter = iter(cycle(train_loader))\n",
    "    losses_exp = []\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        model_exp.train()\n",
    "        lr = get_lr(step, warmup_steps_exp, max_steps, max_lr_exp, min_lr)\n",
    "        for pg in opt_exp.param_groups:\n",
    "            pg['lr'] = lr\n",
    "        x, y = next(exp_iter)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model_exp(x, y)\n",
    "        opt_exp.zero_grad()\n",
    "        loss.backward()\n",
    "        if use_clipping:\n",
    "            clip_grad_norm_(model_exp.parameters(), max_norm=1.0)\n",
    "        opt_exp.step()\n",
    "        losses_exp.append(loss.item())\n",
    "        if math.isnan(loss.item()):\n",
    "            print(f'{label}: NaN at step {step}, stopping.')\n",
    "            break\n",
    "    return losses_exp\n",
    "\n",
    "# Run experiments\n",
    "losses_baseline = train_losses  # from Exercise 4\n",
    "losses_no_warmup = train_experiment(0, 6e-4, True, 'No warmup')\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "window = 50\n",
    "smooth = lambda L: [sum(L[max(0,i-window):i+1])/min(i+1,window) for i in range(len(L))]\n",
    "plt.plot(smooth(losses_baseline), label='Baseline (warmup=100)', linewidth=2)\n",
    "plt.plot(smooth(losses_no_warmup), label='No warmup', linewidth=2)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss (smoothed)')\n",
    "plt.title('Effect of Removing Warmup')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "The experiment makes the lesson's mental model concrete: you can *see* the effect of each training recipe component instead of just reading about it.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Your experiment here ---\n",
    "# Pick one hyperparameter to change, train, and compare to the baseline.\n",
    "#\n",
    "# Suggested structure:\n",
    "# 1. Write a training function that takes the hyperparameter as an argument\n",
    "# 2. Run the baseline and your experiment\n",
    "# 3. Plot both loss curves on the same axes\n",
    "# 4. Print your observations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot your comparison ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations:**\n",
    "\n",
    "(Write your explanation here. What did you change? What happened? Why, in terms of the lesson's mental models?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The training loop is the same loop. Always.** Forward, loss, backward, step. From linear regression to GPT, the algorithm never changes. The data format changes, the model changes, the scale changes. The loop does not.\n",
    "\n",
    "2. **Three new tools for transformer-scale training.** Text dataset preparation (input/target offset for next-token prediction), LR scheduling (warmup + cosine decay), and gradient clipping (one line, essential safety net). Everything else is the same heartbeat from Series 2.\n",
    "\n",
    "3. **Cross-entropy scales to any vocabulary size.** Same formula, same PyTorch call. Reshape logits from (B, T, V) to (B\\*T, V), targets from (B, T) to (B\\*T). The vocabulary size is just another number.\n",
    "\n",
    "4. **Loss-to-text-quality is nonlinear.** The biggest qualitative leaps happen early (gibberish to words, words to phrases). Later improvements are subtle. Do not expect the same dramatic progress all the way down.\n",
    "\n",
    "5. **The initial loss sanity check confirms the entire pipeline.** If $\\text{loss} \\approx \\ln(V)$, everything is wired correctly: tokenization, dataset, model, and loss computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}