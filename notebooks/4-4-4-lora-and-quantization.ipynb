{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA, Quantization & Inference\n",
    "\n",
    "In this notebook, you'll make LLM finetuning and inference practical on real hardware using LoRA and quantization.\n",
    "\n",
    "**What you'll do:**\n",
    "- Calculate training memory requirements and discover why optimizer states dominate the cost\n",
    "- Implement absmax and zero-point quantization by hand, tracing every step with real numbers\n",
    "- Build a LoRA layer from scratch, apply it to GPT-2, and verify the parameter savings\n",
    "- LoRA-finetune a model using the HuggingFace PEFT library\n",
    "- Load a quantized model and compare memory usage and output quality vs full precision\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones â€” they reveal gaps in your mental model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup â€” self-contained for Google Colab\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "\n",
    "print('\\nSetup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 1: Memory Calculator (Guided)\n\nYou have been finetuning GPT-2 for the last three lessons without worrying about memory. GPT-2 has 124M parameters â€” it fits comfortably on any GPU. But the models people actually use â€” Llama 2 7B, Mistral 7B â€” are 50 to 500 times larger.\n\nTraining with AdamW requires storing four things per parameter:\n1. **Weights** in bfloat16 â€” the model itself (2 bytes/param)\n2. **Gradients** in bfloat16 â€” one per parameter for the backward pass (2 bytes/param)\n3. **Adam momentum** in float32 â€” running average of gradients (4 bytes/param)\n4. **Adam variance** in float32 â€” running average of squared gradients (4 bytes/param)\n\nThat is 12 bytes per parameter. Let's calculate the actual memory cost.\n\n**Before running, predict:**\n- For a 7B parameter model in float16, how many GB are the weights alone?\n- For full finetuning with AdamW (bf16 weights + bf16 gradients + fp32 optimizer states = 12 bytes/param), roughly how many GB total?\n- Which component dominates: weights, gradients, or optimizer states?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def bytes_per_param(dtype):\n    \"\"\"Return bytes per parameter for a given dtype string.\"\"\"\n    mapping = {\n        'float32': 4,\n        'float16': 2,\n        'bfloat16': 2,\n        'int8': 1,\n        'int4': 0.5,\n    }\n    return mapping[dtype]\n\n\ndef memory_for_inference(num_params, dtype='float16'):\n    \"\"\"Memory for inference = weights only.\"\"\"\n    return num_params * bytes_per_param(dtype)\n\n\ndef memory_for_training(num_params):\n    \"\"\"Memory for full finetuning with AdamW (bf16 mixed precision).\n    \n    Stores 12 bytes per parameter:\n    - Weights in bfloat16: 2 bytes/param\n    - Gradients in bfloat16: 2 bytes/param\n    - Adam momentum in float32: 4 bytes/param\n    - Adam variance in float32: 4 bytes/param\n    \n    Total: 2 + 2 + 4 + 4 = 12 bytes/param\n    \"\"\"\n    weights_bf16 = num_params * 2        # bfloat16 for forward/backward\n    gradients_bf16 = num_params * 2      # bfloat16 gradients\n    adam_momentum = num_params * 4       # float32\n    adam_variance = num_params * 4       # float32\n    return {\n        'weights_bf16': weights_bf16,\n        'gradients_bf16': gradients_bf16,\n        'adam_momentum': adam_momentum,\n        'adam_variance': adam_variance,\n        'total': weights_bf16 + gradients_bf16 + adam_momentum + adam_variance,\n    }\n\n\ndef to_gb(num_bytes):\n    \"\"\"Convert bytes to GB (decimal, 1 GB = 1e9 bytes).\"\"\"\n    return num_bytes / 1e9\n\n\n# ---------------------------------------------------------------\n# Compare GPT-2 (124M) vs Llama 2 7B\n# ---------------------------------------------------------------\nmodels = {\n    'GPT-2': 124_000_000,\n    'Llama 2 7B': 7_000_000_000,\n    'Llama 2 13B': 13_000_000_000,\n    'Llama 2 70B': 70_000_000_000,\n}\n\nprint(\"=\" * 70)\nprint(\"INFERENCE MEMORY (weights only)\")\nprint(\"=\" * 70)\nfor name, n_params in models.items():\n    mem_fp32 = to_gb(memory_for_inference(n_params, 'float32'))\n    mem_fp16 = to_gb(memory_for_inference(n_params, 'float16'))\n    mem_int8 = to_gb(memory_for_inference(n_params, 'int8'))\n    mem_int4 = to_gb(memory_for_inference(n_params, 'int4'))\n    print(f\"\\n{name} ({n_params/1e9:.1f}B params):\")\n    print(f\"  float32: {mem_fp32:6.1f} GB\")\n    print(f\"  float16: {mem_fp16:6.1f} GB\")\n    print(f\"  int8:    {mem_int8:6.1f} GB\")\n    print(f\"  int4:    {mem_int4:6.1f} GB\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TRAINING MEMORY (full finetuning with AdamW, bf16 mixed precision)\")\nprint(\"=\" * 70)\nprint(\"Model: bf16 weights + bf16 gradients + fp32 Adam states = 12 bytes/param\")\nfor name, n_params in models.items():\n    mem = memory_for_training(n_params)\n    print(f\"\\n{name} ({n_params/1e9:.1f}B params):\")\n    print(f\"  Weights (bf16):        {to_gb(mem['weights_bf16']):6.1f} GB\")\n    print(f\"  Gradients (bf16):      {to_gb(mem['gradients_bf16']):6.1f} GB\")\n    print(f\"  Adam momentum (fp32):  {to_gb(mem['adam_momentum']):6.1f} GB\")\n    print(f\"  Adam variance (fp32):  {to_gb(mem['adam_variance']):6.1f} GB\")\n    print(f\"  ----------------------------------------\")\n    print(f\"  TOTAL:                 {to_gb(mem['total']):6.1f} GB\")\n    \n    # What fraction is optimizer states?\n    optimizer_frac = (mem['adam_momentum'] + mem['adam_variance']) / mem['total'] * 100\n    print(f\"  Optimizer states:      {optimizer_frac:.0f}% of total\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize: where does training memory go?\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nfor ax, (name, n_params) in zip(axes, [('GPT-2', 124_000_000), ('Llama 2 7B', 7_000_000_000)]):\n    mem = memory_for_training(n_params)\n    \n    components = ['Weights\\n(bf16)', 'Gradients\\n(bf16)', 'Adam\\nmomentum\\n(fp32)', 'Adam\\nvariance\\n(fp32)']\n    values = [\n        to_gb(mem['weights_bf16']),\n        to_gb(mem['gradients_bf16']),\n        to_gb(mem['adam_momentum']),\n        to_gb(mem['adam_variance']),\n    ]\n    colors = ['#60a5fa', '#f87171', '#a78bfa', '#fbbf24']\n    \n    bars = ax.bar(components, values, color=colors, alpha=0.8, edgecolor='white', linewidth=0.5)\n    ax.set_title(f'{name} ({n_params/1e9:.1f}B) \\u2014 Training Memory', fontsize=12)\n    ax.set_ylabel('Memory (GB)')\n    \n    # Add value labels on bars\n    for bar, val in zip(bars, values):\n        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02 * max(values),\n                f'{val:.1f}', ha='center', va='bottom', fontsize=9, color='white')\n    \n    ax.set_ylim(0, max(values) * 1.15)\n    ax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"The optimizer states (momentum + variance) are the largest cost.\")\nprint(\"For Llama 2 7B, they alone require 56 GB in float32 \\u2014 two-thirds of the 84 GB total.\")\nprint(\"A consumer RTX 4090 has 24 GB. Full finetuning does NOT fit.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**What you just computed:** The memory wall that motivates LoRA and quantization. Training a 7B model requires **~84 GB** minimum â€” not because of the weights (only 14 GB in bfloat16), but because of the gradients and especially the optimizer states. Adam stores two float32 tensors the same size as the model, accounting for two-thirds of the total.\n\nThis creates two separate problems:\n- **Finetuning is too expensive:** Too many trainable parameters means too many gradients and optimizer states. **Solution: LoRA.**\n- **Inference is too expensive:** The model itself is too large. **Solution: Quantization.**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Quantization by Hand (Guided)\n",
    "\n",
    "In Scaling & Efficiency, you traded float32 for bfloat16 and lost almost nothing. Quantization pushes this further: map floating-point weights to integers (int8 or int4) for even greater memory savings.\n",
    "\n",
    "We'll implement two quantization methods step by step and trace the math with real numbers.\n",
    "\n",
    "### Part A: Absmax Quantization\n",
    "\n",
    "The simplest method. For a vector of weights:\n",
    "1. Find the max absolute value\n",
    "2. Compute scale = max_abs / 127 (for int8, range [-127, 127])\n",
    "3. Quantize: q = round(w / scale)\n",
    "4. Dequantize: w_approx = q * scale\n",
    "\n",
    "**Before running, predict:** Given weights [-0.8, 0.3, 1.2, -0.5], the max absolute value is 1.2 and the scale is 1.2/127 â‰ˆ 0.0094. What will the quantized int8 values be? What will the reconstruction error look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def absmax_quantize(weights):\n    \"\"\"Absmax quantization to int8.\n    \n    Steps:\n    1. Find scale = max(|w|) / 127\n    2. Quantize: q = round(w / scale), clamp to [-127, 127]\n    3. Store q as int8 + scale as float32\n    \n    Returns: quantized (int8), scale (float32)\n    \"\"\"\n    max_abs = weights.abs().max()\n    scale = max_abs / 127.0\n    quantized = torch.clamp(torch.round(weights / scale), -127, 127).to(torch.int8)\n    return quantized, scale\n\n\ndef absmax_dequantize(quantized, scale):\n    \"\"\"Reconstruct float weights from quantized int8 values.\"\"\"\n    return quantized.float() * scale\n\n\n# --- Trace with the exact example from the lesson ---\nweights = torch.tensor([-0.8, 0.3, 1.2, -0.5])\n\nprint(\"ABSMAX QUANTIZATION WALKTHROUGH\")\nprint(\"=\" * 50)\nprint(f\"Original weights:  {weights.tolist()}\")\nprint(f\"Max absolute value: {weights.abs().max().item()}\")\n\nq, scale = absmax_quantize(weights)\nprint(f\"Scale factor:      {scale.item():.6f}\")\nprint(f\"Quantized (int8):  {q.tolist()}\")\n\nw_approx = absmax_dequantize(q, scale)\nprint(f\"Reconstructed:     {[f'{v:.4f}' for v in w_approx.tolist()]}\")\n\nerror = (weights - w_approx).abs()\nprint(f\"Absolute error:    {[f'{v:.6f}' for v in error.tolist()]}\")\nprint(f\"Max error:         {error.max().item():.6f}\")\n\nprint(f\"\\nMemory: 4 bytes (int8) + 4 bytes (scale) = 8 bytes\")\nprint(f\"vs original: 4 * 4 bytes (float32) = 16 bytes\")\nprint(f\"Savings: 2x for this 4-element example.\")\nprint(f\"(In practice, the scale factor is shared across a group of 32-128 values,\")\nprint(f\" so per-weight overhead shrinks and real savings approach ~4x for int8.)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: The Outlier Problem\n",
    "\n",
    "Absmax works well when weights are distributed evenly. But what happens when there are outliers? One extreme value stretches the scale and wastes precision for the majority of weights.\n",
    "\n",
    "**Before running, predict:** Given weights [-0.1, 0.05, 0.02, -0.03, 8.5], the max absolute value is 8.5. The scale will be 8.5/127 â‰ˆ 0.067. What happens to the small values near zero when we quantize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- The outlier problem ---\n",
    "weights_normal = torch.tensor([-0.1, 0.05, 0.02, -0.03, 0.08])\n",
    "weights_outlier = torch.tensor([-0.1, 0.05, 0.02, -0.03, 8.5])\n",
    "\n",
    "print(\"NORMAL DISTRIBUTION (no outliers)\")\n",
    "print(\"=\" * 50)\n",
    "q_normal, scale_normal = absmax_quantize(weights_normal)\n",
    "w_approx_normal = absmax_dequantize(q_normal, scale_normal)\n",
    "error_normal = (weights_normal - w_approx_normal).abs()\n",
    "print(f\"Original:    {weights_normal.tolist()}\")\n",
    "print(f\"Scale:       {scale_normal.item():.6f}\")\n",
    "print(f\"Quantized:   {q_normal.tolist()}\")\n",
    "print(f\"Reconstructed: {[f'{v:.4f}' for v in w_approx_normal.tolist()]}\")\n",
    "print(f\"Errors:      {[f'{v:.6f}' for v in error_normal.tolist()]}\")\n",
    "\n",
    "print(f\"\\nOUTLIER DISTRIBUTION (one extreme value)\")\n",
    "print(\"=\" * 50)\n",
    "q_outlier, scale_outlier = absmax_quantize(weights_outlier)\n",
    "w_approx_outlier = absmax_dequantize(q_outlier, scale_outlier)\n",
    "error_outlier = (weights_outlier - w_approx_outlier).abs()\n",
    "print(f\"Original:    {weights_outlier.tolist()}\")\n",
    "print(f\"Scale:       {scale_outlier.item():.6f}\")\n",
    "print(f\"Quantized:   {q_outlier.tolist()}\")\n",
    "print(f\"Reconstructed: {[f'{v:.4f}' for v in w_approx_outlier.tolist()]}\")\n",
    "print(f\"Errors:      {[f'{v:.6f}' for v in error_outlier.tolist()]}\")\n",
    "\n",
    "print(f\"\\nThe outlier (8.5) hijacks the scale.\")\n",
    "print(f\"Small values near zero lose almost all their information.\")\n",
    "print(f\"Most of the int8 range [-127, 127] is wasted on values that don't exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: quantization error for normal vs outlier distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Large-scale test: random normal weights vs weights with outliers\n",
    "torch.manual_seed(42)\n",
    "w_gaussian = torch.randn(1000) * 0.1  # Typical weight distribution\n",
    "w_with_outliers = w_gaussian.clone()\n",
    "w_with_outliers[::50] = torch.randn(20) * 5.0  # Add outliers every 50 values\n",
    "\n",
    "for ax, (name, w) in zip(axes, [('Gaussian (no outliers)', w_gaussian), ('With outliers', w_with_outliers)]):\n",
    "    q, s = absmax_quantize(w)\n",
    "    w_approx = absmax_dequantize(q, s)\n",
    "    errors = (w - w_approx).abs()\n",
    "    \n",
    "    ax.hist(errors.numpy(), bins=50, color='#f87171', alpha=0.7, edgecolor='white', linewidth=0.5)\n",
    "    ax.set_title(f'{name}\\nMean error: {errors.mean():.6f}, Max error: {errors.max():.6f}', fontsize=10)\n",
    "    ax.set_xlabel('Absolute reconstruction error')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"With outliers, the reconstruction error is much larger for the majority of weights.\")\n",
    "print(\"This motivates zero-point quantization and more sophisticated methods like GPTQ/AWQ.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Zero-Point Quantization\n",
    "\n",
    "Zero-point quantization shifts the range so the minimum maps to -128 and the maximum maps to 127. This better utilizes the integer range for asymmetric distributions.\n",
    "\n",
    "**Before running, predict:** If all weights are positive (e.g., [0.1, 0.3, 0.5, 0.8]), absmax wastes the entire negative range of int8 [-127, 0). Zero-point quantization should use the full [-128, 127] range. Will the reconstruction error be smaller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_point_quantize(weights):\n",
    "    \"\"\"Zero-point quantization to int8.\n",
    "    \n",
    "    Steps:\n",
    "    1. Find min and max of weights\n",
    "    2. Compute scale = (max - min) / 255 (full uint8 range)\n",
    "    3. Compute zero_point = round(-min / scale) - 128\n",
    "    4. Quantize: q = round(w / scale) + zero_point, clamp to [-128, 127]\n",
    "    \n",
    "    Returns: quantized (int8), scale (float32), zero_point (int)\n",
    "    \"\"\"\n",
    "    w_min = weights.min()\n",
    "    w_max = weights.max()\n",
    "    \n",
    "    # Scale maps the float range to 256 int8 values\n",
    "    scale = (w_max - w_min) / 255.0\n",
    "    \n",
    "    # Zero point: which int8 value corresponds to float 0.0\n",
    "    zero_point = torch.round(-w_min / scale).to(torch.int32) - 128\n",
    "    \n",
    "    # Quantize\n",
    "    quantized = torch.clamp(torch.round(weights / scale) + zero_point, -128, 127).to(torch.int8)\n",
    "    \n",
    "    return quantized, scale, zero_point\n",
    "\n",
    "\n",
    "def zero_point_dequantize(quantized, scale, zero_point):\n",
    "    \"\"\"Reconstruct float weights from zero-point quantized values.\"\"\"\n",
    "    return (quantized.float() - zero_point.float()) * scale\n",
    "\n",
    "\n",
    "# Compare absmax vs zero-point on an asymmetric distribution\n",
    "weights_asym = torch.tensor([0.1, 0.3, 0.5, 0.8, 0.2, 0.6, 0.9, 0.15])\n",
    "\n",
    "print(\"ASYMMETRIC DISTRIBUTION (all positive values)\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Original: {weights_asym.tolist()}\")\n",
    "\n",
    "# Absmax\n",
    "q_abs, s_abs = absmax_quantize(weights_asym)\n",
    "w_abs = absmax_dequantize(q_abs, s_abs)\n",
    "err_abs = (weights_asym - w_abs).abs().mean()\n",
    "\n",
    "print(f\"\\nAbsmax:\")\n",
    "print(f\"  Quantized: {q_abs.tolist()}\")\n",
    "print(f\"  Range used: [{q_abs.min().item()}, {q_abs.max().item()}] out of [-127, 127]\")\n",
    "print(f\"  Mean error: {err_abs:.6f}\")\n",
    "\n",
    "# Zero-point\n",
    "q_zp, s_zp, zp = zero_point_quantize(weights_asym)\n",
    "w_zp = zero_point_dequantize(q_zp, s_zp, zp)\n",
    "err_zp = (weights_asym - w_zp).abs().mean()\n",
    "\n",
    "print(f\"\\nZero-point:\")\n",
    "print(f\"  Quantized: {q_zp.tolist()}\")\n",
    "print(f\"  Range used: [{q_zp.min().item()}, {q_zp.max().item()}] out of [-128, 127]\")\n",
    "print(f\"  Zero point: {zp.item()} (this int8 value = float 0.0)\")\n",
    "print(f\"  Mean error: {err_zp:.6f}\")\n",
    "\n",
    "print(f\"\\nZero-point uses the full int8 range, giving {err_abs/err_zp:.1f}x lower error.\")\n",
    "print(f\"Cost: one extra integer (zero_point) stored per group.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply quantization to a REAL model's weights and measure the error\n",
    "print(\"QUANTIZING REAL GPT-2 WEIGHTS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Load GPT-2\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "# Pick a real weight matrix: the first attention query projection\n",
    "# GPT-2 uses Conv1D, so the weight shape is (in_features, out_features)\n",
    "real_weights = gpt2.transformer.h[0].attn.c_attn.weight.data[:, :768].clone()\n",
    "print(f\"Weight matrix shape: {real_weights.shape}\")\n",
    "print(f\"Total values: {real_weights.numel():,}\")\n",
    "print(f\"Min: {real_weights.min():.4f}, Max: {real_weights.max():.4f}\")\n",
    "print(f\"Mean: {real_weights.mean():.4f}, Std: {real_weights.std():.4f}\")\n",
    "\n",
    "# Flatten for quantization\n",
    "w_flat = real_weights.flatten()\n",
    "\n",
    "# Absmax quantization\n",
    "q_abs, s_abs = absmax_quantize(w_flat)\n",
    "w_abs = absmax_dequantize(q_abs, s_abs)\n",
    "err_abs = (w_flat - w_abs).abs()\n",
    "\n",
    "# Zero-point quantization  \n",
    "q_zp, s_zp, zp = zero_point_quantize(w_flat)\n",
    "w_zp = zero_point_dequantize(q_zp, s_zp, zp)\n",
    "err_zp = (w_flat - w_zp).abs()\n",
    "\n",
    "print(f\"\\nAbsmax:     mean error = {err_abs.mean():.6f}, max error = {err_abs.max():.6f}\")\n",
    "print(f\"Zero-point: mean error = {err_zp.mean():.6f}, max error = {err_zp.max():.6f}\")\n",
    "print(f\"\\nRelative error (absmax):     {(err_abs.mean() / w_flat.abs().mean() * 100):.2f}%\")\n",
    "print(f\"Relative error (zero-point): {(err_zp.mean() / w_flat.abs().mean() * 100):.2f}%\")\n",
    "\n",
    "# Visualize weight distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].hist(w_flat.numpy(), bins=100, color='#60a5fa', alpha=0.7, edgecolor='white', linewidth=0.3)\n",
    "axes[0].set_title('GPT-2 Weight Distribution (attention Q projection)', fontsize=10)\n",
    "axes[0].set_xlabel('Weight value')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].hist(err_abs.numpy(), bins=100, color='#f87171', alpha=0.6, label='Absmax', edgecolor='white', linewidth=0.3)\n",
    "axes[1].hist(err_zp.numpy(), bins=100, color='#34d399', alpha=0.6, label='Zero-point', edgecolor='white', linewidth=0.3)\n",
    "axes[1].set_title('Quantization Reconstruction Error', fontsize=10)\n",
    "axes[1].set_xlabel('Absolute error')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe weight distribution is approximately Gaussian (most values near zero).\")\n",
    "print(\"Quantization error is tiny because the dense region maps well to the int8 grid.\")\n",
    "print(\"This is why INT8 (and even INT4) quantization works: neural network weights are compressible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just implemented:** Both absmax and zero-point quantization from scratch, traced every step with real numbers, and applied them to actual GPT-2 weights.\n",
    "\n",
    "Key takeaways:\n",
    "- Absmax is the simplest method: divide by max absolute value, round to int8. Works well for symmetric, Gaussian-like distributions.\n",
    "- Outliers break absmax by stretching the scale and wasting precision. Zero-point quantization handles asymmetric distributions better.\n",
    "- Real neural network weights are approximately Gaussian, which is ideal for quantization. The reconstruction error is tiny.\n",
    "- This is why GPTQ and AWQ can achieve INT4 with less than 1% perplexity degradation: the weight information is highly compressible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: LoRA from Scratch (Supported)\n",
    "\n",
    "Now the other half of the solution. LoRA freezes all base weights and adds tiny trainable low-rank matrices alongside them.\n",
    "\n",
    "The original forward pass: `h = W @ x` (W is frozen).\n",
    "The LoRA forward pass: `h = W @ x + (B @ A) @ x * (alpha / r)`.\n",
    "\n",
    "- **A** is (r x d_in), initialized from a random normal distribution\n",
    "- **B** is (d_out x r), initialized to **zeros** (so LoRA starts as identity)\n",
    "- **alpha / r** is a scaling factor\n",
    "\n",
    "You'll implement the `LoRALinear` class, apply it to GPT-2, and count parameters.\n",
    "\n",
    "Fill in the TODOs below. Each TODO is 1-3 lines.\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The key insight is that LoRA adds a low-rank bypass alongside the frozen original weight matrix. Because B is initialized to zeros, the LoRA output starts at zero â€” the model begins identical to the pretrained model. Training gradually learns the task-specific adaptation through A and B.\n",
    "\n",
    "```python\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, base: nn.Linear, r: int = 8, alpha: int = 16):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.base.weight.requires_grad = False\n",
    "        if self.base.bias is not None:\n",
    "            self.base.bias.requires_grad = False\n",
    "        d_in = base.in_features\n",
    "        d_out = base.out_features\n",
    "        self.A = nn.Parameter(torch.randn(r, d_in) * 0.01)\n",
    "        self.B = nn.Parameter(torch.zeros(d_out, r))\n",
    "        self.scale = alpha / r\n",
    "\n",
    "    def forward(self, x):\n",
    "        base_out = self.base(x)\n",
    "        lora_out = (x @ self.A.T @ self.B.T) * self.scale\n",
    "        return base_out + lora_out\n",
    "```\n",
    "\n",
    "For the parameter count: LoRA rank 8 on a 768x768 matrix adds 768*8 + 8*768 = 12,288 parameters. The full matrix has 589,824. That is about 2% â€” a 48x reduction.\n",
    "\n",
    "Common mistake: forgetting to freeze the base weight with `requires_grad = False`. Without this, the base weights would also receive gradients, defeating the purpose of LoRA.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"A linear layer with a LoRA low-rank bypass.\n",
    "    \n",
    "    Original: h = W @ x\n",
    "    LoRA:     h = W @ x + (B @ A) @ x * (alpha / r)\n",
    "    \n",
    "    W is frozen. Only A and B are trainable.\n",
    "    B initialized to zeros -> LoRA starts as identity.\n",
    "    \"\"\"\n",
    "    def __init__(self, base: nn.Linear, r: int = 8, alpha: int = 16):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        \n",
    "        # TODO: Freeze the base layer's weights (and bias if present).\n",
    "        # Set requires_grad = False on base.weight and base.bias.\n",
    "        # YOUR CODE HERE (2 lines)\n",
    "        \n",
    "        d_in = base.in_features\n",
    "        d_out = base.out_features\n",
    "        \n",
    "        # TODO: Create the LoRA parameters A and B.\n",
    "        # A is (r x d_in), initialized with small random normal values (* 0.01)\n",
    "        # B is (d_out x r), initialized to zeros\n",
    "        # Both should be nn.Parameter so they are trainable.\n",
    "        # YOUR CODE HERE (2 lines)\n",
    "        \n",
    "        self.scale = alpha / r\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # The frozen highway: original linear transformation\n",
    "        base_out = self.base(x)\n",
    "        \n",
    "        # TODO: Compute the LoRA bypass.\n",
    "        # lora_out = (x @ A^T @ B^T) * scale\n",
    "        # This is the \"trainable detour\" that starts at zero.\n",
    "        # YOUR CODE HERE (1 line)\n",
    "        \n",
    "        return base_out + lora_out\n",
    "\n",
    "\n",
    "# --- Test the LoRA layer ---\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a base linear layer (simulating a frozen model weight)\n",
    "base_linear = nn.Linear(768, 768, bias=False)\n",
    "\n",
    "# Wrap it with LoRA\n",
    "lora_layer = LoRALinear(base_linear, r=8, alpha=16)\n",
    "\n",
    "# Test input\n",
    "x = torch.randn(1, 10, 768)  # batch=1, seq_len=10, d_model=768\n",
    "\n",
    "# Forward pass\n",
    "output = lora_layer(x)\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Verify the base output is the same (LoRA starts at zero because B=0)\n",
    "with torch.no_grad():\n",
    "    base_only = base_linear(x)\n",
    "    diff = (output - base_only).abs().max().item()\n",
    "    print(f\"\\nMax difference from base output: {diff:.10f}\")\n",
    "    print(f\"(Should be ~0 because B is initialized to zeros)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameter counting: LoRA vs full finetuning ---\n",
    "\n",
    "# Check which parameters are trainable\n",
    "print(\"PARAMETER ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "for name, param in lora_layer.named_parameters():\n",
    "    print(f\"  {name:15s} | shape: {str(param.shape):15s} | trainable: {param.requires_grad}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in lora_layer.parameters())\n",
    "trainable_params = sum(p.numel() for p in lora_layer.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"\\nTotal parameters:     {total_params:>10,}\")\n",
    "print(f\"Trainable (LoRA):     {trainable_params:>10,}\")\n",
    "print(f\"Frozen (base):        {frozen_params:>10,}\")\n",
    "print(f\"Trainable fraction:   {trainable_params/total_params*100:.2f}%\")\n",
    "print(f\"Reduction factor:     {frozen_params/trainable_params:.0f}x fewer trainable params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Apply LoRA to actual GPT-2 and count parameters ---\n\ndef apply_lora_to_attention(model, r=8, alpha=16):\n    \"\"\"Apply LoRA to all attention Q and V projections in a GPT-2 model.\n    \n    GPT-2 uses Conv1D instead of nn.Linear, so we need to handle this.\n    The combined c_attn projects to Q, K, V concatenated.\n    For simplicity, we apply LoRA to the full c_attn projection.\n    \"\"\"\n    lora_layers = []\n    for i, block in enumerate(model.transformer.h):\n        # GPT-2's c_attn is a Conv1D(768, 2304) -> projects to Q, K, V\n        # We'll wrap it in a LoRA-style bypass\n        old_attn = block.attn.c_attn\n        d_in = old_attn.weight.shape[0]   # 768\n        d_out = old_attn.weight.shape[1]  # 2304\n        \n        # Freeze the original weights\n        old_attn.weight.requires_grad = False\n        old_attn.bias.requires_grad = False\n        \n        # Create LoRA A and B as separate parameters on the block\n        A = nn.Parameter(torch.randn(r, d_in) * 0.01)\n        B = nn.Parameter(torch.zeros(d_out, r))\n        \n        # Store them\n        block.attn.lora_A = A\n        block.attn.lora_B = B\n        block.attn.lora_scale = alpha / r\n        lora_layers.append((A, B))\n    \n    return lora_layers\n\n\n# Load a fresh GPT-2\ngpt2_lora = AutoModelForCausalLM.from_pretrained('gpt2')\n\n# Freeze ALL parameters first\nfor param in gpt2_lora.parameters():\n    param.requires_grad = False\n\n# Apply LoRA\nlora_pairs = apply_lora_to_attention(gpt2_lora, r=8, alpha=16)\n\n# Count parameters\ntotal = sum(p.numel() for p in gpt2_lora.parameters())\ntrainable = sum(p.numel() for p in gpt2_lora.parameters() if p.requires_grad)\nfrozen = total - trainable\n\nprint(\"GPT-2 WITH LoRA (rank=8, attention projections only)\")\nprint(\"=\" * 55)\nprint(f\"Total parameters:     {total:>12,}\")\nprint(f\"Frozen (base model):  {frozen:>12,}\")\nprint(f\"Trainable (LoRA):     {trainable:>12,}\")\nprint(f\"Trainable fraction:   {trainable/total*100:.3f}%\")\nprint(f\"\\nLoRA parameters per layer: {lora_pairs[0][0].numel() + lora_pairs[0][1].numel():,}\")\nprint(f\"Number of layers:          {len(lora_pairs)}\")\nprint(f\"Total LoRA parameters:     {trainable:,}\")\n\n# Memory comparison: LoRA vs full finetuning\n# LoRA: frozen base in bf16 + LoRA params with full optimizer overhead\nlora_training_mem = (\n    frozen * 2 +          # frozen weights in bf16 (no gradients needed)\n    trainable * 2 +       # LoRA weights in bf16\n    trainable * 2 +       # LoRA gradients in bf16\n    trainable * 4 +       # Adam momentum for LoRA (fp32)\n    trainable * 4          # Adam variance for LoRA (fp32)\n)\nfull_training_mem = memory_for_training(total)['total']\n\nprint(f\"\\nMEMORY COMPARISON\")\nprint(f\"Full finetuning:  {to_gb(full_training_mem):.1f} GB\")\nprint(f\"LoRA finetuning:  {to_gb(lora_training_mem):.2f} GB\")\nprint(f\"Savings:          {full_training_mem/lora_training_mem:.1f}x\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just built:** A LoRA layer from scratch, then applied it to GPT-2. The key numbers:\n",
    "\n",
    "- A single LoRA bypass on a 768x768 matrix with rank 8: **12,288 trainable parameters** vs 589,824 frozen. That is about **2%** of the full matrix.\n",
    "- Applied across all attention projections in GPT-2: the trainable fraction is a tiny percentage of the total model.\n",
    "- Because only the LoRA parameters need gradients and optimizer states, the memory cost for training drops dramatically.\n",
    "\n",
    "The conceptual picture: LoRA is the surgical version of \"freeze the backbone.\" Instead of freezing the backbone and adding a head at the end, you freeze everything and add tiny detours *inside* the backbone. Same philosophy â€” preserve pretrained knowledge, adapt minimally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: LoRA Finetuning with PEFT (Supported)\n",
    "\n",
    "Now use the HuggingFace PEFT library to do real LoRA finetuning. The conceptual understanding from Exercise 3 makes the library transparent â€” you know exactly what `LoraConfig` parameters mean because you implemented them yourself.\n",
    "\n",
    "You'll LoRA-finetune GPT-2 on a small instruction dataset and compare trainable parameters, memory usage, and generation quality before/after.\n",
    "\n",
    "Fill in the TODOs. Each is 1-3 lines.\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The PEFT library wraps the model and handles LoRA injection automatically. The key configuration:\n",
    "\n",
    "- `r=8`: rank of the low-rank matrices (same as what you implemented in Exercise 3)\n",
    "- `lora_alpha=16`: the alpha scaling factor (alpha/r = 2.0 in this case)\n",
    "- `target_modules`: which layers to add LoRA to. For GPT-2, `c_attn` is the combined Q/K/V projection and `c_proj` is the output projection.\n",
    "- `lora_dropout=0.05`: dropout on the LoRA path for regularization\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "```\n",
    "\n",
    "The training loop is the exact same heartbeat as always: forward, loss, zero_grad, backward, step. PEFT handles routing gradients only to the LoRA parameters.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a fresh GPT-2 for PEFT finetuning\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# GPT-2 has no pad token â€” set it to eos\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a LoraConfig with:\n",
    "#   r=8 (rank of the low-rank matrices)\n",
    "#   lora_alpha=16 (scaling factor)\n",
    "#   target_modules=[\"c_attn\", \"c_proj\"] (attention projections in GPT-2)\n",
    "#   lora_dropout=0.05\n",
    "#   bias=\"none\" (don't train biases)\n",
    "#   task_type=TaskType.CAUSAL_LM\n",
    "# YOUR CODE HERE (1 block)\n",
    "\n",
    "\n",
    "# TODO: Wrap the model with LoRA using get_peft_model(model, lora_config)\n",
    "# YOUR CODE HERE (1 line)\n",
    "\n",
    "\n",
    "# Print trainable parameter summary\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "# Detailed breakdown\n",
    "print(\"\\nLoRA modules added:\")\n",
    "for name, module in peft_model.named_modules():\n",
    "    if 'lora' in name.lower() and hasattr(module, 'weight'):\n",
    "        print(f\"  {name}: {module.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate text BEFORE LoRA finetuning (baseline) ---\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=60):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "eval_prompts = [\n",
    "    \"The meaning of life is\",\n",
    "    \"Machine learning is a field that\",\n",
    "    \"In a galaxy far, far away,\",\n",
    "]\n",
    "\n",
    "print(\"BEFORE LoRA finetuning:\")\n",
    "print(\"=\" * 60)\n",
    "before_responses = {}\n",
    "for prompt in eval_prompts:\n",
    "    response = generate_text(peft_model, tokenizer, prompt)\n",
    "    before_responses[prompt] = response\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Response: {response[:200]}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare a small finetuning dataset ---\n",
    "# We'll finetune on a small text corpus to shift the model's style.\n",
    "# Using a subset of the Tiny Shakespeare dataset for a visible style shift.\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load a small text dataset\n",
    "raw_dataset = load_dataset(\"karpathy/tiny_shakespeare\", split=\"train\")\n",
    "text = raw_dataset[0]['text'][:50000]  # First 50K characters\n",
    "\n",
    "# Tokenize into fixed-length chunks\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "encodings = tokenizer(text, return_tensors=\"pt\", truncation=False)\n",
    "input_ids = encodings[\"input_ids\"][0]\n",
    "\n",
    "# Create non-overlapping chunks\n",
    "chunks = [input_ids[i:i + MAX_LENGTH] for i in range(0, len(input_ids) - MAX_LENGTH, MAX_LENGTH)]\n",
    "print(f\"Created {len(chunks)} training chunks of length {MAX_LENGTH}\")\n",
    "\n",
    "\n",
    "class TextChunkDataset(Dataset):\n",
    "    def __init__(self, chunks):\n",
    "        self.chunks = chunks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.chunks[idx]\n",
    "        return {\"input_ids\": chunk, \"labels\": chunk.clone()}\n",
    "\n",
    "\n",
    "train_dataset = TextChunkDataset(chunks)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LoRA Finetuning loop ---\n",
    "# Same heartbeat as always: forward, loss, zero_grad, backward, step.\n",
    "# The only difference: PEFT routes gradients only to LoRA parameters.\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 3e-4  # Higher LR is fine for LoRA (fewer parameters)\n",
    "\n",
    "# Only optimize LoRA parameters (PEFT handles this)\n",
    "optimizer = torch.optim.AdamW(peft_model.parameters(), lr=LEARNING_RATE)\n",
    "peft_model.train()\n",
    "\n",
    "losses = []\n",
    "step_count = 0\n",
    "\n",
    "# Track memory if on GPU\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "print(f\"Training for {NUM_EPOCHS} epochs on {len(train_dataset)} chunks...\")\n",
    "print(f\"Batch size: 4, Steps per epoch: ~{len(train_dataloader)}\")\n",
    "print()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = peft_model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "        step_count += 1\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if step_count % 20 == 0:\n",
    "            print(f\"  Step {step_count:4d} | Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS} \\u2014 Avg loss: {avg_loss:.4f}\")\n",
    "    print()\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    peak_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"Peak GPU memory during training: {peak_mem:.2f} GB\")\n",
    "\n",
    "print(f\"Training complete! {step_count} total steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, linewidth=1, color='#a78bfa', alpha=0.5, label='Per-step loss')\n",
    "\n",
    "# Smoothed\n",
    "window = 15\n",
    "if len(losses) > window:\n",
    "    smoothed = [sum(losses[max(0, i - window):i + 1]) / len(losses[max(0, i - window):i + 1]) for i in range(len(losses))]\n",
    "    plt.plot(smoothed, linewidth=2, color='#a78bfa', label=f'Smoothed ({window}-step)')\n",
    "\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('LoRA Finetuning Loss (PEFT)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare before vs after LoRA finetuning ---\n",
    "peft_model.eval()\n",
    "\n",
    "print(\"BEFORE vs AFTER LoRA finetuning:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in eval_prompts:\n",
    "    after_response = generate_text(peft_model, tokenizer, prompt)\n",
    "    \n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"\\n  BEFORE (base GPT-2):\")\n",
    "    print(f\"  {before_responses[prompt][:200]}\")\n",
    "    print(f\"\\n  AFTER (LoRA finetuned on Shakespeare):\")\n",
    "    print(f\"  {after_response[:200]}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nThe LoRA-finetuned model should show a stylistic shift toward Shakespeare.\")\n",
    "print(\"This was achieved by training only the tiny LoRA parameters \\u2014 the base\")\n",
    "print(\"model weights are completely unchanged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just did:** Used the PEFT library to LoRA-finetune GPT-2, achieving a stylistic shift by training a tiny fraction of the total parameters.\n",
    "\n",
    "The library is not magic â€” you know exactly what it does because you built `LoRALinear` from scratch in Exercise 3. `LoraConfig(r=8, lora_alpha=16)` creates the same A and B matrices with the same scaling factor. The library just handles the plumbing: injecting LoRA into the right layers, routing gradients, and managing the adapter weights.\n",
    "\n",
    "Key observation: the learning rate for LoRA finetuning (3e-4) is higher than for full finetuning (5e-5 typically). With fewer trainable parameters, each parameter needs to change more per step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 5: Quantized Inference (Minimal Scaffolding)\n\nThe final exercise brings it full circle. You've seen the theory of quantization (Exercise 2) and LoRA (Exercises 3-4). Now load a quantized model and see quantization in practice: compare memory usage and output quality between full-precision and quantized models.\n\nThis exercise requires a GPU with bitsandbytes support. If running on CPU, the cells will show the expected results as comments.\n\n**Note:** This exercise uses the `generate_text` function defined in Exercise 4. Make sure you have run all previous cells before proceeding.\n\n**Your task:**\n- Load GPT-2 in full precision (float32) and in 8-bit quantized mode using bitsandbytes\n- Compare memory usage between the two\n- Generate text from both and compare output quality\n- Reflect on the memory-quality tradeoff\n\n<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe key insight: bitsandbytes integrates with HuggingFace `transformers` to load models in lower precision automatically. You pass `load_in_8bit=True` (or `load_in_4bit=True`) to `from_pretrained()`, and bitsandbytes handles the quantization.\n\n```python\nfrom transformers import BitsAndBytesConfig\n\n# 8-bit quantization config\nbnb_config_8bit = BitsAndBytesConfig(load_in_8bit=True)\n\n# Load quantized model\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n    \"gpt2\",\n    quantization_config=bnb_config_8bit,\n    device_map=\"auto\",\n)\n\n# For 4-bit:\nbnb_config_4bit = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",  # NormalFloat4 from the QLoRA paper\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n```\n\nFor GPT-2 (124M params), the memory savings are modest because the model is already small. The real impact is on 7B+ models where full precision requires 14+ GB and 4-bit fits in ~3.5 GB.\n\nCommon mistake: trying to run bitsandbytes on CPU. It requires a CUDA GPU.\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load full-precision model and measure memory ---\n",
    "\n",
    "import gc\n",
    "\n",
    "# Clean up previous models\n",
    "del gpt2, gpt2_lora, model, peft_model\n",
    "gc.collect()\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Load full-precision model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "model_fp32 = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# Calculate model memory from parameter sizes\n",
    "fp32_param_mem = sum(p.numel() * p.element_size() for p in model_fp32.parameters())\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    fp32_gpu_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "    print(f\"Full-precision (float32):\")\n",
    "    print(f\"  Parameter memory: {fp32_param_mem / 1e6:.1f} MB\")\n",
    "    print(f\"  GPU memory used:  {fp32_gpu_mem:.1f} MB\")\n",
    "    fp32_gpu = fp32_gpu_mem\n",
    "    \n",
    "    # Generate text\n",
    "    fp32_outputs = {}\n",
    "    test_prompts = [\n",
    "        \"The future of artificial intelligence is\",\n",
    "        \"Once upon a time, in a land far away,\",\n",
    "        \"The most important thing about programming is\",\n",
    "    ]\n",
    "    print(\"\\nFull-precision outputs:\")\n",
    "    for prompt in test_prompts:\n",
    "        response = generate_text(model_fp32, tokenizer, prompt)\n",
    "        fp32_outputs[prompt] = response\n",
    "        print(f\"  {prompt}\")\n",
    "        print(f\"  -> {response[:150]}\")\n",
    "        print()\n",
    "        \n",
    "    print(f\"\\nNote: For GPT-2 (124M params), the absolute savings are modest.\")\n",
    "    print(f\"For a 7B model, float32 = ~28 GB vs int8 = ~7 GB vs int4 = ~3.5 GB.\")\n",
    "    print(f\"That's the difference between 'fits on your GPU' and 'does not fit'.\")\n",
    "    \n",
    "print(f\"\\nParameter memory breakdown:\")\n",
    "print(f\"  float32: {fp32_param_mem / 1e6:.1f} MB ({fp32_param_mem / sum(p.numel() for p in model_fp32.parameters()):.0f} bytes/param)\")\n",
    "print(f\"  float16 would be: {fp32_param_mem / 2 / 1e6:.1f} MB\")\n",
    "print(f\"  int8 would be:    {fp32_param_mem / 4 / 1e6:.1f} MB\")\n",
    "print(f\"  int4 would be:    {fp32_param_mem / 8 / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load quantized model (requires GPU with bitsandbytes) ---\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    # Clean up\n",
    "    del model_fp32\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Load in 8-bit\n",
    "    bnb_config_8bit = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "        \"gpt2\",\n",
    "        quantization_config=bnb_config_8bit,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    int8_gpu_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "    \n",
    "    print(f\"8-bit quantized:\")\n",
    "    print(f\"  GPU memory used: {int8_gpu_mem:.1f} MB\")\n",
    "    print(f\"  vs full precision: {fp32_gpu:.1f} MB\")\n",
    "    print(f\"  Savings: {fp32_gpu / int8_gpu_mem:.1f}x\")\n",
    "    \n",
    "    # Generate text\n",
    "    print(\"\\n8-bit outputs:\")\n",
    "    int8_outputs = {}\n",
    "    for prompt in test_prompts:\n",
    "        response = generate_text(model_8bit, tokenizer, prompt)\n",
    "        int8_outputs[prompt] = response\n",
    "        print(f\"  {prompt}\")\n",
    "        print(f\"  -> {response[:150]}\")\n",
    "        print()\n",
    "    \n",
    "    # Side-by-side comparison\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COMPARISON: Full Precision vs 8-bit Quantized\")\n",
    "    print(\"=\" * 60)\n",
    "    for prompt in test_prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"  FP32: {fp32_outputs[prompt][:150]}\")\n",
    "        print(f\"  INT8: {int8_outputs[prompt][:150]}\")\n",
    "        match = fp32_outputs[prompt][:100] == int8_outputs[prompt][:100]\n",
    "        print(f\"  First 100 chars match: {match}\")\n",
    "    \n",
    "    print(f\"\\nMemory: {fp32_gpu:.0f} MB (FP32) vs {int8_gpu_mem:.0f} MB (INT8)\")\n",
    "    print(f\"Quality: outputs are nearly identical despite using 1/4 the bits per weight.\")\n",
    "    \n",
    "    del model_8bit\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Try 4-bit if available\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    try:\n",
    "        bnb_config_4bit = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",  # NormalFloat4 from the QLoRA paper\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "            \"gpt2\",\n",
    "            quantization_config=bnb_config_4bit,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        \n",
    "        int4_gpu_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "        \n",
    "        print(f\"\\n\\n4-bit quantized (NF4):\")\n",
    "        print(f\"  GPU memory used: {int4_gpu_mem:.1f} MB\")\n",
    "        print(f\"  vs full precision: {fp32_gpu:.1f} MB\")\n",
    "        print(f\"  Savings: {fp32_gpu / int4_gpu_mem:.1f}x\")\n",
    "        \n",
    "        print(\"\\n4-bit outputs:\")\n",
    "        for prompt in test_prompts:\n",
    "            response = generate_text(model_4bit, tokenizer, prompt)\n",
    "            print(f\"  {prompt}\")\n",
    "            print(f\"  -> {response[:150]}\")\n",
    "            print()\n",
    "            \n",
    "        del model_4bit\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n4-bit loading failed: {e}\")\n",
    "        print(\"This may require a newer GPU or updated bitsandbytes.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"THE BIG PICTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"For GPT-2 (124M), the savings are modest because the model is small.\")\n",
    "print(\"For real-world models, quantization is transformative:\")\n",
    "print(\"  Llama 2 7B:  float16 = 14 GB  |  int8 = 7 GB   |  int4 = 3.5 GB\")\n",
    "print(\"  Llama 2 13B: float16 = 26 GB  |  int8 = 13 GB  |  int4 = 6.5 GB\")\n",
    "print(\"  Llama 2 70B: float16 = 140 GB |  int8 = 70 GB  |  int4 = 35 GB\")\n",
    "print(\"\\nA consumer RTX 4090 (24 GB) can run a 13B model in 4-bit.\")\n",
    "print(\"With QLoRA, you can FINETUNE a 7B model on that same GPU.\")\n",
    "print(\"\\nThe 'massive GPU' barrier is largely a myth for parameter-efficient methods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just observed:** Quantized models produce nearly identical outputs to full-precision models while using significantly less memory. For GPT-2 the absolute numbers are small, but the principle scales:\n",
    "\n",
    "| Model | float16 | int8 | int4 (NF4) |\n",
    "|-------|---------|------|------------|\n",
    "| GPT-2 (124M) | 0.25 GB | 0.12 GB | 0.06 GB |\n",
    "| Llama 2 7B | 14 GB | 7 GB | 3.5 GB |\n",
    "| Llama 2 13B | 26 GB | 13 GB | 6.5 GB |\n",
    "| Llama 2 70B | 140 GB | 70 GB | 35 GB |\n",
    "\n",
    "The quality is preserved because neural network weights are highly compressible â€” most cluster near zero in a Gaussian distribution. The information density is much lower than the bit-width suggests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **Full finetuning hits a memory wall.** Training a 7B model requires ~84 GB: bf16 weights (14 GB) + bf16 gradients (14 GB) + fp32 Adam optimizer states (56 GB). The optimizer states â€” not the weights â€” are the bottleneck.\n\n2. **Quantization maps float weights to integers with minimal quality loss.** Absmax and zero-point quantization are the building blocks. Real methods (GPTQ, AWQ) achieve INT4 with less than 1% degradation. The weights are compressible because they cluster near zero.\n\n3. **LoRA freezes base weights and adds tiny trainable low-rank matrices.** Train 0.1-1% of parameters, match full finetuning quality. The low-rank constraint acts as implicit regularization â€” it is a feature, not a compromise.\n\n4. **QLoRA combines both: quantized base + LoRA adapters.** Finetune a 7B model in ~4 GB instead of ~84 GB. A consumer GPU can finetune and serve real LLMs.\n\n5. **Finetuning is a refinement, not a revolution.** Weight changes during finetuning are low-rank because you are adjusting, not rewriting. LoRA captures this insight directly."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}