{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Z-Image & Z-Image Turbo\n",
        "\n",
        "**Module 7.4, Lesson 4** | CourseAI\n",
        "\n",
        "Z-Image takes the same building blocks you know -- transformers, patchify, flow matching, joint attention, adaLN -- and recombines them more efficiently. Its S3-DiT single-stream architecture eliminates the parameter overhead of MMDiT's dual-stream design. A single LLM (Qwen3-4B) replaces triple text encoders. And Z-Image Turbo generates images in 8 steps, fitting in under 16GB of VRAM.\n",
        "\n",
        "**What you will do:**\n",
        "- Load Z-Image Turbo and inspect the architecture: verify the single-stream transformer, the Qwen3-4B text encoder, and parameter counts\n",
        "- Extract the transformer block structure and verify shared Q/K/V projections -- no dual-stream split, no per-modality duplication\n",
        "- Generate images with Z-Image Turbo at different step counts, test bilingual capability (English and Chinese prompts), and measure inference time\n",
        "- Compare Z-Image's architecture and outputs against SD3 or Flux, tracing the full pipeline end-to-end\n",
        "\n",
        "**For each exercise, PREDICT the output before running the cell.**\n",
        "\n",
        "Every concept in this notebook comes from the lesson. S3-DiT as \"translate once, then speak together,\" Qwen3-4B as the endpoint of the text encoder evolution, single-stream as simplification not novelty. No new theory -- just hands-on verification of what you just read.\n",
        "\n",
        "**Estimated time:** 60-90 minutes. All exercises require a GPU runtime with at least 16 GB VRAM (Colab T4 works, A100 is comfortable). Z-Image Turbo fits in under 16GB in bfloat16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run this cell to install dependencies and configure the environment.\n",
        "\n",
        "**Important:** Switch to a GPU runtime in Colab (Runtime > Change runtime type > T4 GPU or better). Z-Image Turbo requires ~12-14 GB VRAM in bfloat16. A T4 (16 GB) works; an A100 (40 GB) is comfortable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install diffusers from source for Z-Image support\n",
        "# Z-Image was merged into diffusers v0.36.0+\n",
        "!pip install -q git+https://github.com/huggingface/diffusers\n",
        "!pip install -q transformers accelerate safetensors sentencepiece protobuf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import gc\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "# Reproducible results\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "# Nice plots\n",
        "plt.style.use('dark_background')\n",
        "plt.rcParams['figure.figsize'] = [14, 5]\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "print(f'Device: {device}')\n",
        "print(f'Dtype: {dtype}')\n",
        "if device.type == 'cpu':\n",
        "    print('WARNING: No GPU detected. All exercises in this notebook require a GPU.')\n",
        "    print('Switch to GPU: Runtime > Change runtime type > T4 GPU')\n",
        "print()\n",
        "print('Setup complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Shared Helpers\n",
        "\n",
        "Utility functions used across multiple exercises. Run this cell now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Count total parameters in a model.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "\n",
        "def count_parameters_by_type(model):\n",
        "    \"\"\"Count parameters grouped by module type (e.g., Linear, LayerNorm).\"\"\"\n",
        "    counts = {}\n",
        "    for name, module in model.named_modules():\n",
        "        module_type = module.__class__.__name__\n",
        "        if module_type == type(model).__name__:\n",
        "            continue\n",
        "        n_params = sum(p.numel() for p in module.parameters(recurse=False))\n",
        "        if n_params > 0:\n",
        "            counts[module_type] = counts.get(module_type, 0) + n_params\n",
        "    return dict(sorted(counts.items(), key=lambda x: -x[1]))\n",
        "\n",
        "\n",
        "def show_image_row(images, titles, suptitle=None, figsize=None):\n",
        "    \"\"\"Display a row of PIL images with titles.\"\"\"\n",
        "    n = len(images)\n",
        "    fig_w = figsize[0] if figsize else max(5 * n, 12)\n",
        "    fig_h = figsize[1] if figsize else 5\n",
        "    fig, axes = plt.subplots(1, n, figsize=(fig_w, fig_h))\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    for ax, img, title in zip(axes, images, titles):\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(title, fontsize=10)\n",
        "        ax.axis('off')\n",
        "    if suptitle:\n",
        "        plt.suptitle(suptitle, fontsize=13, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def free_memory(*objects):\n",
        "    \"\"\"Delete objects and free GPU memory.\"\"\"\n",
        "    for obj in objects:\n",
        "        del obj\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    print('Memory freed.')\n",
        "\n",
        "\n",
        "print('Helpers defined: count_parameters, count_parameters_by_type, show_image_row, free_memory')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 1: Z-Image Pipeline Inspection `[Guided]`\n",
        "\n",
        "The lesson taught that Z-Image uses a **single LLM text encoder** (Qwen3-4B) instead of SD3's triple encoder setup, and an **S3-DiT single-stream transformer** instead of MMDiT's dual-stream design. The result: 6.15B parameters competing with Flux's 32B.\n",
        "\n",
        "In this exercise, you will load Z-Image Turbo and verify all of this concretely:\n",
        "1. Print the text encoder class name and parameter count -- verify it is Qwen3-4B (~4B params)\n",
        "2. Verify embedding output shapes from the text encoder\n",
        "3. Inspect the S3-DiT transformer: count parameters, verify the single-stream architecture\n",
        "4. Compare the overall parameter budget to SD3 and Flux\n",
        "\n",
        "**Before running, predict:**\n",
        "- Z-Image replaced SD3's three text encoders (~5.2B combined) with one LLM (Qwen3-4B). How many parameters does the text encoder have?\n",
        "- The lesson showed the S3-DiT has 30 layers with d_model=3840. How many total transformer parameters would you estimate?\n",
        "- SD3 had separate Q/K/V projections per modality. Z-Image has shared projections. What structural difference should you see when inspecting a transformer block?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Exercise 1: Load Z-Image Turbo and inspect its components\n",
        "# ============================================================\n",
        "\n",
        "# --- Step 1: Load Z-Image Turbo ---\n",
        "# Z-Image Turbo is available via the diffusers ZImagePipeline.\n",
        "# We load in bfloat16 for optimal memory efficiency.\n",
        "# The model fits in <16GB VRAM.\n",
        "\n",
        "from diffusers import ZImagePipeline\n",
        "\n",
        "print('Loading Z-Image Turbo... (this may take a few minutes)')\n",
        "pipe = ZImagePipeline.from_pretrained(\n",
        "    'Tongyi-MAI/Z-Image-Turbo',\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "pipe = pipe.to(device)\n",
        "print('Z-Image Turbo loaded.')\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 2: Inspect the text encoder ---\n",
        "# Z-Image uses a SINGLE text encoder: Qwen3-4B (a chat-capable LLM).\n",
        "# Compare this to SD3's triple encoder setup:\n",
        "#   SD3: CLIP ViT-L (123M) + OpenCLIP ViT-bigG (354M) + T5-XXL (4.7B) = ~5.2B total\n",
        "#   Z-Image: Qwen3-4B (~4B) = one encoder\n",
        "\n",
        "text_encoder = pipe.text_encoder\n",
        "encoder_params = count_parameters(text_encoder)\n",
        "\n",
        "print('Z-Image Text Encoder:')\n",
        "print('=' * 60)\n",
        "print(f'  Class: {type(text_encoder).__name__}')\n",
        "print(f'  Parameters: {encoder_params:,} ({encoder_params / 1e9:.2f}B)')\n",
        "print()\n",
        "\n",
        "# Compare to SD3's text encoder setup\n",
        "print('Text Encoder Comparison:')\n",
        "print('-' * 60)\n",
        "print(f'  SD v1.5:   CLIP ViT-L              ~0.12B (1 encoder)')\n",
        "print(f'  SDXL:      CLIP ViT-L + OpenCLIP    ~0.48B (2 encoders)')\n",
        "print(f'  SD3/Flux:  CLIP + OpenCLIP + T5-XXL ~5.2B  (3 encoders)')\n",
        "print(f'  Z-Image:   Qwen3-4B                 ~{encoder_params / 1e9:.1f}B  (1 encoder)')\n",
        "print()\n",
        "print('The trajectory: from multiple specialized encoders back to one')\n",
        "print('powerful encoder. Simpler pipeline, richer embeddings.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 3: Verify embedding output shapes ---\n",
        "# Encode a test prompt through Qwen3-4B and check the output shape.\n",
        "# The LLM produces per-token embeddings that enter the S3-DiT\n",
        "# as text tokens in the unified sequence.\n",
        "\n",
        "test_prompt = 'a cat sitting on a beach at sunset'\n",
        "\n",
        "# Tokenize and encode\n",
        "tokenizer = pipe.tokenizer\n",
        "tokens = tokenizer(\n",
        "    test_prompt,\n",
        "    return_tensors='pt',\n",
        "    padding='max_length',\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        ").input_ids.to(device)\n",
        "\n",
        "print(f'Prompt: \"{test_prompt}\"')\n",
        "print(f'Tokenized shape: {list(tokens.shape)}')\n",
        "print(f'Max sequence length: 512')\n",
        "print()\n",
        "\n",
        "# Get embeddings from the text encoder\n",
        "with torch.no_grad():\n",
        "    text_outputs = text_encoder(tokens, output_hidden_states=True)\n",
        "\n",
        "# The final hidden state provides the per-token embeddings\n",
        "hidden_states = text_outputs.last_hidden_state\n",
        "print(f'Text encoder output shape: {list(hidden_states.shape)}')\n",
        "print(f'  Batch size: {hidden_states.shape[0]}')\n",
        "print(f'  Sequence length: {hidden_states.shape[1]}')\n",
        "print(f'  Embedding dimension: {hidden_states.shape[2]}')\n",
        "print()\n",
        "print('These per-token embeddings enter the S3-DiT as text tokens.')\n",
        "print('The LLM processes the prompt ONCE to produce embeddings --')\n",
        "print('no chat, no reasoning at inference time. Just rich embeddings')\n",
        "print('from a model trained on diverse language tasks.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 4: Inspect the S3-DiT transformer ---\n",
        "# The denoising network is pipe.transformer (a ZImageTransformer2DModel).\n",
        "# The lesson predicted: 30 layers, d_model=3840, 30 heads, single-stream.\n",
        "\n",
        "transformer = pipe.transformer\n",
        "transformer_params = count_parameters(transformer)\n",
        "\n",
        "print('S3-DiT Transformer (Denoising Network):')\n",
        "print('=' * 60)\n",
        "print(f'  Class: {type(transformer).__name__}')\n",
        "print(f'  Parameters: {transformer_params:,} ({transformer_params / 1e9:.2f}B)')\n",
        "print()\n",
        "\n",
        "# Print config values\n",
        "config = transformer.config\n",
        "print('Config:')\n",
        "for key in ['num_layers', 'num_attention_heads', 'attention_head_dim',\n",
        "            'in_channels', 'patch_size', 'joint_attention_dim']:\n",
        "    val = getattr(config, key, 'N/A')\n",
        "    print(f'  {key}: {val}')\n",
        "print()\n",
        "\n",
        "# Parameter breakdown by top-level component\n",
        "print('Parameter breakdown by top-level component:')\n",
        "groups = {}\n",
        "for name, module in transformer.named_children():\n",
        "    n_params = count_parameters(module)\n",
        "    if n_params > 0:\n",
        "        groups[name] = n_params\n",
        "\n",
        "for group, count in sorted(groups.items(), key=lambda x: -x[1]):\n",
        "    pct = count / transformer_params * 100\n",
        "    print(f'    {group:<35} {count:>14,} ({pct:.1f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 5: Overall parameter budget ---\n",
        "# Compare Z-Image's total parameter count to SD3 and Flux.\n",
        "\n",
        "vae_params = count_parameters(pipe.vae)\n",
        "total_params = encoder_params + transformer_params + vae_params\n",
        "\n",
        "print('Z-Image Turbo: Parameter Budget')\n",
        "print('=' * 60)\n",
        "print(f'  Text encoder (Qwen3-4B):   {encoder_params:>14,} ({encoder_params / 1e9:.2f}B)')\n",
        "print(f'  S3-DiT transformer:        {transformer_params:>14,} ({transformer_params / 1e9:.2f}B)')\n",
        "print(f'  VAE:                       {vae_params:>14,} ({vae_params / 1e6:.0f}M)')\n",
        "print('-' * 60)\n",
        "print(f'  Total:                     {total_params:>14,} ({total_params / 1e9:.2f}B)')\n",
        "print()\n",
        "\n",
        "print('Comparison to other models:')\n",
        "print('-' * 60)\n",
        "print(f'  SD3 Medium:    ~5.2B (text) + ~2B (MMDiT)  = ~7.2B total')\n",
        "print(f'  Flux.1 Dev:    ~5.2B (text) + ~12B (MMDiT) = ~32B total')\n",
        "print(f'  Z-Image:       ~{encoder_params / 1e9:.1f}B (text) + ~{transformer_params / 1e9:.1f}B (S3-DiT) = ~{total_params / 1e9:.1f}B total')\n",
        "print()\n",
        "print(f'Z-Image is roughly {32 / (total_params / 1e9):.0f}x smaller than Flux.1 Dev.')\n",
        "print('The savings come from two sources:')\n",
        "print('  1. One text encoder instead of three (simpler pipeline)')\n",
        "print('  2. Single-stream transformer (no per-modality parameter duplication)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What Just Happened\n",
        "\n",
        "You loaded Z-Image Turbo and verified the architecture the lesson described:\n",
        "\n",
        "- **One text encoder, not three.** Qwen3-4B (~4B params) replaces the CLIP ViT-L + OpenCLIP ViT-bigG + T5-XXL triple encoder setup from SD3. One tokenizer, one forward pass, one set of embeddings. The text encoder evolution: 1 encoder -> 2 -> 3 -> back to 1 (but a much more powerful 1).\n",
        "\n",
        "- **S3-DiT is a standard transformer.** The denoising network is a `ZImageTransformer2DModel` -- a transformer with 30 layers, 3840 hidden dimension, 30 attention heads. The same scaling recipe from the DiT lesson (\"two knobs: d_model and N\"), applied at a larger scale.\n",
        "\n",
        "- **Dramatically fewer parameters than Flux.** Z-Image achieves comparable quality with roughly 1/5 the parameters. The savings come from architectural simplification (single-stream) and a simpler text encoding pipeline (one LLM instead of three specialized encoders).\n",
        "\n",
        "- **The LLM provides embeddings, not chat.** Qwen3-4B processes the prompt once to produce token embeddings. No interactive reasoning at inference time -- just rich embeddings from a model trained on diverse language tasks.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Single-Stream Architecture Exploration `[Guided]`\n",
        "\n",
        "The lesson explained the key architectural difference between MMDiT and S3-DiT:\n",
        "\n",
        "- **MMDiT (SD3/Flux):** Separate Q/K/V projections and separate FFN for text and image tokens at every layer. \"Shared listening, separate thinking.\"\n",
        "- **S3-DiT (Z-Image):** Shared Q/K/V projections and shared FFN for all token types. Modality-specific processing concentrated in 2 lightweight refiner layers upfront. \"Translate once, then speak together.\"\n",
        "\n",
        "In this exercise, you will:\n",
        "1. Extract the transformer block structure from Z-Image\n",
        "2. Verify shared Q/K/V projections (no modality-specific duplication)\n",
        "3. Identify the refiner layers\n",
        "4. Count parameter savings vs what a dual-stream equivalent would need\n",
        "\n",
        "**Before running, predict:**\n",
        "- In MMDiT, each block has separate text and image Q/K/V projections. In S3-DiT, you should see ONE set of Q/K/V projections. What does this look like in the module listing?\n",
        "- The lesson claimed single-stream saves ~50% of projection + FFN parameters per block. For d_model=3840, how many parameters does one set of Q/K/V projections cost? (Hint: 3 matrices of d_model x d_model)\n",
        "- Where will you find the refiner layers -- inside each transformer block, or as a separate module?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Exercise 2: Inspect the single-stream block structure\n",
        "# ============================================================\n",
        "\n",
        "# --- Step 1: List all transformer blocks ---\n",
        "# The transformer has a list of blocks. In MMDiT, these would have\n",
        "# separate per-modality components. In S3-DiT, they should be\n",
        "# single-stream: one set of projections, one FFN.\n",
        "\n",
        "transformer = pipe.transformer\n",
        "\n",
        "# Count transformer blocks\n",
        "print('Transformer block structure:')\n",
        "print('=' * 60)\n",
        "\n",
        "# List all top-level children\n",
        "for name, module in transformer.named_children():\n",
        "    n_params = count_parameters(module)\n",
        "    if n_params > 0:\n",
        "        # Check if it's a list of blocks\n",
        "        if hasattr(module, '__len__'):\n",
        "            print(f'  {name}: {type(module).__name__} ({len(module)} items, {n_params:,} total params)')\n",
        "        else:\n",
        "            print(f'  {name}: {type(module).__name__} ({n_params:,} params)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 2: Inspect one S3-DiT block ---\n",
        "# The key architectural claim: ONE set of Q/K/V projections shared\n",
        "# across all token types (text, image, visual semantic).\n",
        "# Compare this to MMDiT where you would see SEPARATE projections.\n",
        "\n",
        "# Get the blocks -- look for the main transformer blocks\n",
        "blocks = None\n",
        "block_container_name = None\n",
        "for name, module in transformer.named_children():\n",
        "    if hasattr(module, '__len__') and len(module) > 10:\n",
        "        blocks = module\n",
        "        block_container_name = name\n",
        "        break\n",
        "\n",
        "if blocks is None:\n",
        "    # Fallback: try common names\n",
        "    for attr in ['transformer_blocks', 'blocks', 'layers']:\n",
        "        if hasattr(transformer, attr):\n",
        "            blocks = getattr(transformer, attr)\n",
        "            block_container_name = attr\n",
        "            break\n",
        "\n",
        "print(f'Main transformer blocks: {block_container_name}')\n",
        "print(f'Number of blocks: {len(blocks)}')\n",
        "print()\n",
        "\n",
        "# Inspect block 0 in detail\n",
        "block_0 = blocks[0]\n",
        "print(f'Block 0 type: {type(block_0).__name__}')\n",
        "print(f'Block 0 total params: {count_parameters(block_0):,}')\n",
        "print()\n",
        "print('Sub-modules (looking for SHARED projections, not per-modality):')\n",
        "print('-' * 80)\n",
        "for name, module in block_0.named_modules():\n",
        "    if name == '':\n",
        "        continue\n",
        "    n_params = sum(p.numel() for p in module.parameters(recurse=False))\n",
        "    if n_params > 0:\n",
        "        print(f'  {name:<55} {type(module).__name__:<15} ({n_params:,})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 3: Identify attention projections ---\n",
        "# In MMDiT, you would see:\n",
        "#   attn.to_q, attn.to_k, attn.to_v         (image projections)\n",
        "#   attn.add_q_proj, attn.add_k_proj, ...    (text projections)\n",
        "# In S3-DiT single-stream, you should see:\n",
        "#   ONE set of Q/K/V projections, shared across all modalities\n",
        "\n",
        "print('Attention module analysis:')\n",
        "print('=' * 60)\n",
        "\n",
        "# Find the attention module\n",
        "attn_module = None\n",
        "attn_name = None\n",
        "for name, module in block_0.named_modules():\n",
        "    mod_type = type(module).__name__\n",
        "    if 'Attention' in mod_type or 'attn' in name.lower():\n",
        "        if sum(p.numel() for p in module.parameters(recurse=False)) > 0 or \\\n",
        "           sum(1 for _ in module.children()) > 0:\n",
        "            attn_module = module\n",
        "            attn_name = name\n",
        "            break\n",
        "\n",
        "if attn_module is None:\n",
        "    # Try direct attribute access\n",
        "    for attr in ['attn', 'attn1', 'self_attn', 'attention']:\n",
        "        if hasattr(block_0, attr):\n",
        "            attn_module = getattr(block_0, attr)\n",
        "            attn_name = attr\n",
        "            break\n",
        "\n",
        "print(f'Attention module: {attn_name} ({type(attn_module).__name__})')\n",
        "print(f'Attention params: {count_parameters(attn_module):,}')\n",
        "print()\n",
        "\n",
        "# List all children of the attention module\n",
        "print('Attention sub-modules:')\n",
        "for name, child in attn_module.named_children():\n",
        "    n_params = count_parameters(child)\n",
        "    print(f'  {name:<30} {type(child).__name__:<15} ({n_params:,})')\n",
        "\n",
        "print()\n",
        "print('Key observation: Look for the Q/K/V projections.')\n",
        "print('In MMDiT you would see DUPLICATE sets (one per modality).')\n",
        "print('In S3-DiT you should see ONE set (shared across modalities).')\n",
        "print('This is the \"translate once, then speak together\" design.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 4: Identify refiner layers ---\n",
        "# The lesson taught that S3-DiT uses 2 lightweight refiner layers\n",
        "# per modality to pre-process embeddings before the main transformer.\n",
        "# These should be separate from the main 30-layer backbone.\n",
        "\n",
        "print('Looking for refiner layers:')\n",
        "print('=' * 60)\n",
        "\n",
        "# Search for refiner-related modules in the transformer\n",
        "refiner_found = False\n",
        "for name, module in transformer.named_children():\n",
        "    name_lower = name.lower()\n",
        "    if 'refin' in name_lower or 'pre' in name_lower or 'embed' in name_lower:\n",
        "        n_params = count_parameters(module)\n",
        "        if n_params > 0:\n",
        "            print(f'  {name}: {type(module).__name__} ({n_params:,} params)')\n",
        "            refiner_found = True\n",
        "\n",
        "if not refiner_found:\n",
        "    print('  No obviously named refiner layers found at top level.')\n",
        "    print('  Searching all named modules for refiner-like components...')\n",
        "    for name, module in transformer.named_modules():\n",
        "        if 'refin' in name.lower():\n",
        "            n_params = sum(p.numel() for p in module.parameters(recurse=False))\n",
        "            print(f'  {name}: {type(module).__name__} ({n_params:,})')\n",
        "            refiner_found = True\n",
        "\n",
        "if not refiner_found:\n",
        "    print('  Refiner layers may be integrated into the embedding or')\n",
        "    print('  projection stages. Check the input projection modules above.')\n",
        "\n",
        "print()\n",
        "print('The refiner layers serve a critical role: they pre-process each')\n",
        "print(\"modality's raw embeddings into a shared representation space.\")\n",
        "print('Without them, shared Q/K/V projections would fail -- the same')\n",
        "print('problem MMDiT solved with per-modality projections at every layer.')\n",
        "print('S3-DiT solves it ONCE upfront instead of at every layer.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 5: Parameter savings calculation ---\n",
        "# The lesson claimed single-stream saves ~50% of projection + FFN\n",
        "# parameters per block. Let's verify with real numbers.\n",
        "\n",
        "d_model = None\n",
        "# Try to get d_model from config\n",
        "for attr in ['joint_attention_dim', 'hidden_size', 'inner_dim']:\n",
        "    val = getattr(transformer.config, attr, None)\n",
        "    if val is not None:\n",
        "        d_model = val\n",
        "        break\n",
        "\n",
        "# Fallback: compute from attention heads\n",
        "if d_model is None:\n",
        "    n_heads = transformer.config.num_attention_heads\n",
        "    head_dim = transformer.config.attention_head_dim\n",
        "    d_model = n_heads * head_dim\n",
        "\n",
        "n_blocks = len(blocks)\n",
        "block_params = count_parameters(block_0)\n",
        "\n",
        "print('Parameter Savings: Single-Stream vs Dual-Stream')\n",
        "print('=' * 60)\n",
        "print(f'  d_model: {d_model}')\n",
        "print(f'  Number of blocks: {n_blocks}')\n",
        "print(f'  Actual params per S3-DiT block: {block_params:,}')\n",
        "print()\n",
        "\n",
        "# Calculate what a dual-stream equivalent would need\n",
        "# MMDiT dual-stream adds: separate Q/K/V for second modality + separate FFN\n",
        "qkv_params = 3 * d_model * d_model  # Q, K, V matrices for one modality\n",
        "# SwiGLU FFN: gate_proj (d * 8/3*d) + up_proj (d * 8/3*d) + down_proj (8/3*d * d)\n",
        "ffn_dim = int(d_model * 8 / 3)  # Approximate SwiGLU hidden dim\n",
        "ffn_params = 3 * d_model * ffn_dim  # gate + up + down projections\n",
        "\n",
        "extra_dual_stream = qkv_params + ffn_params  # Per-modality duplication cost\n",
        "\n",
        "print(f'Per-modality duplication cost (if dual-stream):')\n",
        "print(f'  Extra Q/K/V projections:  {qkv_params:,} ({qkv_params / 1e6:.1f}M)')\n",
        "print(f'  Extra FFN:               {ffn_params:,} ({ffn_params / 1e6:.1f}M)')\n",
        "print(f'  Total extra per block:   {extra_dual_stream:,} ({extra_dual_stream / 1e6:.1f}M)')\n",
        "print(f'  Over {n_blocks} blocks:          {extra_dual_stream * n_blocks:,} ({extra_dual_stream * n_blocks / 1e9:.2f}B)')\n",
        "print()\n",
        "\n",
        "savings_pct = extra_dual_stream / (block_params + extra_dual_stream) * 100\n",
        "print(f'Parameter savings from single-stream:')\n",
        "print(f'  Dual-stream block would be: {block_params + extra_dual_stream:,}')\n",
        "print(f'  Single-stream block is:     {block_params:,}')\n",
        "print(f'  Savings per block:          {savings_pct:.0f}% of projection + FFN params')\n",
        "print(f'  Total savings:              {extra_dual_stream * n_blocks / 1e9:.2f}B parameters')\n",
        "print()\n",
        "print('This confirms the lesson: single-stream saves ~50% of per-block')\n",
        "print('parameters by eliminating modality-specific duplication.')\n",
        "print('Not from a novel mechanism, but from eliminating redundancy.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What Just Happened\n",
        "\n",
        "You explored Z-Image's single-stream architecture and verified the design choices from the lesson:\n",
        "\n",
        "- **Shared Q/K/V projections confirmed.** Unlike MMDiT's separate per-modality projections, each S3-DiT block has ONE set of Q/K/V projections. Text tokens, image tokens, and visual semantic tokens all share the same projections. This is the architectural source of the parameter savings.\n",
        "\n",
        "- **Shared FFN confirmed.** One SwiGLU FFN per block instead of separate FFNs per modality. Same story: shared computation across all token types.\n",
        "\n",
        "- **Refiner layers handle modality pre-processing.** The refiner layers (or equivalent input projection modules) pre-process each modality's raw embeddings into a shared representation space. This solves the same problem MMDiT solved with per-modality projections at every layer -- but solves it once upfront instead of 30 times.\n",
        "\n",
        "- **~50% parameter savings verified.** A dual-stream equivalent of Z-Image's transformer would require roughly double the projection and FFN parameters per block. Over 30 layers, that adds up to billions of parameters. Single-stream is not a trick -- it is a fundamental architectural simplification.\n",
        "\n",
        "- **The \"translate once, then speak together\" analogy holds.** MMDiT translates at every layer (separate projections). S3-DiT translates once (refiner layers), then everyone shares the same projections and FFN. Same communication, less overhead.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Z-Image Turbo Generation `[Supported]`\n",
        "\n",
        "The lesson taught that Z-Image Turbo generates images in 8 steps via Decoupled-DMD distillation, fitting in under 16GB of VRAM. The distilled model uses `guidance_scale=0.0` (no classifier-free guidance needed -- the distillation bakes in the quality boost).\n",
        "\n",
        "In this exercise, you will:\n",
        "1. Generate images with Z-Image Turbo at different step counts (4, 8, 16 steps)\n",
        "2. Compare quality across step counts to observe the Decoupled-DMD sweet spot\n",
        "3. Test bilingual capability (English and Chinese prompts)\n",
        "4. Measure inference time\n",
        "\n",
        "Fill in the TODO markers to complete the comparison.\n",
        "\n",
        "**Important note:** Z-Image Turbo uses `num_inference_steps=N+1` to produce N actual DiT forward passes. So `num_inference_steps=9` gives 8 steps. Also, guidance scale should be 0.0 for the Turbo variant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Exercise 3: Generate with Z-Image Turbo\n",
        "# ============================================================\n",
        "\n",
        "# --- Step 1: Generate at different step counts ---\n",
        "# The Decoupled-DMD distillation means Z-Image Turbo produces\n",
        "# good results at 8 steps. Let's compare 4, 8, and 16 steps.\n",
        "#\n",
        "# IMPORTANT: Z-Image Turbo uses num_inference_steps = N+1 for N actual\n",
        "# DiT forward passes. So for 8 steps, use num_inference_steps=9.\n",
        "# Also: guidance_scale=0.0 for Turbo models (no CFG needed).\n",
        "\n",
        "prompt = 'a cat sitting on a beach at sunset, photorealistic, golden hour lighting'\n",
        "height, width = 1024, 1024\n",
        "\n",
        "# Map desired steps to num_inference_steps (N+1 pattern)\n",
        "step_configs = [\n",
        "    (4, 5),   # 4 actual steps -> num_inference_steps=5\n",
        "    (8, 9),   # 8 actual steps -> num_inference_steps=9\n",
        "    (16, 17), # 16 actual steps -> num_inference_steps=17\n",
        "]\n",
        "\n",
        "images = []\n",
        "titles = []\n",
        "\n",
        "for actual_steps, inference_steps in step_configs:\n",
        "    generator = torch.Generator(device='cuda').manual_seed(42)\n",
        "    start = time.time()\n",
        "\n",
        "    # TODO: Generate an image using pipe() with these parameters:\n",
        "    #   prompt=prompt\n",
        "    #   height=height, width=width\n",
        "    #   num_inference_steps=inference_steps\n",
        "    #   guidance_scale=0.0  (important: 0 for Turbo models)\n",
        "    #   generator=generator\n",
        "    # Store the result in a variable called 'result'.\n",
        "    raise NotImplementedError(\n",
        "        'TODO: Call pipe() with the correct arguments.'\n",
        "    )\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    images.append(result.images[0])\n",
        "    titles.append(f'{actual_steps} steps\\n{elapsed:.1f}s')\n",
        "    print(f'  {actual_steps} steps ({inference_steps} inference): {elapsed:.1f}s')\n",
        "\n",
        "print('\\nGeneration complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 2: Display the step count comparison ---\n",
        "\n",
        "show_image_row(\n",
        "    images, titles,\n",
        "    suptitle='Z-Image Turbo: Quality vs Step Count (Decoupled-DMD)',\n",
        "    figsize=(18, 6),\n",
        ")\n",
        "\n",
        "print('What you should observe:')\n",
        "print('  4 steps:  Reasonable quality, may lack fine detail')\n",
        "print('  8 steps:  The Decoupled-DMD sweet spot -- high quality')\n",
        "print('  16 steps: Marginal improvement over 8 steps')\n",
        "print()\n",
        "print('Compare to the speed landscape from the lesson:')\n",
        "print('  DDPM:         50+ steps  (baseline)')\n",
        "print('  DDIM:         20-50 steps')\n",
        "print('  LCM:          4-8 steps  (noticeable quality loss)')\n",
        "print('  SDXL Turbo:   1-4 steps  (good quality, limited diversity)')\n",
        "print('  Z-Image Turbo: 8 steps   (competitive with many-step models)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 3: Test bilingual capability ---\n",
        "# Z-Image uses Qwen3-4B, which is bilingual (English and Chinese).\n",
        "# The lesson taught that Qwen3-4B provides richer text understanding\n",
        "# including native bilingual support.\n",
        "\n",
        "bilingual_prompts = [\n",
        "    ('A majestic mountain landscape at sunrise with a river flowing through a valley',\n",
        "     'English'),\n",
        "    ('\\u65e5\\u51fa\\u65f6\\u5206\\uff0c\\u4e00\\u6761\\u6cb3\\u6d41\\u7a7f\\u8fc7\\u5c71\\u8c37\\uff0c\\u58ee\\u4e3d\\u7684\\u5c71\\u8109\\u666f\\u89c2',\n",
        "     'Chinese (same meaning)'),\n",
        "]\n",
        "\n",
        "bi_images = []\n",
        "bi_titles = []\n",
        "\n",
        "for prompt_text, label in bilingual_prompts:\n",
        "    generator = torch.Generator(device='cuda').manual_seed(42)\n",
        "\n",
        "    # TODO: Generate an image with 8 steps (num_inference_steps=9).\n",
        "    # Use guidance_scale=0.0 and the prompt_text variable.\n",
        "    raise NotImplementedError(\n",
        "        'TODO: Generate an image for each bilingual prompt.'\n",
        "    )\n",
        "\n",
        "    bi_images.append(result.images[0])\n",
        "    bi_titles.append(f'{label}\\n\"{prompt_text[:50]}...\"' if len(prompt_text) > 50 else f'{label}\\n\"{prompt_text}\"')\n",
        "\n",
        "show_image_row(\n",
        "    bi_images, bi_titles,\n",
        "    suptitle='Z-Image Turbo: Bilingual Capability (English vs Chinese)',\n",
        "    figsize=(14, 6),\n",
        ")\n",
        "\n",
        "print('Qwen3-4B is natively bilingual (English + Chinese).')\n",
        "print('Both prompts describe the same scene. The images should be')\n",
        "print('semantically similar, demonstrating that the LLM text encoder')\n",
        "print('understands both languages equally well.')\n",
        "print()\n",
        "print('CLIP-based models would need a separate Chinese CLIP or')\n",
        "print('translation step. Z-Image handles this natively because')\n",
        "print('the text encoder is a chat-capable LLM trained on diverse data.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 4: Measure inference time ---\n",
        "# Time the full pipeline for 8 steps, averaged over 3 runs.\n",
        "# This includes text encoding, denoising, and VAE decode.\n",
        "\n",
        "prompt = 'a futuristic city skyline at night with neon lights'\n",
        "n_runs = 3\n",
        "times = []\n",
        "\n",
        "# Warmup run\n",
        "generator = torch.Generator(device='cuda').manual_seed(0)\n",
        "_ = pipe(\n",
        "    prompt=prompt, height=1024, width=1024,\n",
        "    num_inference_steps=9, guidance_scale=0.0,\n",
        "    generator=generator,\n",
        ")\n",
        "\n",
        "# Timed runs\n",
        "for i in range(n_runs):\n",
        "    generator = torch.Generator(device='cuda').manual_seed(i)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "\n",
        "    _ = pipe(\n",
        "        prompt=prompt, height=1024, width=1024,\n",
        "        num_inference_steps=9, guidance_scale=0.0,\n",
        "        generator=generator,\n",
        "    )\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "    elapsed = time.time() - start\n",
        "    times.append(elapsed)\n",
        "    print(f'  Run {i+1}: {elapsed:.2f}s')\n",
        "\n",
        "avg_time = sum(times) / len(times)\n",
        "print(f'\\nAverage generation time (8 steps, 1024x1024): {avg_time:.2f}s')\n",
        "print(f'Steps per second: {8 / avg_time:.1f}')\n",
        "print()\n",
        "print('The lesson reported sub-second on H800 GPUs.')\n",
        "print('On Colab GPUs (T4/A100), expect 2-10 seconds depending on the GPU.')\n",
        "print('The key point: 8 steps is dramatically fewer than DDPM\\'s 50+.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "The key insights: Z-Image Turbo uses `guidance_scale=0.0` because the Decoupled-DMD distillation has already baked in the quality boost that CFG provides. And `num_inference_steps` must be set to N+1 for N actual DiT forward passes.\n",
        "\n",
        "**Step 1: Generate at different step counts**\n",
        "```python\n",
        "result = pipe(\n",
        "    prompt=prompt,\n",
        "    height=height,\n",
        "    width=width,\n",
        "    num_inference_steps=inference_steps,\n",
        "    guidance_scale=0.0,\n",
        "    generator=generator,\n",
        ")\n",
        "```\n",
        "\n",
        "**Step 3: Bilingual generation**\n",
        "```python\n",
        "result = pipe(\n",
        "    prompt=prompt_text,\n",
        "    height=1024,\n",
        "    width=1024,\n",
        "    num_inference_steps=9,\n",
        "    guidance_scale=0.0,\n",
        "    generator=generator,\n",
        ")\n",
        "```\n",
        "\n",
        "**What to observe:**\n",
        "- At 8 steps, Z-Image Turbo produces high-quality photorealistic images. This is the sweet spot from Decoupled-DMD distillation.\n",
        "- The jump from 4 to 8 steps is more significant than from 8 to 16. Decoupled-DMD was optimized for ~8 steps.\n",
        "- English and Chinese prompts produce semantically similar images, demonstrating Qwen3-4B's bilingual capability. CLIP-based models would struggle with Chinese text without additional translation.\n",
        "- Inference time on Colab should be a few seconds. On enterprise H800 GPUs, Z-Image Turbo achieves sub-second generation.\n",
        "\n",
        "**Why guidance_scale=0.0?**\n",
        "The Decoupled-DMD distillation process uses CFG-augmented teacher outputs (the \"spear\") during training. The quality boost from CFG is baked into the student model's weights. At inference time, no additional CFG is needed -- the model generates high-quality images directly. This is why the Turbo variant is faster: no need for the unconditional pass that CFG requires.\n",
        "\n",
        "**Common mistakes:**\n",
        "- Using `guidance_scale > 0` with the Turbo model. This can degrade quality because the model was not trained to work with inference-time CFG.\n",
        "- Using `num_inference_steps=8` instead of 9. The pipeline uses N+1 steps internally.\n",
        "- Forgetting to set the generator seed, making results non-reproducible across step counts.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What Just Happened\n",
        "\n",
        "You generated images with Z-Image Turbo and verified the lesson's claims:\n",
        "\n",
        "- **8 steps is the sweet spot.** The Decoupled-DMD distillation was optimized for 8 steps. Quality at 8 steps is competitive with many-step models. The jump from 4 to 8 is significant; from 8 to 16 is marginal.\n",
        "\n",
        "- **No guidance needed.** `guidance_scale=0.0` because the distillation baked in the quality boost. The \"spear\" (CFG-augmented teacher outputs) was used during training, so the student model generates high-quality images directly. No unconditional pass needed at inference.\n",
        "\n",
        "- **Bilingual generation works natively.** English and Chinese prompts produce semantically similar images because Qwen3-4B is natively bilingual. No translation step, no separate encoder. This is a practical benefit of using an LLM as the text encoder.\n",
        "\n",
        "- **Fast inference.** Sub-second on enterprise GPUs, a few seconds on Colab GPUs. 8 steps instead of DDPM's 50+. The combination of fewer steps (Decoupled-DMD) and a smaller model (single-stream) makes Z-Image Turbo practical for real-time applications.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Architecture Comparison `[Independent]`\n",
        "\n",
        "The lesson concluded with the claim that Z-Image takes the same building blocks as SD3/Flux and recombines them more efficiently. This exercise tests that claim by comparing Z-Image directly against SD3 (or Flux if available).\n",
        "\n",
        "### Your Task\n",
        "\n",
        "1. **Load SD3 Medium** (or reuse from a previous notebook) alongside the already-loaded Z-Image Turbo\n",
        "2. **Compare architectures:** parameter counts (text encoder, denoiser, VAE, total), text encoder setup (triple vs single), transformer block structure (dual-stream vs single-stream), attention configuration\n",
        "3. **Generate the same prompt with both models** and compare output quality\n",
        "4. **Trace the full Z-Image pipeline end-to-end**, annotating each step with the lesson that covered the relevant concept\n",
        "\n",
        "### Hints\n",
        "\n",
        "- To load SD3 alongside Z-Image, you may need to use `enable_model_cpu_offload()` on one or both pipelines to fit in GPU memory\n",
        "- SD3 pipeline: `StableDiffusion3Pipeline.from_pretrained('stabilityai/stable-diffusion-3-medium-diffusers', torch_dtype=torch.float16)`\n",
        "- Z-Image transformer config: `pipe.transformer.config` has `num_layers`, `num_attention_heads`, `attention_head_dim`, `patch_size`\n",
        "- Z-Image uses `FlowMatchEulerDiscreteScheduler` (check `type(pipe.scheduler).__name__`)\n",
        "- Z-Image's VAE reuses the Flux VAE -- check `pipe.vae.config`\n",
        "- For the pipeline trace, follow this structure:\n",
        "  ```\n",
        "  1. Prompt -> Qwen3-4B text encoder [seq_len, d_model]     (this lesson)\n",
        "  2. Text embeddings -> refiner layers                       (this lesson: S3-DiT)\n",
        "  3. Noisy latent -> patchify -> image tokens               (Diffusion Transformers)\n",
        "  4. Concatenate text + image + visual semantic tokens      (this lesson: S3-DiT)\n",
        "  5. N S3-DiT blocks with shared attention + adaLN          (this lesson)\n",
        "  6. Split output -> image tokens                           (this lesson)\n",
        "  7. Unpatchify -> latent                                   (Diffusion Transformers)\n",
        "  8. Flow matching sampling step                            (Flow Matching)\n",
        "  9. Repeat for 8 steps (Decoupled-DMD)                    (this lesson)\n",
        "  10. VAE decode -> pixels                                  (From Pixels to Latents)\n",
        "  ```\n",
        "\n",
        "### What to Print\n",
        "\n",
        "- A side-by-side parameter comparison table\n",
        "- Key architectural differences (text encoders, block structure, conditioning)\n",
        "- Generated images from both models on the same prompt\n",
        "- The full Z-Image pipeline trace with lesson annotations\n",
        "\n",
        "If you cannot load SD3 Medium due to GPU memory constraints, compare against the SD3 numbers from the lesson (or from your previous notebook) instead of loading the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Exercise 4: Architecture Comparison (Independent)\n",
        "# ============================================================\n",
        "#\n",
        "# Compare Z-Image's architecture against SD3 (or Flux).\n",
        "# Trace the full Z-Image pipeline end-to-end.\n",
        "#\n",
        "# Your code here:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Architecture comparison table ---\n",
        "#\n",
        "# Print a side-by-side comparison of Z-Image vs SD3 (or Flux).\n",
        "# Include: text encoders, denoiser architecture, parameter counts,\n",
        "# block structure, conditioning mechanism, training objective.\n",
        "#\n",
        "# Your code here:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Generate the same prompt with both models ---\n",
        "#\n",
        "# If you can load SD3, generate the same prompt with both\n",
        "# and display side-by-side. If not, generate with Z-Image only\n",
        "# and compare against a description of typical SD3 output.\n",
        "#\n",
        "# Your code here:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Full Z-Image pipeline trace ---\n",
        "#\n",
        "# Trace each stage of the Z-Image pipeline, printing shapes\n",
        "# and annotating which lesson covered each concept.\n",
        "#\n",
        "# Your code here:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "The comparison makes concrete what the lesson taught: Z-Image uses the same building blocks as SD3/Flux but combines them more efficiently. Every component traces to a lesson you completed.\n",
        "\n",
        "**Architecture comparison (using Z-Image's actual config + SD3 numbers from the lesson):**\n",
        "\n",
        "```python\n",
        "# ============================================================\n",
        "# Part 1: Architecture comparison table\n",
        "# ============================================================\n",
        "\n",
        "# Z-Image config from the loaded pipeline\n",
        "zi_config = pipe.transformer.config\n",
        "zi_transformer_params = count_parameters(pipe.transformer)\n",
        "zi_encoder_params = count_parameters(pipe.text_encoder)\n",
        "zi_vae_params = count_parameters(pipe.vae)\n",
        "zi_total = zi_transformer_params + zi_encoder_params + zi_vae_params\n",
        "\n",
        "print('Architecture Comparison: Z-Image vs SD3')\n",
        "print('=' * 70)\n",
        "print(f'{\"Component\":<30} {\"SD3 Medium\":<20} {\"Z-Image Turbo\":<20}')\n",
        "print('-' * 70)\n",
        "print(f'{\"Text encoders\":<30} {\"3 (CLIP+OpenCLIP+T5)\":<20} {\"1 (Qwen3-4B)\":<20}')\n",
        "print(f'{\"Text encoder params\":<30} {\"~5.2B\":<20} {f\"~{zi_encoder_params/1e9:.1f}B\":<20}')\n",
        "print(f'{\"Denoiser type\":<30} {\"MMDiT (dual-stream)\":<20} {\"S3-DiT (single-stream)\":<20}')\n",
        "print(f'{\"Denoiser params\":<30} {\"~2B\":<20} {f\"~{zi_transformer_params/1e9:.1f}B\":<20}')\n",
        "print(f'{\"Total params\":<30} {\"~7.2B\":<20} {f\"~{zi_total/1e9:.1f}B\":<20}')\n",
        "print(f'{\"Num layers\":<30} {\"24\":<20} {f\"{getattr(zi_config, \\\"num_layers\\\", \\\"?\\\")!s}\":<20}')\n",
        "print(f'{\"Attention heads\":<30} {\"24\":<20} {f\"{getattr(zi_config, \\\"num_attention_heads\\\", \\\"?\\\")!s}\":<20}')\n",
        "print(f'{\"Block structure\":<30} {\"Separate Q/K/V+FFN\":<20} {\"Shared Q/K/V+FFN\":<20}')\n",
        "print(f'{\"Position encoding\":<30} {\"Learned\":<20} {\"3D Unified RoPE\":<20}')\n",
        "print(f'{\"Training objective\":<30} {\"Flow matching\":<20} {\"Flow matching\":<20}')\n",
        "print(f'{\"Turbo steps\":<30} {\"N/A\":<20} {\"8 steps\":<20}')\n",
        "print(f'{\"Guidance (Turbo)\":<30} {\"N/A\":<20} {\"0.0 (baked in)\":<20}')\n",
        "print()\n",
        "print('Key differences:')\n",
        "print('  1. Text encoding: 3 specialized encoders -> 1 LLM')\n",
        "print('  2. Block structure: dual-stream (per-modality Q/K/V+FFN) -> single-stream (shared)')\n",
        "print('  3. Position: learned embeddings -> 3D Unified RoPE')\n",
        "print('  4. Distillation: none -> Decoupled-DMD + DMDR')\n",
        "print()\n",
        "print('What is PRESERVED:')\n",
        "print('  - Latent space generation (VAE encode/decode)')\n",
        "print('  - Iterative denoising loop')\n",
        "print('  - Flow matching training objective')\n",
        "print('  - Transformer-based denoiser with patchify')\n",
        "print('  - Pipeline structure: text encode -> denoise -> VAE decode')\n",
        "```\n",
        "\n",
        "**Generate with both (memory-efficient approach):**\n",
        "\n",
        "```python\n",
        "# Generate with Z-Image Turbo (already loaded)\n",
        "prompt = 'a red ball to the left of a blue cube on a wooden table'\n",
        "generator = torch.Generator(device='cuda').manual_seed(42)\n",
        "\n",
        "zi_image = pipe(\n",
        "    prompt=prompt,\n",
        "    height=1024,\n",
        "    width=1024,\n",
        "    num_inference_steps=9,\n",
        "    guidance_scale=0.0,\n",
        "    generator=generator,\n",
        ").images[0]\n",
        "\n",
        "# If GPU memory allows, load SD3 as well.\n",
        "# Otherwise, just display the Z-Image result.\n",
        "try:\n",
        "    from diffusers import StableDiffusion3Pipeline\n",
        "    sd3_pipe = StableDiffusion3Pipeline.from_pretrained(\n",
        "        'stabilityai/stable-diffusion-3-medium-diffusers',\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    sd3_pipe.enable_model_cpu_offload()\n",
        "\n",
        "    generator = torch.Generator(device='cpu').manual_seed(42)\n",
        "    sd3_image = sd3_pipe(\n",
        "        prompt=prompt,\n",
        "        height=512, width=512,  # Lower res for memory\n",
        "        num_inference_steps=28,\n",
        "        guidance_scale=7.0,\n",
        "        generator=generator,\n",
        "    ).images[0]\n",
        "\n",
        "    show_image_row(\n",
        "        [sd3_image, zi_image],\n",
        "        ['SD3 Medium\\n(28 steps, ~7.2B params)',\n",
        "         'Z-Image Turbo\\n(8 steps, ~6B params)'],\n",
        "        suptitle=f'\"{prompt}\"',\n",
        "        figsize=(14, 6),\n",
        "    )\n",
        "    free_memory(sd3_pipe)\n",
        "except Exception as e:\n",
        "    print(f'Could not load SD3 (likely GPU memory): {e}')\n",
        "    show_image_row(\n",
        "        [zi_image],\n",
        "        ['Z-Image Turbo\\n(8 steps)'],\n",
        "        suptitle=f'\"{prompt}\"',\n",
        "        figsize=(8, 6),\n",
        "    )\n",
        "```\n",
        "\n",
        "**Full Z-Image pipeline trace:**\n",
        "\n",
        "```python\n",
        "prompt = 'a cat sitting on a beach at sunset'\n",
        "\n",
        "print('Z-Image Pipeline Trace')\n",
        "print('=' * 70)\n",
        "print()\n",
        "\n",
        "# Get config values\n",
        "config = pipe.transformer.config\n",
        "scheduler_name = type(pipe.scheduler).__name__\n",
        "vae_scale = pipe.vae_scale_factor\n",
        "height, width = 1024, 1024\n",
        "latent_h = height // vae_scale\n",
        "latent_w = width // vae_scale\n",
        "latent_c = getattr(config, 'in_channels', 16)\n",
        "patch_size = getattr(config, 'patch_size', 2)\n",
        "n_patches = (latent_h // patch_size) * (latent_w // patch_size)\n",
        "n_layers = getattr(config, 'num_layers', 30)\n",
        "d_model_val = getattr(config, 'joint_attention_dim', None)\n",
        "if d_model_val is None:\n",
        "    n_heads = config.num_attention_heads\n",
        "    head_dim = config.attention_head_dim\n",
        "    d_model_val = n_heads * head_dim\n",
        "\n",
        "print('STAGE 1: TEXT ENCODING')\n",
        "print('-' * 70)\n",
        "print(f'  1. Prompt -> Qwen3-4B text encoder [seq_len, {d_model_val}]')\n",
        "print(f'     One LLM replaces three encoders (CLIP + OpenCLIP + T5-XXL)')\n",
        "print(f'     -> Lesson: Z-Image (this lesson), CLIP (6.3.3), SD3 (7.4.3)')\n",
        "print()\n",
        "\n",
        "print('STAGE 2: MODALITY PRE-PROCESSING (REFINER LAYERS)')\n",
        "print('-' * 70)\n",
        "print(f'  2. Text embeddings -> refiner layers -> aligned text tokens')\n",
        "print(f'     Image patch embeddings -> refiner layers -> aligned image tokens')\n",
        "print(f'     \"Translate once, then speak together\"')\n",
        "print(f'     -> Lesson: Z-Image (this lesson: S3-DiT design)')\n",
        "print()\n",
        "\n",
        "print('STAGE 3: PATCHIFY')\n",
        "print('-' * 70)\n",
        "print(f'  3. Noisy latent [{latent_c}, {latent_h}, {latent_w}] -> patchify -> [{n_patches}, {d_model_val}]')\n",
        "print(f'     Patch size: {patch_size}')\n",
        "print(f'     -> Lesson: Diffusion Transformers (7.4.2)')\n",
        "print()\n",
        "\n",
        "print('STAGE 4: CONCATENATE (UNIFIED SEQUENCE)')\n",
        "print('-' * 70)\n",
        "print(f'  4. Concatenate: text tokens + image tokens + visual semantic tokens')\n",
        "print(f'     All three token types in one unified sequence')\n",
        "print(f'     -> Lesson: Z-Image (this lesson: S3-DiT)')\n",
        "print()\n",
        "\n",
        "print('STAGE 5: S3-DiT BLOCKS')\n",
        "print('-' * 70)\n",
        "print(f'  5. {n_layers} S3-DiT blocks: shared attention + shared FFN + adaLN')\n",
        "print(f'     Single-stream: one set of Q/K/V, one SwiGLU FFN')\n",
        "print(f'     Position: 3D Unified RoPE (temporal + spatial)')\n",
        "print(f'     -> Lesson: Z-Image (this lesson) + DiT (7.4.2) + RoPE (4.2.3)')\n",
        "print()\n",
        "\n",
        "print('STAGE 6-7: SPLIT AND UNPATCHIFY')\n",
        "print('-' * 70)\n",
        "print(f'  6. Split: extract image tokens from unified sequence')\n",
        "print(f'  7. Unpatchify: [{n_patches}, {d_model_val}] -> [{latent_c}, {latent_h}, {latent_w}]')\n",
        "print(f'     -> Lesson: Diffusion Transformers (7.4.2)')\n",
        "print()\n",
        "\n",
        "print('STAGE 8-9: FLOW MATCHING SAMPLING')\n",
        "print('-' * 70)\n",
        "print(f'  8. Flow matching sampling step (scheduler: {scheduler_name})')\n",
        "print(f'  9. Repeat for 8 steps (Decoupled-DMD distillation)')\n",
        "print(f'     -> Lesson: Flow Matching (7.2.2) + Z-Image (this lesson)')\n",
        "print()\n",
        "\n",
        "print('STAGE 10: VAE DECODE')\n",
        "print('-' * 70)\n",
        "print(f' 10. VAE decode: [{latent_c}, {latent_h}, {latent_w}] -> [3, {height}, {width}]')\n",
        "print(f'     Reuses the Flux VAE -- same proven reconstruction quality')\n",
        "print(f'     -> Lesson: From Pixels to Latents (6.3.5)')\n",
        "print()\n",
        "print('=' * 70)\n",
        "print('Every step traces to a lesson you completed.')\n",
        "print('Nothing in this pipeline is unexplained.')\n",
        "print('The code IS your knowledge, implemented.')\n",
        "```\n",
        "\n",
        "**Key observations:**\n",
        "- The pipeline structure is preserved from SD3 to Z-Image: text encode, denoise in latent space, VAE decode. The components inside each stage evolved, but the pipeline did not.\n",
        "- Z-Image's innovations are in simplification (single-stream, one encoder) and post-training (Decoupled-DMD, DMDR), not in new pipeline stages.\n",
        "- Every component traces to a specific lesson: transformers (Series 4), RoPE (4.2.3), DiT/patchify (7.4.2), flow matching (7.2.2), latent diffusion (6.3.5), CLIP and text encoding (6.3.3), and the new concepts from this lesson.\n",
        "- The comparison between Z-Image and SD3 on compositional prompts is particularly revealing: Z-Image's Qwen3-4B text encoder often handles spatial relationships and counting better than SD3's triple encoder setup, despite using fewer parameters.\n",
        "\n",
        "**Common mistakes:**\n",
        "- Not accounting for GPU memory when loading both SD3 and Z-Image. Use `enable_model_cpu_offload()` or compare against lesson numbers instead.\n",
        "- Forgetting that Z-Image Turbo uses `guidance_scale=0.0` while SD3 uses `guidance_scale=7.0`. These are fundamentally different inference configurations.\n",
        "- Conflating parameter count with quality. Z-Image achieves comparable quality with 1/5 the parameters, but the quality comes from training innovation (three-phase curriculum, Decoupled-DMD, DMDR), not just architecture.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **\"Translate once, then speak together.\"** Z-Image's S3-DiT uses shared Q/K/V projections and shared FFN across all token types. Lightweight refiner layers handle modality-specific pre-processing upfront, eliminating the ~50% parameter overhead of MMDiT's dual-stream design. You verified this by inspecting the actual transformer blocks.\n",
        "\n",
        "2. **The text encoder trajectory completes.** CLIP (1 encoder) -> dual CLIP (2) -> CLIP + OpenCLIP + T5-XXL (3) -> Qwen3-4B (1 powerful LLM). Z-Image replaces three specialized encoders with one LLM. Simpler pipeline, richer embeddings, native bilingual support.\n",
        "\n",
        "3. **Decoupled-DMD delivers at 8 steps.** Z-Image Turbo achieves high quality at 8 steps with `guidance_scale=0.0`. The distillation baked in the quality boost -- no CFG needed at inference. Sub-second on enterprise GPUs, under 16GB VRAM.\n",
        "\n",
        "4. **Same building blocks, better combination.** Every component of Z-Image traces to a lesson you completed: transformers (Series 4), RoPE (4.2.3), patchify and adaLN (7.4.2), flow matching (7.2.2), latent diffusion (6.3.5). The innovation is in how they are combined: simpler architecture + better training + RL post-training.\n",
        "\n",
        "5. **Simplicity beats complexity.** Z-Image matches 32B Flux with ~6B parameters. Not through architectural novelty but through eliminating unnecessary duplication and investing in better training. The architecture is simpler than MMDiT. The papers are not beyond your understanding -- they ARE your understanding, combined."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}