{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating LLMs\n",
    "\n",
    "In this notebook, you'll apply the evaluation concepts from the lesson by dissecting what benchmarks actually measure, detecting systematic biases in LLM-as-judge evaluation, and designing a multi-layer evaluation strategy that integrates the full module.\n",
    "\n",
    "**What you'll do:**\n",
    "- Examine actual benchmark questions, classify what they test (recognition vs reasoning vs knowledge recall), identify what they do NOT test, and spot contamination signals in score distributions\n",
    "- Use an LLM as a judge to evaluate pairs of model responses, then swap presentation order and test verbose vs concise pairs to measure position bias and verbosity bias empirically\n",
    "- Design a complete evaluation strategy for a specific use case, integrating benchmarks, human evaluation, LLM judges, and red teaming from the full module\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones \u2014 they reveal gaps in your mental model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup \u2014 self-contained for Google Colab\n",
    "!pip install -q openai\n",
    "\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "from openai import OpenAI\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- API Key Setup ---\n",
    "# Option 1: Set your API key as an environment variable (recommended)\n",
    "#   In Colab: go to the key icon in the left sidebar, add OPENAI_API_KEY\n",
    "# Option 2: Paste it directly (less secure, don't commit this)\n",
    "#   os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "# You can also use any OpenAI-compatible API (e.g., local Ollama, Together AI)\n",
    "# by changing the base_url:\n",
    "#   client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Use a small, cheap model for the exercises\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Reproducible results where possible\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def call_llm(system_prompt: str, user_prompt: str, temperature: float = 0.3) -> str:\n",
    "    \"\"\"Call the LLM with a system prompt and user prompt. Returns the response text.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def print_wrapped(text: str, width: int = 80, prefix: str = \"\"):\n",
    "    \"\"\"Print text with word wrapping for readability.\"\"\"\n",
    "    for line in text.split(\"\\n\"):\n",
    "        wrapped = textwrap.fill(line, width=width, initial_indent=prefix, subsequent_indent=prefix)\n",
    "        print(wrapped)\n",
    "\n",
    "\n",
    "# Quick test to verify the API is working\n",
    "test = call_llm(\"You are a helpful assistant.\", \"Say 'API connection successful' and nothing else.\")\n",
    "print(test)\n",
    "print(f\"\\nUsing model: {MODEL}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Benchmark Autopsy (Guided)\n",
    "\n",
    "The lesson introduced the **evaluation stack** \u2014 the layers between actual model capability and the number on the leaderboard. Every benchmark makes design choices (question format, scoring rubric, coverage) that determine what it *actually* measures, which may differ significantly from what its name implies.\n",
    "\n",
    "In this exercise, you'll perform an autopsy on a specific benchmark: **MMLU** (Massive Multitask Language Understanding). You'll examine actual questions, classify what they test, identify what they do NOT test, and look for contamination signals in a simulated score distribution.\n",
    "\n",
    "**Before running each cell, predict:** What does this benchmark question actually test \u2014 recognition, reasoning, knowledge recall, or formatting compliance? What capability does the name \"understanding\" suggest that the question does NOT measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part A: Examine actual benchmark questions ---\n",
    "# These are representative examples of MMLU-style questions across categories.\n",
    "# For each, we'll classify what cognitive skill it ACTUALLY tests.\n",
    "\n",
    "benchmark_questions = [\n",
    "    {\n",
    "        \"category\": \"Abstract Algebra\",\n",
    "        \"question\": \"Find the degree of the extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\",\n",
    "        \"choices\": [\"0\", \"4\", \"2\", \"6\"],\n",
    "        \"correct\": \"B\",\n",
    "        \"analysis\": {\n",
    "            \"what_it_tests\": \"Recognition of a known result. A student who has seen this type of \"\n",
    "                             \"problem can recognize the pattern (sqrt(18) = 3*sqrt(2), so the \"\n",
    "                             \"extension is generated by sqrt(2) and sqrt(3), giving degree 4). \"\n",
    "                             \"The multiple-choice format means the model only needs to SELECT the \"\n",
    "                             \"right answer, not DERIVE it.\",\n",
    "            \"what_name_suggests\": \"Understanding of abstract algebra \u2014 the ability to reason about \"\n",
    "                                  \"field extensions, prove properties, and work through novel problems.\",\n",
    "            \"proxy_gap\": \"A model could get this right by pattern-matching on similar problems in \"\n",
    "                         \"its training data without understanding WHY sqrt(18) simplifies. The \"\n",
    "                         \"multiple-choice format eliminates the need to show work or explain reasoning.\",\n",
    "            \"skill_tested\": \"Knowledge recall + recognition\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"US History\",\n",
    "        \"question\": \"Which of the following was a major cause of the Mexican-American War?\",\n",
    "        \"choices\": [\n",
    "            \"The U.S. annexation of Texas\",\n",
    "            \"The discovery of gold in California\",\n",
    "            \"The abolition of slavery in Mexico\",\n",
    "            \"The construction of the transcontinental railroad\",\n",
    "        ],\n",
    "        \"correct\": \"A\",\n",
    "        \"analysis\": {\n",
    "            \"what_it_tests\": \"Factual recall. This is a straightforward history fact. The model \"\n",
    "                             \"needs to recognize which option matches the well-documented cause. \"\n",
    "                             \"No reasoning or analysis required \u2014 just retrieval of a fact.\",\n",
    "            \"what_name_suggests\": \"Understanding of US history \u2014 the ability to analyze causes, \"\n",
    "                                  \"evaluate perspectives, and draw connections between events.\",\n",
    "            \"proxy_gap\": \"This question is almost certainly in the training data (it is a standard \"\n",
    "                         \"textbook question). A model could answer it through memorization alone. \"\n",
    "                         \"The same model might fail completely when asked to ANALYZE the \"\n",
    "                         \"geopolitical dynamics that led to the war \u2014 a question that actually \"\n",
    "                         \"requires understanding.\",\n",
    "            \"skill_tested\": \"Knowledge recall\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Formal Logic\",\n",
    "        \"question\": \"Select the best translation into predicate logic: \"\n",
    "                    \"'Some monitor combats are not televised.'\",\n",
    "        \"choices\": [\n",
    "            \"(\u2203x)(Mx \u2022 ~Tx)\",\n",
    "            \"(\u2203x)(Mx \u2283 ~Tx)\",\n",
    "            \"(\u2200x)(Mx \u2022 ~Tx)\",\n",
    "            \"~(\u2200x)(Mx \u2283 Tx)\",\n",
    "        ],\n",
    "        \"correct\": \"A\",\n",
    "        \"analysis\": {\n",
    "            \"what_it_tests\": \"Pattern matching on formal logic notation. The model must recognize \"\n",
    "                             \"the standard translation pattern: 'Some X are Y' maps to (\u2203x)(Xx \u2022 Yx). \"\n",
    "                             \"This IS closer to reasoning, but it is still a formulaic translation \"\n",
    "                             \"with a known mapping.\",\n",
    "            \"what_name_suggests\": \"Understanding of formal logic \u2014 the ability to construct proofs, \"\n",
    "                                  \"evaluate arguments, and reason about validity.\",\n",
    "            \"proxy_gap\": \"Translation into predicate logic is a mechanical skill. A model that can \"\n",
    "                         \"translate well might still fail at constructing a proof or identifying a \"\n",
    "                         \"logical fallacy in natural language. The format (selecting from options) \"\n",
    "                         \"further reduces the task to recognition.\",\n",
    "            \"skill_tested\": \"Recognition + formulaic reasoning\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Clinical Knowledge\",\n",
    "        \"question\": \"A patient presents with fatigue, weight gain, and cold intolerance. \"\n",
    "                    \"Which laboratory test is most appropriate?\",\n",
    "        \"choices\": [\n",
    "            \"Complete blood count\",\n",
    "            \"Thyroid function tests\",\n",
    "            \"Liver function tests\",\n",
    "            \"Blood glucose\",\n",
    "        ],\n",
    "        \"correct\": \"B\",\n",
    "        \"analysis\": {\n",
    "            \"what_it_tests\": \"Symptom-to-diagnosis pattern matching. The three symptoms (fatigue, \"\n",
    "                             \"weight gain, cold intolerance) are the classic textbook presentation \"\n",
    "                             \"of hypothyroidism. A model that has seen medical textbooks will \"\n",
    "                             \"recognize this pattern. This is closer to 'clinical reasoning' than \"\n",
    "                             \"pure recall, but the textbook-typical presentation makes it mostly \"\n",
    "                             \"recognition.\",\n",
    "            \"what_name_suggests\": \"Clinical knowledge \u2014 the ability to reason about patient \"\n",
    "                                  \"presentations, consider differential diagnoses, and make \"\n",
    "                                  \"evidence-based recommendations.\",\n",
    "            \"proxy_gap\": \"A real clinical scenario would involve atypical presentations, \"\n",
    "                         \"comorbidities, patient history, and the need to reason about multiple \"\n",
    "                         \"possible diagnoses simultaneously. This question tests whether the model \"\n",
    "                         \"recognizes the TEXTBOOK pattern, not whether it can reason about a \"\n",
    "                         \"complex clinical case.\",\n",
    "            \"skill_tested\": \"Knowledge recall + pattern recognition\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Moral Scenarios\",\n",
    "        \"question\": \"For which of these two scenarios does the main character \"\n",
    "                    \"(who I'll call 'Pat') do something clearly morally wrong? \"\n",
    "                    \"Scenario 1: Pat told the teacher that a classmate was cheating \"\n",
    "                    \"when the classmate was actually cheating. \"\n",
    "                    \"Scenario 2: Pat told the teacher that a classmate was cheating \"\n",
    "                    \"when the classmate was not cheating.\",\n",
    "        \"choices\": [\n",
    "            \"Wrong, Wrong\",\n",
    "            \"Wrong, Not wrong\",\n",
    "            \"Not wrong, Wrong\",\n",
    "            \"Not wrong, Not wrong\",\n",
    "        ],\n",
    "        \"correct\": \"C\",\n",
    "        \"analysis\": {\n",
    "            \"what_it_tests\": \"Formatting compliance + simple moral classification. The 'reasoning' \"\n",
    "                             \"here is trivial (truthful reporting vs false accusation), but the \"\n",
    "                             \"FORMAT is tricky: the model must correctly map two scenarios to a \"\n",
    "                             \"paired answer format. Models often fail this not because of moral \"\n",
    "                             \"reasoning failure but because of FORMAT parsing failure.\",\n",
    "            \"what_name_suggests\": \"Moral reasoning \u2014 the ability to evaluate ethical scenarios, \"\n",
    "                                  \"weigh competing values, and make nuanced judgments.\",\n",
    "            \"proxy_gap\": \"This is perhaps the widest proxy gap of all the examples. The question \"\n",
    "                         \"tests whether the model can parse a specific answer format for a trivial \"\n",
    "                         \"moral distinction. A model that 'understands' morality could fail this \"\n",
    "                         \"question due to formatting, and a model with no moral understanding \"\n",
    "                         \"could pass it through pattern matching on the structure.\",\n",
    "            \"skill_tested\": \"Formatting compliance + trivial classification\",\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# Display the questions with guided analysis\n",
    "print(\"BENCHMARK AUTOPSY: MMLU\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n'Massive Multitask Language Understanding' \u2014 57 subjects,\")\n",
    "print(\"multiple-choice format. The name says 'understanding.'\")\n",
    "print(\"Let's see what it actually tests.\\n\")\n",
    "\n",
    "for i, q in enumerate(benchmark_questions):\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Question {i + 1}: {q['category']}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"\\n{q['question']}\\n\")\n",
    "    for j, choice in enumerate(q['choices']):\n",
    "        letter = chr(65 + j)  # A, B, C, D\n",
    "        marker = \" <<\" if letter == q['correct'] else \"\"\n",
    "        print(f\"  {letter}. {choice}{marker}\")\n",
    "    print(f\"\\n  Skill actually tested: {q['analysis']['skill_tested']}\")\n",
    "    print(f\"\\n  What it tests:\")\n",
    "    print_wrapped(q['analysis']['what_it_tests'], prefix=\"    \")\n",
    "    print(f\"\\n  What the name suggests:\")\n",
    "    print_wrapped(q['analysis']['what_name_suggests'], prefix=\"    \")\n",
    "    print(f\"\\n  The proxy gap:\")\n",
    "    print_wrapped(q['analysis']['proxy_gap'], prefix=\"    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part B: Classify the skill distribution ---\n",
    "# Let's visualize what MMLU actually measures, based on our question analysis.\n",
    "\n",
    "# Skill categories across MMLU (approximate, based on analysis of question types)\n",
    "skill_categories = {\n",
    "    \"Knowledge\\nRecall\": 45,\n",
    "    \"Recognition /\\nPattern Match\": 30,\n",
    "    \"Formulaic\\nReasoning\": 12,\n",
    "    \"Formatting\\nCompliance\": 8,\n",
    "    \"Genuine\\nReasoning\": 5,\n",
    "}\n",
    "\n",
    "labels = list(skill_categories.keys())\n",
    "values = list(skill_categories.values())\n",
    "colors = [\"#6366f1\", \"#8b5cf6\", \"#a78bfa\", \"#f59e0b\", \"#10b981\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: what MMLU actually tests\n",
    "bars = ax1.bar(labels, values, color=colors, edgecolor=\"white\", linewidth=0.5, width=0.6)\n",
    "for bar, val in zip(bars, values):\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,\n",
    "        f\"{val}%\", ha=\"center\", va=\"bottom\", fontsize=11, fontweight=\"bold\", color=\"white\",\n",
    "    )\n",
    "ax1.set_ylabel(\"Approximate % of Questions\", fontsize=11)\n",
    "ax1.set_title(\"What MMLU Actually Tests\", fontsize=13, fontweight=\"bold\")\n",
    "ax1.set_ylim(0, 55)\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1.spines[\"right\"].set_visible(False)\n",
    "ax1.tick_params(axis='x', labelsize=9)\n",
    "\n",
    "# Right: what the name \"understanding\" suggests\n",
    "suggested_skills = {\n",
    "    \"Knowledge\\nRecall\": 10,\n",
    "    \"Recognition /\\nPattern Match\": 10,\n",
    "    \"Formulaic\\nReasoning\": 15,\n",
    "    \"Novel\\nReasoning\": 35,\n",
    "    \"Explanation /\\nSynthesis\": 30,\n",
    "}\n",
    "s_labels = list(suggested_skills.keys())\n",
    "s_values = list(suggested_skills.values())\n",
    "s_colors = [\"#6366f1\", \"#8b5cf6\", \"#a78bfa\", \"#10b981\", \"#06b6d4\"]\n",
    "\n",
    "bars2 = ax2.bar(s_labels, s_values, color=s_colors, edgecolor=\"white\", linewidth=0.5, width=0.6)\n",
    "for bar, val in zip(bars2, s_values):\n",
    "    ax2.text(\n",
    "        bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,\n",
    "        f\"{val}%\", ha=\"center\", va=\"bottom\", fontsize=11, fontweight=\"bold\", color=\"white\",\n",
    "    )\n",
    "ax2.set_ylabel(\"Approximate % of Questions\", fontsize=11)\n",
    "ax2.set_title('What \"Understanding\" Suggests', fontsize=13, fontweight=\"bold\")\n",
    "ax2.set_ylim(0, 45)\n",
    "ax2.spines[\"top\"].set_visible(False)\n",
    "ax2.spines[\"right\"].set_visible(False)\n",
    "ax2.tick_params(axis='x', labelsize=9)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"The Proxy Gap: What MMLU Measures vs What Its Name Implies\",\n",
    "    fontsize=14, fontweight=\"bold\", y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe proxy gap: MMLU is ~75% recall/recognition, but 'understanding'\")\n",
    "print(\"suggests the majority should be reasoning and synthesis.\")\n",
    "print(\"A model scoring 90% on MMLU has demonstrated recall, not understanding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part C: Spot the contamination signal ---\n",
    "# A model's scores across MMLU categories. Some are suspiciously high.\n",
    "#\n",
    "# Before running, predict: if a model has contamination for SOME\n",
    "# categories but not others, what would the score distribution look like?\n",
    "# Would scores be uniformly high, uniformly medium, or uneven?\n",
    "\n",
    "# Simulated model scores across 10 MMLU categories\n",
    "# Some categories have their questions widely available online (forums,\n",
    "# study guides, solutions manuals). Others have less public exposure.\n",
    "\n",
    "categories = [\n",
    "    \"Abstract\\nAlgebra\",\n",
    "    \"US\\nHistory\",\n",
    "    \"Clinical\\nKnowledge\",\n",
    "    \"Formal\\nLogic\",\n",
    "    \"Moral\\nScenarios\",\n",
    "    \"Econometrics\",\n",
    "    \"Computer\\nSecurity\",\n",
    "    \"Virology\",\n",
    "    \"Philosophy\",\n",
    "    \"Electrical\\nEngineering\",\n",
    "]\n",
    "\n",
    "# Scores: notice the pattern\n",
    "scores = [72, 94, 91, 68, 58, 71, 93, 70, 92, 69]\n",
    "\n",
    "# Which categories have high public availability of Q&A content?\n",
    "high_availability = [False, True, True, False, False, False, True, False, True, False]\n",
    "\n",
    "colors_bars = [\"#ef4444\" if ha else \"#6366f1\" for ha in high_availability]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "bars = ax.bar(categories, scores, color=colors_bars, edgecolor=\"white\", linewidth=0.5, width=0.7)\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,\n",
    "        f\"{score}%\", ha=\"center\", va=\"bottom\", fontsize=10, fontweight=\"bold\", color=\"white\",\n",
    "    )\n",
    "\n",
    "# Average lines\n",
    "avg_high = np.mean([s for s, ha in zip(scores, high_availability) if ha])\n",
    "avg_low = np.mean([s for s, ha in zip(scores, high_availability) if not ha])\n",
    "\n",
    "ax.axhline(y=avg_high, color=\"#ef4444\", linestyle=\"--\", alpha=0.6, linewidth=1.5)\n",
    "ax.text(9.8, avg_high + 1, f\"Avg (high availability): {avg_high:.0f}%\",\n",
    "        ha=\"right\", fontsize=9, color=\"#ef4444\")\n",
    "ax.axhline(y=avg_low, color=\"#6366f1\", linestyle=\"--\", alpha=0.6, linewidth=1.5)\n",
    "ax.text(9.8, avg_low + 1, f\"Avg (low availability): {avg_low:.0f}%\",\n",
    "        ha=\"right\", fontsize=9, color=\"#6366f1\")\n",
    "\n",
    "ax.set_ylabel(\"Score (%)\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"MMLU Scores by Category: Spot the Contamination Signal\",\n",
    "    fontsize=13, fontweight=\"bold\",\n",
    ")\n",
    "ax.set_ylim(40, 100)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.tick_params(axis='x', labelsize=9)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"#ef4444\", label=\"High public Q&A availability\"),\n",
    "    Patch(facecolor=\"#6366f1\", label=\"Low public Q&A availability\"),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc=\"lower right\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "gap = avg_high - avg_low\n",
    "print(f\"\\nGap between high-availability and low-availability categories: {gap:.0f} points\")\n",
    "print(f\"\\nThis is the forensic evidence of contamination. Both sets of\")\n",
    "print(f\"categories are similar difficulty \u2014 the {gap:.0f}-point gap is the\")\n",
    "print(f\"contamination signal. The model scores higher on categories where\")\n",
    "print(f\"Q&A content is widely available in web crawls.\")\n",
    "print(f\"\\nThe score is real (the model got the answers right), but the\")\n",
    "print(f\"capability is inflated (the model memorized rather than reasoned).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part D: What's NOT measured? ---\n",
    "# MMLU covers 57 subjects. What dimensions of model quality are entirely absent?\n",
    "\n",
    "print(\"WHAT MMLU DOES NOT MEASURE\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "unmeasured = [\n",
    "    {\n",
    "        \"dimension\": \"Open-ended generation quality\",\n",
    "        \"why_it_matters\": \"Users interact with LLMs through open-ended prompts, not \"\n",
    "                          \"multiple-choice questions. A model that excels at selecting \"\n",
    "                          \"answers from options may produce verbose, unfocused, or poorly \"\n",
    "                          \"structured text when generating freely.\",\n",
    "        \"example\": \"A model scores 90% on MMLU but produces rambling, hedging responses \"\n",
    "                   \"when asked to explain a concept. Users prefer a lower-scoring model \"\n",
    "                   \"that gives clear, concise explanations.\",\n",
    "    },\n",
    "    {\n",
    "        \"dimension\": \"Instruction following\",\n",
    "        \"why_it_matters\": \"Can the model follow complex, multi-step instructions? Can it \"\n",
    "                          \"maintain constraints across a long response? MMLU tests none of this.\",\n",
    "        \"example\": \"A model aces MMLU but ignores format constraints ('respond in JSON'), \"\n",
    "                   \"forgets earlier instructions in long conversations, or fails to \"\n",
    "                   \"maintain a specified persona.\",\n",
    "    },\n",
    "    {\n",
    "        \"dimension\": \"Calibration (knowing what it doesn't know)\",\n",
    "        \"why_it_matters\": \"A good model should express appropriate uncertainty. MMLU's \"\n",
    "                          \"forced-choice format means the model MUST pick an answer \u2014 it \"\n",
    "                          \"cannot say 'I am not sure.' This systematically fails to \"\n",
    "                          \"measure calibration.\",\n",
    "        \"example\": \"A model that confidently picks wrong answers on MMLU could be \"\n",
    "                   \"dangerous in practice. A model that says 'I don't know' when \"\n",
    "                   \"uncertain is more trustworthy but MMLU cannot detect the difference.\",\n",
    "    },\n",
    "    {\n",
    "        \"dimension\": \"Safety and alignment\",\n",
    "        \"why_it_matters\": \"As you saw in Lesson 3 (Red Teaming), a model can pass safety \"\n",
    "                          \"benchmarks while failing on adversarial probing. MMLU does not \"\n",
    "                          \"test safety at all. A high-scoring model on MMLU could be \"\n",
    "                          \"completely unaligned.\",\n",
    "        \"example\": \"The model from the lesson's hook: passed benchmarks, failed on \"\n",
    "                   \"demographic bias, sycophancy, and indirect requests.\",\n",
    "    },\n",
    "    {\n",
    "        \"dimension\": \"Real-world task completion\",\n",
    "        \"why_it_matters\": \"Can the model actually help someone debug code, write an email, \"\n",
    "                          \"summarize a document, or plan a project? MMLU tests academic \"\n",
    "                          \"knowledge in isolated questions, not practical usefulness.\",\n",
    "        \"example\": \"A model scores 92% on MMLU but cannot maintain context across a \"\n",
    "                   \"multi-turn debugging session or misunderstands what the user \"\n",
    "                   \"actually needs.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, item in enumerate(unmeasured):\n",
    "    print(f\"{i + 1}. {item['dimension']}\")\n",
    "    print_wrapped(f\"Why it matters: {item['why_it_matters']}\", prefix=\"   \")\n",
    "    print_wrapped(f\"Example: {item['example']}\", prefix=\"   \")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nKey insight: a benchmark score tells you about performance on THAT\")\n",
    "print(\"BENCHMARK. It does not tell you about any dimension it does not measure.\")\n",
    "print(\"MMLU measures recognition on academic knowledge. Everything else \u2014\")\n",
    "print(\"generation quality, instruction following, calibration, safety, and\")\n",
    "print(\"real-world usefulness \u2014 is unmeasured. The blind spots are enormous.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just practiced:** Reading benchmark results critically. Not cynically \u2014 the goal is not to dismiss all benchmarks, but to understand the gap between what the name promises and what the mechanism measures.\n",
    "\n",
    "The autopsy revealed three layers of the proxy gap:\n",
    "\n",
    "1. **Skill mismatch:** MMLU tests mostly recall and recognition, but \"understanding\" suggests reasoning and synthesis. A 90% score means \"90% recall accuracy,\" not \"90% understanding.\"\n",
    "\n",
    "2. **Contamination signal:** Uneven performance across categories of similar difficulty is forensic evidence. Categories with high public availability of Q&A content score ~22 points higher \u2014 the model memorized the answers, not the reasoning.\n",
    "\n",
    "3. **Coverage blind spots:** Five major dimensions of model quality (generation, instruction following, calibration, safety, task completion) are entirely unmeasured. A model's MMLU score tells you nothing about any of them.\n",
    "\n",
    "This is the benchmark anatomy the lesson described: every layer between \"actual capability\" and \"the number on the leaderboard\" adds distortion. The number is real. The capability it implies is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: LLM-as-Judge Bias Detection (Supported)\n",
    "\n",
    "The lesson introduced four systematic biases in LLM-as-judge evaluation: verbosity bias, confidence bias, self-preference bias, and format sensitivity. In this exercise, you'll measure two of these biases empirically: **position bias** (does the judge prefer whichever response appears first?) and **verbosity bias** (does the judge prefer longer responses regardless of quality?).\n",
    "\n",
    "You'll use an LLM to judge pairs of model responses. Then you'll swap the order and re-judge. Then you'll test with a verbose-vs-concise pair of equal quality. The biases are measurable and predictable \u2014 the evaluator's blind spots become the evaluation's blind spots.\n",
    "\n",
    "Fill in the TODOs below. Each TODO is 1-3 lines.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "For position bias testing: if the judge is unbiased, swapping the order should not change the verdict. If it does, position bias is present.\n",
    "\n",
    "For verbosity bias testing: create two responses to the same question that are equally accurate, but one is 3-4x longer with more detail, filler, and caveats. If the judge rates the verbose response higher, verbosity bias is present.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Shared judge function ---\n",
    "\n",
    "JUDGE_SYSTEM_PROMPT = (\n",
    "    \"You are an expert evaluator of AI assistant responses. \"\n",
    "    \"Given a user question and two responses (Response A and Response B), \"\n",
    "    \"determine which response is better overall in terms of accuracy, \"\n",
    "    \"helpfulness, and clarity.\\n\\n\"\n",
    "    \"Respond with ONLY one of these three options:\\n\"\n",
    "    \"  A  (if Response A is better)\\n\"\n",
    "    \"  B  (if Response B is better)\\n\"\n",
    "    \"  TIE  (if they are roughly equal)\\n\\n\"\n",
    "    \"Respond with a single word: A, B, or TIE.\"\n",
    ")\n",
    "\n",
    "\n",
    "def judge_pair(question: str, response_a: str, response_b: str) -> str:\n",
    "    \"\"\"Ask the LLM judge to compare two responses. Returns 'A', 'B', or 'TIE'.\"\"\"\n",
    "    user_prompt = (\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Response A: {response_a}\\n\\n\"\n",
    "        f\"Response B: {response_b}\"\n",
    "    )\n",
    "    verdict = call_llm(JUDGE_SYSTEM_PROMPT, user_prompt, temperature=0.0)\n",
    "    # Normalize the verdict\n",
    "    v = verdict.strip().upper()\n",
    "    if \"TIE\" in v:\n",
    "        return \"TIE\"\n",
    "    if \"A\" in v and \"B\" not in v:\n",
    "        return \"A\"\n",
    "    if \"B\" in v and \"A\" not in v:\n",
    "        return \"B\"\n",
    "    return v[:3]  # fallback\n",
    "\n",
    "\n",
    "print(\"Judge function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part A: Position bias test ---\n",
    "# We present the same two responses in both orders.\n",
    "# If the judge is unbiased, swapping order should not change the verdict.\n",
    "\n",
    "# 5 question-response pairs. For each, we'll judge both orderings.\n",
    "test_pairs = [\n",
    "    {\n",
    "        \"question\": \"What is the capital of Australia?\",\n",
    "        \"response_1\": \"The capital of Australia is Canberra. It was chosen as a compromise between Sydney and Melbourne when the country federated in 1901, and the city was purpose-built as the capital.\",\n",
    "        \"response_2\": \"Canberra is the capital of Australia. Many people mistakenly think it's Sydney, but Canberra has been the capital since 1927.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Explain what a neural network is in simple terms.\",\n",
    "        \"response_1\": \"A neural network is a computer system inspired by the human brain. It consists of layers of interconnected nodes that process information. Data flows through these layers, with each layer learning to detect different patterns. Through training on examples, the network adjusts its connections to get better at the task.\",\n",
    "        \"response_2\": \"A neural network is a type of machine learning model that learns patterns from data. Think of it like a series of filters: raw data goes in one end, each layer extracts increasingly complex features, and a prediction comes out the other end. You train it by showing it examples and adjusting the filters when it gets things wrong.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the benefits of regular exercise?\",\n",
    "        \"response_1\": \"Regular exercise improves cardiovascular health, strengthens muscles and bones, boosts mood through endorphin release, helps manage weight, improves sleep quality, and reduces risk of chronic diseases like diabetes and heart disease.\",\n",
    "        \"response_2\": \"Exercise benefits include better heart health, stronger muscles, improved mental health (it reduces anxiety and depression), better sleep, weight management, and a lower risk of conditions like type 2 diabetes and certain cancers.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does photosynthesis work?\",\n",
    "        \"response_1\": \"Photosynthesis converts sunlight, water, and carbon dioxide into glucose and oxygen. Light energy is captured by chlorophyll in the leaves, which drives a series of chemical reactions: water molecules are split, CO2 is fixed into organic molecules, and glucose is produced as stored energy.\",\n",
    "        \"response_2\": \"Plants use photosynthesis to make their own food. They absorb sunlight through chlorophyll in their leaves, take in CO2 from the air and water from the soil, then use the sun's energy to convert these into glucose (sugar) for energy and release oxygen as a byproduct.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the difference between a virus and a bacterium?\",\n",
    "        \"response_1\": \"Bacteria are single-celled living organisms that can reproduce on their own. Viruses are much smaller, not considered fully alive, and need a host cell to reproduce. Bacteria can be treated with antibiotics; viruses cannot. Both can cause disease, but through different mechanisms.\",\n",
    "        \"response_2\": \"The key differences: bacteria are living cells that reproduce independently, while viruses are non-living particles that hijack host cells to replicate. Size-wise, viruses are much smaller. Treatment-wise, antibiotics work on bacteria but not viruses, which is why you don't take antibiotics for a cold.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"POSITION BIAS TEST\")\n",
    "print(\"=\" * 70)\n",
    "print(\"For each pair, we judge twice: once with R1 as A, once with R1 as B.\")\n",
    "print(\"If the judge is unbiased, the verdicts should be consistent.\\n\")\n",
    "\n",
    "position_results = []\n",
    "\n",
    "for i, pair in enumerate(test_pairs):\n",
    "    # Order 1: Response 1 = A, Response 2 = B\n",
    "    verdict_1 = judge_pair(pair[\"question\"], pair[\"response_1\"], pair[\"response_2\"])\n",
    "\n",
    "    # Order 2: Response 2 = A, Response 1 = B (swapped)\n",
    "    verdict_2 = judge_pair(pair[\"question\"], pair[\"response_2\"], pair[\"response_1\"])\n",
    "\n",
    "    # Check consistency: if verdict_1 = \"A\" (preferring R1), then\n",
    "    # verdict_2 should = \"B\" (still preferring R1, which is now B)\n",
    "    consistent = (\n",
    "        (verdict_1 == \"A\" and verdict_2 == \"B\")\n",
    "        or (verdict_1 == \"B\" and verdict_2 == \"A\")\n",
    "        or (verdict_1 == \"TIE\" and verdict_2 == \"TIE\")\n",
    "    )\n",
    "\n",
    "    position_results.append({\n",
    "        \"question\": pair[\"question\"][:50] + \"...\",\n",
    "        \"order1\": verdict_1,\n",
    "        \"order2\": verdict_2,\n",
    "        \"consistent\": consistent,\n",
    "    })\n",
    "\n",
    "    status = \"Consistent\" if consistent else \"FLIPPED (position bias!)\"\n",
    "    print(f\"  Pair {i + 1}: Order 1 = {verdict_1}, Order 2 = {verdict_2} \u2014 {status}\")\n",
    "\n",
    "flipped = sum(1 for r in position_results if not r[\"consistent\"])\n",
    "print(f\"\\nResults: {flipped}/{len(position_results)} pairs showed position bias.\")\n",
    "if flipped > 0:\n",
    "    print(f\"The judge changed its mind when the order changed \u2014 the verdict\")\n",
    "    print(f\"depends on presentation, not quality.\")\n",
    "if flipped == 0:\n",
    "    print(f\"The judge was consistent across orderings for these pairs.\")\n",
    "    print(f\"Note: 5 pairs is a small sample. Position bias may appear at larger scale.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part B: Verbosity bias test ---\n",
    "# For each question, we create two responses of EQUAL quality:\n",
    "# one concise, one verbose (with extra caveats, examples, and filler).\n",
    "# If the judge rates the verbose one higher, verbosity bias is present.\n",
    "\n",
    "verbosity_pairs = [\n",
    "    {\n",
    "        \"question\": \"What is a Python list comprehension?\",\n",
    "        \"concise\": (\n",
    "            \"A list comprehension is a compact syntax for creating lists. \"\n",
    "            \"Instead of writing a for loop, you write `[expression for item in iterable]`. \"\n",
    "            \"For example, `[x**2 for x in range(5)]` gives `[0, 1, 4, 9, 16]`.\"\n",
    "        ),\n",
    "        \"verbose\": (\n",
    "            \"That's a great question! A list comprehension is one of Python's most \"\n",
    "            \"powerful and elegant features, and it's something that every Python \"\n",
    "            \"developer should definitely understand well. Essentially, a list \"\n",
    "            \"comprehension provides a concise and readable way to create new lists \"\n",
    "            \"by applying an expression to each item in an existing iterable, such as \"\n",
    "            \"a list, range, or other sequence. The general syntax follows the pattern \"\n",
    "            \"`[expression for item in iterable]`, which replaces what would otherwise \"\n",
    "            \"require a multi-line for loop with append statements. For instance, if \"\n",
    "            \"you wanted to create a list of squared numbers, you could write \"\n",
    "            \"`[x**2 for x in range(5)]`, which would elegantly produce `[0, 1, 4, 9, 16]`. \"\n",
    "            \"It's worth noting that list comprehensions can also include conditional \"\n",
    "            \"filtering with an optional `if` clause, making them even more versatile. \"\n",
    "            \"Many experienced Python developers prefer list comprehensions for their \"\n",
    "            \"readability and Pythonic style, though it's important to keep them \"\n",
    "            \"simple enough to remain readable.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Why is the sky blue?\",\n",
    "        \"concise\": (\n",
    "            \"Sunlight contains all colors. When it hits Earth's atmosphere, shorter \"\n",
    "            \"wavelengths (blue) scatter more than longer ones (red) due to Rayleigh \"\n",
    "            \"scattering off air molecules. So you see blue light coming from all \"\n",
    "            \"directions in the sky.\"\n",
    "        ),\n",
    "        \"verbose\": (\n",
    "            \"This is actually a really fascinating question that has intrigued \"\n",
    "            \"scientists and philosophers for centuries! The explanation involves a \"\n",
    "            \"phenomenon known as Rayleigh scattering, which is a fundamental concept \"\n",
    "            \"in atmospheric physics. Here's how it works: sunlight, which appears \"\n",
    "            \"white to us, is actually composed of all the colors of the visible \"\n",
    "            \"spectrum \u2014 from red to violet. When this sunlight enters Earth's \"\n",
    "            \"atmosphere, it encounters countless tiny gas molecules (primarily \"\n",
    "            \"nitrogen and oxygen). Now, here's the key insight: shorter wavelengths \"\n",
    "            \"of light (blue and violet) are scattered much more effectively by these \"\n",
    "            \"molecules than longer wavelengths (red and orange). Specifically, the \"\n",
    "            \"scattering intensity is inversely proportional to the fourth power of \"\n",
    "            \"the wavelength, meaning blue light is scattered roughly 5.5 times more \"\n",
    "            \"than red light. As a result, when you look up at the sky, you see blue \"\n",
    "            \"light that has been scattered from all directions. You might wonder why \"\n",
    "            \"the sky isn't violet (since violet has an even shorter wavelength), and \"\n",
    "            \"that's because our eyes are more sensitive to blue and because some \"\n",
    "            \"violet light is absorbed in the upper atmosphere.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the difference between HTTP and HTTPS?\",\n",
    "        \"concise\": (\n",
    "            \"HTTPS is HTTP with encryption. HTTP sends data in plain text \u2014 anyone \"\n",
    "            \"on the network can read it. HTTPS uses TLS to encrypt the connection, \"\n",
    "            \"so data is private between your browser and the server. Always use HTTPS \"\n",
    "            \"for sensitive data like passwords and payments.\"\n",
    "        ),\n",
    "        \"verbose\": (\n",
    "            \"Great question! Understanding the difference between HTTP and HTTPS is \"\n",
    "            \"really important for anyone interested in web security and development. \"\n",
    "            \"Let me break this down for you comprehensively. HTTP, which stands for \"\n",
    "            \"HyperText Transfer Protocol, is the foundational protocol used for \"\n",
    "            \"transmitting data on the web. However, HTTP has a significant limitation: \"\n",
    "            \"it transmits data in plain text, which means that any data sent between \"\n",
    "            \"your browser and the web server can potentially be intercepted and read \"\n",
    "            \"by anyone with access to the network. This is where HTTPS comes in \u2014 \"\n",
    "            \"the 'S' stands for 'Secure'. HTTPS uses TLS (Transport Layer Security) \"\n",
    "            \"encryption to create a secure, encrypted connection between your browser \"\n",
    "            \"and the server. This means that even if someone intercepts the data, they \"\n",
    "            \"won't be able to read it because it's encrypted. It's absolutely essential \"\n",
    "            \"to use HTTPS whenever you're transmitting sensitive information like \"\n",
    "            \"passwords, credit card numbers, or personal data. Most modern browsers \"\n",
    "            \"now show a warning when you visit an HTTP site, which is a good practice \"\n",
    "            \"that encourages the adoption of HTTPS across the web.\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "# TODO: For each pair, judge twice:\n",
    "#   1. Concise as A, Verbose as B\n",
    "#   2. Verbose as A, Concise as B\n",
    "# Track which response the judge prefers in each ordering.\n",
    "# Determine: does the judge systematically prefer the verbose response?\n",
    "\n",
    "print(\"VERBOSITY BIAS TEST\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Both responses are equally accurate. The verbose version adds filler,\")\n",
    "print(\"caveats, and enthusiastic phrasing but no additional substance.\")\n",
    "print(\"An unbiased judge should rate them as TIE or split evenly.\\n\")\n",
    "\n",
    "verbose_preferred = 0\n",
    "concise_preferred = 0\n",
    "ties = 0\n",
    "\n",
    "for i, pair in enumerate(verbosity_pairs):\n",
    "    # TODO: Judge with concise as A and verbose as B.\n",
    "    # Then judge with verbose as A and concise as B.\n",
    "    # Determine which RESPONSE (concise or verbose) the judge preferred\n",
    "    # in each ordering, accounting for the swap.\n",
    "    # YOUR CODE HERE (4-8 lines)\n",
    "    verdict_cv = judge_pair(pair[\"question\"], pair[\"concise\"], pair[\"verbose\"])\n",
    "    verdict_vc = judge_pair(pair[\"question\"], pair[\"verbose\"], pair[\"concise\"])\n",
    "\n",
    "    # Determine preference for each ordering\n",
    "    # Order 1: A=concise, B=verbose. Verdict \"B\" means verbose preferred.\n",
    "    # Order 2: A=verbose, B=concise. Verdict \"A\" means verbose preferred.\n",
    "    verbose_wins = 0\n",
    "    concise_wins = 0\n",
    "\n",
    "    if verdict_cv == \"B\":\n",
    "        verbose_wins += 1\n",
    "    elif verdict_cv == \"A\":\n",
    "        concise_wins += 1\n",
    "\n",
    "    if verdict_vc == \"A\":\n",
    "        verbose_wins += 1\n",
    "    elif verdict_vc == \"B\":\n",
    "        concise_wins += 1\n",
    "\n",
    "    if verbose_wins > concise_wins:\n",
    "        verbose_preferred += 1\n",
    "        result = \"VERBOSE preferred\"\n",
    "    elif concise_wins > verbose_wins:\n",
    "        concise_preferred += 1\n",
    "        result = \"Concise preferred\"\n",
    "    else:\n",
    "        ties += 1\n",
    "        result = \"Tie / mixed\"\n",
    "\n",
    "    print(f\"  Pair {i + 1} ({pair['question'][:40]}...)\")\n",
    "    print(f\"    Order 1 (A=concise, B=verbose): {verdict_cv}\")\n",
    "    print(f\"    Order 2 (A=verbose, B=concise): {verdict_vc}\")\n",
    "    print(f\"    Result: {result}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Summary: Verbose preferred {verbose_preferred}/{len(verbosity_pairs)}, \"\n",
    "      f\"Concise preferred {concise_preferred}/{len(verbosity_pairs)}, \"\n",
    "      f\"Ties {ties}/{len(verbosity_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part C: Visualize the bias ---\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Position bias chart\n",
    "consistent_count = sum(1 for r in position_results if r[\"consistent\"])\n",
    "flipped_count = sum(1 for r in position_results if not r[\"consistent\"])\n",
    "\n",
    "pos_labels = [\"Consistent\\n(no bias)\", \"Flipped\\n(position bias)\"]\n",
    "pos_values = [consistent_count, flipped_count]\n",
    "pos_colors = [\"#10b981\", \"#ef4444\"]\n",
    "\n",
    "bars1 = ax1.bar(pos_labels, pos_values, color=pos_colors, edgecolor=\"white\", linewidth=0.5, width=0.5)\n",
    "for bar, val in zip(bars1, pos_values):\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,\n",
    "        str(val), ha=\"center\", va=\"bottom\", fontsize=14, fontweight=\"bold\", color=\"white\",\n",
    "    )\n",
    "ax1.set_ylabel(\"Number of Pairs\", fontsize=11)\n",
    "ax1.set_title(\"Position Bias\", fontsize=13, fontweight=\"bold\")\n",
    "ax1.set_ylim(0, max(pos_values) + 2)\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Verbosity bias chart\n",
    "verb_labels = [\"Verbose\\nPreferred\", \"Concise\\nPreferred\", \"Tie /\\nMixed\"]\n",
    "verb_values = [verbose_preferred, concise_preferred, ties]\n",
    "verb_colors = [\"#ef4444\", \"#10b981\", \"#6366f1\"]\n",
    "\n",
    "bars2 = ax2.bar(verb_labels, verb_values, color=verb_colors, edgecolor=\"white\", linewidth=0.5, width=0.5)\n",
    "for bar, val in zip(bars2, verb_values):\n",
    "    ax2.text(\n",
    "        bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,\n",
    "        str(val), ha=\"center\", va=\"bottom\", fontsize=14, fontweight=\"bold\", color=\"white\",\n",
    "    )\n",
    "ax2.set_ylabel(\"Number of Pairs\", fontsize=11)\n",
    "ax2.set_title(\"Verbosity Bias\", fontsize=13, fontweight=\"bold\")\n",
    "ax2.set_ylim(0, max(verb_values) + 2)\n",
    "ax2.spines[\"top\"].set_visible(False)\n",
    "ax2.spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"LLM-as-Judge Systematic Biases\",\n",
    "    fontsize=14, fontweight=\"bold\", y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWhat the visualization shows:\")\n",
    "print(\"- Position bias: the judge's verdict depends on which response\")\n",
    "print(\"  appears first, not on quality.\")\n",
    "print(\"- Verbosity bias: the judge systematically prefers longer responses,\")\n",
    "print(\"  even when the extra length adds no substance.\")\n",
    "print(\"\\nThese biases are the SAME failure modes as reward hacking from\")\n",
    "print(\"Series 4: the reward model (now acting as judge) is fooled by\")\n",
    "print(\"surface features (length, confidence, formatting) rather than\")\n",
    "print(\"evaluating actual quality. The evaluator's blind spots become\")\n",
    "print(\"the evaluation's blind spots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Why the bias detection works:** The key insight is that position bias and verbosity bias are *measurable* properties of the judge, not random noise. By testing with swapped orderings and controlled length differences, you can quantify exactly how much the judge's verdict depends on presentation vs content.\n",
    "\n",
    "The TODO code for Part B:\n",
    "\n",
    "```python\n",
    "verdict_cv = judge_pair(pair[\"question\"], pair[\"concise\"], pair[\"verbose\"])\n",
    "verdict_vc = judge_pair(pair[\"question\"], pair[\"verbose\"], pair[\"concise\"])\n",
    "\n",
    "# Order 1: A=concise, B=verbose. \"B\" means verbose preferred.\n",
    "# Order 2: A=verbose, B=concise. \"A\" means verbose preferred.\n",
    "verbose_wins = 0\n",
    "concise_wins = 0\n",
    "\n",
    "if verdict_cv == \"B\":\n",
    "    verbose_wins += 1\n",
    "elif verdict_cv == \"A\":\n",
    "    concise_wins += 1\n",
    "\n",
    "if verdict_vc == \"A\":\n",
    "    verbose_wins += 1\n",
    "elif verdict_vc == \"B\":\n",
    "    concise_wins += 1\n",
    "```\n",
    "\n",
    "**Why this connects to the lesson:** The verbose responses in this exercise are deliberately designed to mimic reward hacking patterns from Series 4: enthusiastic openers (\"Great question!\"), confident filler (\"really important\", \"absolutely essential\"), and excessive qualification. These are the same surface features that reward models learn to prefer. When an LLM judge exhibits the same biases, it confirms the lesson's core pattern: **the evaluator's limitations become the evaluation's limitations.**\n",
    "\n",
    "**Common findings:**\n",
    "- Position bias: most models show some position bias, preferring the first response (recency effects vary by model)\n",
    "- Verbosity bias: most models systematically prefer the verbose response, even though it adds no new information\n",
    "- Both biases are model-dependent \u2014 GPT-4 shows less verbosity bias than GPT-3.5, but neither is bias-free\n",
    "\n",
    "**Mitigation strategies used in practice:**\n",
    "- Run each comparison twice with swapped order, take the majority vote\n",
    "- Ask the judge to evaluate each response independently, then compare scores\n",
    "- Use rubric-based evaluation instead of direct comparison\n",
    "- Calibrate the judge on known-quality pairs before deployment\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just measured:** Systematic biases in LLM-as-judge evaluation \u2014 empirically, not theoretically.\n",
    "\n",
    "The lesson described four biases: verbosity, confidence, self-preference, and format sensitivity. You measured two of them (position and verbosity) and confirmed they are real and predictable. The judge's verdict depends on *how* a response is presented, not just *what* it says.\n",
    "\n",
    "This is the third time this module has demonstrated the same pattern: human annotators have biases (Constitutional AI), red team models have blind spots (Red Teaming), and now LLM judges have biases (this exercise). The evaluator's limitations always become the evaluation's limitations. No single evaluation source is reliable \u2014 the best approach combines multiple methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Design an Evaluation Strategy (Supported) \u2014 Module Synthesis\n",
    "\n",
    "This exercise integrates the full module. You've spent four lessons building a sophisticated understanding of alignment: how to build it (Constitutional AI, Alignment Landscape), how to break it (Red Teaming), and how to measure it (this lesson). Now you'll put it all together.\n",
    "\n",
    "**The scenario:** You are evaluating a **customer support chatbot** for a mid-size e-commerce company. The chatbot handles order inquiries, returns, product recommendations, and complaint resolution. It processes ~10,000 conversations per day.\n",
    "\n",
    "**Your task:** Design a multi-layer evaluation strategy. For each evaluation layer, specify what it measures, what it misses, and why no single layer is sufficient.\n",
    "\n",
    "Fill in the TODOs below. Each TODO is a text response (no code), structured as evaluation design decisions with reasoning.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Think about the defense-in-depth principle from Lesson 3: just as no single defense layer covers the alignment surface, no single evaluation method captures model quality. Your strategy should have multiple layers, each covering what the others miss.\n",
    "\n",
    "The four evaluation layers from the lesson: benchmarks, human evaluation, LLM-as-judge, and red teaming. For each, think about: what is it good at measuring? What are its blind spots? What does it cost?\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- The Use Case ---\n",
    "\n",
    "print(\"EVALUATION STRATEGY DESIGN\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Use Case: Customer Support Chatbot\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Company: Mid-size e-commerce (clothing and electronics)\")\n",
    "print(\"Volume: ~10,000 conversations per day\")\n",
    "print(\"Tasks: Order inquiries, returns, product recommendations,\")\n",
    "print(\"       complaint resolution, policy questions\")\n",
    "print(\"Users: General public, varying technical literacy\")\n",
    "print(\"Stakes: Customer satisfaction, brand reputation,\")\n",
    "print(\"        legal compliance (refund policies, warranty claims)\")\n",
    "print()\n",
    "print(\"Your job: Design a multi-layer evaluation strategy.\")\n",
    "print(\"For each layer, specify:\")\n",
    "print(\"  1. WHAT to evaluate\")\n",
    "print(\"  2. HOW to evaluate it (which method)\")\n",
    "print(\"  3. WHAT IT MISSES (the blind spots)\")\n",
    "print(\"  4. WHY this layer alone is insufficient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Layer 1: Benchmarks ---\n",
    "# Which existing benchmarks would you use? What would they tell you?\n",
    "# What would they NOT tell you?\n",
    "\n",
    "# TODO: Fill in your evaluation design for the benchmark layer.\n",
    "# Think about: which benchmarks are relevant for a customer support chatbot?\n",
    "# What do they actually measure (remember the proxy gap)?\n",
    "# What dimensions of customer support quality do they miss?\n",
    "\n",
    "benchmark_layer = {\n",
    "    \"benchmarks_to_use\": [\n",
    "        # TODO: List 2-3 benchmarks and what each would tell you.\n",
    "        # Example: (\"BenchmarkName\", \"What it measures\", \"What it misses\")\n",
    "    ],\n",
    "    \"what_this_layer_catches\": \"\",  # TODO: What does this layer tell you?\n",
    "    \"blind_spots\": \"\",  # TODO: What does this layer miss?\n",
    "    \"why_insufficient\": \"\",  # TODO: Why can't you stop here?\n",
    "}\n",
    "\n",
    "print(\"LAYER 1: BENCHMARKS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nBenchmarks to use:\")\n",
    "for b in benchmark_layer[\"benchmarks_to_use\"]:\n",
    "    if isinstance(b, tuple) and len(b) == 3:\n",
    "        print(f\"  - {b[0]}\")\n",
    "        print(f\"    Measures: {b[1]}\")\n",
    "        print(f\"    Misses: {b[2]}\")\n",
    "    else:\n",
    "        print(f\"  - {b}\")\n",
    "print(f\"\\nWhat this catches: {benchmark_layer['what_this_layer_catches']}\")\n",
    "print(f\"Blind spots: {benchmark_layer['blind_spots']}\")\n",
    "print(f\"Why insufficient: {benchmark_layer['why_insufficient']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Layer 2: Human Evaluation ---\n",
    "# What would you have humans evaluate? Who are the evaluators?\n",
    "# What are the tradeoffs (cost, consistency, scale)?\n",
    "\n",
    "# TODO: Fill in your evaluation design for the human evaluation layer.\n",
    "# Think about: who should the evaluators be (domain experts, random users, both)?\n",
    "# What should they evaluate (accuracy, tone, helpfulness)?\n",
    "# How do you handle inter-annotator disagreement?\n",
    "\n",
    "human_layer = {\n",
    "    \"what_to_evaluate\": [],  # TODO: List 2-3 specific dimensions\n",
    "    \"who_evaluates\": \"\",  # TODO: Who are the evaluators and why?\n",
    "    \"evaluation_method\": \"\",  # TODO: Rating scale, pairwise comparison, or other?\n",
    "    \"sample_size_and_cost\": \"\",  # TODO: How many conversations, rough cost estimate\n",
    "    \"blind_spots\": \"\",  # TODO: What does human evaluation miss here?\n",
    "    \"why_insufficient\": \"\",  # TODO: Why can't you stop here?\n",
    "}\n",
    "\n",
    "print(\"LAYER 2: HUMAN EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nWhat to evaluate:\")\n",
    "for item in human_layer[\"what_to_evaluate\"]:\n",
    "    print(f\"  - {item}\")\n",
    "print(f\"\\nWho evaluates: {human_layer['who_evaluates']}\")\n",
    "print(f\"Method: {human_layer['evaluation_method']}\")\n",
    "print(f\"Sample & cost: {human_layer['sample_size_and_cost']}\")\n",
    "print(f\"Blind spots: {human_layer['blind_spots']}\")\n",
    "print(f\"Why insufficient: {human_layer['why_insufficient']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Layer 3: LLM-as-Judge ---\n",
    "# What would you have an LLM judge evaluate? How do you mitigate the biases\n",
    "# you just measured in Exercise 2?\n",
    "\n",
    "# TODO: Fill in your evaluation design for the LLM-as-judge layer.\n",
    "# Think about: what rubric would you give the judge?\n",
    "# How do you mitigate position bias and verbosity bias?\n",
    "# What can the LLM judge evaluate at scale that humans cannot?\n",
    "\n",
    "llm_judge_layer = {\n",
    "    \"what_to_evaluate\": [],  # TODO: List 2-3 specific dimensions\n",
    "    \"rubric_design\": \"\",  # TODO: What criteria does the judge use?\n",
    "    \"bias_mitigation\": [],  # TODO: How do you handle the biases from Exercise 2?\n",
    "    \"scale\": \"\",  # TODO: How many conversations can you evaluate?\n",
    "    \"blind_spots\": \"\",  # TODO: What does LLM-as-judge miss?\n",
    "    \"why_insufficient\": \"\",  # TODO: Why can't you stop here?\n",
    "}\n",
    "\n",
    "print(\"LAYER 3: LLM-AS-JUDGE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nWhat to evaluate:\")\n",
    "for item in llm_judge_layer[\"what_to_evaluate\"]:\n",
    "    print(f\"  - {item}\")\n",
    "print(f\"\\nRubric: {llm_judge_layer['rubric_design']}\")\n",
    "print(f\"Bias mitigation:\")\n",
    "for m in llm_judge_layer[\"bias_mitigation\"]:\n",
    "    print(f\"  - {m}\")\n",
    "print(f\"Scale: {llm_judge_layer['scale']}\")\n",
    "print(f\"Blind spots: {llm_judge_layer['blind_spots']}\")\n",
    "print(f\"Why insufficient: {llm_judge_layer['why_insufficient']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Layer 4: Red Teaming ---\n",
    "# What dimensions of the chatbot require adversarial testing?\n",
    "# What attacks from the Lesson 3 taxonomy are relevant here?\n",
    "\n",
    "# TODO: Fill in your evaluation design for the red teaming layer.\n",
    "# Think about: what could go wrong with a customer support chatbot?\n",
    "# Which attack categories from Lesson 3 are most relevant?\n",
    "# What failures would benchmarks and judges never catch?\n",
    "\n",
    "red_team_layer = {\n",
    "    \"what_to_test\": [],  # TODO: List 3-4 specific failure modes to probe\n",
    "    \"attack_categories\": [],  # TODO: Which taxonomy categories are relevant?\n",
    "    \"automated_vs_manual\": \"\",  # TODO: What requires human creativity vs automated scale?\n",
    "    \"blind_spots\": \"\",  # TODO: What does even red teaming miss?\n",
    "    \"why_insufficient\": \"\",  # TODO: Why can't you stop here?\n",
    "}\n",
    "\n",
    "print(\"LAYER 4: RED TEAMING\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nWhat to test:\")\n",
    "for item in red_team_layer[\"what_to_test\"]:\n",
    "    print(f\"  - {item}\")\n",
    "print(f\"\\nRelevant attack categories:\")\n",
    "for cat in red_team_layer[\"attack_categories\"]:\n",
    "    print(f\"  - {cat}\")\n",
    "print(f\"\\nAutomated vs manual: {red_team_layer['automated_vs_manual']}\")\n",
    "print(f\"Blind spots: {red_team_layer['blind_spots']}\")\n",
    "print(f\"Why insufficient: {red_team_layer['why_insufficient']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Synthesis: The Complete Strategy ---\n",
    "\n",
    "print(\"COMPLETE EVALUATION STRATEGY\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Layer 1 (Benchmarks): Broad baseline capability check\")\n",
    "print(\"  \\u2192 Catches: general capability, known failure modes\")\n",
    "print(\"  \\u2192 Misses: domain-specific quality, real-world usefulness\")\n",
    "print()\n",
    "print(\"Layer 2 (Human Evaluation): Deep quality assessment on a sample\")\n",
    "print(\"  \\u2192 Catches: nuanced quality, tone, accuracy in context\")\n",
    "print(\"  \\u2192 Misses: scale (can only evaluate a tiny fraction), consistency\")\n",
    "print()\n",
    "print(\"Layer 3 (LLM-as-Judge): Scalable quality monitoring\")\n",
    "print(\"  \\u2192 Catches: systematic patterns at scale, regression detection\")\n",
    "print(\"  \\u2192 Misses: novel failure modes, biases the judge shares with the model\")\n",
    "print()\n",
    "print(\"Layer 4 (Red Teaming): Adversarial failure discovery\")\n",
    "print(\"  \\u2192 Catches: edge cases, manipulation, policy violations\")\n",
    "print(\"  \\u2192 Misses: day-to-day quality, user satisfaction on normal queries\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"No single layer is sufficient. Each covers what the others miss.\")\n",
    "print(\"This is defense-in-depth applied to evaluation \\u2014 the same principle\")\n",
    "print(\"from Lesson 3 (Red Teaming), applied to measurement.\")\n",
    "print()\n",
    "print(\"The key insight: evaluation design requires the same tradeoff thinking\")\n",
    "print(\"as alignment technique selection (Lesson 2). There is no single right\")\n",
    "print(\"evaluation \\u2014 there are tradeoffs between cost, coverage, depth, and\")\n",
    "print(\"the specific blind spots of each method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**The reasoning matters more than the specific choices.** There is no single \"right\" evaluation strategy \u2014 the goal is demonstrating that you can identify what each layer catches, what it misses, and why multiple layers are necessary.\n",
    "\n",
    "**Example Layer 1 (Benchmarks):**\n",
    "```python\n",
    "benchmark_layer = {\n",
    "    \"benchmarks_to_use\": [\n",
    "        (\"MT-Bench (customer service subset)\",\n",
    "         \"Multi-turn conversation quality \u2014 can the model maintain context across a conversation\",\n",
    "         \"Tested on generic conversations, not domain-specific customer support scenarios\"),\n",
    "        (\"TruthfulQA\",\n",
    "         \"Whether the model makes false claims \u2014 critical for policy and product information\",\n",
    "         \"Tests general truthfulness, not domain-specific accuracy (shipping times, return policies)\"),\n",
    "        (\"Custom policy accuracy test\",\n",
    "         \"Whether the model correctly states company policies (returns, warranties, shipping)\",\n",
    "         \"Only tests the policies you wrote questions for; does not test edge cases or novel situations\"),\n",
    "    ],\n",
    "    \"what_this_layer_catches\": \"Baseline capability: can the model hold conversations, avoid obvious falsehoods, and recall policy information. Quick and cheap to run on every model update.\",\n",
    "    \"blind_spots\": \"Benchmarks test static questions, not real customer interactions. They miss tone, empathy, de-escalation, and the ability to handle ambiguous or emotional customer messages. The policy test only covers known scenarios.\",\n",
    "    \"why_insufficient\": \"A model could score perfectly on benchmarks by memorizing answers (contamination) while being terrible at handling a frustrated customer whose order was wrong \u2014 a scenario that requires empathy, flexible problem-solving, and accurate policy application simultaneously.\",\n",
    "}\n",
    "```\n",
    "\n",
    "**Example Layer 2 (Human Evaluation):**\n",
    "```python\n",
    "human_layer = {\n",
    "    \"what_to_evaluate\": [\n",
    "        \"Accuracy of policy information (returns, shipping, warranties)\",\n",
    "        \"Tone and empathy (does the chatbot feel like talking to a helpful person or a script?)\",\n",
    "        \"Resolution quality (did the customer's issue actually get resolved?)\",\n",
    "    ],\n",
    "    \"who_evaluates\": \"Two groups: (1) experienced customer support agents who know the policies and common issues, (2) a sample of real customers who rate their own interactions. Agents catch accuracy errors; customers catch tone and satisfaction issues.\",\n",
    "    \"evaluation_method\": \"Pairwise comparison (current model vs previous version or vs human agent) rather than absolute rating \u2014 more reliable, same insight as RLHF preference pairs.\",\n",
    "    \"sample_size_and_cost\": \"200 conversations per evaluation round (2% of daily volume). At ~5 min per evaluation, ~17 hours of evaluator time. Cost: ~$500-1000 per round with skilled evaluators.\",\n",
    "    \"blind_spots\": \"200 conversations cannot cover the full distribution. Evaluators have their own biases (verbosity bias, preference for certain communication styles). Inter-annotator agreement will be low on 'tone' judgments.\",\n",
    "    \"why_insufficient\": \"Evaluating 200 out of 10,000 daily conversations means 98% go unexamined. Rare but serious failures (giving incorrect refund information, escalating a complaint) may not appear in the sample. Cannot run on every model update due to cost.\",\n",
    "}\n",
    "```\n",
    "\n",
    "**Example Layer 3 (LLM-as-Judge):**\n",
    "```python\n",
    "llm_judge_layer = {\n",
    "    \"what_to_evaluate\": [\n",
    "        \"Policy accuracy (did the chatbot state correct return/shipping/warranty information?)\",\n",
    "        \"Response completeness (did the chatbot address all parts of the customer's question?)\",\n",
    "        \"Tone appropriateness (professional, empathetic, not dismissive or overly casual)\",\n",
    "    ],\n",
    "    \"rubric_design\": \"Structured rubric with 5 specific criteria, each scored 1-5 with definitions for each score level. The rubric includes examples of 1-rated and 5-rated responses for calibration. Company policies are included in the judge's context so it can verify accuracy.\",\n",
    "    \"bias_mitigation\": [\n",
    "        \"Run each comparison twice with swapped order to detect position bias (from Exercise 2)\",\n",
    "        \"Score against a rubric rather than direct comparison to reduce verbosity bias\",\n",
    "        \"Calibrate the judge on 50 human-evaluated conversations first to check agreement\",\n",
    "        \"Flag conversations where the judge disagrees with itself across orderings for human review\",\n",
    "    ],\n",
    "    \"scale\": \"Can evaluate all 10,000 daily conversations. Cost: ~$50-100/day with GPT-4o-mini as judge. Enables monitoring every conversation, not just a sample.\",\n",
    "    \"blind_spots\": \"The judge has the same biases as the chatbot (both are LLMs). It may rate sycophantic responses highly. It cannot evaluate whether the customer's problem was ACTUALLY resolved \u2014 only whether the response SEEMS helpful. Subtle policy errors that sound plausible will be missed.\",\n",
    "    \"why_insufficient\": \"LLM-as-judge catches systematic patterns but misses the failures that look like successes. A chatbot that gives a confident, well-formatted wrong answer about a return policy will score high with the LLM judge but cost the company a customer.\",\n",
    "}\n",
    "```\n",
    "\n",
    "**Example Layer 4 (Red Teaming):**\n",
    "```python\n",
    "red_team_layer = {\n",
    "    \"what_to_test\": [\n",
    "        \"Social engineering: can a customer manipulate the chatbot into granting unauthorized refunds or discounts?\",\n",
    "        \"Information leakage: can a customer extract internal pricing, inventory, or policy override information?\",\n",
    "        \"Emotional manipulation: does the chatbot become inappropriately agreeable when the customer is aggressive or emotional (sycophancy)?\",\n",
    "        \"Policy boundary testing: does the chatbot correctly apply return window limits, or can edge cases trick it into extending policies?\",\n",
    "    ],\n",
    "    \"attack_categories\": [\n",
    "        \"Category 2 (reframing): customers reframing unreasonable demands as reasonable requests\",\n",
    "        \"Category 3 (multi-step): building up to an unreasonable request through a series of reasonable questions\",\n",
    "        \"Category 5 (persona): claiming to be a manager, VIP customer, or quality assurance tester\",\n",
    "    ],\n",
    "    \"automated_vs_manual\": \"Automated for scale (generate 1000+ variations of social engineering attempts, test all policy boundaries systematically). Manual for creativity (human red teamers try novel manipulation strategies that an automated generator would not think of). The combination mirrors automated + human red teaming from Lesson 3.\",\n",
    "    \"blind_spots\": \"Red teaming tests adversarial scenarios, not typical usage. A chatbot could pass all red teaming while being unhelpful or confusing for normal customers. Red teaming finds failures but does not measure everyday quality.\",\n",
    "    \"why_insufficient\": \"Red teaming tells you where the model fails under adversarial pressure, but 99%+ of real conversations are not adversarial. A chatbot that is bulletproof against manipulation but gives mediocre answers to 'where is my order?' is not a good chatbot.\",\n",
    "}\n",
    "```\n",
    "\n",
    "**The meta-insight:** Each layer compensates for the others' blind spots. Benchmarks are cheap and broad but shallow. Human evaluation is deep but expensive and narrow. LLM-as-judge scales but inherits biases. Red teaming finds edge cases but misses everyday quality. The combination is defense-in-depth for evaluation \u2014 exactly the principle from Lesson 3.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just designed:** A multi-layer evaluation strategy that integrates every concept from the module. This is the \"Measure\" phase of the Build-Break-Measure arc, and it draws on all four lessons:\n",
    "\n",
    "- **From Constitutional AI (Lesson 1):** The annotation bottleneck \u2014 human evaluation is expensive, inconsistent, and does not scale. LLM-as-judge is the evaluation equivalent of RLAIF.\n",
    "- **From the Alignment Landscape (Lesson 2):** Tradeoff thinking \u2014 there is no single best evaluation method, just as there is no single best alignment technique. Constraints drive choice.\n",
    "- **From Red Teaming (Lesson 3):** Defense-in-depth \u2014 no single evaluation layer is sufficient, just as no single defense layer covers the alignment surface. And the attack taxonomy directly informs what to red-team.\n",
    "- **From this lesson:** The proxy gap, contamination, Goodhart's law, and judge biases \u2014 every evaluation method has blind spots, and the evaluator's limitations become the evaluation's limitations.\n",
    "\n",
    "The recurring pattern across the module: **the challenge shifts, not disappears.** From building alignment to testing it to measuring it \u2014 each step reveals deeper difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Benchmark scores are proxies, not measurements.** MMLU tests recognition on academic knowledge, not \"understanding.\" The proxy gap between what a benchmark measures and what its name implies is wide, and the gap widens under contamination and optimization pressure.\n",
    "\n",
    "2. **Contamination is visible in the data.** Uneven performance across categories of similar difficulty is forensic evidence. Categories with high public Q&A availability score significantly higher \u2014 the model memorized answers, not reasoning. This is structural, not accidental: any public benchmark eventually leaks into training data.\n",
    "\n",
    "3. **LLM judges have measurable, predictable biases.** Position bias (verdict depends on presentation order) and verbosity bias (longer = better, regardless of substance) are empirically demonstrable. The evaluator's limitations become the evaluation's limitations \u2014 the same pattern from Constitutional AI and Red Teaming.\n",
    "\n",
    "4. **No single evaluation method is sufficient.** Benchmarks are cheap but shallow. Human evaluation is deep but expensive. LLM-as-judge scales but inherits biases. Red teaming finds edge cases but misses everyday quality. Defense-in-depth from Lesson 3 applies to evaluation: combine multiple methods, each covering what the others miss.\n",
    "\n",
    "5. **Evaluation design requires tradeoff thinking, not benchmark shopping.** Choosing evaluation methods is the same kind of tradeoff reasoning as choosing alignment techniques (Lesson 2). The right strategy depends on your specific use case, constraints, and what you need to measure. \"What are the blind spots?\" is always the right question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}