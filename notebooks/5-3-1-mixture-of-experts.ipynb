{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Mixture of Experts\n\nIn this notebook, you'll build and explore the core mechanics of mixture-of-experts (MoE) layers using toy-scale PyTorch models. No pretrained models or GPUs needed—everything runs in seconds on CPU.\n\n**What you'll do:**\n- Build a router from scratch (a single linear layer + softmax + top-k) and observe how different input vectors route to different experts\n- Build a complete MoE layer with 4 expert FFNs and a router, run a forward pass on a batch of 8 tokens, and compare active parameters to total parameters\n- Simulate expert routing on real tokenized sentences and visualize per-token expert assignments—looking for emergent specialization patterns\n- Design an experiment that trains a toy MoE model with and without an auxiliary load-balancing loss, tracking expert utilization to observe router collapse\n\n**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones—they reveal gaps in your mental model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup—self-contained for Google Colab\n# No extra pip installs needed—torch and matplotlib are in Colab by default.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport numpy as np\n\n# Reproducible results\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Nice plots\nplt.style.use('dark_background')\nplt.rcParams['figure.figsize'] = [10, 4]\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\nprint('Setup complete.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 1: Build a Router from Scratch (Guided)\n\nThe router is the new component that MoE adds to the transformer block. From the lesson, you know it is a single linear layer followed by softmax and top-k selection—the same dot-product + softmax pattern as attention, but selecting experts instead of tokens.\n\nIn this exercise, you'll build a router for **4 experts** on a toy `d_model=64` hidden state. You'll:\n1. Create the router weight matrix `W_router` of shape `(num_experts, d_model)`\n2. Compute router logits via a matrix multiply: `router_logits = W_router @ hidden_state`\n3. Apply softmax to get a probability distribution over experts\n4. Select the top-2 experts\n5. Visualize how 5 different input vectors route to different experts\n\n**Before running, predict:**\n- Will all 5 input vectors route to the same 2 experts, or different ones?\n- How confident will the router be? Will the top-2 probabilities be close (e.g., 0.30 and 0.28) or will one dominate (e.g., 0.80 and 0.10)?\n- The router is initialized with random weights. Does it still produce *different* routing decisions for different inputs? Why?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- The entire router in ~10 lines ---\n",
    "# Compare to attention: same dot-product + softmax pattern.\n",
    "\n",
    "d_model = 64\n",
    "num_experts = 4\n",
    "k = 2  # top-k: how many experts to activate per token\n",
    "\n",
    "# The router is ONE linear layer. No bias.\n",
    "# Each row of W_router is a learned \"expert embedding.\"\n",
    "# The dot product measures relevance of the token to each expert.\n",
    "W_router = torch.randn(num_experts, d_model) * 0.1  # small init\n",
    "\n",
    "# Generate 5 different input vectors (simulating 5 different token hidden states)\n",
    "inputs = torch.randn(5, d_model)\n",
    "\n",
    "# Step 1: Router logits — one score per expert, for each input\n",
    "# Shape: (5, num_experts) = (5, 4)\n",
    "router_logits = inputs @ W_router.T  # dot product: (5, 64) @ (64, 4) = (5, 4)\n",
    "print(f'Router logits shape: {router_logits.shape}')\n",
    "print(f'Router logits (raw scores):\\n{router_logits}')\n",
    "print()\n",
    "\n",
    "# Step 2: Softmax — convert to probability distribution over experts\n",
    "# Same softmax you've used dozens of times for attention weights.\n",
    "router_probs = F.softmax(router_logits, dim=-1)\n",
    "print(f'Router probabilities (sum to 1 per input):\\n{router_probs}')\n",
    "print(f'Sum per input: {router_probs.sum(dim=-1)}')\n",
    "print()\n",
    "\n",
    "# Step 3: Top-k selection — pick the top-2 experts per input\n",
    "top_k_probs, top_k_indices = torch.topk(router_probs, k=k, dim=-1)\n",
    "print(f'Top-{k} expert indices per input:\\n{top_k_indices}')\n",
    "print(f'Top-{k} probabilities per input:\\n{top_k_probs}')\n",
    "print()\n",
    "\n",
    "# Normalize the top-k probabilities so they sum to 1\n",
    "# (we only use k experts, so renormalize to get valid weights)\n",
    "top_k_weights = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n",
    "print(f'Normalized top-{k} weights (sum to 1):\\n{top_k_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize: router probabilities for all 5 inputs ---\n",
    "# Bar chart showing the probability distribution over 4 experts for each input.\n",
    "# The top-2 selected experts are highlighted.\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(14, 3.5), sharey=True)\n",
    "\n",
    "expert_colors = ['#60a5fa', '#f59e0b', '#34d399', '#a78bfa']  # blue, amber, green, violet\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    probs = router_probs[i].detach().numpy()\n",
    "    selected = set(top_k_indices[i].tolist())\n",
    "\n",
    "    bars = ax.bar(\n",
    "        range(num_experts), probs,\n",
    "        color=[expert_colors[j] if j in selected else '#334155' for j in range(num_experts)],\n",
    "        edgecolor='white', linewidth=0.5,\n",
    "    )\n",
    "\n",
    "    # Annotate probabilities\n",
    "    for j, (bar, p) in enumerate(zip(bars, probs)):\n",
    "        label = f'{p:.2f}'\n",
    "        if j in selected:\n",
    "            label += ' *'\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "                label, ha='center', va='bottom', fontsize=8, color='white')\n",
    "\n",
    "    ax.set_title(f'Input {i+1}', fontsize=10, fontweight='bold')\n",
    "    ax.set_xticks(range(num_experts))\n",
    "    ax.set_xticklabels([f'E{j}' for j in range(num_experts)], fontsize=9)\n",
    "    ax.set_ylim(0, 0.55)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "axes[0].set_ylabel('Router Probability', fontsize=10)\n",
    "fig.suptitle('Router Probabilities for 5 Different Inputs (top-2 selected = colored, * marked)',\n",
    "             fontsize=12, fontweight='bold', y=1.04)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print('\\nRouting summary:')\n",
    "for i in range(5):\n",
    "    experts = top_k_indices[i].tolist()\n",
    "    weights = top_k_weights[i].tolist()\n",
    "    print(f'  Input {i+1} -> Expert {experts[0]} (weight {weights[0]:.2f}), '\n",
    "          f'Expert {experts[1]} (weight {weights[1]:.2f})')\n",
    "\n",
    "# Count how many unique expert pairs we see\n",
    "pairs = [tuple(sorted(top_k_indices[i].tolist())) for i in range(5)]\n",
    "unique_pairs = set(pairs)\n",
    "print(f'\\nUnique expert pairs: {len(unique_pairs)} out of 5 inputs')\n",
    "print(f'Expert pairs: {pairs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**What just happened:** The router—a single matrix multiply + softmax—naturally produces different routing decisions for different inputs. Even with random initialization, different input vectors have different dot products with the expert embeddings (rows of `W_router`), leading to different top-k selections.\n\nThis is the same mechanism as attention scores: `Q @ K^T` produces different attention patterns for different query tokens because the dot products depend on the input. Here, `input @ W_router^T` produces different expert scores for different hidden states.\n\n**Key observation:** The probabilities are relatively flat with random initialization (close to uniform 0.25 each). After training, the router would learn sharper distributions—confidently routing tokens to the most relevant experts. But even before training, the mechanism works: different inputs route differently.\n\n---\n\n## Exercise 2: MoE Forward Pass on a Toy Model (Supported)\n\nNow build a complete MoE layer: 4 expert FFNs (each with `d_model=64`, `d_ff=256`) plus the router from Exercise 1. You'll run a forward pass on a batch of 8 tokens and compare the output shape, active parameters, and total parameters to a single dense FFN.\n\nThe first expert FFN and the router are provided. You'll build the remaining experts and implement the weighted combination of expert outputs.\n\n**Before running, predict:**\n- What will the output shape be? (Same as input? Different?)\n- How many total parameters will the MoE layer have vs a single dense FFN?\n- How many parameters are *active* per token (with top-2 routing)?\n- Will the MoE output be identical to a dense FFN output? Why or why not?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A single expert FFN (provided) ---\n",
    "# Same structure as the FFN in a transformer block:\n",
    "# Linear(d_model -> d_ff) -> GELU -> Linear(d_ff -> d_model)\n",
    "\n",
    "class ExpertFFN(nn.Module):\n",
    "    \"\"\"A single expert: standard two-layer FFN with GELU.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w2(F.gelu(self.w1(x)))\n",
    "\n",
    "\n",
    "# --- The MoE layer (you complete this) ---\n",
    "\n",
    "class MoELayer(nn.Module):\n",
    "    \"\"\"Mixture of Experts layer: N expert FFNs + a router.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, num_experts: int, top_k: int):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "\n",
    "        # Router: single linear layer (no bias), maps d_model -> num_experts\n",
    "        self.router = nn.Linear(d_model, num_experts, bias=False)\n",
    "\n",
    "        # TODO: Create a list of expert FFNs using nn.ModuleList.\n",
    "        # Each expert is an ExpertFFN(d_model, d_ff).\n",
    "        # You need num_experts of them.\n",
    "        # Hint: nn.ModuleList([ExpertFFN(...) for _ in range(...)])\n",
    "        self.experts = None  # Replace this line\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: shape (batch_size, d_model)\n",
    "        Returns: shape (batch_size, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Step 1: Compute router probabilities\n",
    "        router_logits = self.router(x)  # (batch_size, num_experts)\n",
    "        router_probs = F.softmax(router_logits, dim=-1)  # (batch_size, num_experts)\n",
    "\n",
    "        # Step 2: Select top-k experts\n",
    "        top_k_probs, top_k_indices = torch.topk(router_probs, k=self.top_k, dim=-1)\n",
    "        # Normalize top-k weights to sum to 1\n",
    "        top_k_weights = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # Step 3: Run selected experts and combine outputs\n",
    "        # TODO: For each token in the batch, run the top-k selected experts\n",
    "        # and compute the weighted sum of their outputs.\n",
    "        #\n",
    "        # The result should be:\n",
    "        #   output[i] = sum(top_k_weights[i, j] * expert_j(x[i]) for j in selected)\n",
    "        #\n",
    "        # Approach:\n",
    "        #   1. Initialize output as zeros: torch.zeros_like(x)\n",
    "        #   2. Loop over each token i in range(batch_size)\n",
    "        #   3. For each of the top_k selected experts (j in range(self.top_k)):\n",
    "        #      - Get the expert index: expert_idx = top_k_indices[i, j].item()\n",
    "        #      - Get the weight: weight = top_k_weights[i, j]\n",
    "        #      - Run the expert: expert_output = self.experts[expert_idx](x[i:i+1])\n",
    "        #      - Add weighted output: output[i:i+1] += weight * expert_output\n",
    "        #\n",
    "        # Note: x[i:i+1] keeps the batch dimension (shape [1, d_model]) instead\n",
    "        # of x[i] which would be (d_model,). The expert expects a batch dim.\n",
    "\n",
    "        output = None  # Replace this — follow the approach above\n",
    "\n",
    "        return output, router_probs, top_k_indices\n",
    "\n",
    "\n",
    "print('Classes defined. Fill in the TODOs and run the next cell.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\nThe key insight is that each expert is an independent FFN, and we only run the top-k selected ones per token. The weighted combination mirrors attention's weighted sum of values—but over expert outputs instead of token values.\n\n```python\n# In __init__:\nself.experts = nn.ModuleList([ExpertFFN(d_model, d_ff) for _ in range(num_experts)])\n\n# In forward, Step 3:\noutput = torch.zeros_like(x)\nfor i in range(batch_size):\n    for j in range(self.top_k):\n        expert_idx = top_k_indices[i, j].item()\n        weight = top_k_weights[i, j]\n        expert_output = self.experts[expert_idx](x[i:i+1])\n        output[i:i+1] += weight * expert_output\n```\n\nThis is a simple loop implementation. Production MoE uses batched expert execution for efficiency (grouping all tokens assigned to the same expert), but the loop makes the logic clear: for each token, run only the selected experts, weight their outputs, and sum.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "source": "### Helper: Complete MoELayer for Remaining Exercises\n\n**Run the cell below** to get a working `MoELayer` and `ExpertFFN` for Exercises 3 and 4. This ensures the remaining exercises work correctly regardless of whether your Exercise 2 implementation has bugs. If you completed Exercise 2 successfully, this cell just redefines the same classes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# --- Complete ExpertFFN and MoELayer (reference implementation) ---\n# Run this cell to ensure a working MoELayer is available for Exercises 3 and 4.\n\nclass ExpertFFN(nn.Module):\n    \"\"\"A single expert: standard two-layer FFN with GELU.\"\"\"\n\n    def __init__(self, d_model: int, d_ff: int):\n        super().__init__()\n        self.w1 = nn.Linear(d_model, d_ff)\n        self.w2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.w2(F.gelu(self.w1(x)))\n\n\nclass MoELayer(nn.Module):\n    \"\"\"Mixture of Experts layer: N expert FFNs + a router.\"\"\"\n\n    def __init__(self, d_model: int, d_ff: int, num_experts: int, top_k: int):\n        super().__init__()\n        self.num_experts = num_experts\n        self.top_k = top_k\n        self.router = nn.Linear(d_model, num_experts, bias=False)\n        self.experts = nn.ModuleList([ExpertFFN(d_model, d_ff) for _ in range(num_experts)])\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size = x.shape[0]\n        router_logits = self.router(x)\n        router_probs = F.softmax(router_logits, dim=-1)\n        top_k_probs, top_k_indices = torch.topk(router_probs, k=self.top_k, dim=-1)\n        top_k_weights = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n\n        output = torch.zeros_like(x)\n        for i in range(batch_size):\n            for j in range(self.top_k):\n                expert_idx = top_k_indices[i, j].item()\n                weight = top_k_weights[i, j]\n                expert_output = self.experts[expert_idx](x[i:i+1])\n                output[i:i+1] += weight * expert_output\n\n        return output, router_probs, top_k_indices\n\n\nprint('ExpertFFN and MoELayer defined. Ready for remaining exercises.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run the MoE forward pass and compare to a dense FFN ---\n",
    "\n",
    "d_model = 64\n",
    "d_ff = 256\n",
    "num_experts = 4\n",
    "top_k = 2\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create the MoE layer\n",
    "moe = MoELayer(d_model, d_ff, num_experts, top_k)\n",
    "\n",
    "# Create a single dense FFN for comparison\n",
    "dense_ffn = ExpertFFN(d_model, d_ff)\n",
    "\n",
    "# Input: batch of 8 tokens, each with d_model=64 features\n",
    "x = torch.randn(batch_size, d_model)\n",
    "\n",
    "# Forward pass through MoE\n",
    "moe_output, moe_probs, moe_indices = moe(x)\n",
    "\n",
    "# Forward pass through dense FFN\n",
    "dense_output = dense_ffn(x)\n",
    "\n",
    "# --- Compare shapes ---\n",
    "print('=== Shape Comparison ===')\n",
    "print(f'Input shape:       {x.shape}')\n",
    "print(f'MoE output shape:  {moe_output.shape}')\n",
    "print(f'Dense output shape: {dense_output.shape}')\n",
    "print(f'Same shape? {moe_output.shape == dense_output.shape}')\n",
    "print()\n",
    "\n",
    "# --- Compare parameter counts ---\n",
    "def count_parameters(module: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "moe_total_params = count_parameters(moe)\n",
    "dense_params = count_parameters(dense_ffn)\n",
    "router_params = count_parameters(moe.router)\n",
    "single_expert_params = count_parameters(moe.experts[0])\n",
    "\n",
    "# Active params per token: router + top_k experts\n",
    "active_params_per_token = router_params + top_k * single_expert_params\n",
    "\n",
    "print('=== Parameter Comparison ===')\n",
    "print(f'Dense FFN parameters:          {dense_params:>8,}')\n",
    "print(f'Single expert parameters:      {single_expert_params:>8,}')\n",
    "print(f'Router parameters:             {router_params:>8,}')\n",
    "print(f'MoE total parameters:          {moe_total_params:>8,}')\n",
    "print(f'MoE active params per token:   {active_params_per_token:>8,} (router + {top_k} experts)')\n",
    "print()\n",
    "print(f'MoE total / dense:  {moe_total_params / dense_params:.1f}x more total params')\n",
    "print(f'MoE active / dense: {active_params_per_token / dense_params:.2f}x active params per token')\n",
    "print()\n",
    "\n",
    "# --- Routing decisions ---\n",
    "print('=== Routing Decisions ===')\n",
    "for i in range(batch_size):\n",
    "    experts = moe_indices[i].tolist()\n",
    "    probs = moe_probs[i].detach().numpy()\n",
    "    print(f'Token {i}: Expert {experts[0]} & Expert {experts[1]}  '\n",
    "          f'(probs: [{\", \".join(f\"{p:.3f}\" for p in probs)}])')\n",
    "\n",
    "# Count expert usage\n",
    "expert_usage = torch.zeros(num_experts)\n",
    "for i in range(batch_size):\n",
    "    for j in range(top_k):\n",
    "        expert_usage[moe_indices[i, j]] += 1\n",
    "\n",
    "print(f'\\nExpert usage across {batch_size} tokens (top-{top_k}):')\n",
    "for e in range(num_experts):\n",
    "    bar = '#' * int(expert_usage[e].item())\n",
    "    print(f'  Expert {e}: {int(expert_usage[e].item()):>2} tokens  {bar}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**What just happened:** The MoE layer produces the exact same output shape as a dense FFN—`(batch_size, d_model)`. This is critical: the MoE layer is a *drop-in replacement* for the FFN sub-layer. The rest of the transformer block (attention, residual stream, layer norm) is completely unchanged.\n\nThe parameter comparison makes the decoupling concrete:\n- **Total parameters**: MoE has ~4x more than the dense FFN (4 experts, each roughly the same size as the dense FFN, plus the router)\n- **Active parameters per token**: roughly ~2x the dense FFN (only 2 of 4 experts fire, plus the tiny router)\n\nScale this up to Mixtral's dimensions: 8 experts instead of 4, and the total/active ratio becomes ~47B total / ~13B active. More stored knowledge, same per-token compute.\n\n---\n\n## Exercise 3: Visualize Expert Routing on Real Text (Supported)\n\nIn this exercise, you'll simulate how a router assigns experts to real tokens. We'll use a small pretrained model's tokenizer to tokenize sentences, generate embeddings via a randomly initialized embedding layer (a proxy for real hidden states), and route each token through a trained-from-scratch router.\n\nThe goal: see per-token routing in action and look for patterns. Do function words cluster? Do domain words cluster? Is the routing different across sentences?\n\nThe first sentence (\"The mitochondria is the powerhouse of the cell\") is fully set up. You extend the pattern to additional sentences.\n\n**Before running, predict:**\n- Will identical tokens (e.g., \"the\" appearing multiple times) always route to the same expert?\n- Will tokens from the same sentence all route to the same expert?\n- After a few training steps on synthetic data, will you see any grouping patterns emerge?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tokenizer setup ---\n",
    "# We use a simple word-level tokenizer for clarity.\n",
    "# Production models use BPE (byte-pair encoding), but word-level makes\n",
    "# the routing patterns easier to interpret.\n",
    "\n",
    "sentences = [\n",
    "    'The mitochondria is the powerhouse of the cell',\n",
    "    'The stock market crashed yesterday after the announcement',\n",
    "    'def forward self x return self linear x',\n",
    "    'Le chat est sur le tapis',\n",
    "    'The gradient flows backward through the computation graph',\n",
    "]\n",
    "\n",
    "# Build a simple vocabulary from all tokens\n",
    "all_tokens = []\n",
    "for sent in sentences:\n",
    "    all_tokens.extend(sent.lower().split())\n",
    "vocab = sorted(set(all_tokens))\n",
    "token_to_id = {t: i for i, t in enumerate(vocab)}\n",
    "id_to_token = {i: t for t, i in token_to_id.items()}\n",
    "\n",
    "print(f'Vocabulary size: {len(vocab)}')\n",
    "print(f'Tokens: {vocab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build a tiny model: embedding + router ---\n",
    "# We train the router for a few steps on a simple proxy task so that\n",
    "# routing patterns can emerge. Without training, the random router\n",
    "# produces near-uniform distributions (as you saw in Exercise 1).\n",
    "\n",
    "d_model = 32\n",
    "num_experts = 8\n",
    "top_k = 2\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "embedding = nn.Embedding(len(vocab), d_model)\n",
    "router = nn.Linear(d_model, num_experts, bias=False)\n",
    "\n",
    "# Simple proxy task: train the router to differentiate token types.\n",
    "# We create soft target distributions that push function words toward\n",
    "# some experts and content words toward others. This simulates what\n",
    "# emerges naturally during real MoE training.\n",
    "\n",
    "function_words = {'the', 'is', 'of', 'a', 'an', 'and', 'or', 'in', 'on', 'at',\n",
    "                  'le', 'est', 'sur', 'after', 'self', 'return'}\n",
    "\n",
    "def get_target_distribution(token: str) -> torch.Tensor:\n",
    "    \"\"\"Create a soft target distribution that pushes different token types\n",
    "    toward different experts. This simulates emergent specialization.\"\"\"\n",
    "    dist = torch.ones(num_experts) * 0.05  # small uniform baseline\n",
    "    if token in function_words:\n",
    "        dist[0] += 0.5  # function words prefer Expert 0\n",
    "        dist[1] += 0.3  # and Expert 1\n",
    "    elif any(c.isdigit() for c in token):\n",
    "        dist[4] += 0.6  # numbers prefer Expert 4\n",
    "        dist[5] += 0.2\n",
    "    else:\n",
    "        # Content words get distributed across experts 2-7\n",
    "        # based on a hash of the token (simulating learned specialization)\n",
    "        h = hash(token) % 6\n",
    "        dist[2 + h % 6] += 0.5\n",
    "        dist[2 + (h + 1) % 6] += 0.3\n",
    "    return dist / dist.sum()\n",
    "\n",
    "# Train the router for a few steps\n",
    "optimizer = torch.optim.Adam(list(embedding.parameters()) + list(router.parameters()), lr=0.01)\n",
    "\n",
    "for step in range(200):\n",
    "    total_loss = 0.0\n",
    "    for token_str, token_id in token_to_id.items():\n",
    "        x = embedding(torch.tensor([token_id]))\n",
    "        logits = router(x)  # (1, num_experts)\n",
    "        target = get_target_distribution(token_str).unsqueeze(0)  # (1, num_experts)\n",
    "        loss = F.kl_div(F.log_softmax(logits, dim=-1), target, reduction='batchmean')\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f'Step {step:>3}: avg loss = {total_loss / len(token_to_id):.4f}')\n",
    "\n",
    "print('\\nRouter training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize routing for the first sentence (fully set up) ---\n",
    "\n",
    "expert_colors_8 = [\n",
    "    '#60a5fa',  # E0 - blue (function words)\n",
    "    '#38bdf8',  # E1 - light blue (function words)\n",
    "    '#f59e0b',  # E2 - amber\n",
    "    '#34d399',  # E3 - emerald\n",
    "    '#a78bfa',  # E4 - violet\n",
    "    '#fb923c',  # E5 - orange\n",
    "    '#f87171',  # E6 - rose\n",
    "    '#e879f9',  # E7 - fuchsia\n",
    "]\n",
    "\n",
    "\n",
    "def visualize_routing(sentence: str, ax: plt.Axes) -> list[int]:\n",
    "    \"\"\"Route each token in the sentence and visualize expert assignments.\"\"\"\n",
    "    tokens = sentence.lower().split()\n",
    "    token_ids = torch.tensor([token_to_id[t] for t in tokens])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = embedding(token_ids)  # (seq_len, d_model)\n",
    "        logits = router(hidden)  # (seq_len, num_experts)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        _, top_indices = torch.topk(probs, k=1, dim=-1)  # top-1 for visualization clarity\n",
    "\n",
    "    expert_ids = top_indices.squeeze(-1).tolist()\n",
    "\n",
    "    # Draw colored boxes for each token\n",
    "    x_pos = 0\n",
    "    for i, (token, expert_id) in enumerate(zip(tokens, expert_ids)):\n",
    "        box_width = len(token) * 0.12 + 0.15\n",
    "        color = expert_colors_8[expert_id]\n",
    "        rect = mpatches.FancyBboxPatch(\n",
    "            (x_pos, 0.2), box_width, 0.5,\n",
    "            boxstyle='round,pad=0.05',\n",
    "            facecolor=color, edgecolor='white', linewidth=0.8, alpha=0.7\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_pos + box_width / 2, 0.45, token,\n",
    "                ha='center', va='center', fontsize=9, color='white', fontweight='bold')\n",
    "        ax.text(x_pos + box_width / 2, 0.05, f'E{expert_id}',\n",
    "                ha='center', va='center', fontsize=7, color=color)\n",
    "        x_pos += box_width + 0.05\n",
    "\n",
    "    ax.set_xlim(-0.1, x_pos + 0.1)\n",
    "    ax.set_ylim(-0.1, 0.85)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "\n",
    "    return expert_ids\n",
    "\n",
    "\n",
    "# Plot the first sentence\n",
    "fig, ax = plt.subplots(figsize=(14, 1.5))\n",
    "expert_ids = visualize_routing(sentences[0], ax)\n",
    "ax.set_title(f'\"{sentences[0]}\"', fontsize=11, fontweight='bold', pad=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze\n",
    "tokens_0 = sentences[0].lower().split()\n",
    "print('Token -> Expert mapping:')\n",
    "for t, e in zip(tokens_0, expert_ids):\n",
    "    category = 'function' if t in function_words else 'content'\n",
    "    print(f'  \"{t}\" -> Expert {e}  ({category} word)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TODO: Visualize routing for ALL 5 sentences ---\n",
    "# Create a figure with 5 subplots (one per sentence) stacked vertically.\n",
    "# Use the visualize_routing function from above.\n",
    "#\n",
    "# After plotting, analyze the routing patterns:\n",
    "#   1. Do function words consistently route to the same expert(s)?\n",
    "#   2. Do content words from different domains route differently?\n",
    "#   3. Does \"the\" always go to the same expert regardless of sentence?\n",
    "#\n",
    "# Hints:\n",
    "#   fig, axes = plt.subplots(len(sentences), 1, figsize=(14, len(sentences) * 1.8))\n",
    "#   for i, (sent, ax) in enumerate(zip(sentences, axes)):\n",
    "#       expert_ids = visualize_routing(sent, ax)\n",
    "#       ax.set_title(f'Sentence {i+1}: \"{sent}\"', ...)\n",
    "#\n",
    "# Then print a cross-sentence analysis:\n",
    "#   - For each unique token that appears in multiple sentences,\n",
    "#     check if it routes to the same expert.\n",
    "#   - Count how many tokens per expert across all sentences.\n",
    "\n",
    "# YOUR CODE HERE (15-30 lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The key insight is that identical tokens should route consistently (same embedding produces same router scores), while different tokens route based on their learned embeddings. Function words cluster, content words spread across experts.\n",
    "\n",
    "```python\n",
    "fig, axes = plt.subplots(len(sentences), 1, figsize=(14, len(sentences) * 1.8))\n",
    "\n",
    "all_routing = {}  # token -> list of expert ids across sentences\n",
    "expert_counts = torch.zeros(num_experts)\n",
    "\n",
    "for i, (sent, ax) in enumerate(zip(sentences, axes)):\n",
    "    expert_ids = visualize_routing(sent, ax)\n",
    "    ax.set_title(f'Sentence {i+1}: \"{sent}\"', fontsize=10, fontweight='bold', pad=8)\n",
    "\n",
    "    # Track routing per token\n",
    "    for token, eid in zip(sent.lower().split(), expert_ids):\n",
    "        if token not in all_routing:\n",
    "            all_routing[token] = []\n",
    "        all_routing[token].append(eid)\n",
    "        expert_counts[eid] += 1\n",
    "\n",
    "plt.suptitle('Per-Token Expert Routing Across 5 Sentences', fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cross-sentence analysis\n",
    "print('\\n=== Cross-Sentence Routing Consistency ===')\n",
    "for token, expert_ids in sorted(all_routing.items()):\n",
    "    if len(expert_ids) > 1:\n",
    "        consistent = len(set(expert_ids)) == 1\n",
    "        category = 'function' if token in function_words else 'content'\n",
    "        print(f'  \"{token}\" ({category}): Expert(s) {expert_ids} '\n",
    "              f'{\"-- consistent\" if consistent else \"-- VARIES\"}')\n",
    "\n",
    "print(f'\\n=== Expert Utilization (across all tokens) ===')\n",
    "total_tokens = expert_counts.sum().item()\n",
    "for e in range(num_experts):\n",
    "    count = int(expert_counts[e].item())\n",
    "    pct = count / total_tokens * 100\n",
    "    bar = '#' * int(pct / 2)\n",
    "    print(f'  Expert {e}: {count:>3} tokens ({pct:>5.1f}%)  {bar}')\n",
    "```\n",
    "\n",
    "**Expected findings:** Identical tokens (\"the\" appearing in multiple sentences) should always route to the same expert because they produce the same embedding, which produces the same router scores. Function words should cluster toward Experts 0-1, while content words spread across Experts 2-7. This mirrors the per-token routing from the lesson diagram: specialization is emergent and per-token, not per-topic.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**What just happened:** You saw per-token routing in action on real sentences. The key observations from the lesson are visible even in this toy model:\n\n1. **Different tokens in the same sentence route to different experts.** \"The\" and \"mitochondria\" do not share an expert.\n2. **Identical tokens route consistently.** The same token always produces the same embedding, so the router always assigns it the same way.\n3. **Function words cluster.** Words like \"the,\" \"is,\" \"of\" tend to share experts, while content words spread across different experts.\n4. **Specialization is per-token, not per-topic.** There is no single \"biology expert\"—different content words route to different experts based on their learned representations.\n\nIn a real MoE model (like Mixtral), these patterns emerge from training on natural language. Early layers show more syntactic specialization, later layers show more semantic patterns. The boundaries do not map cleanly to human-interpretable categories.\n\n---\n\n## Exercise 4: Router Collapse Experiment (Independent)\n\nThe lesson described router collapse: without load balancing, one expert dominates and others atrophy. The positive feedback loop—popular expert gets more gradient updates, improves, gets even more tokens—causes the model to degenerate to approximately a dense model.\n\n**Your task:** Train a toy MoE model (4 experts, tiny dataset) with and without an auxiliary load-balancing loss. Track expert utilization over training steps. Plot the distribution of tokens across experts at different checkpoints.\n\n**Requirements:**\n1. Create a simple regression or classification task (e.g., learn a function from random inputs to outputs)\n2. Build a small MoE model using the `MoELayer` from Exercise 2\n3. Train it **without** an auxiliary loss—observe collapse\n4. Train it **with** an auxiliary loss that penalizes uneven expert utilization\n5. Plot expert utilization at steps 0, 100, 500, 1000 for both conditions\n\n**The auxiliary loss:** A simple approach is to compute the fraction of tokens sent to each expert in a batch, then penalize deviation from uniform. If `f_i` is the fraction of tokens routed to expert `i`, the load-balancing loss is:\n\n```\nL_balance = num_experts * sum(f_i * P_i)\n```\n\nwhere `P_i` is the mean router probability for expert `i` across the batch. This product is minimized when both `f_i` and `P_i` are uniform.\n\n**No skeleton is provided.** Design the experiment yourself. The solution is in the `<details>` block below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiment here.\n",
    "#\n",
    "# 1. Define a simple toy task (e.g., random regression: x -> y)\n",
    "# 2. Build a model with an MoE layer (reuse MoELayer or build a similar one)\n",
    "# 3. Train WITHOUT auxiliary loss, tracking expert utilization per step\n",
    "# 4. Train WITH auxiliary loss, tracking expert utilization per step\n",
    "# 5. Plot expert utilization over time for both conditions\n",
    "#\n",
    "# Tips:\n",
    "# - Use a small model: d_model=32, d_ff=64, 4 experts, top-2\n",
    "# - Track utilization as: count how many tokens each expert is selected for\n",
    "# - The auxiliary loss weight (alpha) matters: too small and collapse still happens,\n",
    "#   too large and it overrides the task loss. Start with alpha=0.1.\n",
    "# - Train for 1000 steps with batches of 64.\n",
    "# - Record utilization at steps 0, 50, 100, 200, 500, 1000.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n**Design rationale:** We create a simple regression task (learn a random linear mapping). The MoE model has 4 experts with top-2 routing. We train two copies: one with only the task loss, one with task loss + auxiliary load-balancing loss. We track which experts are selected at each step.\n\n```python\nclass ToyMoEModel(nn.Module):\n    \"\"\"Tiny model: input projection -> MoE layer -> output projection.\"\"\"\n\n    def __init__(self, d_in: int, d_model: int, d_ff: int,\n                 d_out: int, num_experts: int, top_k: int):\n        super().__init__()\n        self.input_proj = nn.Linear(d_in, d_model)\n        self.moe = MoELayer(d_model, d_ff, num_experts, top_k)\n        self.output_proj = nn.Linear(d_model, d_out)\n\n    def forward(self, x: torch.Tensor):\n        h = F.gelu(self.input_proj(x))\n        moe_out, router_probs, top_k_indices = self.moe(h)\n        out = self.output_proj(moe_out)\n        return out, router_probs, top_k_indices\n\n\ndef compute_load_balance_loss(\n    router_probs: torch.Tensor,\n    top_k_indices: torch.Tensor,\n    num_experts: int,\n) -> torch.Tensor:\n    \"\"\"Auxiliary loss penalizing uneven expert utilization.\n\n    router_probs: (batch_size, num_experts)—softmax probabilities\n    top_k_indices: (batch_size, top_k)—selected expert indices\n    \"\"\"\n    batch_size = router_probs.shape[0]\n\n    # f_i: fraction of tokens routed to expert i\n    # Create a one-hot mask for selected experts, then average over batch\n    expert_mask = torch.zeros(batch_size, num_experts)\n    expert_mask.scatter_(1, top_k_indices, 1.0)\n    f = expert_mask.mean(dim=0)  # (num_experts,)\n\n    # P_i: mean router probability for expert i\n    P = router_probs.mean(dim=0)  # (num_experts,)\n\n    # Auxiliary loss: num_experts * sum(f_i * P_i)\n    # Minimized when both f and P are uniform (1/num_experts each)\n    return num_experts * (f * P).sum()\n\n\ndef get_expert_utilization(\n    top_k_indices: torch.Tensor,\n    num_experts: int,\n) -> list[float]:\n    \"\"\"Compute fraction of tokens assigned to each expert.\"\"\"\n    counts = torch.zeros(num_experts)\n    for e in range(num_experts):\n        counts[e] = (top_k_indices == e).sum().item()\n    total = counts.sum().item()\n    if total == 0:\n        return [0.0] * num_experts\n    return (counts / total).tolist()\n\n\n# --- Training setup ---\nd_in = 16\nd_model = 32\nd_ff = 64\nd_out = 8\nnum_experts = 4\ntop_k = 2\nbatch_size = 64\nnum_steps = 1000\nalpha = 0.1  # weight for auxiliary loss\n\n# Random regression target\ntorch.manual_seed(42)\ntarget_W = torch.randn(d_in, d_out) * 0.5\n\n# Checkpoints to record\ncheckpoints = [0, 50, 100, 200, 500, 1000]\n\n\ndef train_model(use_aux_loss: bool, seed: int = 42):\n    \"\"\"Train a toy MoE model, return utilization history.\"\"\"\n    torch.manual_seed(seed)\n    model = ToyMoEModel(d_in, d_model, d_ff, d_out, num_experts, top_k)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    utilization_history = {}  # step -> [frac_expert_0, frac_expert_1, ...]\n    losses = []\n\n    for step in range(num_steps + 1):\n        # Generate random batch\n        x = torch.randn(batch_size, d_in)\n        y = x @ target_W  # target output\n\n        # Forward pass\n        pred, router_probs, top_k_indices = model(x)\n\n        # Task loss\n        task_loss = F.mse_loss(pred, y)\n\n        # Auxiliary loss\n        if use_aux_loss:\n            aux_loss = compute_load_balance_loss(router_probs, top_k_indices, num_experts)\n            total_loss = task_loss + alpha * aux_loss\n        else:\n            total_loss = task_loss\n\n        # Record utilization at checkpoints\n        if step in checkpoints:\n            util = get_expert_utilization(top_k_indices, num_experts)\n            utilization_history[step] = util\n\n        # Backward pass\n        if step < num_steps:  # don't optimize after the last step\n            optimizer.zero_grad()\n            total_loss.backward()\n            optimizer.step()\n\n        losses.append(task_loss.item())\n\n    return utilization_history, losses\n\n\n# Train both conditions\nprint('Training WITHOUT auxiliary loss...')\nutil_no_aux, losses_no_aux = train_model(use_aux_loss=False)\nprint('Training WITH auxiliary loss...')\nutil_with_aux, losses_with_aux = train_model(use_aux_loss=True)\nprint('Done.\\n')\n\n# --- Plot expert utilization over training ---\nfig, axes = plt.subplots(2, len(checkpoints), figsize=(16, 6), sharey=True)\n\nexpert_colors_4 = ['#60a5fa', '#f59e0b', '#34d399', '#a78bfa']\n\nfor col, step in enumerate(checkpoints):\n    # No auxiliary loss\n    ax_top = axes[0, col]\n    util = util_no_aux[step]\n    ax_top.bar(range(num_experts), util, color=expert_colors_4,\n               edgecolor='white', linewidth=0.5)\n    ax_top.set_title(f'Step {step}', fontsize=9)\n    ax_top.set_ylim(0, 1.0)\n    ax_top.axhline(y=1/num_experts, color='white', linestyle='--',\n                   alpha=0.3, linewidth=0.8)\n    ax_top.set_xticks(range(num_experts))\n    ax_top.set_xticklabels([f'E{i}' for i in range(num_experts)], fontsize=8)\n    ax_top.spines['top'].set_visible(False)\n    ax_top.spines['right'].set_visible(False)\n    if col == 0:\n        ax_top.set_ylabel('No Aux Loss\\nToken Fraction', fontsize=9)\n\n    # With auxiliary loss\n    ax_bot = axes[1, col]\n    util = util_with_aux[step]\n    ax_bot.bar(range(num_experts), util, color=expert_colors_4,\n               edgecolor='white', linewidth=0.5)\n    ax_bot.set_ylim(0, 1.0)\n    ax_bot.axhline(y=1/num_experts, color='white', linestyle='--',\n                   alpha=0.3, linewidth=0.8)\n    ax_bot.set_xticks(range(num_experts))\n    ax_bot.set_xticklabels([f'E{i}' for i in range(num_experts)], fontsize=8)\n    ax_bot.spines['top'].set_visible(False)\n    ax_bot.spines['right'].set_visible(False)\n    if col == 0:\n        ax_bot.set_ylabel('With Aux Loss\\nToken Fraction', fontsize=9)\n\nfig.suptitle('Expert Utilization Over Training: Collapse vs Balanced',\n             fontsize=13, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\n# --- Summary statistics ---\nprint('\\nFinal expert utilization (step 1000):')\nprint('  Without aux loss:', [f'{u:.2f}' for u in util_no_aux[1000]])\nprint('  With aux loss:   ', [f'{u:.2f}' for u in util_with_aux[1000]])\n\n# Compute imbalance metric: max utilization / uniform\nuniform = 1 / num_experts\nimbalance_no_aux = max(util_no_aux[1000]) / uniform\nimbalance_with_aux = max(util_with_aux[1000]) / uniform\nprint(f'\\n  Imbalance ratio (1.0 = perfectly uniform):')\nprint(f'    Without aux: {imbalance_no_aux:.2f}x')\nprint(f'    With aux:    {imbalance_with_aux:.2f}x')\n\n# Plot training losses\nfig, ax = plt.subplots(figsize=(10, 4))\nax.plot(losses_no_aux, color='#f87171', alpha=0.7, linewidth=0.8, label='No aux loss')\nax.plot(losses_with_aux, color='#34d399', alpha=0.7, linewidth=0.8, label='With aux loss')\nax.set_xlabel('Step', fontsize=11)\nax.set_ylabel('Task Loss (MSE)', fontsize=11)\nax.set_title('Training Loss: Collapse vs Balanced', fontsize=13, fontweight='bold')\nax.legend(fontsize=10)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.tight_layout()\nplt.show()\n\nprint('\\nKey insight: without the auxiliary loss, the router collapses to')\nprint('favoring 1-2 experts. The other experts get fewer tokens, fewer')\nprint('gradient updates, and fall further behind. With the auxiliary loss,')\nprint('utilization stays more uniform, allowing all experts to learn.')\nprint('Load balancing is not just an efficiency trick—it prevents training collapse.')\n```\n\n**Expected findings:**\n- **Without auxiliary loss:** Expert utilization becomes increasingly skewed over training. By step 1000, one or two experts handle the majority of tokens. The others are effectively dead.\n- **With auxiliary loss:** Utilization stays roughly uniform (close to 25% each with 4 experts). The dashed line at 25% represents perfect balance.\n- **Training loss:** Both conditions converge, but the balanced model may converge slightly better because all experts contribute useful computation.\n\nThe collapse is the positive feedback loop from the lesson: popular expert gets more gradient updates, improves, gets even more tokens. The auxiliary loss breaks this loop by gently penalizing imbalance.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **The router is a single linear layer with softmax—the same dot-product + softmax pattern as attention.** One matrix multiplication produces expert scores, softmax converts to probabilities, top-k selects which experts to activate. Simpler than a single attention head.\n\n2. **The MoE layer is a drop-in replacement for the FFN.** Same input shape, same output shape. The rest of the transformer block (attention, residual stream, layer norm) is unchanged. More total parameters, but only top-k experts activate per token.\n\n3. **Expert specialization is emergent and per-token.** Different tokens in the same sentence route to different experts. Function words cluster, content words spread across experts. The patterns are learned, not designed—there is no \"biology expert\" in any clean sense.\n\n4. **Without load balancing, the router collapses.** One expert dominates, others atrophy, and the model degenerates to a dense model with wasted parameters. The auxiliary loss breaks the positive feedback loop by penalizing uneven utilization.\n\n5. **MoE decouples total parameters from per-token compute.** Four experts means ~4x total parameters but only ~2x active parameters (with top-2). Scale to Mixtral: 8 experts, ~47B total, ~13B active. More stored knowledge at the same inference cost per token. The library got bigger, but you still only talk to two librarians."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}