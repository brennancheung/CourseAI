{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Thought Reasoning\n",
    "\n",
    "In this notebook, you'll explore the mechanism behind chain-of-thought prompting—that intermediate reasoning tokens give the model additional forward passes worth of computation, expanding its effective capacity beyond a single forward pass.\n",
    "\n",
    "**What you'll do:**\n",
    "- Compare direct answers vs chain-of-thought on arithmetic problems of varying complexity, and see the accuracy difference empirically\n",
    "- Count intermediate tokens generated during CoT and plot them against problem complexity—each token is an additional forward pass\n",
    "- Manually corrupt intermediate reasoning steps and observe error propagation—the model does not \"catch\" mistakes\n",
    "- Design your own experiment to find the complexity boundary where CoT starts helping\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones—they reveal gaps in your mental model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup — self-contained for Google Colab\n",
    "!pip install -q openai\n",
    "\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "import random\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- API Key Setup ---\n",
    "# Option 1: Set your API key as an environment variable (recommended)\n",
    "#   In Colab: go to the key icon in the left sidebar, add OPENAI_API_KEY\n",
    "# Option 2: Paste it directly (less secure, don't commit this)\n",
    "#   os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "# You can also use any OpenAI-compatible API (e.g., local Ollama, Together AI)\n",
    "# by changing the base_url:\n",
    "#   client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Use a small, cheap model for the exercises\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Reproducible results where possible\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def call_llm(prompt: str, temperature: float = 0.0, max_tokens: int = 300) -> str:\n",
    "    \"\"\"Call the LLM with a single prompt. Returns the response text.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def call_llm_with_usage(prompt: str, temperature: float = 0.0,\n",
    "                        max_tokens: int = 300) -> tuple[str, int]:\n",
    "    \"\"\"Call the LLM and return (response_text, completion_tokens).\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    text = response.choices[0].message.content.strip()\n",
    "    tokens = response.usage.completion_tokens\n",
    "    return text, tokens\n",
    "\n",
    "\n",
    "def print_wrapped(text: str, width: int = 80, prefix: str = \"\"):\n",
    "    \"\"\"Print text with word wrapping for readability.\"\"\"\n",
    "    for line in text.split(\"\\n\"):\n",
    "        wrapped = textwrap.fill(line, width=width, initial_indent=prefix,\n",
    "                                subsequent_indent=prefix)\n",
    "        print(wrapped)\n",
    "\n",
    "\n",
    "def extract_number(text: str) -> int | None:\n",
    "    \"\"\"Extract the last number from a response string.\"\"\"\n",
    "    numbers = re.findall(r'-?\\d[\\d,]*', text.replace(',', ''))\n",
    "    if not numbers:\n",
    "        return None\n",
    "    try:\n",
    "        return int(numbers[-1])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Quick test to verify the API is working\n",
    "test = call_llm(\"Say 'API connection successful' and nothing else.\")\n",
    "print(test)\n",
    "print(f\"\\nUsing model: {MODEL}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Data\n",
    "\n",
    "All exercises use arithmetic and word problems of varying complexity. Problems are defined here so exercises can share them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Arithmetic problems: single-step and multi-step ---\n",
    "# Each entry: (description, expression or problem text, correct answer, num_steps)\n",
    "\n",
    "PROBLEMS = [\n",
    "    # Single-step (should NOT benefit from CoT)\n",
    "    (\"simple addition\", \"What is 23 + 45?\", 68, 1),\n",
    "    (\"simple subtraction\", \"What is 91 - 37?\", 54, 1),\n",
    "    (\"simple multiplication\", \"What is 6 × 9?\", 54, 1),\n",
    "    (\"single-digit multiply\", \"What is 8 × 7?\", 56, 1),\n",
    "\n",
    "    # Multi-step (SHOULD benefit from CoT)\n",
    "    (\"two-digit multiply\", \"What is 17 × 24?\", 408, 3),\n",
    "    (\"two-digit multiply\", \"What is 34 × 56?\", 1904, 3),\n",
    "    (\"three operations\", \"What is (15 × 8) + (12 × 3)?\", 156, 3),\n",
    "    (\"word problem\",\n",
    "     \"A store has 3 shelves with 8 books each. They remove 5 books. How many books are left?\",\n",
    "     19, 2),\n",
    "    (\"word problem\",\n",
    "     \"A farmer has 4 fields, each with 6 rows of corn. Each row has 15 plants. How many plants total?\",\n",
    "     360, 3),\n",
    "    (\"chained operations\", \"What is 13 × 17 + 8 × 9 - 45?\", 248, 4),\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(PROBLEMS)} problems:\")\n",
    "print(f\"  Single-step: {sum(1 for _, _, _, s in PROBLEMS if s == 1)}\")\n",
    "print(f\"  Multi-step:  {sum(1 for _, _, _, s in PROBLEMS if s > 1)}\")\n",
    "print(\"\\nData loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Direct vs CoT Comparison (Guided)\n",
    "\n",
    "The lesson demonstrated that the same model gets different answers with and without chain-of-thought prompting. In this exercise, you'll test that claim systematically across 10 problems of varying complexity.\n",
    "\n",
    "For each problem, you'll run two prompts:\n",
    "- **Direct:** Ask for the answer only, no intermediate steps\n",
    "- **CoT:** Ask to think step by step before answering\n",
    "\n",
    "**Before running, predict:**\n",
    "- Which problems will benefit from CoT? (Hint: the lesson says the criterion is computational complexity—whether the problem exceeds single-forward-pass capacity)\n",
    "- Will CoT ever *hurt* accuracy on the simple problems?\n",
    "- How large will the accuracy gap be on multi-step problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Run all problems with both prompting strategies ---\n",
    "\n",
    "results = []\n",
    "\n",
    "for desc, problem, answer, steps in PROBLEMS:\n",
    "    # Direct prompt: ask for the answer only\n",
    "    direct_prompt = f\"{problem} Answer with ONLY the number, nothing else.\"\n",
    "    direct_response = call_llm(direct_prompt)\n",
    "    direct_answer = extract_number(direct_response)\n",
    "    direct_correct = direct_answer == answer\n",
    "\n",
    "    # CoT prompt: ask to think step by step\n",
    "    cot_prompt = f\"{problem} Let's work through this step by step, then give the final answer.\"\n",
    "    cot_response = call_llm(cot_prompt, max_tokens=500)\n",
    "    cot_answer = extract_number(cot_response)\n",
    "    cot_correct = cot_answer == answer\n",
    "\n",
    "    results.append({\n",
    "        \"desc\": desc,\n",
    "        \"problem\": problem,\n",
    "        \"answer\": answer,\n",
    "        \"steps\": steps,\n",
    "        \"direct_response\": direct_response,\n",
    "        \"direct_answer\": direct_answer,\n",
    "        \"direct_correct\": direct_correct,\n",
    "        \"cot_response\": cot_response,\n",
    "        \"cot_answer\": cot_answer,\n",
    "        \"cot_correct\": cot_correct,\n",
    "    })\n",
    "\n",
    "    symbol_d = \"✓\" if direct_correct else \"✗\"\n",
    "    symbol_c = \"✓\" if cot_correct else \"✗\"\n",
    "    print(f\"{desc:25s} | correct={answer:5d} | direct={str(direct_answer):>6s} {symbol_d} | cot={str(cot_answer):>6s} {symbol_c}\")\n",
    "\n",
    "print(\"\\nAll problems tested.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Summarize and visualize ---\n",
    "\n",
    "single_step = [r for r in results if r[\"steps\"] == 1]\n",
    "multi_step = [r for r in results if r[\"steps\"] > 1]\n",
    "\n",
    "single_direct_acc = sum(r[\"direct_correct\"] for r in single_step) / len(single_step)\n",
    "single_cot_acc = sum(r[\"cot_correct\"] for r in single_step) / len(single_step)\n",
    "multi_direct_acc = sum(r[\"direct_correct\"] for r in multi_step) / len(multi_step)\n",
    "multi_cot_acc = sum(r[\"cot_correct\"] for r in multi_step) / len(multi_step)\n",
    "\n",
    "print(\"ACCURACY SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Single-step problems ({len(single_step)}):\")\n",
    "print(f\"  Direct: {single_direct_acc:.0%}\")\n",
    "print(f\"  CoT:    {single_cot_acc:.0%}\")\n",
    "print(f\"  Improvement: {single_cot_acc - single_direct_acc:+.0%}\")\n",
    "print(f\"\\nMulti-step problems ({len(multi_step)}):\")\n",
    "print(f\"  Direct: {multi_direct_acc:.0%}\")\n",
    "print(f\"  CoT:    {multi_cot_acc:.0%}\")\n",
    "print(f\"  Improvement: {multi_cot_acc - multi_direct_acc:+.0%}\")\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "bars1 = ax.bar(x - width/2, [single_direct_acc * 100, multi_direct_acc * 100],\n",
    "               width, label='Direct', color='#f59e0b', edgecolor='white', linewidth=0.5)\n",
    "bars2 = ax.bar(x + width/2, [single_cot_acc * 100, multi_cot_acc * 100],\n",
    "               width, label='CoT', color='#8b5cf6', edgecolor='white', linewidth=0.5)\n",
    "\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=11)\n",
    "ax.set_title('Direct vs Chain-of-Thought Accuracy', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Single-Step', 'Multi-Step'])\n",
    "ax.set_ylim(0, 110)\n",
    "ax.legend()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 2,\n",
    "            f'{bar.get_height():.0f}%', ha='center', va='bottom', fontsize=10, color='white')\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 2,\n",
    "            f'{bar.get_height():.0f}%', ha='center', va='bottom', fontsize=10, color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: CoT helps on multi-step problems that exceed single-forward-pass capacity.\")\n",
    "print(\"Single-step problems fit within one forward pass — CoT adds nothing (and may waste tokens).\")\n",
    "print(\"The criterion is computational complexity, not difficulty in the human sense.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Inspect a CoT response ---\n",
    "# Let's look at one CoT response in detail to see the intermediate tokens.\n",
    "\n",
    "# Find a multi-step problem where CoT was correct\n",
    "cot_wins = [r for r in results if r[\"cot_correct\"] and r[\"steps\"] > 1]\n",
    "if cot_wins:\n",
    "    example = cot_wins[0]\n",
    "    print(f\"Problem: {example['problem']}\")\n",
    "    print(f\"Correct answer: {example['answer']}\")\n",
    "    print(f\"\\nDirect response: {example['direct_response']}\")\n",
    "    print(f\"Direct correct: {example['direct_correct']}\")\n",
    "    print(f\"\\nCoT response:\")\n",
    "    print_wrapped(example['cot_response'])\n",
    "    print(f\"\\nCoT correct: {example['cot_correct']}\")\n",
    "    print(\"\\nNotice the intermediate results in the CoT response.\")\n",
    "    print(\"Each one was generated token by token — each token is a forward pass.\")\n",
    "    print(\"The intermediate results are IN THE CONTEXT for subsequent forward passes.\")\n",
    "else:\n",
    "    print(\"No multi-step CoT wins found — try adjusting the problems or model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** You tested 10 problems with both direct and CoT prompting. The results should confirm the lesson's claim: CoT improves accuracy on multi-step problems (where the computation exceeds a single forward pass) but adds little on single-step problems (where one pass is enough).\n",
    "\n",
    "The key is not that CoT makes the model \"think harder\" — it's that CoT generates intermediate tokens, and each token triggers another forward pass. More forward passes = more computation. The intermediate results (like \"340\" in the 17 × 24 example) are generated into the context, where subsequent forward passes can attend to them.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 2: Token Counting as Computation Measurement (Supported)\n",
    "\n",
    "If each generated token is a forward pass, then counting tokens is counting computation. In this exercise, you'll measure how many tokens the model generates when solving problems with CoT, and compare that to problem complexity.\n",
    "\n",
    "The API returns `completion_tokens` — the number of tokens the model generated. For CoT, this includes all the intermediate reasoning tokens plus the final answer. For direct prompting, it's just the answer tokens.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Will more complex problems generate more tokens? (The lesson says yes — more complex problems require more intermediate results)\n",
    "- How will the relationship look? Linear? Exponential?\n",
    "- How many more tokens will CoT use compared to direct prompting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Measure tokens for both strategies on all problems ---\n",
    "\n",
    "token_data = []\n",
    "\n",
    "for desc, problem, answer, steps in PROBLEMS:\n",
    "    # Direct: answer only\n",
    "    direct_prompt = f\"{problem} Answer with ONLY the number, nothing else.\"\n",
    "    direct_text, direct_tokens = call_llm_with_usage(direct_prompt)\n",
    "\n",
    "    # CoT: step by step\n",
    "    cot_prompt = f\"{problem} Let's work through this step by step, then give the final answer.\"\n",
    "    cot_text, cot_tokens = call_llm_with_usage(cot_prompt, max_tokens=500)\n",
    "\n",
    "    token_data.append({\n",
    "        \"desc\": desc,\n",
    "        \"steps\": steps,\n",
    "        \"direct_tokens\": direct_tokens,\n",
    "        \"cot_tokens\": cot_tokens,\n",
    "        \"ratio\": cot_tokens / max(direct_tokens, 1),\n",
    "    })\n",
    "\n",
    "    print(f\"{desc:25s} | steps={steps} | direct_tokens={direct_tokens:3d} | cot_tokens={cot_tokens:3d} | ratio={cot_tokens/max(direct_tokens,1):.1f}x\")\n",
    "\n",
    "print(\"\\nToken counting complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Step 2: Plot tokens vs problem complexity ---\n# TODO: Create two plots that visualize the relationship between\n# problem complexity and token count.\n#\n# Left plot (scatter): CoT tokens vs problem steps\n#   - X axis: number of reasoning steps (token_data[i][\"steps\"])\n#   - Y axis: CoT tokens generated (token_data[i][\"cot_tokens\"])\n#   - Color single-step (#f59e0b) vs multi-step (#8b5cf6) differently\n#\n# Right plot (horizontal bar): token ratio per problem\n#   - Y axis: problem descriptions (token_data[i][\"desc\"])\n#   - X axis: ratio of CoT/direct tokens (token_data[i][\"ratio\"])\n#   - Same color scheme as above\n#\n# Useful variables from the previous cell:\n#   token_data[i][\"steps\"]         — number of reasoning steps\n#   token_data[i][\"cot_tokens\"]    — tokens generated with CoT\n#   token_data[i][\"direct_tokens\"] — tokens generated without CoT\n#   token_data[i][\"ratio\"]         — cot_tokens / direct_tokens\n#   token_data[i][\"desc\"]          — problem description string\n#\n# Hint: fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# YOUR CODE HERE (20-30 lines)\n\n"
  },
  {
   "cell_type": "markdown",
   "source": "<details>\n<summary>Solution for Step 2</summary>\n\n```python\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left plot: CoT tokens vs problem steps\nsteps_arr = [d[\"steps\"] for d in token_data]\ncot_tokens_arr = [d[\"cot_tokens\"] for d in token_data]\ndirect_tokens_arr = [d[\"direct_tokens\"] for d in token_data]\ncolors = ['#f59e0b' if s == 1 else '#8b5cf6' for s in steps_arr]\n\nax1.scatter(steps_arr, cot_tokens_arr, c=colors, s=80, edgecolors='white',\n            linewidth=0.5, zorder=3)\nax1.set_xlabel('Problem Complexity (reasoning steps)', fontsize=11)\nax1.set_ylabel('CoT Tokens Generated', fontsize=11)\nax1.set_title('More Complex Problems → More Tokens → More Computation',\n              fontsize=12, fontweight='bold')\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\n\n# Right plot: token ratio (CoT / direct)\nratios = [d[\"ratio\"] for d in token_data]\ndescs = [d[\"desc\"][:15] for d in token_data]\nbar_colors = ['#f59e0b' if d[\"steps\"] == 1 else '#8b5cf6' for d in token_data]\n\nbars = ax2.barh(range(len(ratios)), ratios, color=bar_colors,\n                edgecolor='white', linewidth=0.5)\nax2.set_yticks(range(len(descs)))\nax2.set_yticklabels(descs, fontsize=9)\nax2.set_xlabel('Token Ratio (CoT / Direct)', fontsize=11)\nax2.set_title('CoT Computation Multiplier per Problem', fontsize=12, fontweight='bold')\nax2.spines['top'].set_visible(False)\nax2.spines['right'].set_visible(False)\nax2.axvline(x=1, color='white', linewidth=0.5, alpha=0.3)\n\nfor bar, ratio in zip(bars, ratios):\n    ax2.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height() / 2,\n             f'{ratio:.1f}×', va='center', fontsize=9, color='white')\n\n# Legend\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor='#f59e0b', label='Single-step'),\n                   Patch(facecolor='#8b5cf6', label='Multi-step')]\nax1.legend(handles=legend_elements)\n\nplt.suptitle('Tokens as Computation: Each Token = One Forward Pass',\n             fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\navg_single_ratio = np.mean([d[\"ratio\"] for d in token_data if d[\"steps\"] == 1])\navg_multi_ratio = np.mean([d[\"ratio\"] for d in token_data if d[\"steps\"] > 1])\nprint(f\"\\nAverage token ratio (CoT/Direct):\")\nprint(f\"  Single-step: {avg_single_ratio:.1f}×\")\nprint(f\"  Multi-step:  {avg_multi_ratio:.1f}×\")\nprint(f\"\\nThe model 'allocates' computation proportional to problem difficulty —\")\nprint(f\"not because it decides to, but because more complex problems require\")\nprint(f\"more intermediate results, and each result is generated token by token.\")\n```\n\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** You measured the computation cost of CoT in tokens. More complex problems generated more intermediate tokens — and each token is an additional forward pass through N transformer blocks. The model doesn't \"decide\" to think harder; it generates more intermediate results because the problem requires them, and those results happen to provide the additional computation needed for a correct answer.\n",
    "\n",
    "The token ratio shows the computation multiplier: a multi-step problem might use 10-20× more computation with CoT than without. That's 10-20× more forward passes, each running through the same N transformer blocks. Same model, same weights, dramatically more computation.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 3: Error Propagation Experiment (Supported)\n",
    "\n",
    "The lesson emphasized that the model does not \"catch\" errors in intermediate reasoning steps. If step 2 is wrong, subsequent steps build on the error. In this exercise, you'll test that claim by manually corrupting intermediate steps and observing how errors propagate.\n",
    "\n",
    "The setup: take a multi-step problem, get the model's CoT solution, then replace one intermediate result with a wrong number. Feed the corrupted chain back to the model and ask it to continue. Observe that it continues from the error without catching it.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Will the model notice the error and correct it? Or continue from it?\n",
    "- Will an error early in the chain have a larger impact than an error late in the chain?\n",
    "- What does this tell you about whether the model is \"reasoning\" vs \"generating\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Get a correct CoT solution ---\n",
    "\n",
    "problem = \"What is 17 × 24?\"\n",
    "correct_answer = 408\n",
    "\n",
    "cot_prompt = f\"{problem} Let's work through this step by step.\"\n",
    "correct_cot = call_llm(cot_prompt, max_tokens=300)\n",
    "\n",
    "print(\"CORRECT CoT SOLUTION\")\n",
    "print(\"=\" * 50)\n",
    "print_wrapped(correct_cot)\n",
    "print(f\"\\nExtracted answer: {extract_number(correct_cot)}\")\n",
    "print(f\"Correct answer: {correct_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Corrupt an early intermediate step ---\n",
    "# We'll give the model a partial CoT with a WRONG intermediate result\n",
    "# and ask it to continue.\n",
    "\n",
    "# Early corruption: change 17 × 20 = 340 to 17 × 20 = 350 (off by 10)\n",
    "early_corruption = f\"\"\"What is 17 × 24? Let's work through this step by step.\n",
    "\n",
    "Step 1: Break 24 into 20 + 4\n",
    "Step 2: 17 × 20 = 350\n",
    "\n",
    "Continue from here and give the final answer.\"\"\"\n",
    "\n",
    "early_result = call_llm(early_corruption, max_tokens=300)\n",
    "\n",
    "print(\"EARLY CORRUPTION (17 × 20 = 350 instead of 340)\")\n",
    "print(\"=\" * 50)\n",
    "print_wrapped(early_result)\n",
    "print(f\"\\nExtracted answer: {extract_number(early_result)}\")\n",
    "print(f\"Correct answer: {correct_answer}\")\n",
    "print(f\"\\nDid the model catch the error? {'Yes' if extract_number(early_result) == correct_answer else 'No — it continued from the wrong intermediate result.'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Corrupt a late intermediate step ---\n",
    "# Late corruption: correct first step, wrong second step\n",
    "\n",
    "late_corruption = f\"\"\"What is 17 × 24? Let's work through this step by step.\n",
    "\n",
    "Step 1: Break 24 into 20 + 4\n",
    "Step 2: 17 × 20 = 340\n",
    "Step 3: 17 × 4 = 72\n",
    "\n",
    "Continue from here and give the final answer.\"\"\"\n",
    "\n",
    "late_result = call_llm(late_corruption, max_tokens=300)\n",
    "\n",
    "print(\"LATE CORRUPTION (17 × 4 = 72 instead of 68)\")\n",
    "print(\"=\" * 50)\n",
    "print_wrapped(late_result)\n",
    "print(f\"\\nExtracted answer: {extract_number(late_result)}\")\n",
    "print(f\"Correct answer: {correct_answer}\")\n",
    "print(f\"\\nDid the model catch the error? {'Yes' if extract_number(late_result) == correct_answer else 'No — it continued from the wrong intermediate result.'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Test with more problems ---\n",
    "# TODO: Try corruption on a different problem. Choose one of the multi-step\n",
    "# problems from PROBLEMS, get the correct CoT, then corrupt one step.\n",
    "#\n",
    "# Suggested problem: \"A farmer has 4 fields, each with 6 rows of corn.\n",
    "#   Each row has 15 plants. How many plants total?\"\n",
    "#   Correct: 4 × 6 = 24 rows, 24 × 15 = 360 plants\n",
    "#   Corruption: change 4 × 6 = 24 to 4 × 6 = 28\n",
    "#\n",
    "# YOUR CODE HERE (5-15 lines)\n",
    "# 1. Write the corrupted prompt (with the wrong intermediate result)\n",
    "# 2. Call the LLM\n",
    "# 3. Print the result and check if the model caught the error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Summary of corruption results ---\n",
    "\n",
    "print(\"ERROR PROPAGATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nProblem: 17 × 24 = {correct_answer}\")\n",
    "print(f\"  No corruption:     {extract_number(correct_cot)}\")\n",
    "print(f\"  Early corruption:  {extract_number(early_result)} (17 × 20 = 350 instead of 340)\")\n",
    "print(f\"  Late corruption:   {extract_number(late_result)} (17 × 4 = 72 instead of 68)\")\n",
    "print()\n",
    "print(\"The model does not 'catch' errors. It continues from whatever context\")\n",
    "print(\"exists — because it is not 'reasoning,' it is generating tokens that\")\n",
    "print(\"feed back as context for subsequent forward passes.\")\n",
    "print()\n",
    "print(\"This is why CoT quality matters more than quantity. An error in an\")\n",
    "print(\"early step corrupts all subsequent steps. The model builds on the\")\n",
    "print(\"context, and if the context is wrong, the building is on a bad foundation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** You corrupted intermediate reasoning steps and observed that the model continues from the error without catching it. This demonstrates two key points from the lesson:\n",
    "\n",
    "1. **The model is not \"reasoning\"** — it is generating tokens that feed back as context. If the context contains an error, subsequent forward passes build on the error. There is no \"checking\" step.\n",
    "\n",
    "2. **CoT quality matters more than quantity** — a wrong intermediate step produces a wrong final answer. Longer chains are not automatically better; they need to contain *correct* intermediate results. This connects to the ICL finding that more examples don't always help, and the RAG finding that irrelevant context dilutes attention.\n",
    "\n",
    "<details>\n",
    "<summary>Solution for Step 4</summary>\n",
    "\n",
    "```python\n",
    "farmer_corruption = \"\"\"A farmer has 4 fields, each with 6 rows of corn. Each row has 15 plants. How many plants total?\n",
    "Let's work through this step by step.\n",
    "\n",
    "Step 1: Find total rows: 4 fields × 6 rows = 28 rows\n",
    "\n",
    "Continue from here and give the final answer.\"\"\"\n",
    "\n",
    "farmer_result = call_llm(farmer_corruption, max_tokens=300)\n",
    "print(\"FARMER PROBLEM CORRUPTION (4 × 6 = 28 instead of 24)\")\n",
    "print(\"=\" * 50)\n",
    "print_wrapped(farmer_result)\n",
    "print(f\"\\nExtracted answer: {extract_number(farmer_result)}\")\n",
    "print(f\"Correct answer: 360\")\n",
    "print(f\"Expected wrong answer: 28 × 15 = 420\")\n",
    "caught = extract_number(farmer_result) == 360\n",
    "print(f\"Did the model catch the error? {'Yes' if caught else 'No'}\")\n",
    "```\n",
    "\n",
    "The model should produce 420 (28 × 15) instead of 360 (24 × 15). It continues from the corrupted intermediate result \"28 rows\" without noticing that 4 × 6 = 24, not 28.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 4: Find the CoT Boundary (Independent)\n",
    "\n",
    "You've seen that CoT helps on multi-step problems but not on single-step ones. Where exactly is the boundary? In this exercise, you design your own experiment to find it.\n",
    "\n",
    "**Your task:**\n",
    "1. Choose a domain (arithmetic, logic puzzles, word problems, or something else)\n",
    "2. Design a set of problems with increasing complexity (at least 6 problems, spanning from trivially easy to genuinely hard)\n",
    "3. Test each with and without CoT\n",
    "4. Plot accuracy vs complexity for both strategies\n",
    "5. Identify the approximate threshold where CoT starts helping\n",
    "\n",
    "**No skeleton is provided.** Design the experiment yourself. Think about:\n",
    "- How will you measure \"complexity\"? (Number of operations? Digit count? Reasoning steps?)\n",
    "- How will you ensure the problems are comparable?\n",
    "- How many trials per problem for reliable accuracy estimates?\n",
    "\n",
    "The solution is in the `<details>` block at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiment here.\n",
    "#\n",
    "# 1. Define your problems with increasing complexity\n",
    "# 2. Test each with direct and CoT prompting\n",
    "# 3. Measure accuracy (run multiple trials if using temperature > 0)\n",
    "# 4. Plot the results\n",
    "# 5. Identify the boundary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reflection:\n",
    "#\n",
    "# 1. Where was the boundary? At what complexity did CoT start helping?\n",
    "# 2. Was the boundary sharp or gradual?\n",
    "# 3. Is the boundary task-dependent? Would it be different for logic puzzles vs arithmetic?\n",
    "# 4. Is the boundary model-dependent? Would a larger model have the boundary at a different point?\n",
    "#\n",
    "# Print your observations:\n",
    "print(\"Reflection:\")\n",
    "print(\"  1. Boundary location: ...\")\n",
    "print(\"  2. Sharp or gradual: ...\")\n",
    "print(\"  3. Task-dependent: ...\")\n",
    "print(\"  4. Model-dependent: ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Design rationale:** Use arithmetic with increasing digit count as the complexity axis. This gives a clean, measurable complexity metric (number of digits) and unambiguous correctness (the answer is a number).\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "# Generate multiplication problems with increasing digit counts\n",
    "# Complexity = total digits across both operands\n",
    "rng = random.Random(42)\n",
    "\n",
    "boundary_problems = []\n",
    "for complexity in [2, 3, 4, 5, 6]:\n",
    "    # Generate 3 problems at each complexity level\n",
    "    for trial in range(3):\n",
    "        if complexity == 2:  # 1×1 digit\n",
    "            a, b = rng.randint(2, 9), rng.randint(2, 9)\n",
    "        elif complexity == 3:  # 2×1 digit\n",
    "            a, b = rng.randint(10, 99), rng.randint(2, 9)\n",
    "        elif complexity == 4:  # 2×2 digit\n",
    "            a, b = rng.randint(10, 99), rng.randint(10, 99)\n",
    "        elif complexity == 5:  # 3×2 digit\n",
    "            a, b = rng.randint(100, 999), rng.randint(10, 99)\n",
    "        else:  # 3×3 digit\n",
    "            a, b = rng.randint(100, 999), rng.randint(100, 999)\n",
    "        boundary_problems.append((complexity, a, b, a * b))\n",
    "\n",
    "# Test each problem\n",
    "boundary_results = []\n",
    "for complexity, a, b, answer in boundary_problems:\n",
    "    problem = f\"What is {a} × {b}?\"\n",
    "    \n",
    "    direct = call_llm(f\"{problem} Answer with ONLY the number.\")\n",
    "    direct_correct = extract_number(direct) == answer\n",
    "    \n",
    "    cot = call_llm(f\"{problem} Let's work through this step by step.\", max_tokens=500)\n",
    "    cot_correct = extract_number(cot) == answer\n",
    "    \n",
    "    boundary_results.append({\n",
    "        \"complexity\": complexity,\n",
    "        \"problem\": f\"{a}×{b}\",\n",
    "        \"answer\": answer,\n",
    "        \"direct_correct\": direct_correct,\n",
    "        \"cot_correct\": cot_correct,\n",
    "    })\n",
    "    \n",
    "    d_sym = \"✓\" if direct_correct else \"✗\"\n",
    "    c_sym = \"✓\" if cot_correct else \"✗\"\n",
    "    print(f\"complexity={complexity} | {a}×{b}={answer} | direct {d_sym} | cot {c_sym}\")\n",
    "\n",
    "# Aggregate by complexity level\n",
    "complexities = sorted(set(r[\"complexity\"] for r in boundary_results))\n",
    "direct_accs = []\n",
    "cot_accs = []\n",
    "for c in complexities:\n",
    "    level_results = [r for r in boundary_results if r[\"complexity\"] == c]\n",
    "    direct_accs.append(sum(r[\"direct_correct\"] for r in level_results) / len(level_results))\n",
    "    cot_accs.append(sum(r[\"cot_correct\"] for r in level_results) / len(level_results))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(complexities, [a * 100 for a in direct_accs], 'o-', color='#f59e0b',\n",
    "        label='Direct', linewidth=2, markersize=8)\n",
    "ax.plot(complexities, [a * 100 for a in cot_accs], 's-', color='#8b5cf6',\n",
    "        label='CoT', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Complexity (total digits)', fontsize=11)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=11)\n",
    "ax.set_title('Finding the CoT Boundary', fontsize=13, fontweight='bold')\n",
    "ax.set_ylim(-5, 110)\n",
    "ax.legend(fontsize=11)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Annotate the boundary region\n",
    "for i, c in enumerate(complexities):\n",
    "    if direct_accs[i] < cot_accs[i]:\n",
    "        ax.axvspan(c - 0.3, c + 0.3, alpha=0.1, color='#8b5cf6')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the boundary\n",
    "boundary = None\n",
    "for i, c in enumerate(complexities):\n",
    "    if direct_accs[i] < cot_accs[i]:\n",
    "        boundary = c\n",
    "        break\n",
    "\n",
    "print(f\"\\nApproximate boundary: complexity = {boundary}\")\n",
    "print(f\"Below this, direct prompting is sufficient (single-pass capacity).\")\n",
    "print(f\"Above this, CoT provides needed additional computation.\")\n",
    "print(f\"\\nThe boundary is task-dependent (arithmetic vs logic) and model-dependent\")\n",
    "print(f\"(larger models can handle more complexity in a single pass).\")\n",
    "```\n",
    "\n",
    "**Expected findings:** The boundary typically appears around 2×2 digit multiplication (complexity 4). Single-digit multiplication fits in one forward pass. Two-digit multiplication starts to exceed it. The boundary is somewhat model-dependent — larger models may handle 2×2 in a single pass but fail at 3×2.\n",
    "\n",
    "**Key insight:** The boundary corresponds to the point where the problem exceeds single-forward-pass computational capacity. Below the boundary, the model can map input to answer in N transformer blocks. Above it, the intermediate results provide the additional computation needed.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Chain-of-thought works because each intermediate token triggers an additional forward pass.** The model does not \"think harder.\" It runs more forward passes, each building on the context of previous ones. You measured this: multi-step problems used 10-20× more tokens (and therefore forward passes) than direct answers.\n",
    "\n",
    "2. **CoT helps when the problem exceeds single-forward-pass capacity.** Single-step arithmetic fits in one pass. Multi-step problems do not. The boundary is task-dependent and model-dependent, but the criterion is always computational complexity.\n",
    "\n",
    "3. **The model does not catch errors in intermediate steps.** Corrupted intermediate results propagate to the final answer. The model continues from whatever context exists. This proves it is generating tokens, not \"reasoning\" — it has no mechanism to verify its own intermediate results.\n",
    "\n",
    "4. **CoT quality matters more than quantity.** Each intermediate token must provide a useful result. More tokens add more forward passes, but only useful tokens add useful computation. Noise in the chain dilutes attention, just like irrelevant documents in RAG.\n",
    "\n",
    "5. **The mechanism is the autoregressive loop you already understand.** Same `generate()` method. Same forward pass. Same N transformer blocks. The only difference is how many tokens are generated. CoT is not a new mechanism — it is a new way to see the existing one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}