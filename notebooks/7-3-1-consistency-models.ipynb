{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistency Models\n",
    "\n",
    "**Module 7.3, Lesson 1** | CourseAI\n",
    "\n",
    "You know the theory from the lesson: consistency models use the self-consistency property of deterministic ODE trajectories—any point on the same trajectory maps to the same clean endpoint—as a training objective, bypassing multi-step ODE solving entirely. This notebook makes that concrete.\n",
    "\n",
    "**What you will do:**\n",
    "- Visualize ODE trajectories from a pretrained 2D diffusion model and verify the self-consistency property with real numbers\n",
    "- Compare one-step Euler, one-step DDIM, and the true ODE endpoint—see why single-step ODE methods fail on curved trajectories\n",
    "- Train a toy consistency model via distillation on 2D two-moons data and generate one-step samples\n",
    "- Compare multi-step consistency generation (1, 2, 4, 8 steps) to the teacher's ODE solving (1, 5, 10, 20, 50 steps)\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "Every concept in this notebook comes from the lesson. The self-consistency property, the consistency function, consistency distillation, multi-step consistency. No new theory—just hands-on practice with the math and models.\n",
    "\n",
    "**Estimated time:** 35–50 minutes. Exercises 1–2 use a pretrained model (no training). Exercises 3–4 train small MLPs on 2D data (~2–3 minutes on CPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to import everything and configure the environment.\n",
    "\n",
    "No GPU required for this notebook. Everything runs on CPU. The models are tiny MLPs trained on 2D point distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print('Setup complete. No GPU needed for this notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Helpers\n",
    "\n",
    "A small MLP for velocity prediction (the pretrained teacher), data generation utilities, and ODE solving helpers. Run this cell now—it defines everything needed for all four exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Shared: MLP for 2D generative models\n",
    "# ============================================================\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    \"\"\"MLP that takes (x_t, t) and outputs a 2D vector.\n",
    "    \n",
    "    For flow matching: output = predicted velocity v_theta(x_t, t)\n",
    "    For consistency models: output = predicted clean point f_theta(x_t, t)\n",
    "    Same architecture, different training target.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2 + hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        if t.dim() == 1:\n",
    "            t = t.unsqueeze(-1)\n",
    "        t_emb = self.time_mlp(t)\n",
    "        inp = torch.cat([x_t, t_emb], dim=-1)\n",
    "        return self.net(inp)\n",
    "\n",
    "\n",
    "class ConsistencyModel(nn.Module):\n",
    "    \"\"\"Consistency model with skip connection enforcing the boundary condition.\n",
    "    \n",
    "    f(x_t, t) = c_skip(t) * x_t + c_out(t) * F_theta(x_t, t)\n",
    "    \n",
    "    At t = epsilon (near 0): c_skip -> 1, c_out -> 0, so f(x, epsilon) = x.\n",
    "    At t = T (high noise): c_skip -> 0, c_out -> 1, so the network has full control.\n",
    "    \n",
    "    This enforces the boundary condition f(x, epsilon) = x by construction.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=128, sigma_min=0.002):\n",
    "        super().__init__()\n",
    "        self.sigma_min = sigma_min\n",
    "        self.backbone = ToyModel(hidden_dim=hidden_dim)\n",
    "    \n",
    "    def forward(self, x_t, t):\n",
    "        \"\"\"Predict the clean endpoint f(x_t, t).\n",
    "        \n",
    "        x_t: (batch, 2) -- noisy point\n",
    "        t: (batch,) or (batch, 1) -- noise level in [sigma_min, 1]\n",
    "        \"\"\"\n",
    "        if t.dim() == 2:\n",
    "            t_scalar = t.squeeze(-1)\n",
    "        else:\n",
    "            t_scalar = t\n",
    "        \n",
    "        # Skip connection coefficients\n",
    "        # c_skip(t) = sigma_min / t -- goes to 1 as t -> sigma_min\n",
    "        # c_out(t) = (t - sigma_min) / t -- goes to 0 as t -> sigma_min\n",
    "        c_skip = (self.sigma_min / t_scalar).unsqueeze(-1)  # (batch, 1)\n",
    "        c_out = ((t_scalar - self.sigma_min) / t_scalar).unsqueeze(-1)  # (batch, 1)\n",
    "        \n",
    "        # Backbone prediction\n",
    "        F = self.backbone(x_t, t_scalar)\n",
    "        \n",
    "        # f(x_t, t) = c_skip * x_t + c_out * F_theta(x_t, t)\n",
    "        return c_skip * x_t + c_out * F\n",
    "\n",
    "\n",
    "def sample_two_moons(n, noise=0.06):\n",
    "    \"\"\"Sample from the two-moons distribution.\"\"\"\n",
    "    data, _ = make_moons(n_samples=n, noise=noise)\n",
    "    data = (data - data.mean(axis=0)) * 2.0\n",
    "    return torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def solve_ode(model, x_start, t_start, t_end, n_steps):\n",
    "    \"\"\"Solve the flow matching ODE from t_start to t_end using Euler's method.\n",
    "    \n",
    "    Returns the full trajectory as a list of (x, t) pairs.\n",
    "    The model predicts velocity v(x_t, t), and we step:\n",
    "        x_{t-dt} = x_t - dt * v(x_t, t)\n",
    "    \"\"\"\n",
    "    dt = (t_start - t_end) / n_steps\n",
    "    x = x_start.clone()\n",
    "    trajectory = [(x.clone(), t_start)]\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        t = t_start - i * dt\n",
    "        t_tensor = torch.full((x.shape[0], 1), t)\n",
    "        v = model(x, t_tensor)\n",
    "        x = x - dt * v\n",
    "        trajectory.append((x.clone(), t - dt))\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ode_endpoint(model, x_start, t_start, t_end, n_steps):\n",
    "    \"\"\"Solve the ODE and return only the final point.\"\"\"\n",
    "    traj = solve_ode(model, x_start, t_start, t_end, n_steps)\n",
    "    return traj[-1][0]\n",
    "\n",
    "\n",
    "print('Shared helpers defined.')\n",
    "print('- ToyModel: MLP that takes (x_t, t) -> 2D vector')\n",
    "print('- ConsistencyModel: MLP with skip connection enforcing f(x, eps) = x')\n",
    "print('- sample_two_moons: 2D two-moons distribution')\n",
    "print('- solve_ode / ode_endpoint: Euler ODE solver for flow matching models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Teacher Model\n",
    "\n",
    "We need a pretrained 2D diffusion model (flow matching) that defines ODE trajectories. We train it here—same as Exercise 3 from the flow matching notebook. This takes ~30 seconds.\n",
    "\n",
    "This teacher model is used in all four exercises. Think of it as the \"pretrained diffusion model\" from the lesson—it has already learned the ODE trajectory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Train the teacher: a flow matching model on two-moons\n",
    "# ============================================================\n",
    "# This is identical to what you built in the flow matching notebook.\n",
    "# The teacher defines the ODE trajectories that consistency models\n",
    "# will learn to bypass.\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "teacher = ToyModel(hidden_dim=128)\n",
    "optimizer = torch.optim.Adam(teacher.parameters(), lr=3e-4)\n",
    "\n",
    "print('Training teacher (flow matching) on two-moons...')\n",
    "print('(~30 seconds on CPU)')\n",
    "\n",
    "for epoch in range(500):\n",
    "    x_0 = sample_two_moons(512)\n",
    "    epsilon = torch.randn_like(x_0)\n",
    "    t = torch.rand(512, 1)\n",
    "    \n",
    "    # Flow matching interpolation\n",
    "    x_t = (1 - t) * x_0 + t * epsilon\n",
    "    \n",
    "    # Target velocity\n",
    "    target_v = epsilon - x_0\n",
    "    \n",
    "    # Predict and optimize\n",
    "    pred_v = teacher(x_t, t)\n",
    "    loss = nn.functional.mse_loss(pred_v, target_v)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'  Epoch {epoch+1}/500, loss: {loss.item():.4f}')\n",
    "\n",
    "teacher.eval()\n",
    "print(f'\\nTeacher trained. Final loss: {loss.item():.4f}')\n",
    "print('This model defines the ODE trajectories used in all exercises.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check: generate samples from the teacher\n",
    "torch.manual_seed(0)\n",
    "noise = torch.randn(500, 2)\n",
    "teacher_samples = ode_endpoint(teacher, noise, t_start=1.0, t_end=0.0, n_steps=50)\n",
    "\n",
    "real_data = sample_two_moons(500)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axes[0].scatter(real_data[:, 0], real_data[:, 1], s=3, alpha=0.5, c='#60a5fa')\n",
    "axes[0].set_title('Real Data (two-moons)', fontsize=11)\n",
    "axes[0].set_xlim(-4, 4); axes[0].set_ylim(-4, 4)\n",
    "axes[0].set_aspect('equal'); axes[0].grid(alpha=0.15)\n",
    "\n",
    "axes[1].scatter(teacher_samples[:, 0], teacher_samples[:, 1], s=3, alpha=0.5, c='#34d399')\n",
    "axes[1].set_title('Teacher (50-step ODE)', fontsize=11)\n",
    "axes[1].set_xlim(-4, 4); axes[1].set_ylim(-4, 4)\n",
    "axes[1].set_aspect('equal'); axes[1].grid(alpha=0.15)\n",
    "\n",
    "plt.suptitle('Teacher Model Sanity Check', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('The teacher generates clean two-moons samples with 50 ODE steps.')\n",
    "print('This teacher defines the ODE trajectories for all exercises.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Visualize Self-Consistency on ODE Trajectories `[Guided]`\n",
    "\n",
    "From the lesson: the self-consistency property says that any point on the same ODE trajectory maps to the same clean endpoint. This is a trivial consequence of the ODE being deterministic—but it becomes a powerful training objective.\n",
    "\n",
    "We will use the pretrained teacher to:\n",
    "1. Generate several ODE trajectories by starting from different noise points and solving to completion\n",
    "2. Pick one trajectory and highlight 5 points at different noise levels\n",
    "3. Run the ODE from each of these 5 points and verify they all reach the same endpoint\n",
    "\n",
    "**Before running, predict:**\n",
    "- If you start from the midpoint of a trajectory (t=0.5) and run the ODE to completion, will you reach the same endpoint as starting from the beginning (t=1.0)?\n",
    "- How close will the endpoints be? Exactly the same, or approximately the same? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1: Visualize ODE trajectories and self-consistency\n",
    "# ============================================================\n",
    "\n",
    "torch.manual_seed(17)\n",
    "\n",
    "# Step 1: Generate 8 ODE trajectories from random noise to clean data\n",
    "n_trajectories = 8\n",
    "n_ode_steps = 200  # Dense steps for smooth trajectory visualization\n",
    "\n",
    "noise_starts = torch.randn(n_trajectories, 2)\n",
    "all_trajectories = []\n",
    "\n",
    "for i in range(n_trajectories):\n",
    "    x_i = noise_starts[i:i+1]  # (1, 2)\n",
    "    traj = solve_ode(teacher, x_i, t_start=1.0, t_end=0.0, n_steps=n_ode_steps)\n",
    "    # Extract just the points\n",
    "    points = torch.stack([pt[0].squeeze(0) for pt in traj])  # (n_steps+1, 2)\n",
    "    times = [pt[1] for pt in traj]  # list of t values\n",
    "    all_trajectories.append((points, times))\n",
    "\n",
    "# Step 2: Pick one trajectory and highlight 5 points\n",
    "chosen_idx = 2  # Pick the 3rd trajectory\n",
    "chosen_points, chosen_times = all_trajectories[chosen_idx]\n",
    "\n",
    "# Select 5 points at t = 0.9, 0.7, 0.5, 0.3, 0.1\n",
    "highlight_t_values = [0.9, 0.7, 0.5, 0.3, 0.1]\n",
    "highlight_indices = []\n",
    "for t_val in highlight_t_values:\n",
    "    # Find the index closest to this t value\n",
    "    idx = min(range(len(chosen_times)), key=lambda i: abs(chosen_times[i] - t_val))\n",
    "    highlight_indices.append(idx)\n",
    "\n",
    "highlight_points = chosen_points[highlight_indices]  # (5, 2)\n",
    "highlight_actual_t = [chosen_times[i] for i in highlight_indices]\n",
    "\n",
    "# The true endpoint of this trajectory (t=0)\n",
    "true_endpoint = chosen_points[-1]  # (2,)\n",
    "\n",
    "print(f'Chosen trajectory: starts at noise ({chosen_points[0][0]:.3f}, {chosen_points[0][1]:.3f})')\n",
    "print(f'True endpoint: ({true_endpoint[0]:.3f}, {true_endpoint[1]:.3f})')\n",
    "print()\n",
    "print('Highlighted points on this trajectory:')\n",
    "for i, (pt, t_val) in enumerate(zip(highlight_points, highlight_actual_t)):\n",
    "    print(f'  Point {i+1}: t={t_val:.3f}, position=({pt[0]:.3f}, {pt[1]:.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Run the ODE from each highlighted point to t=0\n",
    "# If the self-consistency property holds, all 5 should reach the same endpoint.\n",
    "\n",
    "endpoints_from_midpoints = []\n",
    "\n",
    "print('Running ODE from each highlighted point to t=0...')\n",
    "print()\n",
    "\n",
    "for i, (pt, t_val) in enumerate(zip(highlight_points, highlight_actual_t)):\n",
    "    x_start = pt.unsqueeze(0)  # (1, 2)\n",
    "    # Use enough steps for a good approximation\n",
    "    n_steps_from_here = max(int(t_val * 200), 10)\n",
    "    endpoint = ode_endpoint(teacher, x_start, t_start=t_val, t_end=0.0, n_steps=n_steps_from_here)\n",
    "    endpoint = endpoint.squeeze(0)  # (2,)\n",
    "    endpoints_from_midpoints.append(endpoint)\n",
    "    \n",
    "    distance = torch.norm(endpoint - true_endpoint).item()\n",
    "    print(f'  From t={t_val:.3f}: endpoint = ({endpoint[0]:.4f}, {endpoint[1]:.4f}), '\n",
    "          f'distance from true endpoint = {distance:.6f}')\n",
    "\n",
    "# Compute overall statistics\n",
    "all_endpoints = torch.stack(endpoints_from_midpoints)\n",
    "distances = torch.norm(all_endpoints - true_endpoint.unsqueeze(0), dim=1)\n",
    "print(f'\\nMax distance from true endpoint: {distances.max().item():.6f}')\n",
    "print(f'Mean distance from true endpoint: {distances.mean().item():.6f}')\n",
    "print()\n",
    "print('All 5 points on the same trajectory reach (approximately) the same endpoint.')\n",
    "print('The small differences are due to numerical ODE solver precision, not a failure')\n",
    "print('of the self-consistency property. With infinite precision, they would be identical.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Visualize all trajectories and the self-consistency verification\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# --- Left panel: All 8 ODE trajectories ---\n",
    "ax = axes[0]\n",
    "colors = plt.cm.cool(np.linspace(0.2, 0.9, n_trajectories))\n",
    "\n",
    "for i, (pts, times) in enumerate(all_trajectories):\n",
    "    alpha = 0.8 if i == chosen_idx else 0.25\n",
    "    width = 2.5 if i == chosen_idx else 1.0\n",
    "    ax.plot(pts[:, 0], pts[:, 1], color=colors[i], linewidth=width, alpha=alpha)\n",
    "    # Mark start (noise) and end (data)\n",
    "    ax.plot(pts[0, 0], pts[0, 1], 'o', color=colors[i], markersize=4, alpha=alpha)\n",
    "    ax.plot(pts[-1, 0], pts[-1, 1], '*', color=colors[i], markersize=8 if i == chosen_idx else 5, alpha=alpha)\n",
    "\n",
    "# Highlight the 5 points on the chosen trajectory\n",
    "for i, (pt, t_val) in enumerate(zip(highlight_points, highlight_actual_t)):\n",
    "    ax.plot(pt[0], pt[1], 's', color='#f59e0b', markersize=10, zorder=10,\n",
    "            markeredgecolor='white', markeredgewidth=1.5)\n",
    "    ax.annotate(f't={t_val:.1f}', (pt[0], pt[1]), textcoords='offset points',\n",
    "                xytext=(8, 5), fontsize=8, color='#f59e0b')\n",
    "\n",
    "ax.set_title('8 ODE Trajectories (noise to data)\\nHighlighted: chosen trajectory with 5 points', fontsize=11)\n",
    "ax.set_xlabel('$x_1$'); ax.set_ylabel('$x_2$')\n",
    "ax.grid(alpha=0.15)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# --- Right panel: Self-consistency verification ---\n",
    "ax = axes[1]\n",
    "\n",
    "# Plot the chosen trajectory\n",
    "pts = all_trajectories[chosen_idx][0]\n",
    "ax.plot(pts[:, 0], pts[:, 1], color='#60a5fa', linewidth=1.5, alpha=0.4, label='ODE trajectory')\n",
    "\n",
    "# Plot the 5 starting points\n",
    "for i, (pt, t_val) in enumerate(zip(highlight_points, highlight_actual_t)):\n",
    "    ax.plot(pt[0], pt[1], 's', color='#f59e0b', markersize=10, zorder=10,\n",
    "            markeredgecolor='white', markeredgewidth=1.5)\n",
    "    ax.annotate(f't={t_val:.1f}', (pt[0], pt[1]), textcoords='offset points',\n",
    "                xytext=(8, 5), fontsize=8, color='#f59e0b')\n",
    "\n",
    "# Plot the endpoints reached from each starting point\n",
    "for i, ep in enumerate(endpoints_from_midpoints):\n",
    "    label = 'Endpoints from midpoints' if i == 0 else None\n",
    "    ax.plot(ep[0], ep[1], 'o', color='#34d399', markersize=8, zorder=10, label=label)\n",
    "    # Draw arrow from starting point to endpoint\n",
    "    ax.annotate('', xy=(ep[0], ep[1]), xytext=(highlight_points[i][0], highlight_points[i][1]),\n",
    "                arrowprops=dict(arrowstyle='->', color='#34d399', alpha=0.4, lw=1.5))\n",
    "\n",
    "# Mark the true endpoint\n",
    "ax.plot(true_endpoint[0], true_endpoint[1], '*', color='#ef4444', markersize=18, zorder=11,\n",
    "        markeredgecolor='white', markeredgewidth=1.5, label='True endpoint')\n",
    "\n",
    "ax.set_title('Self-Consistency Verification\\n5 starting points → same endpoint', fontsize=11)\n",
    "ax.set_xlabel('$x_1$'); ax.set_ylabel('$x_2$')\n",
    "ax.legend(fontsize=9, loc='upper left')\n",
    "ax.grid(alpha=0.15)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Left: 8 ODE trajectories from random noise to the two-moons distribution.')\n",
    "print('The highlighted trajectory has 5 marked points at different noise levels.')\n",
    "print()\n",
    "print('Right: Running the ODE from each of the 5 points reaches the same endpoint.')\n",
    "print('The green circles (endpoints from midpoints) cluster tightly around the')\n",
    "print('red star (true endpoint). This IS the self-consistency property:')\n",
    "print('f(x_t, t) = f(x_t\\', t\\') for any t, t\\' on the same trajectory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You verified the self-consistency property with real numbers on real ODE trajectories:\n",
    "\n",
    "- **The self-consistency property is real.** Starting the ODE from t=0.9, t=0.7, t=0.5, t=0.3, or t=0.1—all on the same trajectory—produces (nearly) the same endpoint. The small differences are numerical precision from the ODE solver, not a violation of the property.\n",
    "\n",
    "- **This is just what \"deterministic\" means.** The ODE is deterministic: same starting point, same path, same endpoint. If two points share a trajectory, they share an endpoint. This is the trivially true fact that the lesson calls \"not a new mathematical result.\"\n",
    "\n",
    "- **The insight is what we can do with it.** If a neural network could learn the function f(x_t, t) = endpoint for ANY x_t and t, we would not need to run the ODE at all. We would just evaluate f once and get the clean data point. That is the consistency model idea.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: One-Step ODE vs Consistency Model Prediction `[Guided]`\n",
    "\n",
    "From the lesson: a consistency model is NOT the same as an ODE solver with 1 step. An ODE solver with 1 step computes a direction at x_T and takes a single (massive, inaccurate) step. The consistency model maps x_T directly to x_0—no direction, no step, a direct mapping.\n",
    "\n",
    "We will compare three one-step approaches:\n",
    "1. **True endpoint**—run the ODE with many steps (ground truth)\n",
    "2. **One Euler step**—compute velocity at x_T, take one step to t=0\n",
    "3. **DDIM-style 1-step prediction**—use the velocity to estimate x_0 directly\n",
    "\n",
    "For the flow matching ODE, the DDIM-style 1-step prediction computes:\n",
    "\n",
    "$$\\hat{x}_0 = x_T - T \\cdot v_\\theta(x_T, T) = x_T - 1.0 \\cdot v_\\theta(x_T, 1.0)$$\n",
    "\n",
    "(Since $x_t = (1-t) \\cdot x_0 + t \\cdot \\epsilon$ implies $x_0 = x_t - t \\cdot v$ where $v = \\epsilon - x_0$.)\n",
    "\n",
    "Note: for flow matching with perfectly straight trajectories, the Euler 1-step and the DDIM 1-step are actually the same computation. But the learned velocity field is not perfectly straight (individual paths are straight; the aggregate field has curvature), so there can be subtle differences.\n",
    "\n",
    "**Before running, predict:**\n",
    "- How far off will the 1-step Euler estimate be from the true endpoint?\n",
    "- Will the 1-step predictions land anywhere near the two-moons distribution, or will they be scattered randomly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 2: Compare true ODE endpoint vs 1-step approximations\n",
    "# ============================================================\n",
    "\n",
    "torch.manual_seed(42)\n",
    "n_samples = 300\n",
    "\n",
    "# Start from pure noise\n",
    "x_T = torch.randn(n_samples, 2)\n",
    "\n",
    "# --- Ground truth: 200-step ODE ---\n",
    "true_endpoints = ode_endpoint(teacher, x_T, t_start=1.0, t_end=0.0, n_steps=200)\n",
    "\n",
    "# --- 1 Euler step ---\n",
    "# From t=1.0 to t=0.0 in one step:\n",
    "# x_0_euler = x_T - 1.0 * v_theta(x_T, 1.0)\n",
    "with torch.no_grad():\n",
    "    t_one = torch.ones(n_samples, 1)\n",
    "    v_at_T = teacher(x_T, t_one)\n",
    "    euler_1step = x_T - 1.0 * v_at_T\n",
    "\n",
    "# --- DDIM-style 1-step prediction ---\n",
    "# For flow matching: x_0_hat = x_T - T * v_theta(x_T, T)\n",
    "# This is actually the same formula as Euler 1-step for flow matching\n",
    "# (confirming that for flow matching, Euler from t=1 to t=0 IS the x_0 prediction)\n",
    "ddim_1step = x_T - 1.0 * v_at_T\n",
    "\n",
    "# --- Also compare: 5-step ODE (a reasonable middle ground) ---\n",
    "ode_5step = ode_endpoint(teacher, x_T, t_start=1.0, t_end=0.0, n_steps=5)\n",
    "\n",
    "# Compute errors\n",
    "euler_errors = torch.norm(euler_1step - true_endpoints, dim=1)\n",
    "ode5_errors = torch.norm(ode_5step - true_endpoints, dim=1)\n",
    "\n",
    "print('Endpoint errors (distance from true 200-step ODE endpoint):')\n",
    "print(f'  1 Euler step:  mean={euler_errors.mean():.4f}, max={euler_errors.max():.4f}')\n",
    "print(f'  5 ODE steps:   mean={ode5_errors.mean():.4f}, max={ode5_errors.max():.4f}')\n",
    "print()\n",
    "print('Note: for flow matching, 1 Euler step and DDIM 1-step x_0 prediction')\n",
    "print('are the same computation: x_0 = x_T - v_theta(x_T, 1.0).')\n",
    "print('The error comes from curvature in the LEARNED aggregate velocity field.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4.5))\n",
    "\n",
    "# Real data\n",
    "real = sample_two_moons(500)\n",
    "axes[0].scatter(real[:, 0], real[:, 1], s=3, alpha=0.3, c='#60a5fa')\n",
    "axes[0].set_title('Real Data\\n(two-moons)', fontsize=11)\n",
    "\n",
    "# True endpoints (200-step ODE)\n",
    "axes[1].scatter(true_endpoints[:, 0], true_endpoints[:, 1], s=3, alpha=0.5, c='#34d399')\n",
    "axes[1].set_title('True Endpoint\\n(200-step ODE)', fontsize=11)\n",
    "\n",
    "# 1-step prediction\n",
    "axes[2].scatter(euler_1step[:, 0], euler_1step[:, 1], s=3, alpha=0.5, c='#f59e0b')\n",
    "axes[2].set_title(f'1-Step Prediction\\n(mean error: {euler_errors.mean():.3f})', fontsize=11)\n",
    "\n",
    "# 5-step ODE\n",
    "axes[3].scatter(ode_5step[:, 0], ode_5step[:, 1], s=3, alpha=0.5, c='#a78bfa')\n",
    "axes[3].set_title(f'5-Step ODE\\n(mean error: {ode5_errors.mean():.3f})', fontsize=11)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlim(-5, 5); ax.set_ylim(-5, 5)\n",
    "    ax.set_aspect('equal'); ax.grid(alpha=0.15)\n",
    "\n",
    "plt.suptitle(\n",
    "    'Why We Need Consistency Models: Single-Step ODE Methods Are Inaccurate',\n",
    "    fontsize=13, y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Observations:')\n",
    "print('- The true endpoint (200-step ODE) produces clean two-moons samples.')\n",
    "print('- The 1-step prediction is recognizable but noticeably noisier/messier.')\n",
    "print('  Points scatter away from the clean crescents because the aggregate')\n",
    "print('  velocity field has curvature that one step cannot account for.')\n",
    "print('- The 5-step ODE is much closer to the true endpoint.')\n",
    "print()\n",
    "print('THIS is the problem consistency models solve. A trained consistency model')\n",
    "print('would map x_T directly to x_0 in one step WITHOUT the curvature error.')\n",
    "print('It does not compute a direction and step—it predicts the endpoint directly.')\n",
    "print('The self-consistency training objective teaches it to be accurate at this.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You saw concretely why consistency models are needed—and why they are NOT just an ODE solver with 1 step:\n",
    "\n",
    "- **One-step ODE prediction is inaccurate.** Even with flow matching's nearly-straight trajectories, the aggregate learned velocity field has enough curvature that a single step from pure noise produces noticeably worse samples. The points are roughly in the right area but scatter away from the clean crescent shapes.\n",
    "\n",
    "- **More steps fix it, but cost more.** 5 steps dramatically improves the result; 200 steps is nearly perfect. Every additional step costs one model evaluation. This is the core tension: accuracy costs compute.\n",
    "\n",
    "- **A consistency model takes a different approach.** Instead of computing a direction and stepping (which requires multiple steps for accuracy), it learns a direct mapping: f(x_T, T) = x_0. No direction, no steps, no curvature error. But this requires special training—the self-consistency objective—which is what Exercise 3 implements.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Train a Toy Consistency Model on 2D Data `[Supported]`\n",
    "\n",
    "From the lesson: consistency distillation uses a pretrained teacher to provide ODE trajectory estimates. The training procedure:\n",
    "\n",
    "1. Sample a data point $x_0$, add noise to get $x_{t_{n+1}}$ at a higher noise level\n",
    "2. Use the teacher to take one ODE step from $x_{t_{n+1}}$ to estimate $\\hat{x}_{t_n}$ at the next lower noise level\n",
    "3. Train the consistency model so that $f_\\theta(x_{t_{n+1}}, t_{n+1})$ matches $f_{\\theta^-}(\\hat{x}_{t_n}, t_n)$\n",
    "4. $\\theta^-$ is an EMA of $\\theta$ (prevents collapse)\n",
    "\n",
    "The loss: $\\mathcal{L} = \\| f_\\theta(x_{t_{n+1}}, t_{n+1}) - f_{\\theta^-}(\\hat{x}_{t_n}, t_n) \\|^2$\n",
    "\n",
    "Your task: fill in the TODO markers to implement the consistency distillation training loop.\n",
    "\n",
    "**Before running, predict:**\n",
    "- After training, will the consistency model's 1-step samples be better or worse than the 1-step ODE prediction from Exercise 2?\n",
    "- The consistency model and the teacher have the same architecture size. How can the consistency model produce better 1-step results than the teacher with 1 ODE step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 3: Consistency distillation training\n",
    "# ============================================================\n",
    "# NOTE: Fill in ALL four TODOs before running this cell.\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "sigma_min = 0.002    # Minimum noise level (boundary condition point)\n",
    "sigma_max = 1.0      # Maximum noise level\n",
    "n_timesteps = 20     # Number of discretization steps for the noise schedule\n",
    "ema_decay = 0.999    # EMA decay rate for target network\n",
    "n_epochs = 1000\n",
    "batch_size = 512\n",
    "lr = 3e-4\n",
    "\n",
    "# Create the noise level schedule: evenly spaced from sigma_min to sigma_max\n",
    "# These are the discrete noise levels t_1 < t_2 < ... < t_N\n",
    "sigmas = torch.linspace(sigma_min, sigma_max, n_timesteps)\n",
    "\n",
    "# Create the consistency model (online) and EMA target\n",
    "cm_model = ConsistencyModel(hidden_dim=128, sigma_min=sigma_min)\n",
    "cm_target = ConsistencyModel(hidden_dim=128, sigma_min=sigma_min)\n",
    "\n",
    "# Initialize target as a copy of the model\n",
    "cm_target.load_state_dict(cm_model.state_dict())\n",
    "\n",
    "# The target does NOT receive gradients\n",
    "for param in cm_target.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "optimizer = torch.optim.Adam(cm_model.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "\n",
    "print('Training consistency model via distillation...')\n",
    "print(f'Teacher: pretrained flow matching model')\n",
    "print(f'Noise schedule: {n_timesteps} levels from {sigma_min} to {sigma_max}')\n",
    "print(f'EMA decay: {ema_decay}')\n",
    "print('(~1-2 minutes on CPU)')\n",
    "print()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1: Sample clean data\n",
    "    x_0 = sample_two_moons(batch_size)\n",
    "    \n",
    "    # Step 2: Sample random adjacent timestep pairs (t_n, t_{n+1})\n",
    "    # Pick random indices into the sigma schedule\n",
    "    # n ranges from 0 to n_timesteps-2 (so n+1 is valid)\n",
    "    n_idx = torch.randint(0, n_timesteps - 1, (batch_size,))\n",
    "    t_n = sigmas[n_idx]          # Lower noise level\n",
    "    t_n1 = sigmas[n_idx + 1]     # Higher noise level (one step noisier)\n",
    "    \n",
    "    # Step 3: Create the noisy sample at the higher noise level t_{n+1}\n",
    "    # Using flow matching interpolation: x_t = (1-t) * x_0 + t * epsilon\n",
    "    epsilon = torch.randn_like(x_0)\n",
    "    \n",
    "    # TODO: Compute x at the higher noise level t_{n+1}\n",
    "    # x_tn1 = (1 - t_{n+1}) * x_0 + t_{n+1} * epsilon\n",
    "    # Remember: t_n1 has shape (batch_size,), so unsqueeze to (batch_size, 1)\n",
    "    x_tn1 = None  # <-- Replace this line\n",
    "    \n",
    "    # Step 4: Teacher takes one ODE step from t_{n+1} to t_n\n",
    "    # This estimates where x_{t_{n+1}} would be at noise level t_n\n",
    "    # Euler step: x_hat_tn = x_tn1 - (t_{n+1} - t_n) * v_teacher(x_tn1, t_{n+1})\n",
    "    with torch.no_grad():\n",
    "        v_teacher = teacher(x_tn1, t_n1.unsqueeze(-1))\n",
    "        dt = (t_n1 - t_n).unsqueeze(-1)  # (batch, 1)\n",
    "        \n",
    "        # TODO: Compute the teacher's one-step ODE estimate at noise level t_n\n",
    "        # x_hat_tn = x_tn1 - dt * v_teacher\n",
    "        x_hat_tn = None  # <-- Replace this line\n",
    "    \n",
    "    # Step 5: Consistency model predictions\n",
    "    # Online model: f_theta(x_{t_{n+1}}, t_{n+1})\n",
    "    # Target model: f_theta_minus(x_hat_{t_n}, t_n) [no gradients!]\n",
    "    \n",
    "    pred_online = cm_model(x_tn1, t_n1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_target = cm_target(x_hat_tn, t_n)\n",
    "    \n",
    "    # TODO: Compute the consistency distillation loss\n",
    "    # L = MSE between the online prediction and the target prediction\n",
    "    # The online model's prediction at the higher noise level should match\n",
    "    # the target model's prediction at the teacher-estimated lower noise level.\n",
    "    loss = None  # <-- Replace this line\n",
    "    \n",
    "    # Guard: make sure TODOs are filled in\n",
    "    if x_tn1 is None or x_hat_tn is None or loss is None:\n",
    "        raise NotImplementedError(\n",
    "            'Fill in the TODOs (x_tn1, x_hat_tn, loss) before running this cell.'\n",
    "        )\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # TODO: Update the EMA target network\n",
    "    # For each parameter pair (online_param, target_param):\n",
    "    #   target_param = ema_decay * target_param + (1 - ema_decay) * online_param\n",
    "    # Use torch.no_grad() and .data to avoid tracking gradients.\n",
    "    with torch.no_grad():\n",
    "        for p_online, p_target in zip(cm_model.parameters(), cm_target.parameters()):\n",
    "            pass  # <-- Replace this line with the EMA update\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f'  Epoch {epoch+1}/{n_epochs}, loss: {loss.item():.6f}')\n",
    "\n",
    "print(f'\\nTraining complete. Final loss: {losses[-1]:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 3))\n",
    "ax.plot(losses, color='#a78bfa', linewidth=1, alpha=0.7)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Consistency Loss')\n",
    "ax.set_title('Consistency Distillation Training Loss')\n",
    "ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generate 1-step samples from the consistency model\n",
    "# ============================================================\n",
    "# Single-step generation: f(x_T, T) -> x_0\n",
    "# Sample noise, feed through the consistency model at t=sigma_max, done.\n",
    "\n",
    "cm_model.eval()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "n_gen = 500\n",
    "\n",
    "# Start from pure noise\n",
    "x_noise = torch.randn(n_gen, 2)\n",
    "t_max = torch.full((n_gen,), sigma_max)\n",
    "\n",
    "# One-step consistency model generation\n",
    "with torch.no_grad():\n",
    "    cm_samples = cm_model(x_noise, t_max)\n",
    "\n",
    "# Compare to: teacher 1-step (from Exercise 2) and teacher 50-step\n",
    "with torch.no_grad():\n",
    "    v_teacher_at_T = teacher(x_noise, torch.ones(n_gen, 1))\n",
    "    teacher_1step = x_noise - 1.0 * v_teacher_at_T\n",
    "\n",
    "teacher_50step = ode_endpoint(teacher, x_noise, t_start=1.0, t_end=0.0, n_steps=50)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4.5))\n",
    "\n",
    "real = sample_two_moons(500)\n",
    "axes[0].scatter(real[:, 0], real[:, 1], s=3, alpha=0.3, c='#60a5fa')\n",
    "axes[0].set_title('Real Data', fontsize=11)\n",
    "\n",
    "axes[1].scatter(teacher_50step[:, 0], teacher_50step[:, 1], s=3, alpha=0.5, c='#34d399')\n",
    "axes[1].set_title('Teacher (50-step ODE)', fontsize=11)\n",
    "\n",
    "axes[2].scatter(teacher_1step[:, 0], teacher_1step[:, 1], s=3, alpha=0.5, c='#f59e0b')\n",
    "axes[2].set_title('Teacher (1-step ODE)', fontsize=11)\n",
    "\n",
    "axes[3].scatter(cm_samples[:, 0], cm_samples[:, 1], s=3, alpha=0.5, c='#a78bfa')\n",
    "axes[3].set_title('Consistency Model (1 step)', fontsize=11)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlim(-5, 5); ax.set_ylim(-5, 5)\n",
    "    ax.set_aspect('equal'); ax.grid(alpha=0.15)\n",
    "\n",
    "plt.suptitle(\n",
    "    'Consistency Model vs Teacher: One-Step Generation',\n",
    "    fontsize=13, y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Observations:')\n",
    "print('- The consistency model\\'s 1-step samples should be closer to the real')\n",
    "print('  distribution than the teacher\\'s 1-step ODE prediction.')\n",
    "print('- The consistency model was TRAINED to map noise to clean data in one step.')\n",
    "print('  The teacher was trained to predict velocity, not to be accurate in 1 step.')\n",
    "print('- This is the payoff of the self-consistency training objective: the model')\n",
    "print('  learns that f(x_T, T) should be the trajectory endpoint, not just the')\n",
    "print('  direction at x_T.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The key insight is that consistency distillation enforces a LOCAL constraint (adjacent timesteps agree) that implies GLOBAL consistency (any point maps to the endpoint). The teacher provides the trajectory information, and the EMA target prevents the model from collapsing to a constant function.\n",
    "\n",
    "**Training TODOs:**\n",
    "```python\n",
    "# Noisy sample at higher noise level\n",
    "x_tn1 = (1 - t_n1.unsqueeze(-1)) * x_0 + t_n1.unsqueeze(-1) * epsilon\n",
    "\n",
    "# Teacher's one-step ODE estimate\n",
    "x_hat_tn = x_tn1 - dt * v_teacher\n",
    "\n",
    "# Consistency distillation loss\n",
    "loss = nn.functional.mse_loss(pred_online, pred_target)\n",
    "\n",
    "# EMA update\n",
    "with torch.no_grad():\n",
    "    for p_online, p_target in zip(cm_model.parameters(), cm_target.parameters()):\n",
    "        p_target.data.mul_(ema_decay).add_(p_online.data, alpha=1 - ema_decay)\n",
    "```\n",
    "\n",
    "**Why the EMA target matters:** Without it, both sides of the loss are computed by the same model. The model could trivially minimize the loss by outputting a constant—same output for any input means the two predictions always match. The EMA target breaks this symmetry: the target network changes slowly, so the online network must actually learn the correct mapping to match the target's predictions.\n",
    "\n",
    "**Why the teacher takes only one ODE step:** Running the full ODE would require 10-50 model evaluations per training step—prohibitively expensive. One step between adjacent noise levels provides a reasonable estimate (the trajectory changes little between nearby noise levels). The consistency model learns to chain these local constraints into global consistency.\n",
    "\n",
    "**Common mistakes:**\n",
    "- Forgetting to unsqueeze `t_n1` from shape `(batch,)` to `(batch, 1)` for broadcasting with `x_0` of shape `(batch, 2)`\n",
    "- Using `cm_model` instead of `cm_target` for the target prediction (the EMA copy must be used for stability)\n",
    "- Forgetting `torch.no_grad()` around the EMA update (would corrupt the gradient computation)\n",
    "- Using `.copy_()` instead of the weighted combination for the EMA update\n",
    "\n",
    "</details>\n",
    "\n",
    "### What Just Happened\n",
    "\n",
    "You trained a consistency model via distillation and generated one-step samples:\n",
    "\n",
    "- **The consistency model produces better 1-step samples than the teacher's 1-step ODE.** The teacher was trained to predict velocity (direction at each point). The consistency model was trained to predict the endpoint directly. Same architecture, different training objective, different result.\n",
    "\n",
    "- **The training loop enforces local consistency.** At each training step, two adjacent points on the teacher's trajectory should produce the same output. Over training, these local constraints compose into global consistency: any point maps to the endpoint.\n",
    "\n",
    "- **The EMA target prevents collapse.** Without the slowly-moving target network, the model could cheat by outputting a constant. The EMA target provides a stable reference that the online model must actually match.\n",
    "\n",
    "- **One-step quality is decent but not perfect.** Mapping from pure noise to clean data in one step is a hard problem. The consistency model does it better than the teacher's 1-step ODE, but probably not as well as the teacher's 50-step ODE. This motivates multi-step consistency in Exercise 4.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Multi-Step Consistency and Quality Comparison `[Independent]`\n",
    "\n",
    "From the lesson: multi-step consistency is a middle ground between 1-step and multi-step diffusion. The procedure:\n",
    "\n",
    "1. Start at $x_T$ (pure noise)\n",
    "2. Apply $f(x_T, T)$ to get a clean estimate $\\hat{x}_0$\n",
    "3. Add noise to $\\hat{x}_0$ to get $x_{t_2}$ at a lower noise level $t_2$\n",
    "4. Apply $f(x_{t_2}, t_2)$ to get a better clean estimate\n",
    "5. Repeat\n",
    "\n",
    "This is NOT ODE solving—each step restarts by jumping to $x_0$ and re-noising. There is no trajectory being followed between steps.\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "1. **Implement multi-step consistency sampling** using the trained consistency model from Exercise 3\n",
    "2. **Generate samples at 1, 2, 4, and 8 consistency steps**\n",
    "3. **Generate teacher ODE samples at 1, 5, 10, 20, and 50 steps** for comparison\n",
    "4. **Plot both progressions side by side** to visualize the quality-speed tradeoff\n",
    "\n",
    "For multi-step consistency with $K$ steps, use evenly-spaced noise levels from $\\sigma_{\\text{max}}$ down to $\\sigma_{\\text{min}}$.\n",
    "\n",
    "**Before running, predict:**\n",
    "- At how many consistency steps will the samples look comparable to the teacher's 50-step ODE?\n",
    "- Will the 1-step consistency sample be better or worse than the teacher's 5-step ODE?\n",
    "- Will 8 consistency steps look much better than 4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "#\n",
    "# Suggested structure:\n",
    "#\n",
    "# 1. Implement multi-step consistency sampling:\n",
    "#    def sample_consistency_multistep(model, n_samples, n_steps, sigma_min, sigma_max):\n",
    "#        Create evenly-spaced noise levels from sigma_max down to sigma_min\n",
    "#        Start from pure Gaussian noise\n",
    "#        For each step:\n",
    "#            a. Apply the consistency model to get a clean estimate: x_0_hat = f(x_t, t)\n",
    "#            b. Re-noise the clean estimate to the next lower noise level:\n",
    "#               x_next = (1 - t_next) * x_0_hat + t_next * new_noise\n",
    "#            c. (Skip re-noising on the final step -- just return x_0_hat)\n",
    "#        Return the final clean estimate\n",
    "#\n",
    "# 2. Generate samples:\n",
    "#    - Consistency model at 1, 2, 4, 8 steps (same starting noise for all)\n",
    "#    - Teacher ODE at 1, 5, 10, 20, 50 steps (same starting noise)\n",
    "#\n",
    "# 3. Plot a comparison grid:\n",
    "#    Row 1: Teacher ODE at 1, 5, 10, 20, 50 steps\n",
    "#    Row 2: Consistency model at 1, 2, 4, 8 steps + real data\n",
    "#    Use the same axis limits and styling for fair visual comparison.\n",
    "#\n",
    "# Remember:\n",
    "# - cm_model is the trained consistency model from Exercise 3\n",
    "# - teacher is the pretrained flow matching model\n",
    "# - Use torch.no_grad() for all sampling\n",
    "# - For 1-step consistency, just apply f(x_T, T) directly (same as Exercise 3)\n",
    "# - For multi-step, each re-noising uses FRESH random noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The core insight is that multi-step consistency is NOT ODE solving. Each step independently teleports to $x_0$ and then re-noises to a lower noise level. There is no trajectory being followed. Each consistency function evaluation is an independent jump to the endpoint, starting from progressively less noisy inputs.\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "def sample_consistency_multistep(model, n_samples, n_steps, sigma_min=0.002, sigma_max=1.0):\n",
    "    \"\"\"Multi-step consistency sampling.\n",
    "    \n",
    "    Each step: apply f to get x_0 estimate, re-noise to next lower level.\n",
    "    Final step: return the x_0 estimate without re-noising.\n",
    "    \"\"\"\n",
    "    # Noise levels from sigma_max down to sigma_min, evenly spaced\n",
    "    # For 1 step: just [sigma_max]\n",
    "    # For 2 steps: [sigma_max, sigma_max/2]\n",
    "    # For K steps: evenly spaced from sigma_max down\n",
    "    noise_levels = torch.linspace(sigma_max, sigma_min, n_steps + 1)[:-1]  # K levels\n",
    "    \n",
    "    # Start from pure Gaussian noise\n",
    "    x = torch.randn(n_samples, 2)\n",
    "    \n",
    "    for i, t_val in enumerate(noise_levels):\n",
    "        t = torch.full((n_samples,), t_val.item())\n",
    "        \n",
    "        # Apply consistency model: get clean estimate\n",
    "        x_0_hat = model(x, t)\n",
    "        \n",
    "        # If this is the last step, return the clean estimate\n",
    "        if i == len(noise_levels) - 1:\n",
    "            return x_0_hat\n",
    "        \n",
    "        # Otherwise, re-noise to the next lower noise level\n",
    "        t_next = noise_levels[i + 1].item()\n",
    "        fresh_noise = torch.randn_like(x_0_hat)\n",
    "        x = (1 - t_next) * x_0_hat + t_next * fresh_noise\n",
    "    \n",
    "    return x_0_hat\n",
    "\n",
    "\n",
    "# Generate samples\n",
    "cm_model.eval()\n",
    "n_gen = 500\n",
    "\n",
    "consistency_steps = [1, 2, 4, 8]\n",
    "teacher_steps = [1, 5, 10, 20, 50]\n",
    "\n",
    "cm_samples_dict = {}\n",
    "teacher_samples_dict = {}\n",
    "\n",
    "for k in consistency_steps:\n",
    "    torch.manual_seed(42)\n",
    "    cm_samples_dict[k] = sample_consistency_multistep(\n",
    "        cm_model, n_gen, k, sigma_min=0.002, sigma_max=1.0\n",
    "    )\n",
    "\n",
    "for k in teacher_steps:\n",
    "    torch.manual_seed(42)\n",
    "    noise = torch.randn(n_gen, 2)\n",
    "    teacher_samples_dict[k] = ode_endpoint(teacher, noise, t_start=1.0, t_end=0.0, n_steps=k)\n",
    "\n",
    "real = sample_two_moons(n_gen)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 5, figsize=(22, 9))\n",
    "\n",
    "# Row 1: Teacher ODE at varying step counts\n",
    "for col, k in enumerate(teacher_steps):\n",
    "    ax = axes[0, col]\n",
    "    s = teacher_samples_dict[k]\n",
    "    ax.scatter(s[:, 0], s[:, 1], s=3, alpha=0.5, c='#34d399')\n",
    "    ax.set_title(f'Teacher ODE\\n{k} step{\"s\" if k > 1 else \"\"}', fontsize=10)\n",
    "    ax.set_xlim(-5, 5); ax.set_ylim(-5, 5)\n",
    "    ax.set_aspect('equal'); ax.grid(alpha=0.15)\n",
    "\n",
    "axes[0, 0].set_ylabel('Teacher (ODE)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Row 2: Consistency model at varying step counts + real data\n",
    "for col, k in enumerate(consistency_steps):\n",
    "    ax = axes[1, col]\n",
    "    s = cm_samples_dict[k]\n",
    "    ax.scatter(s[:, 0], s[:, 1], s=3, alpha=0.5, c='#a78bfa')\n",
    "    ax.set_title(f'Consistency Model\\n{k} step{\"s\" if k > 1 else \"\"}', fontsize=10)\n",
    "    ax.set_xlim(-5, 5); ax.set_ylim(-5, 5)\n",
    "    ax.set_aspect('equal'); ax.grid(alpha=0.15)\n",
    "\n",
    "# Real data in the last cell\n",
    "axes[1, 4].scatter(real[:, 0], real[:, 1], s=3, alpha=0.5, c='#60a5fa')\n",
    "axes[1, 4].set_title('Real Data\\n(reference)', fontsize=10)\n",
    "axes[1, 4].set_xlim(-5, 5); axes[1, 4].set_ylim(-5, 5)\n",
    "axes[1, 4].set_aspect('equal'); axes[1, 4].grid(alpha=0.15)\n",
    "\n",
    "axes[1, 0].set_ylabel('Consistency (CM)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle(\n",
    "    'Multi-Step Consistency vs Teacher ODE: Quality-Speed Tradeoff\\n'\n",
    "    'Top: Teacher ODE solving (more steps = more model evaluations)\\n'\n",
    "    'Bottom: Consistency model (each step = one teleport + re-noise)',\n",
    "    fontsize=13, y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Expected observations:')\n",
    "print('- Consistency model at 2-4 steps should approach the teacher at 20-50 steps.')\n",
    "print('- Each consistency step is an independent teleportation, not a trajectory step.')\n",
    "print('  Re-noising gives the model a second (third, fourth...) chance to refine.')\n",
    "print('- The teacher at 1 step is poor (curvature error). The consistency model at 1 step')\n",
    "print('  is trained to be accurate at 1 step, so it should be better.')\n",
    "print('- Diminishing returns beyond 4 consistency steps for this simple 2D distribution.')\n",
    "```\n",
    "\n",
    "**Key design decisions:**\n",
    "- Evenly-spaced noise levels work well for this toy example. In practice, the spacing matters (more levels near high noise where the prediction is hardest).\n",
    "- Fresh noise at each re-noising step introduces diversity. Using the same noise would limit the model's ability to refine.\n",
    "- The final step does NOT re-noise—it returns the clean estimate directly.\n",
    "\n",
    "**Common mistakes:**\n",
    "- Re-noising on the final step (produces a noisy output instead of clean)\n",
    "- Using the same noise for re-noising as the initial noise (limits diversity)\n",
    "- Confusing multi-step consistency with ODE solving—there is no trajectory between steps, each step is independent\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The self-consistency property is real and verifiable.** Starting the ODE from any point on a trajectory reaches the same endpoint. You verified this with concrete numbers—not an abstraction, but a measurable property of the pretrained model's ODE.\n",
    "\n",
    "2. **Single-step ODE methods fail because of curvature.** Even with flow matching's nearly-straight trajectories, one Euler step from pure noise produces inaccurate samples. The aggregate velocity field has enough curvature that a single linear extrapolation misses the target. This is why consistency models exist: they are trained to be accurate in one step, rather than relying on a solver that needs multiple steps.\n",
    "\n",
    "3. **Consistency distillation uses local constraints for global consistency.** The training loss only compares adjacent timesteps: f(x at higher noise) should match f(x at slightly lower noise). Over training, these local constraints compose into global consistency—any noise level maps to the same endpoint. The teacher provides the trajectory information; the EMA target prevents collapse.\n",
    "\n",
    "4. **Multi-step consistency is independent teleportation, not trajectory-following.** Each step jumps to the clean estimate and re-noises, rather than continuing along a trajectory. 2-4 consistency steps can approach the quality of 20-50 ODE steps—a massive speedup for the same number of model evaluations.\n",
    "\n",
    "5. **The training objective, not the architecture, makes the difference.** The consistency model and teacher use the same MLP architecture. The consistency model is better at 1-step generation because it was trained for it—the self-consistency objective specifically optimizes for accurate single-step predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
