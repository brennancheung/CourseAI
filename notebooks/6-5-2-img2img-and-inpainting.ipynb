{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Img2Img and Inpainting\n",
    "\n",
    "**Module 6.5, Lesson 2** | CourseAI\n",
    "\n",
    "You know the forward process closed-form formula, the denoising loop, and the alpha-bar curve. You know the full Stable Diffusion pipeline from text prompt to pixel image. This notebook reconfigures the inference process you already understandâ€”starting from a noised real image instead of pure noise (img2img) and applying a spatial mask at each denoising step (inpainting).\n",
    "\n",
    "**What you will do:**\n",
    "- Run img2img at strengths 0.1 through 0.9 on the same image to see the full strength spectrum and connect it to the alpha-bar curve\n",
    "- Implement img2img from scratch: VAE encode, forward process noise, manual denoising loopâ€”no pipeline abstraction\n",
    "- Create binary masks and run inpainting, observing how mask sizing affects boundary blending and the U-Net receptive field\n",
    "- Combine img2img and inpainting in a multi-step creative editing workflow\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "**Estimated time:** 30â€“45 minutes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies and import everything. This notebook requires a GPU for reasonable inference times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import gc\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float16 if device.type == 'cuda' else torch.float32\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB')\n",
    "\n",
    "print('\\nSetup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Helpers\n",
    "\n",
    "Display helpers, model ID, and the source image used across exercises. Each exercise loads only the pipeline it needs and cleans up afterward to stay within free-tier Colab VRAM (~16 GB on a T4).\n",
    "\n",
    "> **VRAM tip:** If you encounter an out-of-memory error, go to Runtime â†’ Restart runtime and rerun from Setup. Exercise 2 uses the most VRAM because it loads individual components rather than a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'stable-diffusion-v1-5/stable-diffusion-v1-5'\n",
    "\n",
    "\n",
    "def show_images(images, titles, figsize=None):\n",
    "    \"\"\"Display a list of PIL images side by side.\"\"\"\n",
    "    n = len(images)\n",
    "    if figsize is None:\n",
    "        figsize = (5 * n, 5)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(np.array(img))\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_image_grid(images, titles, nrows, ncols, figsize=None, suptitle=None):\n",
    "    \"\"\"Display images in a grid with the given number of rows and columns.\"\"\"\n",
    "    if figsize is None:\n",
    "        figsize = (4 * ncols, 4 * nrows)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    axes_flat = axes.flat if nrows > 1 or ncols > 1 else [axes]\n",
    "    for ax, img, title in zip(axes_flat, images, titles):\n",
    "        ax.imshow(np.array(img))\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    for ax in list(axes_flat)[len(images):]:\n",
    "        ax.axis('off')\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle, fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_sample_image(url, size=(512, 512)):\n",
    "    \"\"\"Download an image from a URL and resize it.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    return img.resize(size, Image.LANCZOS)\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Free GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Load the source image used across all exercises.\n",
    "# This is a public-domain landscape photo from Unsplash.\n",
    "source_url = 'https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=512&h=512&fit=crop'\n",
    "source_image = load_sample_image(source_url)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(np.array(source_image))\n",
    "plt.title('Source Image (used across all exercises)', fontsize=11)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Source image size: {source_image.size}')\n",
    "print('Helpers defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Img2img Strength Exploration [Guided]\n",
    "\n",
    "The lesson explained that the strength parameter determines where on the noise schedule the denoising loop starts. Low strength means starting where alpha-bar is high (mostly signal)â€”only fine details change. High strength means starting where alpha-bar is low (mostly noise)â€”the model can reimagine the entire composition.\n",
    "\n",
    "This exercise makes that tangible. You will run img2img on the same input image at five different strength values with the same prompt and seed. The resulting grid directly maps the strength parameter onto the visual changes you see.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Which strength value will preserve the most original structure? (Think: which strength starts the denoising loop closest to the end, where only detail-refinement steps run?)\n",
    "- At strength=0.9, will the output look anything like the original landscape? (Think: strength=0.9 means 90% of the denoising process runs. Where on the alpha-bar curve is the starting point?)\n",
    "- At what strength value would you expect the output to be completely unrelated to the input? (Hint: the boundary case from the lesson.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from diffusers import StableDiffusionImg2ImgPipeline, DDIMScheduler\n\n# Load the img2img pipeline.\n# We use DDIMScheduler throughout this notebook because its deterministic\n# stepping makes comparison between the pipeline (Exercise 1) and our\n# manual implementation (Exercise 2) straightforward.\nprint('Loading StableDiffusionImg2ImgPipeline...')\nimg2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n    model_id,\n    torch_dtype=dtype,\n    safety_checker=None,\n    requires_safety_checker=False,\n).to(device)\nimg2img_pipe.scheduler = DDIMScheduler.from_config(\n    img2img_pipe.scheduler.config\n)\nprint('Pipeline loaded.')\n\n# Run img2img at five strength values with the same prompt and seed.\nprompt = 'a watercolor painting of mountains at sunset'\nstrengths = [0.1, 0.3, 0.5, 0.7, 0.9]\nseed = 42\n\nresults = []\nfor strength in strengths:\n    generator = torch.Generator(device=device).manual_seed(seed)\n    result = img2img_pipe(\n        prompt=prompt,\n        image=source_image,\n        strength=strength,\n        guidance_scale=7.5,\n        num_inference_steps=30,\n        generator=generator,\n    ).images[0]\n    results.append(result)\n    print(f'  strength={strength}: done')\n\n# Display the original image alongside all strength results.\nall_images = [source_image] + results\nall_titles = ['Original'] + [f'strength={s}' for s in strengths]\n\nshow_image_grid(\n    all_images, all_titles,\n    nrows=2, ncols=3,\n    figsize=(18, 12),\n    suptitle=f'Img2img Strength Spectrum: \"{prompt}\"',\n)\n\nprint(f'\\nPrompt: \"{prompt}\"')\nprint(f'Seed: {seed}')\nprint(f'All images used the same input, prompt, and seed.')\nprint(f'The ONLY variable is the strength parameter.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the alpha-bar connection.\n# Map each strength value to its actual position on the alpha-bar curve\n# using the scheduler's 30-step timestep schedule (the same schedule used\n# in the exercises above).\n\nalphas_cumprod = img2img_pipe.scheduler.alphas_cumprod.cpu().numpy()\ntotal_timesteps = len(alphas_cumprod)\n\n# Get the actual 30-step schedule the scheduler uses during inference.\nnum_inference_steps = 30\nimg2img_pipe.scheduler.set_timesteps(num_inference_steps)\nactual_timesteps = img2img_pipe.scheduler.timesteps.cpu().numpy()\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(range(total_timesteps), alphas_cumprod, color='cyan', linewidth=2,\n        label=r'$\\bar{\\alpha}_t$ (cumulative signal fraction)')\n\n# Mark each strength value on the curve using the actual scheduler timesteps.\ncolors = ['#22c55e', '#3b82f6', '#a855f7', '#f59e0b', '#ef4444']\nfor strength, color in zip(strengths, colors):\n    # strength determines how many of the 30 steps run.\n    num_steps_to_run = int(num_inference_steps * strength)\n    start_step_index = num_inference_steps - num_steps_to_run\n    # Clamp to valid range.\n    start_step_index = min(start_step_index, len(actual_timesteps) - 1)\n    t_start = actual_timesteps[start_step_index]\n    t_start_int = int(t_start)\n    alpha_bar_at_start = alphas_cumprod[t_start_int]\n\n    ax.axvline(x=t_start_int, color=color, linestyle='--', alpha=0.7)\n    ax.scatter([t_start_int], [alpha_bar_at_start], color=color, s=100, zorder=5)\n    ax.annotate(\n        f's={strength}\\n$\\\\bar{{\\\\alpha}}$={alpha_bar_at_start:.3f}',\n        (t_start_int, alpha_bar_at_start),\n        textcoords='offset points', xytext=(10, 10),\n        fontsize=9, color=color,\n    )\n\nax.set_xlabel('Timestep t', fontsize=11)\nax.set_ylabel(r'$\\bar{\\alpha}_t$ (signal fraction)', fontsize=11)\nax.set_title('Strength Parameter Mapped onto the Alpha-Bar Curve (30-step DDIM schedule)', fontsize=12)\nax.legend(fontsize=10)\nplt.tight_layout()\nplt.show()\n\nprint('Each dot shows where the denoising loop STARTS for that strength value.')\nprint(f'Positions use the actual {num_inference_steps}-step DDIM schedule, not a linear mapping.')\nprint('Low strength = start near the right (high alpha-bar, mostly signal).')\nprint('High strength = start near the left (low alpha-bar, mostly noise).')\nprint('\\nThe curve is nonlinear. That is why the effect of strength is nonlinear.')\nprint('The jump from 0.3 to 0.5 is qualitatively different from 0.7 to 0.9,')\nprint('because different denoising phases control different aspects of the image.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the img2img pipeline before Exercise 2.\n",
    "del img2img_pipe\n",
    "cleanup()\n",
    "print('Img2img pipeline freed from VRAM.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You saw the full strength spectrum on a single input image. Key observations:\n",
    "\n",
    "1. **strength=0.1** preserves almost everything. Only the finest textures and color tones shift toward the watercolor prompt. The model ran only the detail-refinement stepsâ€”structure is locked in.\n",
    "\n",
    "2. **strength=0.5** is the editing sweet spot. The broad composition (mountains, sky, horizon) is preserved, but the rendering style changes significantly. The model ran both structure-setting and detail steps.\n",
    "\n",
    "3. **strength=0.9** reimagines the image almost entirely. Only the vaguest spatial hints of the original survive. The model ran nearly the entire denoising process, starting from nearly pure noise.\n",
    "\n",
    "4. **The alpha-bar curve explains everything.** At strength=0.1, alpha-bar is high (~0.98)â€”the noised image is almost clean. At strength=0.9, alpha-bar is low (~0.02)â€”the noised image is almost pure noise. The nonlinear curve explains why going from 0.3 to 0.5 changes detail quality, while going from 0.7 to 0.9 changes global structure.\n",
    "\n",
    "5. **At strength=1.0, the input image would have zero influence.** The image would be noised to pure random noise, and the output would be standard text-to-image. This is the boundary case from the lesson.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 2: Implement Img2img From the Denoising Loop [Guided]\n\nThe lesson showed that img2img is a 3-line change to the standard text-to-image pipeline:\n1. **VAE encode** the input image to latent space\n2. **Noise** the latent to a starting timestep using the forward process formula\n3. **Change the loop start** from T (pure noise) to t_start (partially noised image)\n\nThis exercise implements those three changes by handâ€”no `StableDiffusionImg2ImgPipeline`, just the raw components. You will encode, noise, and denoise manually. Both exercises use the same DDIMScheduler, same seed, same prompt, and same strength, so the manual output should match Exercise 1's pipeline output at strength=0.7.\n\n**Before running, predict:**\n- After VAE encoding, what shape will the latent z_0 have? (512/8 = 64, 4 latent channels.)\n- With 30 DDIM steps and strength=0.7, how many denoising steps will actually run? (70% of 30 = 21 steps.)\n- Will the manually constructed output match the pipeline output from Exercise 1? (Same scheduler, same seed, same strength, same prompt.)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from diffusers import UNet2DConditionModel, AutoencoderKL, DDIMScheduler\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n# Load individual components. We use the same DDIMScheduler as Exercise 1\n# so that our manual output can be compared directly to the pipeline output.\nprint('Loading individual pipeline components...')\nvae = AutoencoderKL.from_pretrained(model_id, subfolder='vae', torch_dtype=dtype).to(device)\nunet = UNet2DConditionModel.from_pretrained(model_id, subfolder='unet', torch_dtype=dtype).to(device)\ntokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder='tokenizer')\ntext_encoder = CLIPTextModel.from_pretrained(\n    model_id, subfolder='text_encoder', torch_dtype=dtype\n).to(device)\nscheduler = DDIMScheduler.from_pretrained(model_id, subfolder='scheduler')\n\n# Freeze everythingâ€”we are doing inference, not training.\nvae.requires_grad_(False)\nunet.requires_grad_(False)\ntext_encoder.requires_grad_(False)\n\nprint('Components loaded.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# STEP 1: Encode the input image with the VAE encoder.\n# ============================================================\n# Remember from the lesson: \"We said the VAE encoder is not used during\n# text-to-image inference. That was correct. Img2img IS differentâ€”it is\n# image-to-image, and the encoder converts your input to latent space.\"\n\nfrom torchvision import transforms\n\n# Preprocess the source image to a tensor normalized to [-1, 1].\nimage_transform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5]),\n])\n\nimage_tensor = image_transform(source_image).unsqueeze(0).to(device, dtype=dtype)\nprint(f'Input image tensor shape: {list(image_tensor.shape)}')\nprint(f'Value range: [{image_tensor.min().item():.2f}, {image_tensor.max().item():.2f}]')\n\n# VAE encode: image space -> latent space.\n# This is the VAE encoder that is NOT used in text-to-image but IS used in img2img.\n# We use .mode() (the distribution mean) rather than .sample() for deterministic\n# encoding. The VAE's learned variance is small enough that the mean is an\n# excellent approximation, and this avoids generator-state differences when\n# comparing to the pipeline output in Exercise 1.\nwith torch.no_grad():\n    latent_dist = vae.encode(image_tensor)\n    z_0 = latent_dist.latent_dist.mode() * vae.config.scaling_factor\n\nprint(f'\\nLatent z_0 shape: {list(z_0.shape)}')\nprint(f'VAE scaling factor: {vae.config.scaling_factor}')\nprint(f'\\n8x spatial compression: 512 -> 64. Four latent channels.')\nprint('This is the same encoding from \"From Pixels to Latents.\"')\nprint('In text-to-image, this encoder is skipped (start from pure noise).')\nprint('In img2img, the encoder is NEEDED to bring the input into latent space.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2: Add noise to z_0 using the forward process formula.\n",
    "# ============================================================\n",
    "# This is the formula from \"The Forward Process\":\n",
    "#   z_t = sqrt(alpha_bar_t) * z_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
    "#\n",
    "# The strength parameter determines HOW MUCH noise to addâ€”i.e., which\n",
    "# timestep t_start to noise to.\n",
    "\n",
    "strength = 0.7\n",
    "num_inference_steps = 30\n",
    "\n",
    "# Set up the scheduler's timestep schedule.\n",
    "scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "all_timesteps = scheduler.timesteps  # e.g., [981, 961, ..., 1] for 30 steps\n",
    "\n",
    "# Compute which timestep to start from.\n",
    "# strength=0.7 means 70% of the denoising steps run.\n",
    "num_steps_to_run = int(num_inference_steps * strength)  # 21 steps\n",
    "start_step_index = num_inference_steps - num_steps_to_run  # step index 9\n",
    "t_start = all_timesteps[start_step_index]  # the actual timestep value\n",
    "\n",
    "print(f'Total inference steps:  {num_inference_steps}')\n",
    "print(f'Strength:              {strength}')\n",
    "print(f'Steps to run:          {num_steps_to_run}')\n",
    "print(f'Start step index:      {start_step_index}')\n",
    "print(f'Starting timestep t:   {t_start.item()}')\n",
    "\n",
    "# Noise z_0 to z_{t_start} using the forward process formula.\n",
    "# The generator ensures we use the same noise as the pipeline would.\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "noise = torch.randn(z_0.shape, generator=generator, device=device, dtype=dtype)\n",
    "\n",
    "# Forward process: z_t = sqrt(alpha_bar_t) * z_0 + sqrt(1 - alpha_bar_t) * noise\n",
    "alpha_bar_t = scheduler.alphas_cumprod[t_start.long().cpu()]\n",
    "z_t_start = (\n",
    "    (alpha_bar_t ** 0.5) * z_0\n",
    "    + ((1 - alpha_bar_t) ** 0.5) * noise\n",
    ")\n",
    "\n",
    "print(f'\\nalpha_bar at t={t_start.item()}: {alpha_bar_t.item():.4f}')\n",
    "print(f'Signal fraction:       {alpha_bar_t.item():.4f} ({alpha_bar_t.item()*100:.1f}%)')\n",
    "print(f'Noise fraction:        {(1 - alpha_bar_t.item()):.4f} ({(1 - alpha_bar_t.item())*100:.1f}%)')\n",
    "print(f'\\nNoised latent z_t_start shape: {list(z_t_start.shape)}')\n",
    "print(f'\\nThis is the forward process formula in action. The same formula')\n",
    "print(f'you derived in \"The Forward Process\", used in the capstone, and')\n",
    "print(f'used in the LoRA training loop. Now it appears in a third context:')\n",
    "print(f'inference-time image editing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3: Encode the text prompt with frozen CLIP.\n",
    "# ============================================================\n",
    "# This is identical to standard text-to-image. The text conditioning\n",
    "# mechanism is unchanged in img2img.\n",
    "\n",
    "prompt = 'a watercolor painting of mountains at sunset'\n",
    "negative_prompt = ''\n",
    "\n",
    "# Encode prompt.\n",
    "text_tokens = tokenizer(\n",
    "    prompt, padding='max_length', max_length=tokenizer.model_max_length,\n",
    "    truncation=True, return_tensors='pt',\n",
    ")\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_tokens.input_ids.to(device))[0]\n",
    "\n",
    "# Encode the unconditional (empty) prompt for classifier-free guidance.\n",
    "uncond_tokens = tokenizer(\n",
    "    negative_prompt, padding='max_length', max_length=tokenizer.model_max_length,\n",
    "    truncation=True, return_tensors='pt',\n",
    ")\n",
    "with torch.no_grad():\n",
    "    uncond_embeddings = text_encoder(uncond_tokens.input_ids.to(device))[0]\n",
    "\n",
    "# Concatenate for CFG: [unconditional, conditional].\n",
    "# The U-Net processes both in a single batch, then CFG combines them.\n",
    "text_emb = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "print(f'Prompt: \"{prompt}\"')\n",
    "print(f'Text embeddings shape: {list(text_emb.shape)}')\n",
    "print(f'  [0] = unconditional (empty prompt)')\n",
    "print(f'  [1] = conditional (\"{prompt}\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4: The denoising loopâ€”starting from t_start, not from T.\n",
    "# ============================================================\n",
    "# This is the ONE change from standard text-to-image:\n",
    "# - Text-to-image: loop from step 0 (t=T, pure noise) to the last step (t~0)\n",
    "# - Img2img: loop from start_step_index (t=t_start, partially noised image)\n",
    "#\n",
    "# The denoising loop itself is IDENTICAL. Same U-Net, same CFG, same sampler.\n",
    "\n",
    "guidance_scale = 7.5\n",
    "z = z_t_start.clone()\n",
    "\n",
    "# Only iterate over the timesteps from t_start onward.\n",
    "denoising_timesteps = all_timesteps[start_step_index:]\n",
    "print(f'Denoising from step index {start_step_index} ({denoising_timesteps[0].item()}) '\n",
    "      f'to step index {num_inference_steps - 1} ({denoising_timesteps[-1].item()})')\n",
    "print(f'Running {len(denoising_timesteps)} denoising steps (out of {num_inference_steps} total)\\n')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, t in enumerate(denoising_timesteps):\n",
    "        # Duplicate the latent for CFG (unconditional + conditional).\n",
    "        z_input = torch.cat([z] * 2)\n",
    "\n",
    "        # U-Net predicts noise.\n",
    "        noise_pred = unet(z_input, t, encoder_hidden_states=text_emb).sample\n",
    "\n",
    "        # Classifier-free guidance: amplify the text direction.\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (\n",
    "            noise_pred_text - noise_pred_uncond\n",
    "        )\n",
    "\n",
    "        # Scheduler step (DDIM update).\n",
    "        z = scheduler.step(noise_pred, t, z).prev_sample\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            print(f'  Step {i:>2d}/{len(denoising_timesteps)}: t = {t.item()}')\n",
    "\n",
    "print(f'\\nDenoising complete. Final latent shape: {list(z.shape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5: VAE decode back to pixel space.\n",
    "# ============================================================\n",
    "# This is the same decode step as standard text-to-image.\n",
    "\n",
    "with torch.no_grad():\n",
    "    decoded = vae.decode(z / vae.config.scaling_factor).sample\n",
    "\n",
    "# Convert tensor to PIL image.\n",
    "decoded_np = decoded[0].cpu().float().numpy()\n",
    "decoded_np = ((decoded_np + 1) / 2).clip(0, 1)  # [-1, 1] -> [0, 1]\n",
    "decoded_np = (decoded_np.transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "manual_result = Image.fromarray(decoded_np)\n",
    "\n",
    "# Display the manual result alongside the original.\n",
    "show_images(\n",
    "    [source_image, manual_result],\n",
    "    ['Original', f'Manual img2img (strength={strength})'],\n",
    "    figsize=(12, 6),\n",
    ")\n",
    "\n",
    "print(f'\\nYou just implemented img2img from scratch:')\n",
    "print(f'  1. VAE encode: [3, 512, 512] -> [4, 64, 64]')\n",
    "print(f'  2. Forward process noise to t={t_start.item()} (alpha_bar={alpha_bar_t.item():.4f})')\n",
    "print(f'  3. Denoise from t={t_start.item()} to t~0 ({num_steps_to_run} DDIM steps)')\n",
    "print(f'  4. VAE decode: [4, 64, 64] -> [3, 512, 512]')\n",
    "print(f'\\nNo StableDiffusionImg2ImgPipeline. Just the raw components.')\n",
    "print(f'Img2img IS the forward-process-then-denoise mechanismâ€”not a black box.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up individual components before Exercise 3.\n",
    "del vae, unet, text_encoder, z, z_0, z_t_start, noise, text_emb\n",
    "cleanup()\n",
    "print('Components freed from VRAM.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You implemented img2img by hand using only the individual pipeline componentsâ€”no `StableDiffusionImg2ImgPipeline` abstraction. The core mechanism was three changes:\n",
    "\n",
    "1. **VAE encode** the input image (the encoder that is NOT used in text-to-image but IS needed for img2img).\n",
    "2. **Forward process noise** to the starting timestep using the closed-form formula you derived in Module 6.2.\n",
    "3. **Start the denoising loop from t_start** instead of from T.\n",
    "\n",
    "The denoising loop itself was unchangedâ€”same U-Net, same CFG (two forward passes, amplify text direction), same DDIM scheduler step. The only difference from standard text-to-image was the starting point.\n",
    "\n",
    "This confirms the lesson's core claim: **img2img is not a new algorithm. It is the same denoising process with a different starting point.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Inpainting with Mask Design [Supported]\n",
    "\n",
    "The lesson explained that inpainting adds one operation to the denoising loop: at each step, a spatial mask controls which regions the model can change and which are preserved from the original. The mask formula:\n",
    "\n",
    "$$z_t^{\\text{combined}} = m \\cdot z_t^{\\text{denoised}} + (1 - m) \\cdot \\text{forward}(z_0^{\\text{original}}, t)$$\n",
    "\n",
    "This exercise uses `StableDiffusionInpaintPipeline` to inpaint a region of the source image. You will create masks of different sizes to observe how mask sizing affects boundary qualityâ€”connecting to the lesson's point about the U-Net's receptive field and why boundaries blend seamlessly.\n",
    "\n",
    "**Your tasks:**\n",
    "- Create a binary mask covering a region of the source image\n",
    "- Run inpainting with a descriptive prompt\n",
    "- Compare tight vs generous mask sizing\n",
    "- Observe boundary blending quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "# Load the inpainting pipeline.\n",
    "print('Loading StableDiffusionInpaintPipeline...')\n",
    "inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=dtype,\n",
    "    safety_checker=None,\n",
    "    requires_safety_checker=False,\n",
    ").to(device)\n",
    "inpaint_pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "    inpaint_pipe.scheduler.config\n",
    ")\n",
    "print('Pipeline loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create TWO binary masks for the source image.\n# A binary mask is a PIL Image (mode 'L') where:\n#   - White (255) = edit this region (the model denoises here)\n#   - Black (0) = preserve the original (re-noised original latent used here)\n#\n# We provide a TIGHT mask and you create a GENEROUS mask covering roughly\n# the same area (the sky/upper portion of the landscape image).\n\n# Pre-filled: the tight mask covers the sky region (top 200 pixels).\n# This is intentionally snug against the sky-mountain boundary.\ntight_mask = Image.new('L', (512, 512), 0)  # Start with all black (preserve)\ntight_draw = ImageDraw.Draw(tight_mask)\ntight_draw.rectangle([0, 0, 512, 200], fill=255)\n\n# TODO: Create the generous mask.\n# The generous mask should cover the same sky region but extend ~40 pixels\n# further downward, giving the model more room to blend at the boundary.\n# Use the same pattern as the tight mask above, but with a larger rectangle.\n#\n# generous_mask = Image.new('L', (512, 512), 0)\n# generous_draw = ImageDraw.Draw(generous_mask)\n# generous_draw.rectangle([left, top, right, bottom], fill=255)\ngenerous_mask = None  # Replace this line with the generous mask\n\n# Display the masks.\nif generous_mask is not None:\n    show_images(\n        [source_image, tight_mask, generous_mask],\n        ['Source Image', 'Tight Mask', 'Generous Mask'],\n        figsize=(15, 5),\n    )\nelse:\n    print('Create the generous mask above to continue.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: Run inpainting with both masks and compare.\n",
    "# Use the inpaint_pipe with:\n",
    "#   - prompt: 'dramatic thunderstorm clouds, dark sky'\n",
    "#   - image: source_image\n",
    "#   - mask_image: the mask (tight_mask or generous_mask)\n",
    "#   - guidance_scale: 7.5\n",
    "#   - num_inference_steps: 30\n",
    "#   - generator: seeded for reproducibility\n",
    "#\n",
    "# Run once with tight_mask and once with generous_mask.\n",
    "\n",
    "inpaint_prompt = 'dramatic thunderstorm clouds, dark sky'\n",
    "seed = 42\n",
    "\n",
    "# Inpaint with tight mask.\n",
    "# TODO: Call inpaint_pipe(...) with tight_mask.\n",
    "# generator = torch.Generator(device=device).manual_seed(seed)\n",
    "# tight_result = inpaint_pipe(...).images[0]\n",
    "tight_result = None  # Replace this line\n",
    "\n",
    "# Inpaint with generous mask.\n",
    "# TODO: Call inpaint_pipe(...) with generous_mask.\n",
    "# generator = torch.Generator(device=device).manual_seed(seed)\n",
    "# generous_result = inpaint_pipe(...).images[0]\n",
    "generous_result = None  # Replace this line\n",
    "\n",
    "if tight_result is not None and generous_result is not None:\n",
    "    show_image_grid(\n",
    "        [source_image, tight_mask, tight_result, source_image, generous_mask, generous_result],\n",
    "        ['Original', 'Tight Mask', 'Tight Result', 'Original', 'Generous Mask', 'Generous Result'],\n",
    "        nrows=2, ncols=3,\n",
    "        figsize=(18, 12),\n",
    "        suptitle=f'Inpainting: \"{inpaint_prompt}\"',\n",
    "    )\n",
    "    print(f'\\nCompare the boundary between the inpainted sky and the preserved mountains.')\n",
    "    print(f'The generous mask gives the model more room to blendâ€”look for smoother')\n",
    "    print(f'transitions at the sky-mountain boundary.')\n",
    "    print(f'\\nWhy do boundaries blend at all? The U-Net sees the FULL latent at every')\n",
    "    print(f'denoising step. At the 8x8 bottleneck resolution, each position has global')\n",
    "    print(f'receptive field. The model\\'s predictions for the sky account for the')\n",
    "    print(f'mountain context. This is fundamentally different from cut-and-paste.')\n",
    "else:\n",
    "    print('Fill in TODO 2 to run inpainting.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe key insight is that inpainting is the standard denoising loop plus a per-step mask. The mask tells the model which regions to denoise and which to preserve. Mask sizing matters because the model needs spatial context around the edited region to produce coherent boundaries.\n\n**TODO 1--Create the generous mask:**\n```python\ngenerous_mask = Image.new('L', (512, 512), 0)\ngenerous_draw = ImageDraw.Draw(generous_mask)\ngenerous_draw.rectangle([0, 0, 512, 240], fill=255)  # 40px more than tight mask\n```\nWhy extend by 40 pixels? The lesson noted that slightly oversized masks produce better results because the model has room to create smooth transitions. The tight mask cuts exactly at the sky-mountain boundary, leaving no room for blending. The generous mask extends into the mountains slightly, letting the model handle the transition.\n\n**TODO 2--Run inpainting:**\n```python\ngenerator = torch.Generator(device=device).manual_seed(seed)\ntight_result = inpaint_pipe(\n    prompt=inpaint_prompt,\n    image=source_image,\n    mask_image=tight_mask,\n    guidance_scale=7.5,\n    num_inference_steps=30,\n    generator=generator,\n).images[0]\n\ngenerator = torch.Generator(device=device).manual_seed(seed)\ngenerous_result = inpaint_pipe(\n    prompt=inpaint_prompt,\n    image=source_image,\n    mask_image=generous_mask,\n    guidance_scale=7.5,\n    num_inference_steps=30,\n    generator=generator,\n).images[0]\n```\nNote the re-seeded generator for each call. Same seed ensures the random noise is identical, so differences between tight and generous results come only from the mask size.\n\n**Common mistakes:**\n- Forgetting to re-create the generator between pipeline calls. The first call consumes the random state.\n- Using `mode='RGB'` instead of `mode='L'` for the mask. The pipeline expects a single-channel mask.\n- Inverting the mask convention: white (255) means \"edit here\", black (0) means \"preserve.\" Some implementations use the opposite convention.\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: demonstrate the boundary case from the lesson.\n",
    "# What happens when the mask covers the ENTIRE image?\n",
    "\n",
    "full_mask = Image.new('L', (512, 512), 255)  # All white = edit everything\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "full_mask_result = inpaint_pipe(\n",
    "    prompt=inpaint_prompt,\n",
    "    image=source_image,\n",
    "    mask_image=full_mask,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=30,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "\n",
    "show_images(\n",
    "    [source_image, full_mask_result],\n",
    "    ['Original', 'Full-Image Mask (entire image denoised)'],\n",
    "    figsize=(12, 6),\n",
    ")\n",
    "\n",
    "print('When mask=1 everywhere, EVERY region is denoised and NONE are preserved.')\n",
    "print('This collapses to standard img2img / text-to-image.')\n",
    "print('The mask IS the entire difference between inpainting and standard denoising.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up before Exercise 4.\n",
    "del inpaint_pipe\n",
    "cleanup()\n",
    "print('Inpainting pipeline freed from VRAM.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You created binary masks and ran inpainting, observing two key phenomena:\n",
    "\n",
    "1. **Mask sizing affects boundary quality.** The generous mask with extra padding produces smoother transitions at the sky-mountain boundary. The tight mask can produce more abrupt transitions because the model has less room to blend. The lesson's advice: \"err on the side of slightly too large.\"\n",
    "\n",
    "2. **Boundaries blend naturally.** Even with the tight mask, the boundary is far smoother than you would get from cut-and-paste. This is because the U-Net sees the full latent at every denoising step. At the 8x8 bottleneck, each position has global receptive fieldâ€”the model's predictions for the sky region near the boundary account for the mountain context.\n",
    "\n",
    "3. **Full-image mask = standard denoising.** When the mask covers everything, no regions are preserved. Inpainting collapses to standard generation. The mask is the entire mechanism.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 4: Creative Multi-Step Workflow [Independent]\n\nThe lesson positioned img2img and inpainting as complementary tools: img2img transforms the whole image, inpainting edits specific regions. The most powerful workflows combine them.\n\nYour task: create a multi-step editing workflow that combines both techniques.\n\n**Workflow specification:**\n1. **Create a simple sketch** (or use the source image)--a rough starting point\n2. **Img2img** at high strength (0.6â€“0.8) to transform it into a coherent scene with a descriptive prompt\n3. **Inpainting** to selectively modify one element of the result (e.g., change the sky, add an object, modify a region)\n4. **Display the full progression:** original/sketch â†’ img2img result â†’ inpainted result\n\n**Key APIs you will need:**\n- `StableDiffusionImg2ImgPipeline`--for the initial transformation\n- `StableDiffusionInpaintPipeline`--for the selective edit\n- `Image.new('L', (512, 512), 0)` + `ImageDraw.Draw(mask).rectangle(...)`--for mask creation\n\n**VRAM constraint:** Load only one pipeline at a time. Delete and call `cleanup()` between pipelines.\n\n**What to observe:**\n- How does the img2img strength affect how much of the original survives?\n- Does the inpainted region blend naturally with the img2img result?\n- Could you achieve this result with either technique alone?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CREATIVE WORKFLOW\n",
    "#\n",
    "# Step 1: Create a simple sketch (or use source_image as your starting point).\n",
    "#\n",
    "# To create a simple sketch programmatically:\n",
    "#   sketch = Image.new('RGB', (512, 512), (200, 220, 255))  # light blue sky\n",
    "#   draw = ImageDraw.Draw(sketch)\n",
    "#   draw.polygon([(0, 350), (150, 200), (300, 300), (512, 250), (512, 512), (0, 512)],\n",
    "#                fill=(50, 120, 50))  # green mountains\n",
    "#   draw.rectangle([0, 400, 512, 512], fill=(80, 60, 40))  # brown ground\n",
    "#\n",
    "# Or simply use: starting_image = source_image\n",
    "#\n",
    "# Step 2: Load StableDiffusionImg2ImgPipeline and transform the sketch/image.\n",
    "#   pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "#       model_id, torch_dtype=dtype, safety_checker=None,\n",
    "#       requires_safety_checker=False).to(device)\n",
    "#   pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "#   result = pipe(prompt=..., image=..., strength=..., ...).images[0]\n",
    "#   del pipe; cleanup()\n",
    "#\n",
    "# Step 3: Load StableDiffusionInpaintPipeline and edit a specific region.\n",
    "#   Create a mask, run inpainting on the img2img result.\n",
    "#   pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "#       model_id, torch_dtype=dtype, safety_checker=None,\n",
    "#       requires_safety_checker=False).to(device)\n",
    "#   pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "#   final = pipe(prompt=..., image=..., mask_image=..., ...).images[0]\n",
    "#   del pipe; cleanup()\n",
    "#\n",
    "# Step 4: Display the full progression.\n",
    "#   show_images(\n",
    "#       [starting_image, img2img_result, mask, final_result],\n",
    "#       ['Starting Image', 'After Img2img', 'Inpainting Mask', 'Final Result'],\n",
    "#   )\n",
    "\n",
    "print('Build your creative workflow here.')\n",
    "print('Remember: load one pipeline at a time, cleanup between them.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "**Experimental design reasoning:** The multi-step workflow demonstrates that img2img and inpainting are complementary. Img2img transforms the global composition (you cannot do this with inpainting alone without masking everything). Inpainting edits specific regions (you cannot do this with img2img alone without affecting the whole image). Together, they form a practical editing pipeline: rough composition â†’ global transformation â†’ selective refinement.\n",
    "\n",
    "```python\n",
    "from diffusers import (StableDiffusionImg2ImgPipeline,\n",
    "                       StableDiffusionInpaintPipeline,\n",
    "                       DPMSolverMultistepScheduler)\n",
    "\n",
    "# Step 1: Create a simple sketch.\n",
    "sketch = Image.new('RGB', (512, 512), (180, 210, 240))  # light blue sky\n",
    "draw = ImageDraw.Draw(sketch)\n",
    "# Simple mountain shapes\n",
    "draw.polygon([(0, 350), (100, 180), (200, 280), (300, 200), (400, 250),\n",
    "              (512, 300), (512, 512), (0, 512)], fill=(60, 100, 50))\n",
    "# A simple lake/river in the foreground\n",
    "draw.ellipse([100, 380, 400, 480], fill=(100, 150, 200))\n",
    "\n",
    "# Step 2: Img2img the sketch into a realistic scene.\n",
    "i2i_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    model_id, torch_dtype=dtype, safety_checker=None,\n",
    "    requires_safety_checker=False).to(device)\n",
    "i2i_pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "    i2i_pipe.scheduler.config)\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "img2img_result = i2i_pipe(\n",
    "    prompt='a beautiful mountain landscape with a lake, photorealistic, golden hour',\n",
    "    image=sketch,\n",
    "    strength=0.75,  # High strength: the sketch is rough, give the model creative freedom\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=30,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "\n",
    "del i2i_pipe\n",
    "cleanup()\n",
    "\n",
    "# Step 3: Inpaint the sky to add dramatic clouds.\n",
    "sky_mask = Image.new('L', (512, 512), 0)\n",
    "sky_draw = ImageDraw.Draw(sky_mask)\n",
    "sky_draw.rectangle([0, 0, 512, 220], fill=255)  # Generous sky mask\n",
    "\n",
    "inp_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    model_id, torch_dtype=dtype, safety_checker=None,\n",
    "    requires_safety_checker=False).to(device)\n",
    "inp_pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "    inp_pipe.scheduler.config)\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "final_result = inp_pipe(\n",
    "    prompt='dramatic sunset sky with orange and purple clouds',\n",
    "    image=img2img_result,\n",
    "    mask_image=sky_mask,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=30,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "\n",
    "del inp_pipe\n",
    "cleanup()\n",
    "\n",
    "# Step 4: Display the full progression.\n",
    "show_image_grid(\n",
    "    [sketch, img2img_result, sky_mask, final_result],\n",
    "    ['Sketch', 'After Img2img (strength=0.75)',\n",
    "     'Inpainting Mask', 'After Inpainting (sky replaced)'],\n",
    "    nrows=1, ncols=4, figsize=(20, 5),\n",
    "    suptitle='Multi-Step Creative Workflow: Sketch -> Img2img -> Inpainting',\n",
    ")\n",
    "\n",
    "print('The workflow progression:')\n",
    "print('  1. Rough sketch provides spatial composition (where mountains are, where the lake is).')\n",
    "print('  2. Img2img at strength=0.75 transforms the sketch into a photorealistic scene.')\n",
    "print('     The model has enough creative freedom to add realistic detail, but the')\n",
    "print('     sketch\\'s composition (mountain shapes, lake position) guides the structure.')\n",
    "print('  3. Inpainting replaces ONLY the sky with dramatic clouds. The mountains and')\n",
    "print('     lake are preserved exactly as img2img generated them. The boundary blends')\n",
    "print('     seamlessly because the U-Net sees the full image at every step.')\n",
    "print('\\nNeither technique alone could do this:')\n",
    "print('  - Img2img alone cannot selectively edit the sky while preserving the landscape.')\n",
    "print('  - Inpainting alone cannot transform a sketch into a photorealistic scene.')\n",
    "```\n",
    "\n",
    "**Why strength=0.75 for the sketch?** The sketch is very roughâ€”it needs substantial creative reinterpretation. Low strength (0.3) would preserve the sketch's flat colors and hard edges. High strength (0.75) lets the model reimagine the details while keeping the broad spatial composition. This connects to the coarse-to-fine mental model: at strength=0.75, the model runs the structure-setting steps, but the sketch's spatial layout still provides enough guidance to anchor the composition.\n",
    "\n",
    "**Why a generous sky mask?** The lesson noted that slightly oversized masks produce better results. Extending the mask 20-40 pixels below the visible sky-mountain boundary gives the model room to create a smooth, coherent transition between the new clouds and the preserved mountains.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Img2img starts the denoising loop from a noised real image, not pure noise.** The forward process formula you derived in Module 6.2 noises the input to a specific timestep. The strength parameter maps onto the alpha-bar curveâ€”low strength preserves structure (high alpha-bar, mostly signal), high strength allows creative reinterpretation (low alpha-bar, mostly noise). The effect is nonlinear because the alpha-bar curve is nonlinear.\n",
    "\n",
    "2. **Inpainting adds a per-step spatial mask to the denoising loop.** At each step, masked regions use the model's prediction while unmasked regions are replaced with the re-noised original. Boundaries blend seamlessly because the U-Net sees the full image at every stepâ€”its global receptive field at the bottleneck ensures predictions account for surrounding context.\n",
    "\n",
    "3. **Neither technique requires training.** No new math, no new architecture, no training loop. Just two clever reconfigurations of the denoising process you already know. The same U-Net, VAE, and CLIP from text-to-image work unchanged.\n",
    "\n",
    "4. **Img2img and inpainting are complementary.** Img2img transforms global composition. Inpainting edits specific regions. Together they form a practical editing pipeline: composition â†’ global transformation â†’ selective refinement.\n",
    "\n",
    "5. **Same denoising process, different starting point (img2img) or selective application (inpainting).** The pipeline you traced across 17 lessons is unchanged. Img2img moves the starting line. Inpainting adds a spatial filter. Both use the forward process formulaâ€”now in its third and fourth applications."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}