{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fashion-MNIST Project\n\n**Module 2.3, Lesson 3 (Project)** | CourseAI\n\nThis is your second end-to-end project and the **final lesson in Series 2**. You will build, train, debug, and improve a Fashion-MNIST classifier independently â€” combining every skill from the past ten lessons.\n\n**What you'll do:**\n- Run a baseline model and diagnose the accuracy gap\n- Train longer and observe the scissors (overfitting) pattern\n- Add regularization (BatchNorm, Dropout, weight decay) and measure improvement\n- Analyze per-class accuracy to understand what the model finds hard\n- Build a complete pipeline with GPU, checkpointing, and early stopping\n\n**Structure (decreasing scaffolding):**\n1. **(Guided)** Data loading and baseline model â€” run it, observe results, diagnose the gap\n2. **(Supported)** Experimentation â€” train longer, add regularization, try architectures\n3. **(Supported)** Per-class analysis â€” computation provided, you interpret\n4. **(Independent)** Full pipeline â€” GPU, checkpointing, early stopping, your best model\n\n**For each exercise, PREDICT the output before running the cell.**\n\n**Estimated time:** 30-45 minutes.\n\n**Target accuracy:** ~88-90% with FC layers. Do not expect 97% â€” that requires CNNs (Series 3).\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to import everything and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Reproducible results\ntorch.manual_seed(42)\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n# For nice plots\nplt.style.use('dark_background')\nplt.rcParams['figure.figsize'] = [10, 4]\n\n# Checkpoint directory\nos.makedirs('saved_models', exist_ok=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 1: Load Fashion-MNIST (Guided)\n\nFashion-MNIST is a drop-in replacement for MNIST. Same image size (28x28 grayscale), same number of classes (10), same torchvision API. One word changes in your loading code.\n\n**Before running, predict:** How will Fashion-MNIST images look compared to MNIST digits? Which classes do you think will be hardest to distinguish from each other?\n\nNote the different normalization values: mean=0.2860, std=0.3530 (MNIST was 0.1307, 0.3081)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion-MNIST normalization values (different from MNIST)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Class names for display\n",
    "class_names = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples:     {len(test_dataset)}')\n",
    "print(f'Image shape:      {train_dataset[0][0].shape}')\n",
    "print(f'Classes:          {len(class_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: one row per class, 8 samples each\n",
    "fig, axes = plt.subplots(10, 8, figsize=(10, 14))\n",
    "fig.suptitle('Fashion-MNIST: 8 Samples Per Class', fontsize=14)\n",
    "\n",
    "# Collect indices for each class\n",
    "class_indices = {i: [] for i in range(10)}\n",
    "for idx, (_, label) in enumerate(train_dataset):\n",
    "    if len(class_indices[label]) < 8:\n",
    "        class_indices[label].append(idx)\n",
    "    if all(len(v) >= 8 for v in class_indices.values()):\n",
    "        break\n",
    "\n",
    "for class_idx in range(10):\n",
    "    for col in range(8):\n",
    "        ax = axes[class_idx, col]\n",
    "        image, _ = train_dataset[class_indices[class_idx][col]]\n",
    "        ax.imshow(image.squeeze(), cmap='gray')\n",
    "        ax.axis('off')\n",
    "        if col == 0:\n",
    "            ax.set_title(class_names[class_idx], fontsize=7, loc='left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nNotice: T-shirt, Pullover, Coat, and Shirt look very similar as 28x28 silhouettes.')\n",
    "print('These are the \"hard\" classes your model will struggle with.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 2: Baseline Model (Guided)\n\nStart with the exact architecture from the MNIST project: Flatten -> Linear(784, 256) -> ReLU -> Linear(256, 128) -> ReLU -> Linear(128, 10). No regularization.\n\n**Before running, predict:** The MNIST baseline hit ~97% accuracy. Will Fashion-MNIST be higher, lower, or similar? By how much?\n\nThis is your baseline â€” the number to beat."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.net(x)\n",
    "\n",
    "print(f'Baseline parameters: {sum(p.numel() for p in BaselineModel().parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, criterion):\n",
    "    \"\"\"Train for one epoch. Returns (avg_loss, accuracy).\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return running_loss / total, 100.0 * correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    \"\"\"Evaluate model. Returns (avg_loss, accuracy).\"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return running_loss / total, 100.0 * correct / total\n",
    "\n",
    "print('Helper functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the baseline for 5 epochs\n",
    "baseline = BaselineModel().to(device)\n",
    "baseline_optimizer = optim.Adam(baseline.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "baseline_history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "\n",
    "print('Training baseline for 5 epochs...')\n",
    "print('=' * 70)\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss, train_acc = train_one_epoch(baseline, train_loader, baseline_optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(baseline, test_loader)\n",
    "\n",
    "    baseline_history['train_loss'].append(train_loss)\n",
    "    baseline_history['train_acc'].append(train_acc)\n",
    "    baseline_history['test_loss'].append(test_loss)\n",
    "    baseline_history['test_acc'].append(test_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/5 | '\n",
    "          f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | '\n",
    "          f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "print('=' * 70)\n",
    "print(f'\\nBaseline result: {baseline_history[\"test_acc\"][-1]:.2f}% test accuracy')\n",
    "print(f'Gap from MNIST (~97%): ~{97 - baseline_history[\"test_acc\"][-1]:.0f} percentage points')\n",
    "print(f'\\nObservations:')\n",
    "print(f'  - Training loss still decreasing? {baseline_history[\"train_loss\"][-1] < baseline_history[\"train_loss\"][-2]}')\n",
    "print(f'  - Train/test gap: {baseline_history[\"train_acc\"][-1] - baseline_history[\"test_acc\"][-1]:.1f} points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnose the Baseline\n",
    "\n",
    "Before experimenting, answer these questions (from your debugging checklist):\n",
    "\n",
    "1. The loss is still decreasing at epoch 5. What does that tell you?\n",
    "2. Training accuracy is higher than test accuracy. What pattern is this?\n",
    "3. What should you try first â€” more epochs, or a different model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 3: Experimentation (Supported)\n\nYou have a baseline (~87-88%). Now improve it through three experiments. The structure is given, but you write the code and observe the results.\n\n### Experiment 1: Train Longer\n\nThe simplest thing to try: give the model more time to converge.\n\n**Task:** Train the baseline architecture for 20 epochs instead of 5. Track train and test accuracy.\n\n**Before running, predict:** Will the test accuracy keep improving for all 20 epochs, or will it plateau/get worse? What will the train/test gap look like?\n\n**Observe:**\n- Does test accuracy improve? By how much?\n- Does the train/test gap get wider? (That is the scissors pattern â€” overfitting.)\n- Does the loss plateau?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Train longer (20 epochs)\n",
    "# TODO: Create a fresh BaselineModel, optimizer, and train for 20 epochs\n",
    "# Track train_loss, train_acc, test_loss, test_acc in a history dict\n",
    "# Print results each epoch\n",
    "\n",
    "exp1_model = BaselineModel().to(device)\n",
    "exp1_optimizer = optim.Adam(exp1_model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "exp1_history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "\n",
    "print('Experiment 1: Training baseline for 20 epochs...')\n",
    "print('=' * 70)\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_loss, train_acc = train_one_epoch(exp1_model, train_loader, exp1_optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(exp1_model, test_loader)\n",
    "\n",
    "    exp1_history['train_loss'].append(train_loss)\n",
    "    exp1_history['train_acc'].append(train_acc)\n",
    "    exp1_history['test_loss'].append(test_loss)\n",
    "    exp1_history['test_acc'].append(test_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1:2d}/20 | '\n",
    "          f'Train: {train_acc:.2f}% | Test: {test_acc:.2f}% | '\n",
    "          f'Gap: {train_acc - test_acc:.1f}')\n",
    "\n",
    "print('=' * 70)\n",
    "print(f'Best test accuracy: {max(exp1_history[\"test_acc\"]):.2f}%')\n",
    "print(f'Final train/test gap: {exp1_history[\"train_acc\"][-1] - exp1_history[\"test_acc\"][-1]:.1f} points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scissors pattern\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, 21)\n",
    "\n",
    "ax1.plot(epochs_range, exp1_history['train_loss'], 'o-', linewidth=2, markersize=3, label='Train')\n",
    "ax1.plot(epochs_range, exp1_history['test_loss'], 's-', linewidth=2, markersize=3, label='Test')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss: Train vs Test (Scissors Pattern?)')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.plot(epochs_range, exp1_history['train_acc'], 'o-', linewidth=2, markersize=3, label='Train')\n",
    "ax2.plot(epochs_range, exp1_history['test_acc'], 's-', linewidth=2, markersize=3, label='Test')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Accuracy: Train vs Test')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Experiment 2: Add Regularization\n\nThe scissors are open â€” training accuracy is higher than test accuracy. Close them with:\n- **BatchNorm1d** â€” normalizes activations, stabilizes training\n- **Dropout(0.3)** â€” randomly zeros 30% of activations during training\n- **weight_decay=0.01** â€” L2 regularization via AdamW\n\n**Task:** Build an `ImprovedModel` with the layer ordering: Linear -> BatchNorm -> ReLU -> Dropout.\n\nUse `torch.optim.AdamW` with `weight_decay=0.01`.\n\nTrain for 20 epochs. Compare to Experiment 1.\n\n**Before running, predict:** Will regularization close the train/test gap? Will it improve test accuracy, or just reduce overfitting?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your ImprovedModel\n",
    "# Architecture: Flatten -> Linear(784,256) -> BN -> ReLU -> Dropout(0.3)\n",
    "#                       -> Linear(256,128) -> BN -> ReLU -> Dropout(0.3)\n",
    "#                       -> Linear(128, 10)\n",
    "\n",
    "class ImprovedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # TODO: Define self.net using nn.Sequential\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the improved model for 20 epochs\n",
    "exp2_model = ImprovedModel().to(device)\n",
    "exp2_optimizer = optim.AdamW(exp2_model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "exp2_history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "\n",
    "print('Experiment 2: Training with regularization for 20 epochs...')\n",
    "print('=' * 70)\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_loss, train_acc = train_one_epoch(exp2_model, train_loader, exp2_optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(exp2_model, test_loader)\n",
    "\n",
    "    exp2_history['train_loss'].append(train_loss)\n",
    "    exp2_history['train_acc'].append(train_acc)\n",
    "    exp2_history['test_loss'].append(test_loss)\n",
    "    exp2_history['test_acc'].append(test_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1:2d}/20 | '\n",
    "          f'Train: {train_acc:.2f}% | Test: {test_acc:.2f}% | '\n",
    "          f'Gap: {train_acc - test_acc:.1f}')\n",
    "\n",
    "print('=' * 70)\n",
    "print(f'Best test accuracy: {max(exp2_history[\"test_acc\"]):.2f}%')\n",
    "print(f'Final train/test gap: {exp2_history[\"train_acc\"][-1] - exp2_history[\"test_acc\"][-1]:.1f} points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\n**The key insight:** The layer ordering matters â€” Linear -> BatchNorm -> ReLU -> Dropout. BatchNorm normalizes before activation, and Dropout regularizes after activation. No BatchNorm or Dropout on the output layer.\n\n```python\nclass ImprovedModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.net = nn.Sequential(\n            nn.Linear(784, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n\n            nn.Linear(256, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n\n            nn.Linear(128, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        return self.net(x)\n```\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Experiment 1 vs Experiment 2\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, 21)\n",
    "\n",
    "# Test accuracy comparison\n",
    "ax1.plot(epochs_range, exp1_history['test_acc'], 'o-', linewidth=2, markersize=3, label='Baseline (no reg.)')\n",
    "ax1.plot(epochs_range, exp2_history['test_acc'], 's-', linewidth=2, markersize=3, label='Improved (with reg.)')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Test Accuracy (%)')\n",
    "ax1.set_title('Test Accuracy: Baseline vs Improved')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Train/test gap comparison\n",
    "exp1_gap = [t - v for t, v in zip(exp1_history['train_acc'], exp1_history['test_acc'])]\n",
    "exp2_gap = [t - v for t, v in zip(exp2_history['train_acc'], exp2_history['test_acc'])]\n",
    "ax2.plot(epochs_range, exp1_gap, 'o-', linewidth=2, markersize=3, label='Baseline gap')\n",
    "ax2.plot(epochs_range, exp2_gap, 's-', linewidth=2, markersize=3, label='Improved gap')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Train - Test Accuracy (points)')\n",
    "ax2.set_title('Overfitting Gap: Scissors Closing?')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nBaseline best test acc: {max(exp1_history[\"test_acc\"]):.2f}%')\n",
    "print(f'Improved best test acc: {max(exp2_history[\"test_acc\"]):.2f}%')\n",
    "print(f'Improvement: {max(exp2_history[\"test_acc\"]) - max(exp1_history[\"test_acc\"]):+.2f} points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Architecture Decisions (YOUR CHOICE)\n",
    "\n",
    "This is fully independent. Try different architectures and compare. Some ideas:\n",
    "\n",
    "- **More capacity:** 784 â†’ 512 â†’ 256 â†’ 128 â†’ 10\n",
    "- **Deeper:** 784 â†’ 256 â†’ 256 â†’ 128 â†’ 64 â†’ 10\n",
    "- **Different dropout:** p=0.5 instead of 0.3\n",
    "- **Different weight decay:** 0.001 instead of 0.01\n",
    "\n",
    "**Negative experiment to try:** Build a very large model (784 â†’ 1024 â†’ 512 â†’ 256 â†’ 10) with **no regularization**. Watch the scissors open wide.\n",
    "\n",
    "Run at least 2 different configurations. Use the training curves to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Try at least 2 different architectures or hyperparameter settings.\n",
    "# Track and compare their training curves.\n",
    "#\n",
    "# Remember:\n",
    "# - Always use model.train() during training and model.eval() during evaluation\n",
    "# - The layer ordering is: Linear -> BatchNorm -> ReLU -> Dropout\n",
    "# - No activation, no dropout, no batchnorm on the output layer\n",
    "# - Use AdamW with weight_decay for L2 regularization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 4: Per-Class Analysis (Supported)\n\nA single accuracy number hides important structure. Your model is not uniformly 89% accurate â€” some classes are easy and some are hard.\n\n**Before running, predict:** Which Fashion-MNIST classes do you think the model finds easiest? Which hardest? Think about what 28x28 silhouettes look like.\n\nThe `per_class_accuracy` function is provided. **Your job:** run it on your best model, interpret the results, and identify the easy vs hard classes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_accuracy(model, test_loader, class_names):\n",
    "    \"\"\"Compute and display accuracy for each class.\"\"\"\n",
    "    model.eval()\n",
    "    correct = torch.zeros(10)\n",
    "    total = torch.zeros(10)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            for i in range(10):\n",
    "                mask = labels == i\n",
    "                total[i] += mask.sum().item()\n",
    "                correct[i] += (preds[mask] == labels[mask]).sum().item()\n",
    "\n",
    "    print('\\nPer-class accuracy:')\n",
    "    print('-' * 45)\n",
    "    accs = []\n",
    "    for i in range(10):\n",
    "        acc = 100 * correct[i] / total[i]\n",
    "        accs.append(acc.item())\n",
    "        bar = 'â–ˆ' * int(acc / 2.5)\n",
    "        print(f'{class_names[i]:>12s}: {acc:5.1f}%  {bar}')\n",
    "    print('-' * 45)\n",
    "    print(f'{\"Overall\":>12s}: {100 * correct.sum() / total.sum():5.1f}%')\n",
    "\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run per-class accuracy on your best model from the experiments above\n",
    "# Replace 'exp2_model' with whichever model performed best\n",
    "accs = per_class_accuracy(exp2_model, test_loader, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class accuracy as a horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = ['#ef4444' if a < 85 else '#f59e0b' if a < 90 else '#22c55e' for a in accs]\n",
    "bars = ax.barh(class_names, accs, color=colors)\n",
    "ax.set_xlabel('Accuracy (%)')\n",
    "ax.set_title('Per-Class Accuracy on Fashion-MNIST')\n",
    "ax.set_xlim(60, 100)\n",
    "ax.axvline(x=90, color='white', linestyle='--', alpha=0.3, label='90%')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add accuracy labels on bars\n",
    "for bar, acc in zip(bars, accs):\n",
    "    ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "            f'{acc:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify easy and hard classes\n",
    "easy = [(class_names[i], accs[i]) for i in range(10) if accs[i] >= 90]\n",
    "hard = [(class_names[i], accs[i]) for i in range(10) if accs[i] < 85]\n",
    "\n",
    "print(f'\\nEasy classes (â‰¥90%): {\", \".join(f\"{n} ({a:.1f}%)\" for n, a in easy)}')\n",
    "print(f'Hard classes (<85%): {\", \".join(f\"{n} ({a:.1f}%)\" for n, a in hard)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the Results\n",
    "\n",
    "Answer these questions based on your per-class accuracy:\n",
    "\n",
    "1. Which classes does the model find easiest? Why? (Think about what the silhouettes look like.)\n",
    "2. Which classes are hardest? What do they have in common visually?\n",
    "3. Why can't an FC network distinguish shirts from coats? (Hint: what happens when you flatten a 28x28 image?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stretch Goal: Confusion Matrix\n",
    "\n",
    "A confusion matrix shows exactly which classes get confused with which. The diagonal shows correct predictions; off-diagonal shows mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stretch: Confusion matrix\n",
    "# TODO: Build and display a confusion matrix\n",
    "#\n",
    "# Steps:\n",
    "# 1. Collect all predictions and true labels from test_loader\n",
    "# 2. Build a 10x10 matrix: confusion[true_class][predicted_class] = count\n",
    "# 3. Display as a heatmap with plt.imshow()\n",
    "#\n",
    "# Hint: you can use torch.zeros(10, 10) for the matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Hint: Confusion Matrix</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Collect predictions\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model = exp2_model  # or whichever model you want to analyze\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_labels = torch.cat(all_labels)\n",
    "\n",
    "# Build confusion matrix\n",
    "confusion = torch.zeros(10, 10, dtype=torch.int64)\n",
    "for true, pred in zip(all_labels, all_preds):\n",
    "    confusion[true][pred] += 1\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(confusion.numpy(), cmap='Blues')\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_yticks(range(10))\n",
    "ax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=8)\n",
    "ax.set_yticklabels(class_names, fontsize=8)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Add numbers\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        val = confusion[i, j].item()\n",
    "        if val > 0:\n",
    "            color = 'white' if val > confusion.max().item() * 0.5 else 'black'\n",
    "            ax.text(j, i, str(val), ha='center', va='center',\n",
    "                    fontsize=7, color=color)\n",
    "\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 5: Full Pipeline (Independent)\n\nPut everything together into a complete, production-ready training pipeline:\n\n1. **Device detection** â€” GPU if available\n2. **Data loading** with Fashion-MNIST transforms\n3. **Your best model** with regularization\n4. **Training loop** with GPU, checkpointing, and early stopping (patience=5)\n5. **Restore best model** and run per-class analysis\n\n**Optional:** Add mixed precision if on GPU.\n\nThis is the pattern you carry forward to every future project. Write it from scratch."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Write the complete pipeline from scratch.\n",
    "#\n",
    "# Requirements:\n",
    "#   - Device detection (GPU if available)\n",
    "#   - Fashion-MNIST data loading with correct normalization\n",
    "#   - Your best model architecture (with regularization)\n",
    "#   - AdamW optimizer with weight_decay\n",
    "#   - Training loop with:\n",
    "#       - model.train() / model.eval() correctly\n",
    "#       - Track train loss, train acc, test loss, test acc\n",
    "#       - Save best model checkpoint by test accuracy\n",
    "#       - Early stopping with patience=5\n",
    "#   - After training: restore best model\n",
    "#   - Per-class accuracy analysis\n",
    "#\n",
    "# Optional: mixed precision (autocast + GradScaler) if on GPU\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the full training curves from your pipeline\n",
    "# TODO: Plot train/test loss and accuracy curves\n",
    "# Show where early stopping triggered (if it did)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final per-class analysis on your best model\n",
    "# TODO: Load best checkpoint and run per_class_accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\n**The key insight:** This combines every skill from Series 2 â€” data loading, model design, regularization, training loops, checkpointing, early stopping, and per-class analysis. The pattern is the same one from the GPU Training lesson, extended with early stopping.\n\n```python\n# 1. Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Training on: {device}')\n\n# 2. Data\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.2860,), (0.3530,))\n])\ntrain_data = torchvision.datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\ntest_data = torchvision.datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\ntrain_loader = DataLoader(train_data, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=256)\n\n# 3. Model\nmodel = ImprovedModel().to(device)\noptimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\ncriterion = nn.CrossEntropyLoss()\n\n# 4. Training loop with early stopping\nnum_epochs = 30\npatience = 5\nbest_acc = 0.0\npatience_counter = 0\nhistory = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n\nstart_time = time.time()\n\nfor epoch in range(num_epochs):\n    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n    test_loss, test_acc = evaluate(model, test_loader)\n\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['test_loss'].append(test_loss)\n    history['test_acc'].append(test_acc)\n\n    improved = ''\n    if test_acc > best_acc:\n        best_acc = test_acc\n        patience_counter = 0\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'accuracy': test_acc,\n        }, 'saved_models/fashion_best.pth')\n        improved = ' <- best'\n    else:\n        patience_counter += 1\n\n    print(f'Epoch {epoch+1:2d}/{num_epochs} | '\n          f'Train: {train_acc:.2f}% | Test: {test_acc:.2f}% | '\n          f'Patience: {patience_counter}/{patience}{improved}')\n\n    if patience_counter >= patience:\n        print(f'\\nEarly stopping at epoch {epoch+1}.')\n        break\n\nelapsed = time.time() - start_time\n\n# 5. Restore best model\nbest_ckpt = torch.load('saved_models/fashion_best.pth', map_location=device, weights_only=False)\nmodel.load_state_dict(best_ckpt['model_state_dict'])\nprint(f'\\nRestored best model from epoch {best_ckpt[\"epoch\"] + 1}')\nprint(f'Best accuracy: {best_acc:.2f}%')\nprint(f'Training time: {elapsed:.1f}s')\n\n# Per-class analysis\nper_class_accuracy(model, test_loader, class_names)\n```\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Summary\n",
    "\n",
    "Collect all your results in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table â€” fill in your results\n",
    "print('=' * 65)\n",
    "print(f'{\"Experiment\":<30} {\"Best Test Acc\":>15} {\"Train/Test Gap\":>15}')\n",
    "print('-' * 65)\n",
    "print(f'{\"Baseline (5 epochs)\":<30} {max(baseline_history[\"test_acc\"]):>14.2f}% {baseline_history[\"train_acc\"][-1] - baseline_history[\"test_acc\"][-1]:>14.1f}')\n",
    "print(f'{\"Exp 1: Longer (20 epochs)\":<30} {max(exp1_history[\"test_acc\"]):>14.2f}% {exp1_history[\"train_acc\"][-1] - exp1_history[\"test_acc\"][-1]:>14.1f}')\n",
    "print(f'{\"Exp 2: Regularization\":<30} {max(exp2_history[\"test_acc\"]):>14.2f}% {exp2_history[\"train_acc\"][-1] - exp2_history[\"test_acc\"][-1]:>14.1f}')\n",
    "# Add your Experiment 3 results here if you tracked them\n",
    "print('=' * 65)\n",
    "print()\n",
    "print('Key observations:')\n",
    "print('  - Training longer improves accuracy but widens the overfitting gap')\n",
    "print('  - Regularization closes the gap and improves generalization')\n",
    "print('  - Hard classes (Shirt, Coat, Pullover) share similar silhouettes')\n",
    "print('  - FC networks flatten spatial structure â€” CNNs (Series 3) will help')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **Training longer helps, but overfitting follows.** The scissors pattern (train acc rising, test acc plateauing) tells you when to stop adding epochs and start adding regularization.\n2. **Regularization closes the scissors.** BatchNorm, Dropout, and weight decay each fight overfitting in different ways â€” combined, they let you train longer without diverging.\n3. **Per-class accuracy reveals structure.** A single number hides the fact that T-shirt/Shirt/Pullover/Coat are hard (similar silhouettes) while Trouser/Sandal/Bag are easy (distinctive shapes).\n4. **FC networks flatten spatial structure.** A shifted or rotated garment becomes an entirely different input vector â€” CNNs (Series 3) fix this.\n5. **The complete pipeline pattern** (device detection, training loop, checkpointing, early stopping, analysis) is what you carry forward to every future project.\n\n| Skill | Source Lesson |\n|-------|-------------|\n| Data loading with transforms | Datasets and DataLoaders |\n| nn.Module model design | nn.Module |\n| Training loop (forward, loss, backward, step) | The Training Loop |\n| Cross-entropy loss, accuracy tracking | MNIST Project |\n| model.train() / model.eval() | MNIST Project |\n| BatchNorm, Dropout, weight decay | MNIST Project |\n| Debugging checklist | Debugging and Visualization |\n| Checkpointing and early stopping | Saving, Loading, and Checkpoints |\n| GPU training | GPU Training |\n| Per-class analysis | This project |\n\n**You did not follow a tutorial. You made decisions, observed results, and adapted. That is machine learning.**\n\n**Next:** Series 3 â€” Convolutional Neural Networks. Your FC model tops out at ~89-90%. CNNs reach 93-95%. The remaining gap is what motivates the next series."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: clean up\n",
    "# import shutil\n",
    "# shutil.rmtree('saved_models', ignore_errors=True)\n",
    "# print('Cleaned up saved_models/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}