{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning Models\n",
    "\n",
    "In this notebook, you'll explore the empirical differences between base models using chain-of-thought prompting and models that reason more effectively, and investigate the core concepts from the lesson: self-consistency, process vs outcome evaluation, and test-time compute allocation.\n",
    "\n",
    "**What you'll do:**\n",
    "- Compare a base model with CoT prompting against a reasoning-focused model on 10 math/reasoning problems, predicting which problems each will get right before running\n",
    "- Run self-consistency experiments: generate N chains (N=1,3,5,10,20), compute majority-vote accuracy, and plot accuracy vs N to find the point of diminishing returns\n",
    "- Evaluate reasoning chains two ways (process vs outcome) on 5 problems with known solutions, finding cases where the final answer is correct but a step is wrong\n",
    "- Design a test-time compute allocation experiment: fixed compute budget, equal vs adaptive allocation across problems of varying difficulty\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones—they reveal gaps in your mental model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup — self-contained for Google Colab\n",
    "!pip install -q openai\n",
    "\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from openai import OpenAI\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- API Key Setup ---\n",
    "# Option 1: Set your API key as an environment variable (recommended)\n",
    "#   In Colab: go to the key icon in the left sidebar, add OPENAI_API_KEY\n",
    "# Option 2: Paste it directly (less secure, don't commit this)\n",
    "#   os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "# You can also use any OpenAI-compatible API (e.g., local Ollama, Together AI)\n",
    "# by changing the base_url:\n",
    "#   client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# --- Model Configuration ---\n",
    "# BASE_MODEL: a standard model that follows CoT prompts but wasn't specifically\n",
    "# trained with RL for reasoning. We use temperature>0 for self-consistency.\n",
    "BASE_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# REASONING_MODEL: a model trained with RL to reason effectively.\n",
    "# o3-mini is OpenAI's reasoning model — it generates internal reasoning\n",
    "# tokens before producing an answer. If you don't have access to o3-mini,\n",
    "# you can substitute another reasoning-focused model.\n",
    "REASONING_MODEL = \"o3-mini\"\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Reproducible results where possible\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def call_llm(prompt: str, model: str = BASE_MODEL,\n",
    "             temperature: float = 0.0, max_tokens: int = 1024) -> str:\n",
    "    \"\"\"Call the LLM with a single prompt. Returns the response text.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def call_llm_with_usage(prompt: str, model: str = BASE_MODEL,\n",
    "                        temperature: float = 0.0,\n",
    "                        max_tokens: int = 1024) -> tuple[str, int]:\n",
    "    \"\"\"Call the LLM and return (response_text, completion_tokens).\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    text = response.choices[0].message.content.strip()\n",
    "    tokens = response.usage.completion_tokens\n",
    "    return text, tokens\n",
    "\n",
    "\n",
    "def call_reasoning_model(prompt: str,\n",
    "                         model: str = REASONING_MODEL) -> tuple[str, int]:\n",
    "    \"\"\"Call the reasoning model. Returns (response_text, completion_tokens).\n",
    "\n",
    "    Reasoning models (like o3-mini) handle their own chain-of-thought\n",
    "    internally — you don't need to prompt them with 'think step by step.'\n",
    "    The model generates internal reasoning tokens (which you may or may not\n",
    "    see) and then produces an answer.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    text = response.choices[0].message.content.strip()\n",
    "    tokens = response.usage.completion_tokens\n",
    "    return text, tokens\n",
    "\n",
    "\n",
    "def print_wrapped(text: str, width: int = 80, prefix: str = \"\"):\n",
    "    \"\"\"Print text with word wrapping for readability.\"\"\"\n",
    "    for line in text.split(\"\\n\"):\n",
    "        wrapped = textwrap.fill(line, width=width, initial_indent=prefix,\n",
    "                                subsequent_indent=prefix)\n",
    "        print(wrapped)\n",
    "\n",
    "\n",
    "def extract_number(text: str) -> int | None:\n",
    "    \"\"\"Extract the last number from a response string.\"\"\"\n",
    "    numbers = re.findall(r'-?\\d[\\d,]*', text.replace(',', ''))\n",
    "    if not numbers:\n",
    "        return None\n",
    "    try:\n",
    "        return int(numbers[-1])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def majority_vote(answers: list[int | None]) -> int | None:\n",
    "    \"\"\"Return the most common non-None answer, or None if all are None.\"\"\"\n",
    "    valid = [a for a in answers if a is not None]\n",
    "    if not valid:\n",
    "        return None\n",
    "    counts = Counter(valid)\n",
    "    return counts.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "# Quick test to verify the API is working\n",
    "test = call_llm(\"Say 'API connection successful' and nothing else.\")\n",
    "print(test)\n",
    "print(f\"\\nBase model: {BASE_MODEL}\")\n",
    "print(f\"Reasoning model: {REASONING_MODEL}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Data\n",
    "\n",
    "All exercises use math and reasoning problems of varying complexity. We define them here so exercises can share them. The problems range from single-step arithmetic to multi-step word problems that require chaining several operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Problems for Exercises 1-4 ---\n",
    "# Each entry: (description, problem_text, correct_answer, num_steps, difficulty)\n",
    "# difficulty: \"easy\" (single-step or simple multi-step) or \"hard\" (chained reasoning)\n",
    "\n",
    "PROBLEMS = [\n",
    "    # Easy problems (1-2 steps)\n",
    "    (\"simple multiply\",\n",
    "     \"What is 8 x 7?\",\n",
    "     56, 1, \"easy\"),\n",
    "\n",
    "    (\"two-digit addition\",\n",
    "     \"What is 47 + 86?\",\n",
    "     133, 1, \"easy\"),\n",
    "\n",
    "    (\"simple word problem\",\n",
    "     \"A store has 3 shelves with 8 books each. They remove 5 books. How many books are left?\",\n",
    "     19, 2, \"easy\"),\n",
    "\n",
    "    # Medium problems (2-3 steps)\n",
    "    (\"two-digit multiply\",\n",
    "     \"What is 17 x 24?\",\n",
    "     408, 3, \"medium\"),\n",
    "\n",
    "    (\"three operations\",\n",
    "     \"What is (15 x 8) + (12 x 3)?\",\n",
    "     156, 3, \"medium\"),\n",
    "\n",
    "    (\"word problem\",\n",
    "     \"Alex earns $12/hour. He worked 5 hours Monday, 3 hours Tuesday, and 4 hours Wednesday. How much did he earn total?\",\n",
    "     144, 2, \"medium\"),\n",
    "\n",
    "    # Hard problems (3+ steps, chained reasoning)\n",
    "    (\"chained operations\",\n",
    "     \"What is 13 x 17 + 8 x 9 - 45?\",\n",
    "     248, 4, \"hard\"),\n",
    "\n",
    "    (\"multi-step word problem\",\n",
    "     \"A farmer has 4 fields, each with 6 rows of corn. Each row has 15 plants. He loses 10% of the plants to pests. How many plants survive?\",\n",
    "     324, 4, \"hard\"),\n",
    "\n",
    "    (\"percentage reasoning\",\n",
    "     \"A shirt costs $80. It's on sale for 25% off. Then you have a coupon for 10% off the sale price. What is the final price?\",\n",
    "     54, 3, \"hard\"),\n",
    "\n",
    "    (\"ratio word problem\",\n",
    "     \"Three friends split a bill. The bill is $156. Alice pays twice as much as Bob, and Carol pays three times as much as Bob. How much does Alice pay?\",\n",
    "     52, 4, \"hard\"),\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(PROBLEMS)} problems:\")\n",
    "for diff in [\"easy\", \"medium\", \"hard\"]:\n",
    "    count = sum(1 for _, _, _, _, d in PROBLEMS if d == diff)\n",
    "    print(f\"  {diff}: {count}\")\n",
    "print(\"\\nData loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Base Model CoT vs Reasoning Model (Guided)\n",
    "\n",
    "The lesson demonstrated that RL-trained reasoning models produce *consistently* better reasoning chains than base models with CoT prompting. Same architecture, same parameter count—the difference is training. In this exercise, you'll test that claim on 10 problems.\n",
    "\n",
    "For each problem, you'll run two models:\n",
    "- **Base model + CoT prompt:** A standard model prompted to \"think step by step\"\n",
    "- **Reasoning model:** A model trained with RL to reason effectively (generates internal reasoning tokens)\n",
    "\n",
    "The first 3 problems are fully worked with analysis. The remaining 7 run with the same pattern.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Which problems will the base model get wrong that the reasoning model gets right? (Hint: the computational complexity criterion from the CoT lesson still applies, but reasoning models handle complex problems more reliably)\n",
    "- Will there be problems where the base model gets it right and the reasoning model gets it wrong?\n",
    "- How will the reasoning *quality* differ, beyond just the final answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Run the first 3 problems with detailed analysis ---\n",
    "# We'll show the full reasoning chains to compare quality, not just accuracy.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DETAILED ANALYSIS: First 3 Problems\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "detailed_results = []\n",
    "\n",
    "for i, (desc, problem, answer, steps, diff) in enumerate(PROBLEMS[:3]):\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Problem {i+1}: {desc} (difficulty: {diff}, steps: {steps})\")\n",
    "    print(f\"Question: {problem}\")\n",
    "    print(f\"Correct answer: {answer}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    # Base model with CoT\n",
    "    cot_prompt = f\"{problem}\\n\\nLet's work through this step by step, then give the final answer.\"\n",
    "    base_response = call_llm(cot_prompt, model=BASE_MODEL, max_tokens=500)\n",
    "    base_answer = extract_number(base_response)\n",
    "    base_correct = base_answer == answer\n",
    "\n",
    "    print(f\"\\n--- Base Model + CoT ---\")\n",
    "    print_wrapped(base_response)\n",
    "    print(f\"\\nExtracted: {base_answer} | Correct: {'Yes' if base_correct else 'NO'}\")\n",
    "\n",
    "    # Reasoning model\n",
    "    reasoning_response, reasoning_tokens = call_reasoning_model(problem)\n",
    "    reasoning_answer = extract_number(reasoning_response)\n",
    "    reasoning_correct = reasoning_answer == answer\n",
    "\n",
    "    print(f\"\\n--- Reasoning Model ---\")\n",
    "    print_wrapped(reasoning_response)\n",
    "    print(f\"\\nExtracted: {reasoning_answer} | Correct: {'Yes' if reasoning_correct else 'NO'}\")\n",
    "    print(f\"Reasoning tokens used: {reasoning_tokens}\")\n",
    "\n",
    "    detailed_results.append({\n",
    "        \"desc\": desc, \"answer\": answer, \"steps\": steps, \"diff\": diff,\n",
    "        \"base_correct\": base_correct, \"reasoning_correct\": reasoning_correct,\n",
    "        \"base_response\": base_response, \"reasoning_response\": reasoning_response,\n",
    "        \"reasoning_tokens\": reasoning_tokens,\n",
    "    })\n",
    "\n",
    "    # Analysis for this problem\n",
    "    print(f\"\\n--- Analysis ---\")\n",
    "    if base_correct and reasoning_correct:\n",
    "        print(\"Both correct. Compare the reasoning quality:\")\n",
    "        print(\"  - Is the base model's chain structured and checkable?\")\n",
    "        print(\"  - Is the reasoning model's chain more systematic?\")\n",
    "    if not base_correct and reasoning_correct:\n",
    "        print(\"Base model WRONG, reasoning model CORRECT.\")\n",
    "        print(\"  The RL training produced a more reliable chain.\")\n",
    "    if base_correct and not reasoning_correct:\n",
    "        print(\"Base model CORRECT, reasoning model WRONG.\")\n",
    "        print(\"  Surprising! Reasoning models aren't infallible.\")\n",
    "\n",
    "print(\"\\nDetailed analysis complete for first 3 problems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Run remaining 7 problems ---\n",
    "\n",
    "all_results = list(detailed_results)  # Copy the first 3\n",
    "\n",
    "print(\"Remaining problems:\")\n",
    "print(f\"{'Problem':<30} | {'Answer':>6} | {'Base':>6} | {'Reasoning':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for desc, problem, answer, steps, diff in PROBLEMS[3:]:\n",
    "    # Base model with CoT\n",
    "    cot_prompt = f\"{problem}\\n\\nLet's work through this step by step, then give the final answer.\"\n",
    "    base_response = call_llm(cot_prompt, model=BASE_MODEL, max_tokens=500)\n",
    "    base_answer = extract_number(base_response)\n",
    "    base_correct = base_answer == answer\n",
    "\n",
    "    # Reasoning model\n",
    "    reasoning_response, reasoning_tokens = call_reasoning_model(problem)\n",
    "    reasoning_answer = extract_number(reasoning_response)\n",
    "    reasoning_correct = reasoning_answer == answer\n",
    "\n",
    "    sym_b = \"\\u2713\" if base_correct else \"\\u2717\"\n",
    "    sym_r = \"\\u2713\" if reasoning_correct else \"\\u2717\"\n",
    "    print(f\"{desc:<30} | {answer:>6} | {str(base_answer):>5} {sym_b} | {str(reasoning_answer):>8} {sym_r}\")\n",
    "\n",
    "    all_results.append({\n",
    "        \"desc\": desc, \"answer\": answer, \"steps\": steps, \"diff\": diff,\n",
    "        \"base_correct\": base_correct, \"reasoning_correct\": reasoning_correct,\n",
    "        \"base_response\": base_response, \"reasoning_response\": reasoning_response,\n",
    "        \"reasoning_tokens\": reasoning_tokens,\n",
    "    })\n",
    "\n",
    "print(\"\\nAll 10 problems tested.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Summary and visualization ---\n",
    "\n",
    "base_total = sum(r[\"base_correct\"] for r in all_results)\n",
    "reasoning_total = sum(r[\"reasoning_correct\"] for r in all_results)\n",
    "\n",
    "print(\"OVERALL ACCURACY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Base model + CoT:  {base_total}/{len(all_results)} ({base_total/len(all_results):.0%})\")\n",
    "print(f\"Reasoning model:   {reasoning_total}/{len(all_results)} ({reasoning_total/len(all_results):.0%})\")\n",
    "\n",
    "# By difficulty\n",
    "for diff in [\"easy\", \"medium\", \"hard\"]:\n",
    "    subset = [r for r in all_results if r[\"diff\"] == diff]\n",
    "    if not subset:\n",
    "        continue\n",
    "    b_acc = sum(r[\"base_correct\"] for r in subset) / len(subset)\n",
    "    r_acc = sum(r[\"reasoning_correct\"] for r in subset) / len(subset)\n",
    "    print(f\"\\n{diff.upper()} ({len(subset)} problems):\")\n",
    "    print(f\"  Base + CoT:    {b_acc:.0%}\")\n",
    "    print(f\"  Reasoning:     {r_acc:.0%}\")\n",
    "    print(f\"  Improvement:   {r_acc - b_acc:+.0%}\")\n",
    "\n",
    "# Bar chart by difficulty\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "diffs = [\"easy\", \"medium\", \"hard\"]\n",
    "base_accs = []\n",
    "reason_accs = []\n",
    "for d in diffs:\n",
    "    subset = [r for r in all_results if r[\"diff\"] == d]\n",
    "    base_accs.append(sum(r[\"base_correct\"] for r in subset) / len(subset) * 100 if subset else 0)\n",
    "    reason_accs.append(sum(r[\"reasoning_correct\"] for r in subset) / len(subset) * 100 if subset else 0)\n",
    "\n",
    "x = np.arange(len(diffs))\n",
    "width = 0.35\n",
    "bars1 = ax.bar(x - width/2, base_accs, width, label='Base + CoT', color='#f59e0b',\n",
    "               edgecolor='white', linewidth=0.5)\n",
    "bars2 = ax.bar(x + width/2, reason_accs, width, label='Reasoning Model', color='#a78bfa',\n",
    "               edgecolor='white', linewidth=0.5)\n",
    "\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=11)\n",
    "ax.set_title('Base Model + CoT vs Reasoning Model by Difficulty', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([d.capitalize() for d in diffs])\n",
    "ax.set_ylim(0, 115)\n",
    "ax.legend()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "for bar in list(bars1) + list(bars2):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 2,\n",
    "            f'{bar.get_height():.0f}%', ha='center', va='bottom', fontsize=10, color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cases where reasoning model succeeded and base failed\n",
    "reasoning_wins = [r for r in all_results if r[\"reasoning_correct\"] and not r[\"base_correct\"]]\n",
    "if reasoning_wins:\n",
    "    print(f\"\\nProblems where reasoning model won and base model lost ({len(reasoning_wins)}):\")\n",
    "    for r in reasoning_wins:\n",
    "        print(f\"  - {r['desc']} (difficulty: {r['diff']}, {r['steps']} steps)\")\n",
    "\n",
    "print(\"\\nKey insight: RL training produces CONSISTENTLY better reasoning chains,\")\n",
    "print(\"not just longer ones. Same architecture, different training. The reasoning\")\n",
    "print(\"model has learned to use the scratchpad (context window) effectively.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** You compared a base model with CoT prompting against a reasoning model on 10 problems of varying difficulty. The reasoning model should show higher and more consistent accuracy, especially on the hard multi-step problems.\n",
    "\n",
    "The key difference is not the architecture or parameter count—it's the training. The reasoning model was trained with RL where the reward signal is answer correctness. It learned to use the scratchpad (context window) more effectively: structured, checkable chains rather than the hit-or-miss quality of base model CoT.\n",
    "\n",
    "This is the lesson's core \"of course\" moment: you already knew tokens are computation (CoT lesson). You already knew RL can shape behavior (RLHF). Of course you can use RL to shape reasoning behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 2: Self-Consistency Experiment (Supported)\n",
    "\n",
    "The lesson DEVELOPED self-consistency from MENTIONED: generate N reasoning chains, take the majority vote. In this exercise, you'll measure how accuracy changes as N increases from 1 to 20.\n",
    "\n",
    "The mechanism: if each chain has probability p > 0.5 of being correct, then N independent chains with majority voting have a higher probability of producing the correct answer. Different chains make different errors—voting averages out the noise.\n",
    "\n",
    "**The first problem is fully set up with code for N=1 and N=3.** You'll extend the pattern to N=5, 10, 20 and then to additional problems.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Will accuracy increase monotonically with N, or will it plateau?\n",
    "- At what N will you see diminishing returns?\n",
    "- Will the benefit of more chains be larger for harder problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Self-consistency for the first problem (fully worked) ---\n",
    "# We use temperature > 0 to get diverse chains.\n",
    "# Each chain is a different sample from the model's distribution.\n",
    "\n",
    "SELF_CONSISTENCY_PROBLEMS = [\n",
    "    (\"two-digit multiply\", \"What is 34 x 56?\", 1904),\n",
    "    (\"chained operations\", \"What is 13 x 17 + 8 x 9 - 45?\", 248),\n",
    "    (\"percentage word problem\",\n",
    "     \"A shirt costs $80. It's on sale for 25% off. Then you have a coupon for 10% off the sale price. What is the final price?\",\n",
    "     54),\n",
    "    (\"ratio word problem\",\n",
    "     \"Three friends split a bill. The bill is $156. Alice pays twice as much as Bob, and Carol pays three times as much as Bob. How much does Alice pay?\",\n",
    "     52),\n",
    "    (\"multi-step word problem\",\n",
    "     \"A farmer has 4 fields, each with 6 rows of corn. Each row has 15 plants. He loses 10% of the plants to pests. How many plants survive?\",\n",
    "     324),\n",
    "]\n",
    "\n",
    "# Demonstrate with the first problem, N=1 and N=3\n",
    "demo_desc, demo_problem, demo_answer = SELF_CONSISTENCY_PROBLEMS[0]\n",
    "print(f\"Problem: {demo_problem}\")\n",
    "print(f\"Correct answer: {demo_answer}\")\n",
    "print()\n",
    "\n",
    "cot_prompt = f\"{demo_problem}\\n\\nLet's work through this step by step, then give the final answer.\"\n",
    "\n",
    "# N=1: single chain\n",
    "chain_1 = call_llm(cot_prompt, temperature=0.7, max_tokens=500)\n",
    "answer_1 = extract_number(chain_1)\n",
    "print(f\"N=1: Single chain answer = {answer_1} ({'correct' if answer_1 == demo_answer else 'WRONG'})\")\n",
    "print(f\"  Chain: {chain_1[:120]}...\")\n",
    "print()\n",
    "\n",
    "# N=3: three chains, majority vote\n",
    "chains_3 = []\n",
    "answers_3 = []\n",
    "for i in range(3):\n",
    "    chain = call_llm(cot_prompt, temperature=0.7, max_tokens=500)\n",
    "    ans = extract_number(chain)\n",
    "    chains_3.append(chain)\n",
    "    answers_3.append(ans)\n",
    "    print(f\"  Chain {i+1}: answer = {ans}\")\n",
    "\n",
    "vote_3 = majority_vote(answers_3)\n",
    "print(f\"\\nN=3: Majority vote = {vote_3} ({'correct' if vote_3 == demo_answer else 'WRONG'})\")\n",
    "print(f\"  Individual answers: {answers_3}\")\n",
    "print(f\"  Voting averages out errors from individual chains.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Step 2: Extend to all N values for all problems ---\n# TODO: For each problem, generate chains at N = 1, 3, 5, 10, 20.\n# At each N, use majority voting to get the answer.\n# Track whether the majority-vote answer is correct.\n#\n# Strategy: Generate all 20 chains once (the max N), then take the\n# majority vote of the first 1, 3, 5, 10, 20 answers. This is more\n# efficient than generating separately for each N.\n#\n# The result should be stored in sc_results:\n#   sc_results[problem_index][N] = {\"correct\": bool, \"vote\": int, \"answers\": list}\n#\n# Starter code below. Fill in the TODO sections.\n\nN_VALUES = [1, 3, 5, 10, 20]\nsc_results = {}\n\nfor p_idx, (desc, problem, answer) in enumerate(SELF_CONSISTENCY_PROBLEMS):\n    print(f\"\\nProblem {p_idx+1}: {desc} (answer: {answer})\")\n    cot_prompt = f\"{problem}\\n\\nLet's work through this step by step, then give the final answer.\"\n\n    # Generate 20 chains\n    all_answers = []\n    for i in range(20):\n        chain = call_llm(cot_prompt, temperature=0.7, max_tokens=500)\n        ans = extract_number(chain)\n        all_answers.append(ans)\n    print(f\"  Generated 20 chains. Answers: {all_answers}\")\n\n    # TODO: For each N in N_VALUES, take the first N answers from all_answers\n    # and use majority_vote() to get the consensus answer.\n    # Store results in sc_results[p_idx] as a dictionary keyed by N.\n    # Each entry should track: the vote, whether it's correct, and the answer subset.\n    # Print the vote and correctness for each N.\n    #\n    # Hint: You already saw the pattern in Step 1 with N=3.\n    # This is the same idea, looped over multiple N values.\n\n    # YOUR CODE HERE (5-8 lines)\n\n\nprint(\"\\nSelf-consistency experiment complete.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Plot accuracy vs N ---\n",
    "# TODO: Create a plot showing accuracy (y-axis) vs N (x-axis).\n",
    "#\n",
    "# Compute the accuracy at each N across all problems:\n",
    "#   For each N, accuracy = (number of problems where majority vote is correct) / total problems\n",
    "#\n",
    "# Also overlay individual problem correctness as scatter points to show\n",
    "# which problems benefit most from more chains.\n",
    "#\n",
    "# Useful data:\n",
    "#   sc_results[p_idx][n][\"correct\"]  — whether majority vote was correct\n",
    "#   N_VALUES = [1, 3, 5, 10, 20]\n",
    "#   len(SELF_CONSISTENCY_PROBLEMS) = 5\n",
    "#\n",
    "# Hints:\n",
    "#   accuracies = []\n",
    "#   for n in N_VALUES:\n",
    "#       acc = sum(sc_results[p][n][\"correct\"] for p in range(5)) / 5\n",
    "#       accuracies.append(acc * 100)\n",
    "#\n",
    "#   fig, ax = plt.subplots(figsize=(8, 5))\n",
    "#   ax.plot(N_VALUES, accuracies, 'o-', ...)\n",
    "\n",
    "# YOUR CODE HERE (15-25 lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution for Steps 2 and 3</summary>\n",
    "\n",
    "**Step 2 — majority vote at each N:**\n",
    "\n",
    "```python\n",
    "    sc_results[p_idx] = {}\n",
    "    for n in N_VALUES:\n",
    "        subset = all_answers[:n]\n",
    "        vote = majority_vote(subset)\n",
    "        sc_results[p_idx][n] = {\n",
    "            \"correct\": vote == answer,\n",
    "            \"vote\": vote,\n",
    "            \"answers\": subset,\n",
    "        }\n",
    "        print(f\"  N={n:>2}: vote={vote}, correct={vote == answer}\")\n",
    "```\n",
    "\n",
    "**Step 3 — plot:**\n",
    "\n",
    "```python\n",
    "# Compute accuracy at each N\n",
    "accuracies = []\n",
    "for n in N_VALUES:\n",
    "    acc = sum(sc_results[p][n][\"correct\"] for p in range(len(SELF_CONSISTENCY_PROBLEMS))) / len(SELF_CONSISTENCY_PROBLEMS)\n",
    "    accuracies.append(acc * 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Main accuracy line\n",
    "ax.plot(N_VALUES, accuracies, 'o-', color='#a78bfa', linewidth=2.5,\n",
    "        markersize=10, label='Majority-Vote Accuracy', zorder=3)\n",
    "\n",
    "# Individual problem results as background dots\n",
    "for p_idx in range(len(SELF_CONSISTENCY_PROBLEMS)):\n",
    "    for n in N_VALUES:\n",
    "        correct = sc_results[p_idx][n][\"correct\"]\n",
    "        color = '#34d399' if correct else '#f87171'\n",
    "        ax.scatter(n, p_idx * 5 - 5, color=color, s=30, alpha=0.5, zorder=2)\n",
    "\n",
    "ax.set_xlabel('Number of Chains (N)', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Self-Consistency: Accuracy vs Number of Chains', fontsize=13, fontweight='bold')\n",
    "ax.set_ylim(-5, 110)\n",
    "ax.set_xticks(N_VALUES)\n",
    "ax.legend(fontsize=11)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Annotate diminishing returns\n",
    "for i, (n, acc) in enumerate(zip(N_VALUES, accuracies)):\n",
    "    ax.annotate(f'{acc:.0f}%', (n, acc), textcoords='offset points',\n",
    "                xytext=(0, 12), ha='center', fontsize=10, color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\nAccuracy by N:\")\n",
    "for n, acc in zip(N_VALUES, accuracies):\n",
    "    print(f\"  N={n:>2}: {acc:.0f}%\")\n",
    "\n",
    "# Identify diminishing returns\n",
    "print(\"\\nDiminishing returns:\")\n",
    "for i in range(1, len(accuracies)):\n",
    "    improvement = accuracies[i] - accuracies[i-1]\n",
    "    print(f\"  N={N_VALUES[i-1]}->{N_VALUES[i]}: {improvement:+.0f}% improvement\")\n",
    "\n",
    "print(\"\\nKey insight: more chains help up to a point. The improvement from\")\n",
    "print(\"1 to 5 chains is typically large. The improvement from 10 to 20 is\")\n",
    "print(\"marginal. And the compute cost is linear — 20 chains cost 20x more.\")\n",
    "print(\"Self-consistency trades compute for reliability, with diminishing returns.\")\n",
    "```\n",
    "\n",
    "**Why this approach:** We generate all 20 chains once and take subsets, rather than re-generating for each N. This is both more efficient and scientifically cleaner — the same chains are used across conditions, isolating the effect of N rather than sampling noise.\n",
    "\n",
    "The accuracy curve should rise steeply from N=1 to N=5, then flatten. The intuition: if each chain has (say) 70% chance of being correct, going from 1 chain to 3 chains is a big jump. Going from 10 to 20 barely matters because the majority is already very likely correct.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** You measured the empirical relationship between the number of reasoning chains and accuracy. Self-consistency works because different chains make different errors. Majority voting averages out these independent errors.\n",
    "\n",
    "The diminishing returns are important: the lesson emphasized that \"more reasoning tokens are not always better.\" Each additional chain costs the same compute, but the marginal accuracy improvement shrinks. This is a concrete example of the test-time compute scaling tradeoff—you want to allocate compute where it matters, not uniformly.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 3: Process vs Outcome Evaluation (Supported)\n",
    "\n",
    "The lesson DEVELOPED process supervision from INTRODUCED: outcome evaluation asks \"did you get the right answer?\" while process evaluation asks \"is each step correct?\" In this exercise, you'll evaluate reasoning chains both ways and find cases where the outcome is correct but a step is wrong—the cancelling errors problem from the lesson.\n",
    "\n",
    "For 5 problems with known step-by-step solutions, you'll:\n",
    "1. Generate reasoning chains\n",
    "2. Check the final answer (outcome evaluation)\n",
    "3. Evaluate each step (process evaluation)\n",
    "4. Find discrepancies—correct answer with wrong reasoning\n",
    "\n",
    "**The first problem has a complete evaluation template.** You extend the pattern.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Will you find cases where the final answer is correct but a step is wrong?\n",
    "- Which type of problem is more likely to have cancelling errors?\n",
    "- If an ORM and PRM evaluated the same chain, when would they disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Problems with known step-by-step solutions ---\n",
    "# Each has the problem, correct answer, and the correct reasoning steps.\n",
    "# We'll use these to evaluate the model's chain step by step.\n",
    "\n",
    "EVAL_PROBLEMS = [\n",
    "    {\n",
    "        \"problem\": \"What is 17 x 24?\",\n",
    "        \"answer\": 408,\n",
    "        \"steps\": [\n",
    "            \"Break 24 into 20 + 4\",\n",
    "            \"17 x 20 = 340\",\n",
    "            \"17 x 4 = 68\",\n",
    "            \"340 + 68 = 408\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"A shirt costs $80. It's on sale for 25% off. Then you have a coupon for 10% off the sale price. What is the final price?\",\n",
    "        \"answer\": 54,\n",
    "        \"steps\": [\n",
    "            \"25% of $80 = $20\",\n",
    "            \"Sale price: $80 - $20 = $60\",\n",
    "            \"10% of $60 = $6\",\n",
    "            \"Final price: $60 - $6 = $54\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"A farmer has 4 fields, each with 6 rows of corn. Each row has 15 plants. He loses 10% of the plants to pests. How many plants survive?\",\n",
    "        \"answer\": 324,\n",
    "        \"steps\": [\n",
    "            \"Total rows: 4 x 6 = 24\",\n",
    "            \"Total plants: 24 x 15 = 360\",\n",
    "            \"Plants lost: 10% of 360 = 36\",\n",
    "            \"Plants surviving: 360 - 36 = 324\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"Three friends split a bill. The bill is $156. Alice pays twice as much as Bob, and Carol pays three times as much as Bob. How much does Alice pay?\",\n",
    "        \"answer\": 52,\n",
    "        \"steps\": [\n",
    "            \"Let Bob's share = x\",\n",
    "            \"Alice pays 2x, Carol pays 3x\",\n",
    "            \"x + 2x + 3x = 156\",\n",
    "            \"6x = 156, so x = 26\",\n",
    "            \"Alice pays 2 x 26 = $52\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"What is 13 x 17 + 8 x 9 - 45?\",\n",
    "        \"answer\": 248,\n",
    "        \"steps\": [\n",
    "            \"13 x 17 = 221\",\n",
    "            \"8 x 9 = 72\",\n",
    "            \"221 + 72 = 293\",\n",
    "            \"293 - 45 = 248\",\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(EVAL_PROBLEMS)} evaluation problems, each with step-by-step solutions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Step 1: First problem with full evaluation template ---\n# We generate multiple chains (using temperature > 0) and evaluate each one\n# both ways: outcome (final answer) and process (each step).\n#\n# NOTE: For process evaluation, we're using the LLM itself as a simplified\n# step evaluator. A real PRM would be a dedicated model trained specifically\n# on step-level correctness labels, as described in the lesson. Using an LLM\n# with a prompt is a practical proxy that lets us explore the concept, but\n# it is less reliable than a trained PRM and may miss subtle errors or\n# flag correct steps incorrectly.\n\np = EVAL_PROBLEMS[0]\nprint(f\"Problem: {p['problem']}\")\nprint(f\"Correct answer: {p['answer']}\")\nprint(f\"Correct steps: {p['steps']}\")\nprint()\n\n# Generate 5 chains\ncot_prompt = f\"{p['problem']}\\n\\nSolve this step by step. Show each calculation on its own line.\"\n\nchains = []\nfor i in range(5):\n    chain = call_llm(cot_prompt, temperature=0.9, max_tokens=500)\n    chains.append(chain)\n\n# Evaluate each chain\nfor i, chain in enumerate(chains):\n    final_answer = extract_number(chain)\n    outcome_correct = final_answer == p[\"answer\"]\n\n    # Process evaluation: use the LLM to check each step\n    process_prompt = f\"\"\"I'm checking a math solution for errors. Here's the problem and solution:\n\nProblem: {p['problem']}\nCorrect answer: {p['answer']}\nCorrect steps: {'; '.join(p['steps'])}\n\nStudent's solution:\n{chain}\n\nFor each step in the student's solution, respond with:\n- CORRECT if the step's math is right\n- WRONG if the step's math has an error (explain what's wrong)\n\nThen on the last line write: ALL_CORRECT or HAS_ERRORS\"\"\"\n\n    process_eval = call_llm(process_prompt, temperature=0.0, max_tokens=500)\n    process_correct = \"ALL_CORRECT\" in process_eval\n\n    # Classification\n    if outcome_correct and process_correct:\n        label = \"GOOD: correct answer, correct reasoning\"\n    elif outcome_correct and not process_correct:\n        label = \"DANGEROUS: correct answer, WRONG reasoning (cancelling errors!)\"\n    elif not outcome_correct and process_correct:\n        label = \"UNLUCKY: wrong answer, but reasoning was valid\"\n    else:\n        label = \"BAD: wrong answer, wrong reasoning\"\n\n    print(f\"\\nChain {i+1}: {label}\")\n    print(f\"  Final answer: {final_answer} ({'correct' if outcome_correct else 'WRONG'})\")\n    print(f\"  Process evaluation: {'all steps correct' if process_correct else 'HAS ERRORS'}\")\n    if outcome_correct and not process_correct:\n        print(f\"  >>> ORM would give +1 reward. PRM would penalize wrong steps.\")\n        print(f\"  >>> This chain would pass outcome evaluation but fail process evaluation.\")\n    print(f\"  Chain preview: {chain[:100]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Evaluate all 5 problems ---\n",
    "# TODO: Run the same outcome + process evaluation pattern on all 5 problems.\n",
    "# Generate 5 chains per problem. Track the counts of each category:\n",
    "#   - GOOD: correct answer + correct reasoning\n",
    "#   - DANGEROUS: correct answer + wrong reasoning\n",
    "#   - UNLUCKY: wrong answer + correct reasoning\n",
    "#   - BAD: wrong answer + wrong reasoning\n",
    "#\n",
    "# Store results in eval_summary:\n",
    "#   eval_summary[problem_index] = {\"good\": int, \"dangerous\": int, \"unlucky\": int, \"bad\": int}\n",
    "#\n",
    "# Follow the same pattern as Step 1:\n",
    "#   1. Generate 5 chains with temperature=0.9\n",
    "#   2. For each chain, check outcome (extract_number == answer)\n",
    "#   3. For each chain, check process (LLM-based step evaluation)\n",
    "#   4. Classify and count\n",
    "#\n",
    "# Hints:\n",
    "#   - Reuse the cot_prompt and process_prompt patterns from Step 1\n",
    "#   - The interesting finding is how many \"DANGEROUS\" cases you find\n",
    "#     (correct answer, wrong reasoning)\n",
    "#   - Print a running summary as you go\n",
    "\n",
    "eval_summary = {}\n",
    "\n",
    "# YOUR CODE HERE (25-40 lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Visualize the outcome vs process evaluation gap ---\n",
    "# TODO: Create a stacked bar chart showing the distribution of categories\n",
    "# for each problem.\n",
    "#\n",
    "# X-axis: problem index (or short description)\n",
    "# Y-axis: count (out of 5 chains)\n",
    "# Colors: good=#34d399, dangerous=#f59e0b, unlucky=#60a5fa, bad=#f87171\n",
    "#\n",
    "# Also compute and print:\n",
    "#   - Total outcome accuracy (chains where final answer is correct)\n",
    "#   - Total process accuracy (chains where all steps are correct)\n",
    "#   - The gap between them (outcome - process) = \"hidden reasoning failures\"\n",
    "#\n",
    "# Useful data: eval_summary[problem_index][\"good\"], [\"dangerous\"], etc.\n",
    "\n",
    "# YOUR CODE HERE (20-30 lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution for Steps 2 and 3</summary>\n",
    "\n",
    "**Step 2 — evaluate all problems:**\n",
    "\n",
    "```python\n",
    "eval_summary = {}\n",
    "\n",
    "for p_idx, p in enumerate(EVAL_PROBLEMS):\n",
    "    print(f\"\\nProblem {p_idx+1}: {p['problem'][:50]}...\")\n",
    "    print(f\"  Correct answer: {p['answer']}\")\n",
    "\n",
    "    counts = {\"good\": 0, \"dangerous\": 0, \"unlucky\": 0, \"bad\": 0}\n",
    "\n",
    "    cot_prompt = f\"{p['problem']}\\n\\nSolve this step by step. Show each calculation on its own line.\"\n",
    "\n",
    "    for i in range(5):\n",
    "        chain = call_llm(cot_prompt, temperature=0.9, max_tokens=500)\n",
    "        final_answer = extract_number(chain)\n",
    "        outcome_correct = final_answer == p[\"answer\"]\n",
    "\n",
    "        process_prompt = f\"\"\"I'm checking a math solution for errors. Here's the problem and solution:\n",
    "\n",
    "Problem: {p['problem']}\n",
    "Correct answer: {p['answer']}\n",
    "Correct steps: {'; '.join(p['steps'])}\n",
    "\n",
    "Student's solution:\n",
    "{chain}\n",
    "\n",
    "For each step in the student's solution, respond with:\n",
    "- CORRECT if the step's math is right\n",
    "- WRONG if the step's math has an error (explain what's wrong)\n",
    "\n",
    "Then on the last line write: ALL_CORRECT or HAS_ERRORS\"\"\"\n",
    "\n",
    "        process_eval = call_llm(process_prompt, temperature=0.0, max_tokens=500)\n",
    "        process_correct = \"ALL_CORRECT\" in process_eval\n",
    "\n",
    "        if outcome_correct and process_correct:\n",
    "            counts[\"good\"] += 1\n",
    "        elif outcome_correct and not process_correct:\n",
    "            counts[\"dangerous\"] += 1\n",
    "        elif not outcome_correct and process_correct:\n",
    "            counts[\"unlucky\"] += 1\n",
    "        else:\n",
    "            counts[\"bad\"] += 1\n",
    "\n",
    "    eval_summary[p_idx] = counts\n",
    "    print(f\"  Results: {counts}\")\n",
    "\n",
    "print(\"\\nAll problems evaluated.\")\n",
    "```\n",
    "\n",
    "**Step 3 — visualization:**\n",
    "\n",
    "```python\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "problems_short = [p[\"problem\"][:25] + \"...\" for p in EVAL_PROBLEMS]\n",
    "x = np.arange(len(EVAL_PROBLEMS))\n",
    "\n",
    "categories = [\"good\", \"dangerous\", \"unlucky\", \"bad\"]\n",
    "colors = {\"good\": \"#34d399\", \"dangerous\": \"#f59e0b\", \"unlucky\": \"#60a5fa\", \"bad\": \"#f87171\"}\n",
    "labels = {\"good\": \"Correct answer + correct reasoning\",\n",
    "          \"dangerous\": \"Correct answer + WRONG reasoning\",\n",
    "          \"unlucky\": \"Wrong answer + correct reasoning\",\n",
    "          \"bad\": \"Wrong answer + wrong reasoning\"}\n",
    "\n",
    "bottom = np.zeros(len(EVAL_PROBLEMS))\n",
    "for cat in categories:\n",
    "    values = [eval_summary[i][cat] for i in range(len(EVAL_PROBLEMS))]\n",
    "    ax.bar(x, values, bottom=bottom, label=labels[cat], color=colors[cat],\n",
    "           edgecolor='white', linewidth=0.5)\n",
    "    bottom += np.array(values)\n",
    "\n",
    "ax.set_xlabel('Problem', fontsize=11)\n",
    "ax.set_ylabel('Chains (out of 5)', fontsize=11)\n",
    "ax.set_title('Process vs Outcome Evaluation: Finding Hidden Failures', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"P{i+1}\" for i in range(len(EVAL_PROBLEMS))], fontsize=10)\n",
    "ax.legend(loc='upper right', fontsize=8)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute the gap\n",
    "total_chains = len(EVAL_PROBLEMS) * 5\n",
    "total_outcome_correct = sum(eval_summary[i][\"good\"] + eval_summary[i][\"dangerous\"]\n",
    "                            for i in range(len(EVAL_PROBLEMS)))\n",
    "total_process_correct = sum(eval_summary[i][\"good\"] + eval_summary[i][\"unlucky\"]\n",
    "                            for i in range(len(EVAL_PROBLEMS)))\n",
    "total_dangerous = sum(eval_summary[i][\"dangerous\"] for i in range(len(EVAL_PROBLEMS)))\n",
    "\n",
    "print(f\"\\nOverall statistics ({total_chains} chains total):\")\n",
    "print(f\"  Outcome accuracy (final answer correct): {total_outcome_correct}/{total_chains} ({total_outcome_correct/total_chains:.0%})\")\n",
    "print(f\"  Process accuracy (all steps correct):     {total_process_correct}/{total_chains} ({total_process_correct/total_chains:.0%})\")\n",
    "print(f\"  Hidden failures (correct answer, wrong reasoning): {total_dangerous}/{total_chains} ({total_dangerous/total_chains:.0%})\")\n",
    "print(f\"\\nThe gap between outcome and process accuracy represents reasoning\")\n",
    "print(f\"failures that an ORM would miss but a PRM would catch.\")\n",
    "print(f\"These are the 'cancelling errors' from the lesson: chains that get\")\n",
    "print(f\"lucky with the right answer despite wrong intermediate steps.\")\n",
    "```\n",
    "\n",
    "**Why this matters:** The \"dangerous\" category—correct answer with wrong reasoning—is exactly what an Outcome Reward Model (ORM) would miss. It would give these chains a positive reward, reinforcing wrong reasoning patterns. A Process Reward Model (PRM) would catch the errors at the step level, providing a richer training signal. This is the same proxy gap from reward hacking: outcome supervision is the proxy, process supervision is closer to the true objective.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** You evaluated reasoning chains two ways and found the gap between outcome accuracy and process accuracy. Chains that get the right answer with wrong reasoning are the \"cancelling errors\" problem from the lesson.\n",
    "\n",
    "The key insight: an ORM gives these chains the same reward as chains with correct reasoning. A PRM penalizes the wrong steps even when the final answer is right. This is why process supervision trains better reasoning—it's closer to the true objective (correct reasoning at every step) rather than the proxy (correct final answer).\n",
    "\n",
    "Connect this to the reward hacking pattern from RLHF & Alignment: outcome supervision is a proxy that can be \"gamed\" (accidentally, through cancelling errors). Process supervision is a tighter specification of what \"good reasoning\" means.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 4: Test-Time Compute Allocation (Independent)\n",
    "\n",
    "The lesson's paradigm shift: instead of building bigger models, let the model think longer. But how much longer? The lesson emphasized that \"more reasoning tokens are not always better\"—there are diminishing returns, and on simple problems, extra reasoning can actually hurt.\n",
    "\n",
    "**Your task:** Design an experiment to test adaptive vs uniform compute allocation.\n",
    "\n",
    "**Setup:**\n",
    "- You have a fixed compute budget: a total of N reasoning chains across all problems\n",
    "- You have a mix of easy and hard problems\n",
    "- Compare two strategies:\n",
    "  - **Equal allocation:** Same number of chains per problem (budget / num_problems)\n",
    "  - **Adaptive allocation:** More chains for harder problems, fewer for easy ones\n",
    "- Measure overall accuracy under each strategy\n",
    "\n",
    "**Think about:**\n",
    "- How will you define \"harder\"? (Number of steps? Past accuracy? Token count?)\n",
    "- What total budget makes the comparison interesting? (Too high and both strategies max out; too low and neither works)\n",
    "- How will you allocate chains adaptively? (proportional to difficulty? all-or-nothing?)\n",
    "\n",
    "**No skeleton is provided.** Design the experiment yourself. The solution is in the `<details>` block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiment here.\n",
    "#\n",
    "# 1. Define a set of problems with varying difficulty (or reuse PROBLEMS)\n",
    "# 2. Choose a fixed total compute budget (total chains across all problems)\n",
    "# 3. Implement equal allocation: budget / num_problems chains per problem\n",
    "# 4. Implement adaptive allocation: more chains for harder problems\n",
    "# 5. Measure accuracy under both strategies\n",
    "# 6. Plot and compare\n",
    "#\n",
    "# Hint: You can reuse the majority_vote() function and the call_llm() helper.\n",
    "# Use temperature > 0 for diverse chains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reflection:\n",
    "#\n",
    "# 1. Did adaptive allocation outperform equal allocation?\n",
    "# 2. By how much? Was the difference large or marginal?\n",
    "# 3. What was the key factor: spending MORE on hard problems, or spending LESS on easy ones?\n",
    "# 4. How does this connect to the lesson's \"bigger brain vs more thinking time\" analogy?\n",
    "#\n",
    "# Print your observations:\n",
    "print(\"Reflection:\")\n",
    "print(\"  1. Adaptive vs equal: ...\")\n",
    "print(\"  2. Magnitude: ...\")\n",
    "print(\"  3. Key factor: ...\")\n",
    "print(\"  4. Connection to lesson: ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Design rationale:** Use the PROBLEMS list (which has easy, medium, and hard problems). Set a total budget that's tight enough to force allocation tradeoffs. Use the number of reasoning steps as a proxy for difficulty.\n",
    "\n",
    "```python\n",
    "# --- Test-Time Compute Allocation Experiment ---\n",
    "\n",
    "# Use a subset of problems with clear difficulty variation\n",
    "ALLOCATION_PROBLEMS = [\n",
    "    # Easy (1-2 steps)\n",
    "    (\"simple multiply\", \"What is 8 x 7?\", 56, 1),\n",
    "    (\"two-digit addition\", \"What is 47 + 86?\", 133, 1),\n",
    "    (\"simple word problem\",\n",
    "     \"A store has 3 shelves with 8 books each. They remove 5 books. How many?\",\n",
    "     19, 2),\n",
    "    # Hard (3-4 steps)\n",
    "    (\"chained operations\", \"What is 13 x 17 + 8 x 9 - 45?\", 248, 4),\n",
    "    (\"percentage reasoning\",\n",
    "     \"A shirt costs $80. It's 25% off, then 10% off the sale price. Final price?\",\n",
    "     54, 3),\n",
    "    (\"ratio word problem\",\n",
    "     \"Bill is $156. Alice pays 2x Bob, Carol pays 3x Bob. How much does Alice pay?\",\n",
    "     52, 4),\n",
    "]\n",
    "\n",
    "NUM_PROBLEMS = len(ALLOCATION_PROBLEMS)\n",
    "TOTAL_BUDGET = 30  # Total chains across all problems\n",
    "\n",
    "# --- Strategy 1: Equal allocation ---\n",
    "chains_per_problem_equal = TOTAL_BUDGET // NUM_PROBLEMS  # 5 each\n",
    "\n",
    "print(f\"Total budget: {TOTAL_BUDGET} chains across {NUM_PROBLEMS} problems\")\n",
    "print(f\"\\n--- EQUAL ALLOCATION: {chains_per_problem_equal} chains per problem ---\")\n",
    "\n",
    "equal_results = []\n",
    "for desc, problem, answer, steps in ALLOCATION_PROBLEMS:\n",
    "    cot_prompt = f\"{problem}\\n\\nLet's work through this step by step.\"\n",
    "    answers = []\n",
    "    for _ in range(chains_per_problem_equal):\n",
    "        chain = call_llm(cot_prompt, temperature=0.7, max_tokens=500)\n",
    "        answers.append(extract_number(chain))\n",
    "\n",
    "    vote = majority_vote(answers)\n",
    "    correct = vote == answer\n",
    "    equal_results.append(correct)\n",
    "    sym = \"\\u2713\" if correct else \"\\u2717\"\n",
    "    print(f\"  {desc:<25} | chains={chains_per_problem_equal} | vote={vote} | {sym}\")\n",
    "\n",
    "equal_accuracy = sum(equal_results) / len(equal_results)\n",
    "print(f\"  Equal accuracy: {equal_accuracy:.0%}\")\n",
    "\n",
    "# --- Strategy 2: Adaptive allocation ---\n",
    "# Allocate proportional to number of steps (difficulty proxy)\n",
    "total_steps = sum(s for _, _, _, s in ALLOCATION_PROBLEMS)\n",
    "adaptive_chains = []\n",
    "for _, _, _, steps in ALLOCATION_PROBLEMS:\n",
    "    # Allocate proportional to steps, minimum 1\n",
    "    alloc = max(1, round(TOTAL_BUDGET * steps / total_steps))\n",
    "    adaptive_chains.append(alloc)\n",
    "\n",
    "# Adjust to hit exact budget\n",
    "while sum(adaptive_chains) > TOTAL_BUDGET:\n",
    "    # Remove from the problem with the most allocation\n",
    "    max_idx = adaptive_chains.index(max(adaptive_chains))\n",
    "    adaptive_chains[max_idx] -= 1\n",
    "while sum(adaptive_chains) < TOTAL_BUDGET:\n",
    "    # Add to the problem with the most steps\n",
    "    max_steps_idx = max(range(NUM_PROBLEMS), key=lambda i: ALLOCATION_PROBLEMS[i][3])\n",
    "    adaptive_chains[max_steps_idx] += 1\n",
    "\n",
    "print(f\"\\n--- ADAPTIVE ALLOCATION ---\")\n",
    "print(f\"  Allocation: {adaptive_chains} (total: {sum(adaptive_chains)})\")\n",
    "\n",
    "adaptive_results = []\n",
    "for i, (desc, problem, answer, steps) in enumerate(ALLOCATION_PROBLEMS):\n",
    "    n_chains = adaptive_chains[i]\n",
    "    cot_prompt = f\"{problem}\\n\\nLet's work through this step by step.\"\n",
    "    answers = []\n",
    "    for _ in range(n_chains):\n",
    "        chain = call_llm(cot_prompt, temperature=0.7, max_tokens=500)\n",
    "        answers.append(extract_number(chain))\n",
    "\n",
    "    vote = majority_vote(answers)\n",
    "    correct = vote == answer\n",
    "    adaptive_results.append(correct)\n",
    "    sym = \"\\u2713\" if correct else \"\\u2717\"\n",
    "    print(f\"  {desc:<25} | chains={n_chains:>2} | vote={vote} | {sym}\")\n",
    "\n",
    "adaptive_accuracy = sum(adaptive_results) / len(adaptive_results)\n",
    "print(f\"  Adaptive accuracy: {adaptive_accuracy:.0%}\")\n",
    "\n",
    "# --- Comparison ---\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"RESULTS (budget = {TOTAL_BUDGET} chains)\")\n",
    "print(f\"  Equal allocation:    {equal_accuracy:.0%}\")\n",
    "print(f\"  Adaptive allocation: {adaptive_accuracy:.0%}\")\n",
    "print(f\"  Difference:          {adaptive_accuracy - equal_accuracy:+.0%}\")\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: chain allocation comparison\n",
    "x = np.arange(NUM_PROBLEMS)\n",
    "width = 0.35\n",
    "short_descs = [d[:12] for d, _, _, _ in ALLOCATION_PROBLEMS]\n",
    "\n",
    "ax1.bar(x - width/2, [chains_per_problem_equal] * NUM_PROBLEMS, width,\n",
    "        label='Equal', color='#f59e0b', edgecolor='white', linewidth=0.5)\n",
    "ax1.bar(x + width/2, adaptive_chains, width,\n",
    "        label='Adaptive', color='#a78bfa', edgecolor='white', linewidth=0.5)\n",
    "ax1.set_xlabel('Problem', fontsize=11)\n",
    "ax1.set_ylabel('Chains Allocated', fontsize=11)\n",
    "ax1.set_title('Compute Allocation per Problem', fontsize=12, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(short_descs, fontsize=8, rotation=30, ha='right')\n",
    "ax1.legend()\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "# Right: accuracy comparison\n",
    "strategies = ['Equal', 'Adaptive']\n",
    "accs = [equal_accuracy * 100, adaptive_accuracy * 100]\n",
    "bars = ax2.bar(strategies, accs, color=['#f59e0b', '#a78bfa'],\n",
    "               edgecolor='white', linewidth=0.5)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=11)\n",
    "ax2.set_title('Overall Accuracy: Same Budget, Different Allocation', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylim(0, 115)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "for bar, acc in zip(bars, accs):\n",
    "    ax2.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 2,\n",
    "             f'{acc:.0f}%', ha='center', fontsize=12, color='white')\n",
    "\n",
    "plt.suptitle(f'Test-Time Compute Allocation (Budget: {TOTAL_BUDGET} chains)',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: adaptive allocation outperforms equal allocation because\")\n",
    "print(\"easy problems don't need many chains (they're correct even with 1-2),\")\n",
    "print(\"while hard problems benefit significantly from more chains. This is\")\n",
    "print(\"the core of test-time compute scaling: allocate compute based on problem\")\n",
    "print(\"difficulty, not uniformly. Same total compute, better overall accuracy.\")\n",
    "```\n",
    "\n",
    "**Expected findings:** Adaptive allocation should outperform equal allocation because easy problems are already correct with 1-2 chains (the extra chains are wasted), while hard problems benefit significantly from more chains (self-consistency from Exercise 2). The improvement comes from *redistributing* compute, not adding more.\n",
    "\n",
    "**Connection to the lesson:** This is test-time compute scaling in practice. The paradigm shift is not just \"think longer\" but \"think longer *where it matters*.\" A reasoning model that allocates variable compute per problem (longer chains for harder problems, shorter for easy ones) outperforms one that uses the same compute for every problem. The \"bigger brain vs more thinking time\" analogy: you don't need to think for 30 minutes about what 8 x 7 is, but you do for a multi-step word problem.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **RL training produces consistently better reasoning, not just longer chains.** The reasoning model outperformed the base model + CoT on the same problems—same architecture, same parameter count, different training. RL shaped how the model uses the scratchpad (context window), not what it knows.\n",
    "\n",
    "2. **Self-consistency trades compute for reliability, with diminishing returns.** Going from 1 to 5 chains is a large accuracy improvement. Going from 10 to 20 is marginal. The compute cost is linear, but the benefit curve flattens. This is why \"more reasoning tokens are not always better.\"\n",
    "\n",
    "3. **Outcome evaluation misses reasoning flaws that process evaluation catches.** Chains can arrive at the correct answer through wrong reasoning (cancelling errors). An ORM would reward these chains; a PRM would penalize the wrong steps. The gap between outcome accuracy and process accuracy represents hidden failures.\n",
    "\n",
    "4. **Adaptive compute allocation outperforms uniform allocation.** Easy problems don't need many chains; hard problems benefit from more. Same total compute, better overall accuracy. This is test-time compute scaling in practice: allocate compute based on problem difficulty, not uniformly.\n",
    "\n",
    "5. **The paradigm shift: from \"how big is the model?\" to \"how much does the model think?\"** Model size and inference compute are two independent axes of scaling. These exercises demonstrated the inference compute axis empirically—self-consistency, process supervision, and adaptive allocation are all mechanisms for trading inference compute for better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}