{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Context Learning\n",
    "\n",
    "In this notebook, you'll explore in-context learning (ICL) empirically — how transformers learn new tasks from examples in the prompt without any weight update, and where that ability breaks down.\n",
    "\n",
    "**What you'll do:**\n",
    "- Compare zero-shot and few-shot classification on the same sentiment task, measuring how examples in the prompt change model behavior\n",
    "- Create novel mappings the model has never seen in training data, testing where ICL generalizes and where it fails\n",
    "- Run a controlled experiment on example ordering, measuring the accuracy swings that reveal attention-based (not comprehension-based) behavior\n",
    "- Compare ICL directly against a finetuned classifier on the same task, discovering when each approach wins\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones — they reveal gaps in your mental model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup — self-contained for Google Colab\n!pip install -q openai\n\nimport os\nimport textwrap\nimport random\nfrom openai import OpenAI\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# --- API Key Setup ---\n# Option 1: Set your API key as an environment variable (recommended)\n#   In Colab: go to the key icon in the left sidebar, add OPENAI_API_KEY\n# Option 2: Paste it directly (less secure, don't commit this)\n#   os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n# You can also use any OpenAI-compatible API (e.g., local Ollama, Together AI)\n# by changing the base_url:\n#   client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n\nclient = OpenAI()\n\n# Use a small, cheap model for the exercises\nMODEL = \"gpt-4o-mini\"\n\n# Nice plots\nplt.style.use('dark_background')\nplt.rcParams['figure.figsize'] = [10, 4]\n\n# Reproducible results where possible\nrandom.seed(42)\nnp.random.seed(42)\n\n\ndef call_llm(prompt: str, temperature: float = 0.0) -> str:\n    \"\"\"Call the LLM with a single prompt (completion-style). Returns the response text.\"\"\"\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=temperature,\n        max_tokens=100,\n    )\n    return response.choices[0].message.content.strip()\n\n\ndef call_llm_with_system(system_prompt: str, user_prompt: str, temperature: float = 0.0) -> str:\n    \"\"\"Call the LLM with a system prompt and user prompt. Returns the response text.\"\"\"\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ],\n        temperature=temperature,\n        max_tokens=100,\n    )\n    return response.choices[0].message.content.strip()\n\n\ndef print_wrapped(text: str, width: int = 80, prefix: str = \"\"):\n    \"\"\"Print text with word wrapping for readability.\"\"\"\n    for line in text.split(\"\\n\"):\n        wrapped = textwrap.fill(line, width=width, initial_indent=prefix, subsequent_indent=prefix)\n        print(wrapped)\n\n\n# Quick test to verify the API is working\ntest = call_llm(\"Say 'API connection successful' and nothing else.\")\nprint(test)\nprint(f\"\\nUsing model: {MODEL}\")\nprint(\"Setup complete.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Data\n",
    "\n",
    "We'll use a common set of sentiment examples across multiple exercises. These are simple, unambiguous movie reviews — the task itself is easy, so we can focus on how ICL behavior changes with different prompting strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Shared sentiment data ---\n",
    "# 5 examples we'll use as few-shot demonstrations\n",
    "FEW_SHOT_EXAMPLES = [\n",
    "    (\"This movie was absolutely amazing, I loved every minute\", \"Positive\"),\n",
    "    (\"Terrible film, complete waste of time and money\", \"Negative\"),\n",
    "    (\"A beautiful and moving story that touched my heart\", \"Positive\"),\n",
    "    (\"The acting was awful and the plot made no sense\", \"Negative\"),\n",
    "    (\"One of the best films I have seen this year\", \"Positive\"),\n",
    "]\n",
    "\n",
    "# 10 test examples with ground truth labels\n",
    "TEST_EXAMPLES = [\n",
    "    (\"The cinematography was stunning and the soundtrack was perfect\", \"Positive\"),\n",
    "    (\"I fell asleep halfway through, incredibly boring\", \"Negative\"),\n",
    "    (\"A masterpiece of modern cinema\", \"Positive\"),\n",
    "    (\"The worst movie I have ever seen\", \"Negative\"),\n",
    "    (\"Heartwarming and funny, great for the whole family\", \"Positive\"),\n",
    "    (\"Predictable plot with flat characters\", \"Negative\"),\n",
    "    (\"An unforgettable experience that left me speechless\", \"Positive\"),\n",
    "    (\"Dull, lifeless, and painfully long\", \"Negative\"),\n",
    "    (\"Brilliant performances from the entire cast\", \"Positive\"),\n",
    "    (\"A complete disappointment from start to finish\", \"Negative\"),\n",
    "]\n",
    "\n",
    "\n",
    "def build_few_shot_prompt(examples: list[tuple[str, str]], test_review: str) -> str:\n",
    "    \"\"\"Build a few-shot prompt from example pairs and a test review.\"\"\"\n",
    "    lines = []\n",
    "    for review, label in examples:\n",
    "        lines.append(f'Review: \"{review}\" -> {label}')\n",
    "    lines.append(f'Review: \"{test_review}\" ->')\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def extract_sentiment(response: str) -> str:\n",
    "    \"\"\"Extract 'Positive' or 'Negative' from a model response.\"\"\"\n",
    "    response_lower = response.lower().strip()\n",
    "    if \"positive\" in response_lower:\n",
    "        return \"Positive\"\n",
    "    if \"negative\" in response_lower:\n",
    "        return \"Negative\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def compute_accuracy(predictions: list[str], ground_truth: list[str]) -> float:\n",
    "    \"\"\"Compute accuracy as fraction of correct predictions.\"\"\"\n",
    "    correct = sum(1 for p, g in zip(predictions, ground_truth) if p == g)\n",
    "    return correct / len(ground_truth)\n",
    "\n",
    "\n",
    "print(f\"Few-shot examples: {len(FEW_SHOT_EXAMPLES)}\")\n",
    "print(f\"Test examples: {len(TEST_EXAMPLES)}\")\n",
    "print(\"\\nSample few-shot prompt:\")\n",
    "print(build_few_shot_prompt(FEW_SHOT_EXAMPLES[:3], TEST_EXAMPLES[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Zero-Shot vs Few-Shot Comparison (Guided)\n",
    "\n",
    "The lesson showed a few-shot sentiment prompt and claimed that examples in the prompt genuinely change model behavior. Let's measure that claim.\n",
    "\n",
    "You'll classify the same 10 reviews twice:\n",
    "1. **Zero-shot:** Just the instruction, no examples\n",
    "2. **Few-shot (3 examples):** Three input-output pairs before the test input\n",
    "\n",
    "**Before running, predict:**\n",
    "- Will the zero-shot approach get any wrong? Sentiment is a simple task — maybe zero-shot is already good enough?\n",
    "- If few-shot is better, by how much? 5%? 20%? 50%?\n",
    "- Will there be any reviews where zero-shot succeeds but few-shot fails?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Zero-shot classification ---\n",
    "# The model gets only an instruction, no examples.\n",
    "\n",
    "ZERO_SHOT_TEMPLATE = (\n",
    "    'Classify the sentiment of the following movie review as Positive or Negative.\\n'\n",
    "    'Respond with a single word: Positive or Negative.\\n\\n'\n",
    "    'Review: \"{review}\"\\n'\n",
    "    'Sentiment:'\n",
    ")\n",
    "\n",
    "ground_truth = [label for _, label in TEST_EXAMPLES]\n",
    "\n",
    "# Zero-shot predictions\n",
    "print(\"ZERO-SHOT CLASSIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "zero_shot_preds = []\n",
    "for review, true_label in TEST_EXAMPLES:\n",
    "    prompt = ZERO_SHOT_TEMPLATE.format(review=review)\n",
    "    raw_response = call_llm(prompt)\n",
    "    pred = extract_sentiment(raw_response)\n",
    "    zero_shot_preds.append(pred)\n",
    "    match = \"correct\" if pred == true_label else \"WRONG\"\n",
    "    print(f\"  [{match:>7}] {true_label:>8} | {pred:>8} | {review[:50]}...\")\n",
    "\n",
    "zero_shot_acc = compute_accuracy(zero_shot_preds, ground_truth)\n",
    "print(f\"\\nZero-shot accuracy: {zero_shot_acc:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Few-shot classification (3 examples) ---\n",
    "# The model sees 3 labeled examples before each test input.\n",
    "# Same model, same weights. The only difference: examples in the prompt.\n",
    "\n",
    "few_shot_examples = FEW_SHOT_EXAMPLES[:3]  # Use first 3 examples\n",
    "\n",
    "print(\"FEW-SHOT CLASSIFICATION (3 examples)\")\n",
    "print(\"=\" * 60)\n",
    "few_shot_preds = []\n",
    "for review, true_label in TEST_EXAMPLES:\n",
    "    prompt = build_few_shot_prompt(few_shot_examples, review)\n",
    "    raw_response = call_llm(prompt)\n",
    "    pred = extract_sentiment(raw_response)\n",
    "    few_shot_preds.append(pred)\n",
    "    match = \"correct\" if pred == true_label else \"WRONG\"\n",
    "    print(f\"  [{match:>7}] {true_label:>8} | {pred:>8} | {review[:50]}...\")\n",
    "\n",
    "few_shot_acc = compute_accuracy(few_shot_preds, ground_truth)\n",
    "print(f\"\\nFew-shot accuracy: {few_shot_acc:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare the two approaches ---\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "labels = [\"Zero-Shot\\n(instruction only)\", \"Few-Shot\\n(3 examples)\"]\n",
    "accs = [zero_shot_acc * 100, few_shot_acc * 100]\n",
    "colors = [\"#f59e0b\", \"#6366f1\"]\n",
    "\n",
    "bars = ax.bar(labels, accs, color=colors, edgecolor=\"white\", linewidth=0.5, width=0.5)\n",
    "for bar, val in zip(bars, accs):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,\n",
    "        f\"{val:.0f}%\", ha=\"center\", va=\"bottom\", fontsize=14, fontweight=\"bold\", color=\"white\",\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "ax.set_title(\"Zero-Shot vs Few-Shot: Same Model, Same Task\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_ylim(0, 110)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "diff = few_shot_acc - zero_shot_acc\n",
    "print(f\"Difference: {diff:+.0%}\")\n",
    "print(f\"\\nThe model's weights did not change between these two runs.\")\n",
    "print(f\"No gradients. No optimizer. No training. The only difference\")\n",
    "print(f\"is what's in the prompt.\")\n",
    "print(f\"\\nIf the accuracy is similar: sentiment is 'easy enough' that the\")\n",
    "print(f\"model's pretraining is sufficient. The examples help less when the\")\n",
    "print(f\"task is already well-represented in training data.\")\n",
    "print(f\"\\nIf few-shot is clearly better: the examples provided a retrieval\")\n",
    "print(f\"structure — the model's attention matched the test input against\")\n",
    "print(f\"example inputs and retrieved the output pattern.\")\n",
    "print(f\"\\nEither result is informative. The question is not 'which is better'\")\n",
    "print(f\"but 'what do the examples change about the model's computation?'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** You measured the effect of in-context learning on a simple task. The model's weights are frozen — no gradients, no optimizer, no training loop. The only difference between zero-shot and few-shot is the content of the prompt.\n",
    "\n",
    "The lesson explained the mechanism: when examples are in the prompt, the test input's query vectors attend to the example inputs (structural matching via QK) and retrieve the output pattern through the value vectors. The few-shot examples create a retrieval structure in the attention weights.\n",
    "\n",
    "For a well-known task like sentiment classification, the improvement may be modest — the model already \"knows\" sentiment from pretraining. The real power of ICL shows on novel tasks. That's Exercise 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Novel Task ICL (Supported)\n",
    "\n",
    "The lesson argued that ICL is more than memorized pattern matching — it can generalize to tasks the model has never seen in training data. The novel symbol mapping (\"sdag\" -> \"happy\") was the proof.\n",
    "\n",
    "In this exercise, you'll test that claim yourself. You'll create made-up mappings of increasing complexity and see where ICL succeeds and where it fails. The boundary between success and failure reveals what attention can compute in a single forward pass.\n",
    "\n",
    "Fill in the TODOs below. Each TODO is 1-3 lines.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "For your custom mappings, think about functions of increasing complexity:\n",
    "- A mapping the model could plausibly pattern-match (e.g., first letter -> a word starting with that letter)\n",
    "- A mapping that requires a simple transformation (e.g., reverse the word, count the letters)\n",
    "- A mapping that is arbitrary and has no learnable pattern from the examples alone\n",
    "\n",
    "ICL should succeed on the first two (attention can match the structural pattern) and struggle on the third (arbitrary mappings require memorization, not computation).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mapping 1 (provided): Made-up words -> emotions ---\n",
    "# This mapping is arbitrary but the OUTPUT domain is familiar (emotions).\n",
    "# The model can't have seen these nonsense words mapped to emotions in training.\n",
    "\n",
    "mapping_1 = {\n",
    "    \"name\": \"Nonsense words -> emotions\",\n",
    "    \"examples\": [\n",
    "        (\"sdag\", \"happy\"),\n",
    "        (\"trel\", \"sad\"),\n",
    "        (\"blix\", \"angry\"),\n",
    "        (\"worp\", \"scared\"),\n",
    "    ],\n",
    "    \"test_input\": \"frum\",\n",
    "    \"expected_behavior\": \"Should produce an emotion word (the specific emotion doesn't matter — the model should follow the PATTERN of nonsense->emotion)\",\n",
    "}\n",
    "\n",
    "\n",
    "def test_mapping(mapping: dict) -> str:\n",
    "    \"\"\"Test an ICL mapping. Returns the model's response.\"\"\"\n",
    "    lines = []\n",
    "    for inp, out in mapping[\"examples\"]:\n",
    "        lines.append(f\"{inp} -> {out}\")\n",
    "    lines.append(f\"{mapping['test_input']} ->\")\n",
    "    prompt = \"\\n\".join(lines)\n",
    "    return call_llm(prompt)\n",
    "\n",
    "\n",
    "# Test mapping 1\n",
    "print(f\"MAPPING 1: {mapping_1['name']}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Prompt:\")\n",
    "for inp, out in mapping_1[\"examples\"]:\n",
    "    print(f\"  {inp} -> {out}\")\n",
    "print(f\"  {mapping_1['test_input']} -> ???\")\n",
    "print(f\"\\nExpected: {mapping_1['expected_behavior']}\")\n",
    "\n",
    "result_1 = test_mapping(mapping_1)\n",
    "print(f\"Model output: {result_1}\")\n",
    "print(f\"\\nDid it follow the pattern? The model has never seen 'sdag' mapped to\")\n",
    "print(f\"'happy' in training data. If it produced an emotion word, ICL worked\")\n",
    "print(f\"on a truly novel mapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mapping 2: A learnable function ---\n",
    "# TODO: Create a mapping where the output is a COMPUTABLE FUNCTION of the input.\n",
    "# Examples: word -> its length as a digit, word -> first letter repeated,\n",
    "#           word -> the word reversed, number -> number * 2, etc.\n",
    "#\n",
    "# Choose a function where you can verify the answer is correct.\n",
    "\n",
    "# TODO: Fill in the mapping dict. Provide 4 examples and 1 test input.\n",
    "# YOUR CODE HERE (6-10 lines)\n",
    "mapping_2 = {\n",
    "    \"name\": \"\",  # Give your mapping a descriptive name\n",
    "    \"examples\": [\n",
    "        # (input, output), ...\n",
    "    ],\n",
    "    \"test_input\": \"\",\n",
    "    \"expected_output\": \"\",  # What the correct answer should be\n",
    "    \"expected_behavior\": \"\",  # Describe what you expect\n",
    "}\n",
    "\n",
    "# Test it\n",
    "print(f\"MAPPING 2: {mapping_2['name']}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Prompt:\")\n",
    "for inp, out in mapping_2[\"examples\"]:\n",
    "    print(f\"  {inp} -> {out}\")\n",
    "print(f\"  {mapping_2['test_input']} -> ???\")\n",
    "print(f\"\\nExpected output: {mapping_2['expected_output']}\")\n",
    "print(f\"Expected behavior: {mapping_2['expected_behavior']}\")\n",
    "\n",
    "result_2 = test_mapping(mapping_2)\n",
    "print(f\"Model output: {result_2}\")\n",
    "\n",
    "correct = mapping_2[\"expected_output\"].lower() in result_2.lower()\n",
    "print(f\"\\nCorrect? {'Yes' if correct else 'No'}\")\n",
    "if correct:\n",
    "    print(\"The model learned a computable function from 4 examples — no weight updates.\")\n",
    "else:\n",
    "    print(\"The model did not get the expected output. What does this tell you\")\n",
    "    print(\"about the limits of what attention can compute in a single forward pass?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mapping 3: A harder or arbitrary mapping ---\n",
    "# TODO: Create a mapping that is HARDER for ICL.\n",
    "# Ideas:\n",
    "#   - A completely arbitrary mapping (no learnable pattern)\n",
    "#   - A function that requires multi-step reasoning\n",
    "#   - A mapping where the output domain is unfamiliar\n",
    "#\n",
    "# The goal is to find a mapping where ICL FAILS or produces inconsistent results.\n",
    "# Finding the boundary is more informative than finding another success.\n",
    "\n",
    "# TODO: Fill in the mapping dict. Provide 4-5 examples and 1 test input.\n",
    "# YOUR CODE HERE (6-10 lines)\n",
    "mapping_3 = {\n",
    "    \"name\": \"\",  # Give your mapping a descriptive name\n",
    "    \"examples\": [\n",
    "        # (input, output), ...\n",
    "    ],\n",
    "    \"test_input\": \"\",\n",
    "    \"expected_output\": \"\",  # What the correct answer should be (or \"any X\" if pattern-based)\n",
    "    \"expected_behavior\": \"\",  # What you think will happen\n",
    "}\n",
    "\n",
    "# Test it\n",
    "print(f\"MAPPING 3: {mapping_3['name']}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Prompt:\")\n",
    "for inp, out in mapping_3[\"examples\"]:\n",
    "    print(f\"  {inp} -> {out}\")\n",
    "print(f\"  {mapping_3['test_input']} -> ???\")\n",
    "print(f\"\\nExpected: {mapping_3['expected_behavior']}\")\n",
    "\n",
    "result_3 = test_mapping(mapping_3)\n",
    "print(f\"Model output: {result_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summary of all three mappings ---\n",
    "\n",
    "print(\"NOVEL TASK ICL: SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Mapping 1 ({mapping_1['name']}): {result_1}\")\n",
    "print(f\"  Mapping 2 ({mapping_2['name']}): {result_2}\")\n",
    "print(f\"  Mapping 3 ({mapping_3['name']}): {result_3}\")\n",
    "print()\n",
    "print(\"What this tells you about ICL:\")\n",
    "print(\"- Mapping 1 (arbitrary but familiar output domain): ICL can follow\")\n",
    "print(\"  the pattern even when the input is nonsense, because the output\")\n",
    "print(\"  domain (emotions) is well-represented in pretraining.\")\n",
    "print(\"- Mapping 2 (computable function): ICL can learn simple functions\")\n",
    "print(\"  from examples — the attention mechanism computes the transformation.\")\n",
    "print(\"- Mapping 3 (harder / arbitrary): ICL struggles when the mapping\")\n",
    "print(\"  requires multi-step reasoning or has no learnable pattern.\")\n",
    "print(\"\\nThe boundary: ICL works within the scope of what attention can\")\n",
    "print(\"compute in a single forward pass. Simple structural patterns and\")\n",
    "print(\"familiar output domains succeed. Complex multi-step reasoning or\")\n",
    "print(\"truly arbitrary mappings fail. This is consistent with the lesson's\")\n",
    "print(\"framing: ICL is attention-based computation, not comprehension.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Why the mapping complexity matters:** ICL works because attention creates retrieval patterns — the test input's queries match example inputs' keys, and the values carry the output pattern. This mechanism can compute structural transformations (reverse a word, count letters) but cannot compute arbitrary lookups (random input-output pairs with no pattern).\n",
    "\n",
    "**Example Mapping 2 (computable function — word reversal):**\n",
    "```python\n",
    "mapping_2 = {\n",
    "    \"name\": \"Word reversal\",\n",
    "    \"examples\": [\n",
    "        (\"cat\", \"tac\"),\n",
    "        (\"dog\", \"god\"),\n",
    "        (\"star\", \"rats\"),\n",
    "        (\"loop\", \"pool\"),\n",
    "    ],\n",
    "    \"test_input\": \"hello\",\n",
    "    \"expected_output\": \"olleh\",\n",
    "    \"expected_behavior\": \"Should output 'olleh' — the model can learn reversal from examples\",\n",
    "}\n",
    "```\n",
    "\n",
    "**Example Mapping 3 (arbitrary — random code mapping):**\n",
    "```python\n",
    "mapping_3 = {\n",
    "    \"name\": \"Arbitrary code (no pattern)\",\n",
    "    \"examples\": [\n",
    "        (\"apple\", \"7X\"),\n",
    "        (\"banana\", \"3Q\"),\n",
    "        (\"cherry\", \"9M\"),\n",
    "        (\"date\", \"1F\"),\n",
    "        (\"elderberry\", \"5R\"),\n",
    "    ],\n",
    "    \"test_input\": \"fig\",\n",
    "    \"expected_output\": \"no correct answer possible\",\n",
    "    \"expected_behavior\": \"Should fail or produce a plausible-looking but meaningless output. There is no pattern to learn — each mapping is arbitrary.\",\n",
    "}\n",
    "```\n",
    "\n",
    "**Common finding:** The model often succeeds on word reversal (a computable structural transformation) and produces a plausible-looking but incorrect output for the arbitrary mapping (e.g., \"2K\" — it follows the format but cannot know the correct answer). This is the boundary: ICL computes, it does not memorize arbitrary mappings.\n",
    "\n",
    "**Alternative Mapping 2 (letter counting):**\n",
    "```python\n",
    "mapping_2 = {\n",
    "    \"name\": \"Letter counting\",\n",
    "    \"examples\": [\n",
    "        (\"cat\", \"3\"),\n",
    "        (\"hello\", \"5\"),\n",
    "        (\"a\", \"1\"),\n",
    "        (\"elephant\", \"8\"),\n",
    "    ],\n",
    "    \"test_input\": \"python\",\n",
    "    \"expected_output\": \"6\",\n",
    "    \"expected_behavior\": \"Should output '6' — counting letters is a simple computable function\",\n",
    "}\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Ordering Sensitivity Experiment (Supported)\n",
    "\n",
    "The lesson claimed that reordering the same few-shot examples can swing accuracy by 20-30 percentage points. That is a strong claim. If the model truly \"understood\" the task, order would not matter — the same examples convey the same information regardless of arrangement.\n",
    "\n",
    "In this exercise, you'll run a controlled experiment: same 5 examples, same 10 test inputs, 5 different orderings. You'll measure accuracy per ordering and plot the results.\n",
    "\n",
    "Fill in the TODOs below. Each TODO is 1-3 lines.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Generate 5 random permutations of the `FEW_SHOT_EXAMPLES` list. For each permutation, classify all 10 test examples and compute accuracy. The key insight is whether accuracy varies significantly across orderings — if it does, the model's behavior depends on surface features (position, recency) rather than task comprehension.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate 5 distinct orderings of the same 5 examples ---\n",
    "\n",
    "# We use a fixed seed for reproducibility but generate genuine permutations\n",
    "rng = random.Random(42)\n",
    "\n",
    "orderings = []\n",
    "seen = set()\n",
    "base = list(range(len(FEW_SHOT_EXAMPLES)))\n",
    "\n",
    "while len(orderings) < 5:\n",
    "    perm = base[:]\n",
    "    rng.shuffle(perm)\n",
    "    perm_tuple = tuple(perm)\n",
    "    if perm_tuple not in seen:\n",
    "        seen.add(perm_tuple)\n",
    "        orderings.append(perm)\n",
    "\n",
    "print(\"5 orderings of the same 5 examples:\")\n",
    "for i, order in enumerate(orderings):\n",
    "    labels = [FEW_SHOT_EXAMPLES[j][1][:3] for j in order]  # Pos/Neg abbreviated\n",
    "    print(f\"  Ordering {i + 1}: indices {order} -> labels [{', '.join(labels)}]\")\n",
    "\n",
    "print(f\"\\nNotice the different patterns: some end with Positive, some with\")\n",
    "print(f\"Negative. Because of causal masking, the LAST example has the strongest\")\n",
    "print(f\"recency effect — the test input always attends to it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Run the experiment ---\n\nground_truth = [label for _, label in TEST_EXAMPLES]\n\nordering_results = []\n\nprint(\"ORDERING SENSITIVITY EXPERIMENT\")\nprint(\"=\" * 60)\n\nfor i, order in enumerate(orderings):\n    # Build the reordered example list\n    reordered_examples = [FEW_SHOT_EXAMPLES[j] for j in order]\n\n    # TODO: Classify all 10 test examples using the reordered few-shot examples.\n    # Use build_few_shot_prompt(reordered_examples, review) to build each prompt,\n    # call_llm() to get the response, and extract_sentiment() to parse it.\n    # Collect predictions in a list called `preds`.\n    # YOUR CODE HERE (4-6 lines)\n\n\n    # TODO: Compute accuracy using compute_accuracy(preds, ground_truth)\n    # Store it in a variable called `acc`.\n    # YOUR CODE HERE (1 line)\n\n\n    ordering_results.append({\n        \"ordering_idx\": i + 1,\n        \"order\": order,\n        \"accuracy\": acc,\n        \"predictions\": preds,\n    })\n\n    last_label = FEW_SHOT_EXAMPLES[order[-1]][1]\n    print(f\"  Ordering {i + 1}: accuracy = {acc:.0%}  (last example: {last_label})\")\n\n# Summary statistics\naccs = [r[\"accuracy\"] for r in ordering_results]\nprint(f\"\\nAccuracy range: {min(accs):.0%} — {max(accs):.0%}\")\nprint(f\"Spread: {(max(accs) - min(accs)) * 100:.0f} percentage points\")\nprint(f\"Mean: {np.mean(accs):.0%}, Std: {np.std(accs):.0%}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize the results ---\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: accuracy per ordering\n",
    "ordering_labels = [f\"Ordering {r['ordering_idx']}\" for r in ordering_results]\n",
    "accs_pct = [r[\"accuracy\"] * 100 for r in ordering_results]\n",
    "last_labels = [FEW_SHOT_EXAMPLES[r[\"order\"][-1]][1] for r in ordering_results]\n",
    "colors = [\"#6366f1\" if l == \"Positive\" else \"#ef4444\" for l in last_labels]\n",
    "\n",
    "bars = ax1.bar(ordering_labels, accs_pct, color=colors, edgecolor=\"white\", linewidth=0.5, width=0.6)\n",
    "for bar, val in zip(bars, accs_pct):\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,\n",
    "        f\"{val:.0f}%\", ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"bold\", color=\"white\",\n",
    "    )\n",
    "\n",
    "ax1.set_ylabel(\"Accuracy (%)\", fontsize=11)\n",
    "ax1.set_title(\"Accuracy by Example Ordering\", fontsize=13, fontweight=\"bold\")\n",
    "ax1.set_ylim(0, 110)\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Add legend for last-example color\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"#6366f1\", label=\"Last example: Positive\"),\n",
    "    Patch(facecolor=\"#ef4444\", label=\"Last example: Negative\"),\n",
    "]\n",
    "ax1.legend(handles=legend_elements, loc=\"lower right\", fontsize=9)\n",
    "\n",
    "# Right: per-test-example consistency across orderings\n",
    "# How many orderings got each test example correct?\n",
    "per_example_correct = []\n",
    "for j in range(len(TEST_EXAMPLES)):\n",
    "    correct_count = sum(\n",
    "        1 for r in ordering_results\n",
    "        if r[\"predictions\"][j] == ground_truth[j]\n",
    "    )\n",
    "    per_example_correct.append(correct_count)\n",
    "\n",
    "test_labels = [f\"Test {j+1}\" for j in range(len(TEST_EXAMPLES))]\n",
    "bar_colors = [\"#10b981\" if c == 5 else \"#f59e0b\" if c >= 3 else \"#ef4444\" for c in per_example_correct]\n",
    "\n",
    "bars2 = ax2.bar(test_labels, per_example_correct, color=bar_colors, edgecolor=\"white\", linewidth=0.5, width=0.6)\n",
    "ax2.set_ylabel(\"Orderings Correct (out of 5)\", fontsize=11)\n",
    "ax2.set_title(\"Per-Example Consistency Across Orderings\", fontsize=13, fontweight=\"bold\")\n",
    "ax2.set_ylim(0, 6)\n",
    "ax2.axhline(y=5, color=\"#10b981\", linestyle=\"--\", alpha=0.3)\n",
    "ax2.spines[\"top\"].set_visible(False)\n",
    "ax2.spines[\"right\"].set_visible(False)\n",
    "\n",
    "legend2 = [\n",
    "    Patch(facecolor=\"#10b981\", label=\"Always correct (robust)\"),\n",
    "    Patch(facecolor=\"#f59e0b\", label=\"Sometimes correct (fragile)\"),\n",
    "    Patch(facecolor=\"#ef4444\", label=\"Mostly wrong (ordering-dependent)\"),\n",
    "]\n",
    "ax2.legend(handles=legend2, loc=\"lower right\", fontsize=9)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Same Examples, Different Orders: ICL Is Not Comprehension\",\n",
    "    fontsize=14, fontweight=\"bold\", y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fragile = sum(1 for c in per_example_correct if c < 5)\n",
    "print(f\"\\n{fragile} out of {len(TEST_EXAMPLES)} test examples changed their prediction\")\n",
    "print(f\"depending on example ordering.\")\n",
    "print(f\"\\nIf the model 'understood' the task, ordering would not matter.\")\n",
    "print(f\"The sensitivity to ordering is consistent with the attention-based\")\n",
    "print(f\"mechanism: causal masking creates recency bias (later examples have\")\n",
    "print(f\"stronger influence), and the position of positive vs negative examples\")\n",
    "print(f\"changes the attention pattern over the context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Why ordering matters from the attention perspective:** With causal masking, each token can only attend to tokens at the same position or earlier. The test input token attends to ALL example tokens, but the attention distribution is not uniform — tokens closer to the end of the context tend to have stronger influence due to positional encoding patterns and the sheer structure of causal attention.\n",
    "\n",
    "The TODO code:\n",
    "\n",
    "```python\n",
    "# Classify all 10 test examples with this ordering\n",
    "preds = []\n",
    "for review, _ in TEST_EXAMPLES:\n",
    "    prompt = build_few_shot_prompt(reordered_examples, review)\n",
    "    response = call_llm(prompt)\n",
    "    preds.append(extract_sentiment(response))\n",
    "\n",
    "# Compute accuracy\n",
    "acc = compute_accuracy(preds, ground_truth)\n",
    "```\n",
    "\n",
    "**What to look for in the results:**\n",
    "- If the accuracy spread is large (10+ points), ordering sensitivity is confirmed.\n",
    "- Check whether orderings ending with a Positive example bias the model toward Positive predictions (and vice versa). This is the recency effect from causal masking.\n",
    "- The per-example consistency chart reveals which test examples are \"easy\" (always correct regardless of ordering) and which are \"fragile\" (ordering-dependent). Fragile examples are likely the ones closest to the decision boundary.\n",
    "\n",
    "**Common finding:** With `gpt-4o-mini` on simple sentiment, the effect may be modest (5-10 points) because the task is too easy. On harder tasks or with more ambiguous examples, the effect is larger. The lesson's claim of 20-30 points comes from research on more challenging benchmarks.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: ICL vs Finetuning (Independent)\n",
    "\n",
    "You have now seen ICL work on sentiment classification (Exercise 1), novel tasks (Exercise 2), and you have measured its fragility (Exercise 3). The last question: how does ICL compare to the approach you already know — finetuning?\n",
    "\n",
    "In Module 4.4, you classified sentiment by adding a classification head and training on labeled data. That required gradient descent, a labeled dataset, and weight updates. ICL uses 3-5 examples and no training.\n",
    "\n",
    "**Your task:** Simulate a comparison between ICL and a finetuned classifier on the same sentiment task. Since we cannot run finetuning in this notebook, you will use the LLM to simulate a finetuned model's behavior with a strong system prompt, and compare against ICL.\n",
    "\n",
    "**Specification:**\n",
    "1. Create an \"ICL classifier\" that uses 3-5 few-shot examples to classify sentiment\n",
    "2. Create a \"simulated finetuned classifier\" that uses a detailed system prompt (acting as a purpose-trained sentiment classifier with high confidence)\n",
    "3. Test both on the same 10 test examples\n",
    "4. Compare accuracy, but also compare **response consistency** — run each classifier 3 times on the same inputs with temperature=0.3 and measure how often the prediction changes\n",
    "5. Visualize the comparison\n",
    "\n",
    "Think about: when would you choose ICL over finetuning? When would finetuning be worth the cost?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**The reasoning behind the comparison:** ICL and finetuning are not competing approaches — they are different tools for different constraints. ICL is fast (no training), flexible (change the task by changing the prompt), and cheap (no compute for training). Finetuning is more accurate (thousands of examples vs 3-5), more robust (not sensitive to example ordering), and more consistent (learned decision boundary vs context-dependent computation).\n",
    "\n",
    "```python\n",
    "# --- ICL Classifier ---\n",
    "icl_examples = FEW_SHOT_EXAMPLES[:3]\n",
    "\n",
    "def icl_classify(review: str) -> str:\n",
    "    prompt = build_few_shot_prompt(icl_examples, review)\n",
    "    response = call_llm(prompt, temperature=0.3)\n",
    "    return extract_sentiment(response)\n",
    "\n",
    "\n",
    "# --- Simulated Finetuned Classifier ---\n",
    "FINETUNED_SYSTEM = (\n",
    "    \"You are a sentiment classification model that has been finetuned on \"\n",
    "    \"50,000 movie reviews. You classify reviews as exactly 'Positive' or \"\n",
    "    \"'Negative'. You are highly accurate and confident. Respond with a \"\n",
    "    \"single word: Positive or Negative.\"\n",
    ")\n",
    "\n",
    "def finetuned_classify(review: str) -> str:\n",
    "    response = call_llm_with_system(\n",
    "        FINETUNED_SYSTEM,\n",
    "        f'Classify: \"{review}\"',\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    return extract_sentiment(response)\n",
    "\n",
    "\n",
    "# --- Accuracy comparison ---\n",
    "ground_truth = [label for _, label in TEST_EXAMPLES]\n",
    "\n",
    "icl_preds = [icl_classify(review) for review, _ in TEST_EXAMPLES]\n",
    "ft_preds = [finetuned_classify(review) for review, _ in TEST_EXAMPLES]\n",
    "\n",
    "icl_acc = compute_accuracy(icl_preds, ground_truth)\n",
    "ft_acc = compute_accuracy(ft_preds, ground_truth)\n",
    "\n",
    "print(f\"ICL accuracy: {icl_acc:.0%}\")\n",
    "print(f\"Finetuned accuracy: {ft_acc:.0%}\")\n",
    "\n",
    "\n",
    "# --- Consistency comparison (3 runs each) ---\n",
    "icl_runs = []\n",
    "ft_runs = []\n",
    "for _ in range(3):\n",
    "    icl_runs.append([icl_classify(r) for r, _ in TEST_EXAMPLES])\n",
    "    ft_runs.append([finetuned_classify(r) for r, _ in TEST_EXAMPLES])\n",
    "\n",
    "# Count inconsistencies per example\n",
    "icl_inconsistent = sum(\n",
    "    1 for j in range(len(TEST_EXAMPLES))\n",
    "    if len(set(run[j] for run in icl_runs)) > 1\n",
    ")\n",
    "ft_inconsistent = sum(\n",
    "    1 for j in range(len(TEST_EXAMPLES))\n",
    "    if len(set(run[j] for run in ft_runs)) > 1\n",
    ")\n",
    "\n",
    "print(f\"\\nICL inconsistent predictions: {icl_inconsistent}/{len(TEST_EXAMPLES)}\")\n",
    "print(f\"Finetuned inconsistent predictions: {ft_inconsistent}/{len(TEST_EXAMPLES)}\")\n",
    "\n",
    "\n",
    "# --- Visualization ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Accuracy\n",
    "methods = [\"ICL\\n(3 examples)\", \"Finetuned\\n(simulated)\"]\n",
    "acc_vals = [icl_acc * 100, ft_acc * 100]\n",
    "bars = ax1.bar(methods, acc_vals, color=[\"#6366f1\", \"#f59e0b\"], width=0.5)\n",
    "for bar, val in zip(bars, acc_vals):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height()+1,\n",
    "             f\"{val:.0f}%\", ha=\"center\", fontsize=14, fontweight=\"bold\", color=\"white\")\n",
    "ax1.set_ylabel(\"Accuracy (%)\")\n",
    "ax1.set_title(\"Accuracy\")\n",
    "ax1.set_ylim(0, 110)\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Consistency\n",
    "cons_vals = [\n",
    "    (len(TEST_EXAMPLES) - icl_inconsistent) / len(TEST_EXAMPLES) * 100,\n",
    "    (len(TEST_EXAMPLES) - ft_inconsistent) / len(TEST_EXAMPLES) * 100,\n",
    "]\n",
    "bars2 = ax2.bar(methods, cons_vals, color=[\"#6366f1\", \"#f59e0b\"], width=0.5)\n",
    "for bar, val in zip(bars2, cons_vals):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height()+1,\n",
    "             f\"{val:.0f}%\", ha=\"center\", fontsize=14, fontweight=\"bold\", color=\"white\")\n",
    "ax2.set_ylabel(\"Consistency (%)\")\n",
    "ax2.set_title(\"Consistency (3 runs, same inputs)\")\n",
    "ax2.set_ylim(0, 110)\n",
    "ax2.spines[\"top\"].set_visible(False)\n",
    "ax2.spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.suptitle(\"ICL vs Finetuning: Accuracy and Consistency\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWhen to choose ICL:\")\n",
    "print(\"  - You need to classify a new task RIGHT NOW (no training time)\")\n",
    "print(\"  - You have very few labeled examples (3-10)\")\n",
    "print(\"  - The task may change frequently (just update the prompt)\")\n",
    "print(\"  - Accuracy does not need to be perfect\")\n",
    "print(\"\\nWhen to choose finetuning:\")\n",
    "print(\"  - You have thousands of labeled examples\")\n",
    "print(\"  - You need high, consistent accuracy\")\n",
    "print(\"  - The task is stable (won't change frequently)\")\n",
    "print(\"  - Robustness matters (no sensitivity to prompt format)\")\n",
    "print(\"\\nThey are complementary tools, not competing approaches.\")\n",
    "```\n",
    "\n",
    "**The meta-insight:** This comparison reveals that ICL and finetuning trade off along different axes. ICL is fast and flexible but fragile. Finetuning is slow and rigid but robust. The choice depends on your constraints: how much labeled data do you have? How much time? How stable is the task? How important is consistency?\n",
    "\n",
    "This is the same tradeoff thinking from the Alignment Techniques Landscape (Module 5.1, Lesson 2) — there is no universally best approach, only tradeoffs along axes.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Refer to the specification above and the solution if needed.\n",
    "# Build an ICL classifier, a simulated finetuned classifier, test both on\n",
    "# the 10 test examples, measure accuracy and consistency, and visualize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Examples in the prompt genuinely change model behavior.** Zero-shot and few-shot classification use the same model with the same frozen weights. The only difference is the prompt content. The improvement (or lack thereof) is the empirical signature of in-context learning.\n",
    "\n",
    "2. **ICL works on novel tasks, not just memorized patterns.** Made-up mappings that could not appear in training data still work — at least when the transformation is computable by attention in a single forward pass. The boundary between success and failure reveals the scope of what attention can compute.\n",
    "\n",
    "3. **Ordering sensitivity confirms the attention-based mechanism.** Same examples, different order, different accuracy. If the model truly \"understood\" the task, order would not matter. The sensitivity to ordering is consistent with causal masking (recency bias) and attention-based retrieval, not comprehension.\n",
    "\n",
    "4. **ICL and finetuning are complementary, not competing.** ICL is fast, flexible, and cheap but fragile and sensitive to prompt design. Finetuning is accurate, robust, and consistent but requires labeled data, compute, and time. The choice depends on your constraints — not on which is \"better.\"\n",
    "\n",
    "5. **The prompt is a program; attention is the interpreter.** Every prompt configures the model for a different task through attention over the context. Different examples, different behavior. Same weights, different programs. This is the core mental model for understanding ICL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}