{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditioning the U-Net\n",
    "\n",
    "**Module 6.3, Lesson 2** | CourseAI\n",
    "\n",
    "The U-Net from the previous lesson was the skeleton -- the spatial architecture. This notebook adds the nervous system: the timestep signal that tells each bone how to move. You will implement both mechanisms that make this work:\n",
    "\n",
    "**What you will do:**\n",
    "- Implement sinusoidal timestep embedding from the formula (the same one from positional encoding)\n",
    "- Visualize how the embedding encodes different noise levels as distinct patterns\n",
    "- Build the 2-layer MLP that refines the raw sinusoidal encoding into a timestep embedding\n",
    "- Implement adaptive group normalization -- making gamma and beta depend on the timestep\n",
    "- Compare the capstone's simple linear projection to sinusoidal + MLP conditioning on real training\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "Everything in this notebook connects to concepts you already know. The sinusoidal encoding is the same formula from *Embeddings and Position*. The adaptive normalization is standard group norm with one change: gamma and beta come from the timestep instead of a fixed parameter table. No new theory -- just practice.\n",
    "\n",
    "**Estimated time:** 45--60 minutes (Exercise 4 training takes ~15 minutes on a Colab GPU).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to import everything and configure the environment.\n",
    "\n",
    "**Important:** If you plan to run Exercise 4 (the training comparison), set the runtime to GPU. In Colab: Runtime > Change runtime type > T4 GPU. Exercises 1--3 work fine on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "print('\\nSetup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Sinusoidal Timestep Embedding [Guided]\n",
    "\n",
    "In *Embeddings and Position*, you implemented sinusoidal positional encoding -- the \"clock with many hands\" that gives each position a unique, smooth pattern. The formula was:\n",
    "\n",
    "$$\\text{PE}(\\text{pos}, 2i) = \\sin\\!\\left(\\frac{\\text{pos}}{10000^{2i/d}}\\right), \\quad \\text{PE}(\\text{pos}, 2i+1) = \\cos\\!\\left(\\frac{\\text{pos}}{10000^{2i/d}}\\right)$$\n",
    "\n",
    "For timestep embedding, the formula is **identical** -- just replace \"pos\" with \"t\":\n",
    "\n",
    "$$\\text{TE}(t, 2i) = \\sin\\!\\left(\\frac{t}{10000^{2i/d}}\\right), \\quad \\text{TE}(t, 2i+1) = \\cos\\!\\left(\\frac{t}{10000^{2i/d}}\\right)$$\n",
    "\n",
    "**Same formula. Different input. Different question.** In transformers, the clock encodes *where* a token sits. In diffusion, the same clock encodes *how noisy* the input is.\n",
    "\n",
    "The four requirements still hold: **unique** (each timestep gets a distinct pattern), **smooth** (nearby timesteps produce similar embeddings), **any range** (works for 1000 timesteps or 10,000), and **deterministic** (no learned parameters).\n",
    "\n",
    "**Before running, predict:**\n",
    "- Will the heatmap for t=500 and t=501 look similar or different?\n",
    "- Will the heatmap for t=500 and t=50 look similar or different?\n",
    "- In the heatmap, which dimensions will change rapidly across timesteps -- the low-index or high-index dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embedding(t, d_emb=256):\n",
    "    \"\"\"Compute sinusoidal timestep embedding.\n",
    "\n",
    "    Same formula as positional encoding from Embeddings and Position:\n",
    "    TE(t, 2i)   = sin(t / 10000^(2i/d))\n",
    "    TE(t, 2i+1) = cos(t / 10000^(2i/d))\n",
    "\n",
    "    Args:\n",
    "        t: timesteps [batch_size] (integer or float tensor)\n",
    "        d_emb: embedding dimension (default 256)\n",
    "\n",
    "    Returns:\n",
    "        embeddings [batch_size, d_emb]\n",
    "    \"\"\"\n",
    "    # Ensure t is a float tensor with shape [batch_size]\n",
    "    if not isinstance(t, torch.Tensor):\n",
    "        t = torch.tensor([t], dtype=torch.float32)\n",
    "    t = t.float()\n",
    "    if t.dim() == 0:\n",
    "        t = t.unsqueeze(0)\n",
    "\n",
    "    # Half the dimensions get sin, half get cos\n",
    "    half_d = d_emb // 2\n",
    "\n",
    "    # Compute the frequencies: 1 / 10000^(2i/d) for i = 0, 1, ..., half_d-1\n",
    "    # Equivalently: exp(-2i/d * log(10000))\n",
    "    i = torch.arange(half_d, dtype=torch.float32, device=t.device)\n",
    "    freq = torch.exp(-i * (math.log(10000.0) / half_d))\n",
    "\n",
    "    # Outer product: [batch_size, 1] * [1, half_d] -> [batch_size, half_d]\n",
    "    # Each row is one timestep, each column is one frequency\n",
    "    angles = t.unsqueeze(1) * freq.unsqueeze(0)\n",
    "\n",
    "    # Interleave sin and cos: [batch_size, d_emb]\n",
    "    emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "\n",
    "    return emb\n",
    "\n",
    "\n",
    "# Compute embeddings for several timesteps\n",
    "timesteps = [0, 50, 500, 501, 950, 999]\n",
    "embeddings = sinusoidal_embedding(torch.tensor(timesteps), d_emb=256)\n",
    "\n",
    "print(f'Embedding shape: {embeddings.shape}')  # [6, 256]\n",
    "print(f'Timesteps: {timesteps}')\n",
    "print()\n",
    "\n",
    "# Show the first few values for t=500 and t=50\n",
    "print('First 8 dimensions of each embedding:')\n",
    "for i, t_val in enumerate(timesteps):\n",
    "    vals = embeddings[i, :8].numpy()\n",
    "    print(f'  t={t_val:>4d}: [{\"  \".join(f\"{v:+.3f}\" for v in vals)}  ...]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize as a heatmap -- like the PositionalEncodingHeatmap from Embeddings and Position\n",
    "# Each row is one timestep. Each column is one dimension of the embedding.\n",
    "# You should see: low-index dimensions oscillate rapidly (the \"second hand\"),\n",
    "# high-index dimensions change slowly (the \"hour hand\").\n",
    "\n",
    "# Compute embeddings for many timesteps (dense sampling)\n",
    "all_timesteps = torch.arange(0, 1000)\n",
    "all_embeddings = sinusoidal_embedding(all_timesteps, d_emb=256)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Full heatmap\n",
    "im1 = ax1.imshow(all_embeddings.numpy(), aspect='auto', cmap='RdBu_r',\n",
    "                  vmin=-1, vmax=1, interpolation='nearest')\n",
    "ax1.set_xlabel('Embedding Dimension', fontsize=11)\n",
    "ax1.set_ylabel('Timestep t', fontsize=11)\n",
    "ax1.set_title('Sinusoidal Timestep Embedding (all 1000 timesteps)', fontsize=12)\n",
    "plt.colorbar(im1, ax=ax1, shrink=0.8)\n",
    "\n",
    "# Zoomed heatmap: t=495 to t=505 (to check smoothness)\n",
    "zoom_range = range(495, 506)\n",
    "zoom_embeddings = sinusoidal_embedding(torch.tensor(list(zoom_range)), d_emb=256)\n",
    "\n",
    "im2 = ax2.imshow(zoom_embeddings.numpy(), aspect='auto', cmap='RdBu_r',\n",
    "                  vmin=-1, vmax=1, interpolation='nearest')\n",
    "ax2.set_xlabel('Embedding Dimension', fontsize=11)\n",
    "ax2.set_ylabel('Timestep t', fontsize=11)\n",
    "ax2.set_yticks(range(len(list(zoom_range))))\n",
    "ax2.set_yticklabels(list(zoom_range))\n",
    "ax2.set_title('Zoomed: t=495 to t=505 (smoothness check)', fontsize=12)\n",
    "plt.colorbar(im2, ax=ax2, shrink=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Left: full heatmap. Low-frequency dimensions (right) change slowly across timesteps.')\n",
    "print('      High-frequency dimensions (left) oscillate rapidly.')\n",
    "print('      This is the \"clock with many hands\" -- the second hand (dim 0) captures')\n",
    "print('      fine differences; the hour hand (dim 255) captures broad noise-level changes.')\n",
    "print()\n",
    "print('Right: zoomed view of t=495 to t=505. Adjacent timesteps produce nearly identical')\n",
    "print('       patterns. The embedding is SMOOTH -- small change in t, small change in embedding.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sinusoidal embedding to a RANDOM embedding\n",
    "# This is the negative example from the lesson: if embeddings were random,\n",
    "# t=500 and t=501 would have completely unrelated patterns.\n",
    "\n",
    "# Random embedding: each timestep gets a random vector\n",
    "torch.manual_seed(0)\n",
    "random_embeddings = torch.randn(1000, 256)\n",
    "\n",
    "# Compute cosine similarity between consecutive timesteps\n",
    "def cosine_sim(a, b):\n",
    "    return F.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0)).item()\n",
    "\n",
    "# Sinusoidal: adjacent timesteps\n",
    "sin_sim_adjacent = cosine_sim(all_embeddings[500], all_embeddings[501])\n",
    "sin_sim_distant = cosine_sim(all_embeddings[500], all_embeddings[50])\n",
    "\n",
    "# Random: adjacent timesteps\n",
    "rand_sim_adjacent = cosine_sim(random_embeddings[500], random_embeddings[501])\n",
    "rand_sim_distant = cosine_sim(random_embeddings[500], random_embeddings[50])\n",
    "\n",
    "print('Cosine similarity comparison:')\n",
    "print(f'                        t=500 vs t=501    t=500 vs t=50')\n",
    "print(f'  Sinusoidal:           {sin_sim_adjacent:+.4f}            {sin_sim_distant:+.4f}')\n",
    "print(f'  Random:               {rand_sim_adjacent:+.4f}            {rand_sim_distant:+.4f}')\n",
    "print()\n",
    "print('With sinusoidal encoding:')\n",
    "print('  - Adjacent timesteps (500 vs 501) are very similar (high cosine similarity).')\n",
    "print('  - Distant timesteps (500 vs 50) are quite different (low similarity).')\n",
    "print('  This is the smoothness property in action.')\n",
    "print()\n",
    "print('With random encoding:')\n",
    "print('  - Adjacent and distant timesteps have similarly random similarity.')\n",
    "print('  - The network would treat t=500 and t=501 as unrelated tasks.')\n",
    "print('  - No smoothness, no generalization across nearby timesteps.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You implemented the sinusoidal timestep embedding -- the **same formula** from positional encoding in transformers, now encoding noise level instead of sequence position.\n",
    "\n",
    "The heatmap shows the \"clock with many hands\":\n",
    "- **Low-index dimensions** (left side) oscillate rapidly, capturing fine differences between adjacent timesteps. These are the \"second hand.\"\n",
    "- **High-index dimensions** (right side) change slowly, capturing the broad distinction between \"lots of noise\" and \"almost clean.\" These are the \"hour hand.\"\n",
    "\n",
    "The smoothness comparison confirms why sinusoidal encoding is superior to random embeddings: adjacent timesteps (500 vs 501) get nearly identical patterns, so the network can generalize across nearby noise levels. A random embedding destroys this structure.\n",
    "\n",
    "This encoding has no learned parameters. The learning happens in the MLP that comes next.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Timestep MLP [Guided]\n",
    "\n",
    "The sinusoidal encoding provides a structured, multi-frequency input. But it is not the final embedding -- it is the *input* to a 2-layer MLP:\n",
    "\n",
    "```\n",
    "t -> sinusoidal_encoding(t) -> Linear(256, 512) -> GELU -> Linear(512, 512) -> timestep embedding\n",
    "```\n",
    "\n",
    "The MLP learns to combine and transform the raw frequency components into features that are useful for denoising. This is a standard pattern: provide a structured input (the sinusoidal encoding), let the network refine it (the MLP). The sinusoidal part has no learned parameters -- all the learning happens in the MLP.\n",
    "\n",
    "**Before running, predict:**\n",
    "- After the MLP transforms the embeddings, will t=500 and t=501 still be similar? (The MLP is a smooth function -- does it preserve the smoothness of its input?)\n",
    "- In the cosine similarity matrix, what pattern do you expect along the diagonal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimestepEmbedding(nn.Module):\n",
    "    \"\"\"Full timestep embedding: sinusoidal encoding + 2-layer MLP.\n",
    "\n",
    "    Pipeline:\n",
    "      t (integer) -> sinusoidal encoding (256-dim) -> MLP -> embedding (512-dim)\n",
    "\n",
    "    The sinusoidal encoding provides a structured, multi-frequency input.\n",
    "    The MLP refines it into features useful for denoising.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_sinusoidal=256, d_emb=512):\n",
    "        super().__init__()\n",
    "        self.d_sinusoidal = d_sinusoidal\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_sinusoidal, d_emb),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_emb, d_emb),\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"Compute timestep embedding.\n",
    "\n",
    "        Args:\n",
    "            t: timesteps [batch_size] (integer tensor)\n",
    "\n",
    "        Returns:\n",
    "            embeddings [batch_size, d_emb]\n",
    "        \"\"\"\n",
    "        # Step 1: sinusoidal encoding (no learned parameters)\n",
    "        sin_emb = sinusoidal_embedding(t, self.d_sinusoidal)\n",
    "        # Step 2: MLP refinement (learned parameters)\n",
    "        return self.mlp(sin_emb)\n",
    "\n",
    "\n",
    "# Create the timestep embedding module\n",
    "torch.manual_seed(42)\n",
    "t_embed = TimestepEmbedding(d_sinusoidal=256, d_emb=512)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in t_embed.parameters())\n",
    "print(f'TimestepEmbedding parameters: {n_params:,}')\n",
    "print(f'  Linear(256, 512): {256 * 512 + 512:,}  (weight + bias)')\n",
    "print(f'  Linear(512, 512): {512 * 512 + 512:,}  (weight + bias)')\n",
    "print(f'  Total:            {256*512 + 512 + 512*512 + 512:,}')\n",
    "print()\n",
    "print('Note: the sinusoidal encoding has ZERO parameters.')\n",
    "print('All learning happens in the MLP.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings for a range of timesteps and visualize the similarity structure\n",
    "# We use the UNTRAINED MLP to see what structure the sinusoidal encoding preserves\n",
    "# even through a random (untrained) network.\n",
    "\n",
    "sample_timesteps = torch.arange(0, 1000, 10)  # every 10th timestep\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Sinusoidal only (no MLP)\n",
    "    sin_embeds = sinusoidal_embedding(sample_timesteps, d_emb=256)\n",
    "    # Sinusoidal + MLP\n",
    "    mlp_embeds = t_embed(sample_timesteps)\n",
    "\n",
    "# Compute cosine similarity matrices\n",
    "sin_normed = F.normalize(sin_embeds, dim=1)\n",
    "mlp_normed = F.normalize(mlp_embeds, dim=1)\n",
    "\n",
    "sin_sim_matrix = sin_normed @ sin_normed.T\n",
    "mlp_sim_matrix = mlp_normed @ mlp_normed.T\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "im1 = ax1.imshow(sin_sim_matrix.numpy(), cmap='RdBu_r', vmin=-1, vmax=1,\n",
    "                  extent=[0, 1000, 1000, 0])\n",
    "ax1.set_xlabel('Timestep', fontsize=11)\n",
    "ax1.set_ylabel('Timestep', fontsize=11)\n",
    "ax1.set_title('Cosine Similarity: Sinusoidal Only', fontsize=12)\n",
    "plt.colorbar(im1, ax=ax1, shrink=0.8)\n",
    "\n",
    "im2 = ax2.imshow(mlp_sim_matrix.numpy(), cmap='RdBu_r', vmin=-1, vmax=1,\n",
    "                  extent=[0, 1000, 1000, 0])\n",
    "ax2.set_xlabel('Timestep', fontsize=11)\n",
    "ax2.set_ylabel('Timestep', fontsize=11)\n",
    "ax2.set_title('Cosine Similarity: Sinusoidal + MLP (untrained)', fontsize=12)\n",
    "plt.colorbar(im2, ax=ax2, shrink=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Left: sinusoidal encoding only. Nearby timesteps are similar (bright diagonal band).')\n",
    "print('      Distant timesteps are dissimilar (dark off-diagonal regions).')\n",
    "print()\n",
    "print('Right: after the MLP (even untrained!). The smoothness structure is PRESERVED.')\n",
    "print('       The MLP is a smooth function -- it cannot destroy the neighborhood')\n",
    "print('       structure of its input. Adjacent timesteps stay similar.')\n",
    "print()\n",
    "print('After training, the MLP will refine WHICH aspects of the timestep are emphasized,')\n",
    "print('but the smooth neighborhood structure is baked in by the sinusoidal encoding.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify specific pairs -- quantitative smoothness check\n",
    "with torch.no_grad():\n",
    "    pairs = [(500, 501), (500, 510), (500, 50), (500, 0)]\n",
    "\n",
    "    print('Cosine similarity between timestep pairs:')\n",
    "    print(f'{\"Pair\":>20s}    {\"Sinusoidal\":>12s}    {\"Sin + MLP\":>12s}')\n",
    "    print('-' * 52)\n",
    "\n",
    "    for t_a, t_b in pairs:\n",
    "        sin_a = sinusoidal_embedding(torch.tensor([t_a]), d_emb=256)\n",
    "        sin_b = sinusoidal_embedding(torch.tensor([t_b]), d_emb=256)\n",
    "        mlp_a = t_embed(torch.tensor([t_a]))\n",
    "        mlp_b = t_embed(torch.tensor([t_b]))\n",
    "\n",
    "        sin_s = F.cosine_similarity(sin_a, sin_b).item()\n",
    "        mlp_s = F.cosine_similarity(mlp_a, mlp_b).item()\n",
    "        print(f'  t={t_a} vs t={t_b:>4d}    {sin_s:+.4f}         {mlp_s:+.4f}')\n",
    "\n",
    "print()\n",
    "print('Key observation: the similarity DECREASES as timesteps get further apart.')\n",
    "print('This holds both before and after the MLP.')\n",
    "print('The network gets a rich signal about how different two noise levels are.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You built the complete timestep embedding pipeline: sinusoidal encoding followed by a 2-layer MLP.\n",
    "\n",
    "The cosine similarity matrices confirm two things:\n",
    "1. **The sinusoidal encoding provides smooth structure.** Nearby timesteps have similar embeddings, distant timesteps have different embeddings. This is the same smoothness property from positional encoding.\n",
    "2. **The MLP preserves this structure.** Even an untrained MLP cannot destroy the neighborhood structure -- it is a continuous function. After training, it will refine *which aspects* of the timestep are emphasized for denoising, but the smooth foundation is baked in by the sinusoidal encoding.\n",
    "\n",
    "The sinusoidal encoding has zero learned parameters. The MLP has ~393K parameters. Together they produce a 512-dimensional embedding that the U-Net uses at every residual block.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Adaptive Group Normalization [Supported]\n",
    "\n",
    "You know how standard group normalization works: normalize the features, then apply a learned scale $\\gamma$ and shift $\\beta$. Those are **fixed parameters** -- the same $\\gamma$ and $\\beta$ for every input, every timestep.\n",
    "\n",
    "Adaptive group normalization makes one change: $\\gamma$ and $\\beta$ **depend on the timestep**.\n",
    "\n",
    "$$\\text{AdaGN}(x, t) = \\gamma(t) \\cdot \\text{GroupNorm}(x) + \\beta(t)$$\n",
    "\n",
    "where $[\\gamma(t), \\beta(t)] = \\text{Linear}(\\text{emb}_t)$\n",
    "\n",
    "The normalization step is standard. The scale and shift come from a linear projection of the timestep embedding rather than a fixed parameter table.\n",
    "\n",
    "**Your task:** Implement the `AdaGN` module. The structure is provided with `# TODO` markers. Each TODO is 1--2 lines.\n",
    "\n",
    "**Hints:**\n",
    "- `nn.GroupNorm(num_groups, num_channels, affine=False)` gives you group norm WITHOUT the learned gamma/beta (we replace those with timestep-dependent ones).\n",
    "- The linear projection maps from `d_emb` to `2 * num_channels` -- half for gamma, half for beta.\n",
    "- Use `.chunk(2, dim=-1)` to split the projection output into gamma and beta.\n",
    "- Remember to reshape gamma and beta for broadcasting: `[batch, channels]` -> `[batch, channels, 1, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGN(nn.Module):\n",
    "    \"\"\"Adaptive Group Normalization.\n",
    "\n",
    "    Standard group norm + timestep-dependent scale (gamma) and shift (beta).\n",
    "    The normalization is standard. The adaptation is in where gamma and beta\n",
    "    come from: not from a fixed parameter table, but from a linear projection\n",
    "    of the timestep embedding.\n",
    "\n",
    "    Args:\n",
    "        num_channels: number of feature map channels\n",
    "        d_emb: dimension of the timestep embedding\n",
    "        num_groups: number of groups for group norm (default 32)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_channels, d_emb, num_groups=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # Group norm WITHOUT learned affine parameters (affine=False)\n",
    "        # We replace gamma/beta with timestep-dependent versions\n",
    "        # TODO: Create self.norm using nn.GroupNorm with affine=False\n",
    "        # Hint: nn.GroupNorm(num_groups, num_channels, affine=False)\n",
    "\n",
    "\n",
    "        # Linear projection: timestep embedding -> [gamma, beta]\n",
    "        # Maps from d_emb to 2 * num_channels (half for gamma, half for beta)\n",
    "        # TODO: Create self.proj using nn.Linear\n",
    "        # Hint: nn.Linear(d_emb, 2 * num_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        \"\"\"Apply adaptive group normalization.\n",
    "\n",
    "        Args:\n",
    "            x: feature maps [batch, channels, height, width]\n",
    "            t_emb: timestep embedding [batch, d_emb]\n",
    "\n",
    "        Returns:\n",
    "            modulated features [batch, channels, height, width]\n",
    "        \"\"\"\n",
    "        # Step 1: standard group norm (normalize only, no affine)\n",
    "        h = self.norm(x)\n",
    "\n",
    "        # Step 2: project timestep embedding to gamma and beta\n",
    "        # TODO: Use self.proj to get gamma_beta from t_emb, then split\n",
    "        # Hint: gamma, beta = self.proj(t_emb).chunk(2, dim=-1)\n",
    "\n",
    "\n",
    "        # Step 3: reshape for broadcasting [batch, channels] -> [batch, channels, 1, 1]\n",
    "        gamma = gamma.unsqueeze(-1).unsqueeze(-1)\n",
    "        beta = beta.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # Step 4: apply scale and shift\n",
    "        # Standard norm uses: gamma * norm(x) + beta\n",
    "        # We add 1 to gamma so that the default (untrained) behavior is identity-like:\n",
    "        # (1 + 0) * norm(x) + 0 = norm(x)\n",
    "        # TODO: Return (1 + gamma) * h + beta\n",
    "\n",
    "\n",
    "print('AdaGN class defined. Fill in the TODO markers before running the next cell.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The three fills implement the adaptive group normalization mechanism. The key insight: the normalization is standard group norm. The only new part is where gamma and beta come from.\n",
    "\n",
    "**`__init__` -- Group norm and projection:**\n",
    "```python\n",
    "self.norm = nn.GroupNorm(num_groups, num_channels, affine=False)\n",
    "self.proj = nn.Linear(d_emb, 2 * num_channels)\n",
    "```\n",
    "`affine=False` tells PyTorch not to learn gamma/beta parameters -- we are replacing them with timestep-dependent versions. The linear projection maps the 512-dim timestep embedding to `2 * num_channels` values: half become gamma (scale), half become beta (shift).\n",
    "\n",
    "**`forward` -- Project and split:**\n",
    "```python\n",
    "gamma, beta = self.proj(t_emb).chunk(2, dim=-1)\n",
    "```\n",
    "One linear layer produces both gamma and beta. `.chunk(2, dim=-1)` splits the output in half along the last dimension. This is the same pattern as producing Q and K from one projection.\n",
    "\n",
    "**`forward` -- Apply modulation:**\n",
    "```python\n",
    "return (1 + gamma) * h + beta\n",
    "```\n",
    "Adding 1 to gamma means the default behavior (when gamma is near 0) is close to identity. Without the `+ 1`, the network would start by zeroing out all features, making training harder.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the AdaGN module: same feature map, different timesteps -> different outputs\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create the modules\n",
    "adagn = AdaGN(num_channels=64, d_emb=512, num_groups=32)\n",
    "t_embed_test = TimestepEmbedding(d_sinusoidal=256, d_emb=512)\n",
    "\n",
    "# Create a dummy feature map (same for both timesteps)\n",
    "feature_map = torch.randn(1, 64, 16, 16)  # [batch=1, channels=64, h=16, w=16]\n",
    "\n",
    "# Compute timestep embeddings for two different timesteps\n",
    "with torch.no_grad():\n",
    "    emb_500 = t_embed_test(torch.tensor([500]))  # [1, 512]\n",
    "    emb_50 = t_embed_test(torch.tensor([50]))     # [1, 512]\n",
    "\n",
    "    # Apply AdaGN with different timesteps\n",
    "    out_500 = adagn(feature_map, emb_500)  # same feature map, t=500\n",
    "    out_50 = adagn(feature_map, emb_50)    # same feature map, t=50\n",
    "\n",
    "print(f'Input feature map shape:  {feature_map.shape}')\n",
    "print(f'Output shape (t=500):     {out_500.shape}')\n",
    "print(f'Output shape (t=50):      {out_50.shape}')\n",
    "print()\n",
    "\n",
    "# Are the outputs different?\n",
    "diff = (out_500 - out_50).abs()\n",
    "print(f'Are outputs identical?    {torch.allclose(out_500, out_50)}')\n",
    "print(f'Mean absolute difference: {diff.mean():.4f}')\n",
    "print(f'Max absolute difference:  {diff.max():.4f}')\n",
    "print()\n",
    "print('Same feature map, same normalization, same conv weights.')\n",
    "print('Different timestep -> different gamma(t) and beta(t) -> different output.')\n",
    "print('This is the core mechanism: the network\\'s behavior changes with the noise level.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what gamma(t) and beta(t) look like for different timesteps\n",
    "# This shows the \"conductor's score\" -- different dynamics for different timesteps\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_timesteps = [0, 250, 500, 750, 999]\n",
    "    gammas = []\n",
    "    betas = []\n",
    "\n",
    "    for t_val in test_timesteps:\n",
    "        emb = t_embed_test(torch.tensor([t_val]))\n",
    "        gamma_beta = adagn.proj(emb)  # [1, 2 * 64]\n",
    "        gamma, beta = gamma_beta.chunk(2, dim=-1)  # each [1, 64]\n",
    "        gammas.append(gamma.squeeze().numpy())\n",
    "        betas.append(beta.squeeze().numpy())\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "for i, t_val in enumerate(test_timesteps):\n",
    "    ax1.plot(gammas[i][:32], label=f't={t_val}', alpha=0.8, linewidth=1.5)\n",
    "    ax2.plot(betas[i][:32], label=f't={t_val}', alpha=0.8, linewidth=1.5)\n",
    "\n",
    "ax1.set_xlabel('Channel index (first 32)', fontsize=11)\n",
    "ax1.set_ylabel('gamma(t)', fontsize=11)\n",
    "ax1.set_title('Scale parameters per channel (different timesteps)', fontsize=12)\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('Channel index (first 32)', fontsize=11)\n",
    "ax2.set_ylabel('beta(t)', fontsize=11)\n",
    "ax2.set_title('Shift parameters per channel (different timesteps)', fontsize=12)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Each timestep produces DIFFERENT gamma and beta values for each channel.')\n",
    "print('This is the conductor\\'s score: different measures (timesteps) call for')\n",
    "print('different dynamics (scale) and different keys (shift).')\n",
    "print()\n",
    "print('Note: these are from an UNTRAINED network, so the patterns are random.')\n",
    "print('After training, the network would learn meaningful gamma/beta patterns:')\n",
    "print('  - At high t: amplify structural channels, suppress detail channels')\n",
    "print('  - At low t: amplify detail channels for fine-grained refinement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You implemented adaptive group normalization -- the mechanism that lets the U-Net change its behavior based on the noise level. The key points:\n",
    "\n",
    "1. **The normalization is standard group norm.** Nothing changes about how features are normalized (zero mean, unit variance within each group).\n",
    "2. **The only change is where gamma and beta come from.** Instead of a fixed parameter table, they come from a linear projection of the timestep embedding. One line of conceptual change.\n",
    "3. **Same feature map + different timestep = different output.** The conv weights are fixed. The architecture is fixed. What changes are the scale and shift parameters after normalization, which changes the effective behavior of the network.\n",
    "\n",
    "In the full U-Net, every residual block has its own AdaGN module with its own linear projection. Same timestep embedding, different \"learned lens\" per block -- the same pattern from Q/K/V in attention.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Compare Simple vs Sinusoidal Conditioning [Independent]\n",
    "\n",
    "In *Build a Diffusion Model*, the capstone used a simple approach: normalize t to [0, 1], pass through a 2-layer MLP, add to the bottleneck. That \"worked\" for MNIST. This exercise tests whether the sinusoidal approach is actually better.\n",
    "\n",
    "**Your task:**\n",
    "1. A minimal U-Net with the capstone's **simple linear** timestep embedding is provided below.\n",
    "2. Create a second version that replaces the simple embedding with **sinusoidal + MLP** conditioning.\n",
    "3. Train both for 10 epochs on MNIST and compare loss curves.\n",
    "\n",
    "**What to change:** Replace the `SimpleTimestepEmbed` module with the `TimestepEmbedding` (sinusoidal + MLP) you built in Exercise 2, and adjust the embedding injection accordingly.\n",
    "\n",
    "**Expected result:** The sinusoidal version should converge faster or to a lower loss. The sinusoidal encoding gives the network a richer, multi-frequency starting point rather than mapping everything onto a single learned direction.\n",
    "\n",
    "**Note:** This exercise requires GPU for reasonable training times. If you are on CPU, read the solution and discussion instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# Shared training infrastructure\n",
    "# =====================================================================\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# Diffusion hyperparameters (same as capstone)\n",
    "T = 1000\n",
    "beta_min = 0.0001\n",
    "beta_max = 0.02\n",
    "betas = torch.linspace(beta_min, beta_max, T)\n",
    "alphas = 1.0 - betas\n",
    "alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "\n",
    "def q_sample(x_0, t, alpha_bars, noise=None):\n",
    "    \"\"\"Forward process: x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * eps\"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_0)\n",
    "    alpha_bar_t = alpha_bars[t].view(-1, 1, 1, 1)\n",
    "    return torch.sqrt(alpha_bar_t) * x_0 + torch.sqrt(1.0 - alpha_bar_t) * noise, noise\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, alpha_bars, T, device):\n",
    "    \"\"\"Train for one epoch. Returns average loss.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for batch_images, _ in dataloader:\n",
    "        x_0 = batch_images.to(device)\n",
    "        batch_size = x_0.shape[0]\n",
    "        t = torch.randint(0, T, (batch_size,), device=device)\n",
    "        noise = torch.randn_like(x_0)\n",
    "        x_t, noise = q_sample(x_0, t, alpha_bars, noise=noise)\n",
    "        epsilon_hat = model(x_t, t)\n",
    "        loss = F.mse_loss(epsilon_hat, noise)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "# Load MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "alpha_bars_device = alpha_bars.to(device)\n",
    "\n",
    "print(f'Dataset: {len(dataset)} images, {len(dataloader)} batches per epoch')\n",
    "print(f'Device: {device}')\n",
    "print('Training infrastructure ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# VERSION A: Simple linear timestep embedding (the capstone approach)\n",
    "# =====================================================================\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Two convolutions with BatchNorm and ReLU.\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class SimpleTimestepEmbed(nn.Module):\n",
    "    \"\"\"The capstone's simple approach: normalize t to [0,1], 2-layer MLP.\"\"\"\n",
    "    def __init__(self, d_emb=128):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(1, d_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_emb, d_emb),\n",
    "        )\n",
    "    def forward(self, t):\n",
    "        t_normalized = t.float().unsqueeze(1) / T  # [batch, 1]\n",
    "        return self.mlp(t_normalized)  # [batch, d_emb]\n",
    "\n",
    "\n",
    "class UNetSimple(nn.Module):\n",
    "    \"\"\"Minimal U-Net with SIMPLE timestep embedding (normalize + MLP).\n",
    "    Same architecture as the capstone: timestep added to bottleneck only.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time_embed = SimpleTimestepEmbed(d_emb=128)\n",
    "\n",
    "        self.enc1 = ConvBlock(1, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = ConvBlock(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = ConvBlock(64, 128)\n",
    "        self.bottleneck = ConvBlock(128, 256)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec3 = ConvBlock(128 + 64, 128)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec2 = ConvBlock(64 + 32, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_embed(t)  # [B, 128]\n",
    "\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        b = self.bottleneck(e3)\n",
    "\n",
    "        # Simple approach: add timestep to bottleneck only\n",
    "        t_emb_spatial = t_emb.view(-1, 128, 1, 1)\n",
    "        b[:, :128, :, :] = b[:, :128, :, :] + t_emb_spatial\n",
    "\n",
    "        d3 = self.dec3(torch.cat([self.up3(b), e2], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e1], dim=1))\n",
    "        return self.final(d2)\n",
    "\n",
    "\n",
    "print('UNetSimple defined (capstone-style simple timestep embedding).')\n",
    "print(f'Parameters: {sum(p.numel() for p in UNetSimple().parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# VERSION B: Your task -- sinusoidal timestep embedding\n",
    "# =====================================================================\n",
    "#\n",
    "# Create UNetSinusoidal: same architecture as UNetSimple, but replace\n",
    "# SimpleTimestepEmbed with the sinusoidal + MLP pipeline from Exercise 2.\n",
    "#\n",
    "# Changes needed:\n",
    "# 1. Use TimestepEmbedding (sinusoidal + MLP) instead of SimpleTimestepEmbed\n",
    "# 2. The embedding dimension changes from 128 to 512\n",
    "# 3. Update the bottleneck injection to match the new dimension\n",
    "#\n",
    "# Write the full class below. Use UNetSimple as your starting point.\n",
    "# The only changes are in the timestep embedding and how it is injected.\n",
    "\n",
    "# YOUR CODE HERE: define class UNetSinusoidal\n",
    "\n",
    "\n",
    "print('Define UNetSinusoidal in the cell above before running this.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The key insight: the only changes are (1) swapping the timestep embedding module and (2) adjusting the injection dimensions. The spatial architecture (encoder, decoder, skip connections) is completely unchanged.\n",
    "\n",
    "```python\n",
    "class UNetSinusoidal(nn.Module):\n",
    "    \"\"\"Minimal U-Net with SINUSOIDAL timestep embedding.\n",
    "    Same spatial architecture as UNetSimple; only the timestep\n",
    "    embedding and its injection change.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Sinusoidal + MLP instead of simple linear\n",
    "        self.time_embed = TimestepEmbedding(d_sinusoidal=256, d_emb=512)\n",
    "        # Project 512-dim embedding to 256 channels to match bottleneck\n",
    "        self.time_proj = nn.Linear(512, 256)\n",
    "\n",
    "        self.enc1 = ConvBlock(1, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = ConvBlock(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = ConvBlock(64, 128)\n",
    "        self.bottleneck = ConvBlock(128, 256)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec3 = ConvBlock(128 + 64, 128)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec2 = ConvBlock(64 + 32, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_embed(t)          # [B, 512]  (sinusoidal + MLP)\n",
    "        t_proj = self.time_proj(t_emb)      # [B, 256]  (project to bottleneck size)\n",
    "\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        b = self.bottleneck(e3)\n",
    "\n",
    "        # Add timestep to ALL bottleneck channels (256-dim projection)\n",
    "        t_spatial = t_proj.view(-1, 256, 1, 1)\n",
    "        b = b + t_spatial\n",
    "\n",
    "        d3 = self.dec3(torch.cat([self.up3(b), e2], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e1], dim=1))\n",
    "        return self.final(d2)\n",
    "```\n",
    "\n",
    "The spatial architecture is identical. The changes:\n",
    "1. `TimestepEmbedding` replaces `SimpleTimestepEmbed` -- sinusoidal encoding + MLP instead of normalize + MLP\n",
    "2. A `time_proj` linear layer maps the 512-dim embedding to 256 channels (matching the bottleneck width)\n",
    "3. The embedding is added to ALL 256 bottleneck channels instead of just the first 128\n",
    "\n",
    "The sinusoidal encoding gives the network a much richer starting representation for the timestep. Instead of a single normalized scalar that the MLP must stretch into 128 dimensions, the sinusoidal encoding provides 256 frequency components that spread the timestep information across many dimensions from the start.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# Train both versions and compare\n",
    "# =====================================================================\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# --- Version A: Simple ---\n",
    "torch.manual_seed(42)\n",
    "model_simple = UNetSimple().to(device)\n",
    "opt_simple = torch.optim.Adam(model_simple.parameters(), lr=2e-4)\n",
    "\n",
    "print(f'UNetSimple parameters:     {sum(p.numel() for p in model_simple.parameters()):,}')\n",
    "\n",
    "# --- Version B: Sinusoidal ---\n",
    "torch.manual_seed(42)\n",
    "model_sin = UNetSinusoidal().to(device)\n",
    "opt_sin = torch.optim.Adam(model_sin.parameters(), lr=2e-4)\n",
    "\n",
    "print(f'UNetSinusoidal parameters: {sum(p.numel() for p in model_sin.parameters()):,}')\n",
    "print()\n",
    "\n",
    "losses_simple = []\n",
    "losses_sin = []\n",
    "\n",
    "print(f'Training both models for {N_EPOCHS} epochs...')\n",
    "print(f'{\"Epoch\":>6s}  {\"Simple\":>10s}  {\"Sinusoidal\":>12s}  {\"Time\":>8s}')\n",
    "print('-' * 42)\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    start = time.time()\n",
    "\n",
    "    loss_a = train_epoch(model_simple, dataloader, opt_simple, alpha_bars_device, T, device)\n",
    "    loss_b = train_epoch(model_sin, dataloader, opt_sin, alpha_bars_device, T, device)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    losses_simple.append(loss_a)\n",
    "    losses_sin.append(loss_b)\n",
    "\n",
    "    print(f'{epoch:>6d}  {loss_a:>10.4f}  {loss_b:>12.4f}  {elapsed:>7.1f}s')\n",
    "\n",
    "print('\\nTraining complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, N_EPOCHS + 1), losses_simple, 'o-', color='#f87171',\n",
    "         linewidth=2, markersize=6, label='Simple (normalize + MLP)')\n",
    "plt.plot(range(1, N_EPOCHS + 1), losses_sin, 's-', color='#86efac',\n",
    "         linewidth=2, markersize=6, label='Sinusoidal + MLP')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Average Loss', fontsize=12)\n",
    "plt.title('Timestep Embedding Comparison: Simple vs Sinusoidal', fontsize=13)\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f'Final loss (Simple):      {losses_simple[-1]:.4f}')\n",
    "print(f'Final loss (Sinusoidal):  {losses_sin[-1]:.4f}')\n",
    "print(f'Difference:               {losses_simple[-1] - losses_sin[-1]:+.4f}')\n",
    "print()\n",
    "\n",
    "if losses_sin[-1] < losses_simple[-1]:\n",
    "    print('The sinusoidal version achieves lower loss.')\n",
    "    print('Why? The sinusoidal encoding spreads timestep information across 256 frequency')\n",
    "    print('components from the start. The simple approach maps a single scalar through an MLP,')\n",
    "    print('which must learn ALL the useful representations from scratch.')\n",
    "    print()\n",
    "    print('Analogy from the lesson: describing a GPS position with latitude + longitude + altitude')\n",
    "    print('(sinusoidal) vs a single distance number (simple). The richer input gives the')\n",
    "    print('network much more to work with.')\n",
    "else:\n",
    "    print('Interesting -- the simple version matched or beat sinusoidal on this run.')\n",
    "    print('This can happen on MNIST because MNIST is easy. The simple approach is sufficient')\n",
    "    print('for very simple data. The advantage of sinusoidal encoding becomes clearer at scale')\n",
    "    print('(higher-resolution images, more complex distributions).')\n",
    "    print()\n",
    "    print('Even so, the sinusoidal approach has structural advantages: guaranteed smoothness')\n",
    "    print('between adjacent timesteps, no dependence on learned representations for the')\n",
    "    print('basic encoding, and a richer input for the MLP to refine.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You compared the capstone's simple timestep embedding to the sinusoidal + MLP approach on the same architecture and data. The sinusoidal version typically converges faster or to a lower loss because:\n",
    "\n",
    "1. **Richer input representation.** The sinusoidal encoding provides 256 frequency components from the start. The simple approach maps a single scalar, so the MLP must learn all structure from scratch.\n",
    "2. **Built-in smoothness.** Adjacent timesteps automatically get similar embeddings. The simple approach must learn this smoothness.\n",
    "3. **Multi-frequency discrimination.** High-frequency components distinguish t=500 from t=501. Low-frequency components distinguish t=500 from t=50. The simple approach has a single number to encode both fine and coarse distinctions.\n",
    "\n",
    "On MNIST (a simple dataset), the gap may be small. On real diffusion tasks (256x256 or 512x512 images), the sinusoidal approach is dramatically better -- which is why every production diffusion model uses it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Sinusoidal timestep embedding is the same formula as positional encoding.** Replace \"position\" with \"timestep\" and you have the diffusion timestep embedding. The multi-frequency encoding provides smooth, unique patterns for each noise level. No learned parameters in the encoding itself.\n",
    "\n",
    "2. **The MLP refines but preserves structure.** The 2-layer MLP transforms the raw frequencies into features useful for denoising. It cannot destroy the smoothness of its input -- adjacent timesteps stay similar after the MLP.\n",
    "\n",
    "3. **Adaptive group normalization makes gamma and beta depend on the timestep.** The normalization is standard. The scale and shift are timestep-dependent, computed by a per-block linear projection. Same architecture, same weights -- different behavior at different noise levels.\n",
    "\n",
    "4. **Sinusoidal encoding outperforms simple linear projection.** The richer, multi-frequency input gives the network more to work with. The built-in smoothness means adjacent timesteps are similar by construction, not by learning.\n",
    "\n",
    "5. **The mental model: same orchestra, different conductor's score.** The U-Net weights are the instruments. The timestep embedding is the conductor's score. Adaptive normalization is how the conductor communicates dynamics (scale) and key (shift) to each section. Different measures (timesteps) produce different performances from the same orchestra."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
