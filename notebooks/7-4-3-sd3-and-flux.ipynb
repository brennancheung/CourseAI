{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SD3 & Flux\n",
    "\n",
    "**Module 7.4, Lesson 3** | CourseAI\n",
    "\n",
    "SD3 and Flux combine the DiT architecture with joint text-image attention (MMDiT), T5-XXL text encoding, and flow matching. Every component traces to a lesson you have already completed. This notebook makes those components concrete and inspectable.\n",
    "\n",
    "**What you will do:**\n",
    "- Load SD3 Medium and inspect the triple text encoder setup: verify parameter counts, embedding shapes, and the scale difference between CLIP and T5-XXL\n",
    "- Extract attention weights from an MMDiT block and visualize the four quadrants of joint attention: text-to-text, text-to-image, image-to-text, image-to-image\n",
    "- Generate images with SD3 at different step counts (10, 20, 30, 50) and observe the flow matching payoff: good results in 20–30 steps\n",
    "- Trace the full SD3 pipeline end-to-end, capturing intermediate outputs and annotating each step with the lesson that covered it\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "Every concept in this notebook comes from the lesson. MMDiT as \"one room, one conversation,\" T5-XXL as a language model complementing CLIP, flow matching as the same objective you trained with in **Flow Matching**. No new theory—just hands-on verification of what you just read.\n",
    "\n",
    "**Estimated time:** 60–90 minutes. All exercises require a GPU runtime with at least 16 GB VRAM (Colab A100 recommended, T4 with float16 may work for some exercises). SD3 Medium is a large model (~12 GB in float16)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies and configure the environment.\n",
    "\n",
    "**Important:** Switch to a GPU runtime in Colab (Runtime > Change runtime type > A100 GPU). SD3 Medium requires ~12 GB VRAM in float16. A T4 (16 GB) is tight but may work; an A100 (40 GB) is comfortable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate safetensors sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [14, 5]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(f'Device: {device}')\n",
    "print(f'Dtype: {dtype}')\n",
    "if device.type == 'cpu':\n",
    "    print('WARNING: No GPU detected. All exercises in this notebook require a GPU.')\n",
    "    print('Switch to GPU: Runtime > Change runtime type > A100 GPU')\n",
    "print()\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Shared Helpers\n",
    "\n",
    "Utility functions used across multiple exercises. Run this cell now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count total parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def count_parameters_by_type(model):\n",
    "    \"\"\"Count parameters grouped by module type (e.g., Linear, LayerNorm).\"\"\"\n",
    "    counts = {}\n",
    "    for name, module in model.named_modules():\n",
    "        module_type = module.__class__.__name__\n",
    "        if module_type == type(model).__name__:\n",
    "            continue\n",
    "        n_params = sum(p.numel() for p in module.parameters(recurse=False))\n",
    "        if n_params > 0:\n",
    "            counts[module_type] = counts.get(module_type, 0) + n_params\n",
    "    return dict(sorted(counts.items(), key=lambda x: -x[1]))\n",
    "\n",
    "\n",
    "def show_image_row(images, titles, suptitle=None, figsize=None):\n",
    "    \"\"\"Display a row of PIL images with titles.\"\"\"\n",
    "    n = len(images)\n",
    "    fig_w = figsize[0] if figsize else max(5 * n, 12)\n",
    "    fig_h = figsize[1] if figsize else 5\n",
    "    fig, axes = plt.subplots(1, n, figsize=(fig_w, fig_h))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle, fontsize=13, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def free_memory(*objects):\n",
    "    \"\"\"Delete objects and free GPU memory.\"\"\"\n",
    "    for obj in objects:\n",
    "        del obj\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print('Memory freed.')\n",
    "\n",
    "\n",
    "print('Helpers defined: count_parameters, count_parameters_by_type, show_image_row, free_memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: SD3 Pipeline Inspection `[Guided]`\n",
    "\n",
    "The lesson taught that SD3 uses **three text encoders simultaneously**: CLIP ViT-L `[77, 768]`, OpenCLIP ViT-bigG `[77, 1280]`, and T5-XXL `[77, 4096]`. Each provides different information—CLIP gives image-aligned text understanding, T5 gives deep linguistic understanding. The denoising network is an MMDiT (Multimodal Diffusion Transformer) with modality-specific Q/K/V projections.\n",
    "\n",
    "In this exercise, you will load SD3 Medium and verify all of this concretely:\n",
    "1. Print the text encoder class names and parameter counts\n",
    "2. Verify the embedding output shapes match the lesson's tensor shape trace\n",
    "3. Inspect the MMDiT transformer: count parameters, identify modality-specific projections\n",
    "4. Compare text encoder parameters vs denoising network parameters\n",
    "\n",
    "**Before running, predict:**\n",
    "- SD3 uses T5-XXL with 4.7B parameters and a denoising network (MMDiT) with ~2B parameters. Which component has more parameters?\n",
    "- The lesson showed CLIP ViT-L produces `[77, 768]` embeddings and T5-XXL produces `[77, 4096]`. How many times wider are T5's embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1: Load SD3 Medium and inspect its components\n",
    "# ============================================================\n",
    "\n",
    "# --- Step 1: Load SD3 Medium ---\n",
    "# SD3 Medium is available via the diffusers StableDiffusion3Pipeline.\n",
    "# We load in float16 to fit in GPU memory.\n",
    "# NOTE: You may need to accept the model license on HuggingFace and\n",
    "# log in with `huggingface-cli login` or pass a token.\n",
    "\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "\n",
    "print('Loading SD3 Medium... (this may take a minute)')\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\n",
    "    'stabilityai/stable-diffusion-3-medium-diffusers',\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "print('SD3 Medium loaded.')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Inspect the three text encoders ---\n",
    "# SD3 has three text encoders: text_encoder, text_encoder_2, text_encoder_3.\n",
    "# The lesson predicted: CLIP ViT-L, OpenCLIP ViT-bigG, T5-XXL.\n",
    "\n",
    "encoders = [\n",
    "    ('text_encoder (CLIP ViT-L)', pipe.text_encoder),\n",
    "    ('text_encoder_2 (OpenCLIP ViT-bigG)', pipe.text_encoder_2),\n",
    "    ('text_encoder_3 (T5-XXL)', pipe.text_encoder_3),\n",
    "]\n",
    "\n",
    "print('SD3 Text Encoders:')\n",
    "print('=' * 75)\n",
    "total_encoder_params = 0\n",
    "for name, encoder in encoders:\n",
    "    if encoder is None:\n",
    "        print(f'  {name}: NOT LOADED (may be optional)')\n",
    "        continue\n",
    "    n_params = count_parameters(encoder)\n",
    "    total_encoder_params += n_params\n",
    "    print(f'  {name}')\n",
    "    print(f'    Class: {type(encoder).__name__}')\n",
    "    print(f'    Parameters: {n_params:,} ({n_params / 1e9:.2f}B)')\n",
    "    print()\n",
    "\n",
    "print(f'Total text encoder parameters: {total_encoder_params:,} ({total_encoder_params / 1e9:.2f}B)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Verify embedding output shapes ---\n",
    "# Encode a test prompt through each encoder and check the output shapes.\n",
    "# Expected from the lesson:\n",
    "#   CLIP ViT-L:       [1, 77, 768]\n",
    "#   OpenCLIP ViT-bigG: [1, 77, 1280]\n",
    "#   T5-XXL:           [1, 77, 4096]\n",
    "\n",
    "test_prompt = 'a cat sitting on a beach at sunset'\n",
    "\n",
    "# Use the pipeline's encode_prompt method to get all embeddings at once\n",
    "with torch.no_grad():\n",
    "    (\n",
    "        prompt_embeds,       # Combined text embeddings for joint attention\n",
    "        negative_prompt_embeds,\n",
    "        pooled_prompt_embeds,  # Pooled CLIP embeddings for adaLN-Zero\n",
    "        negative_pooled_prompt_embeds,\n",
    "    ) = pipe.encode_prompt(\n",
    "        prompt=test_prompt,\n",
    "        prompt_2=test_prompt,\n",
    "        prompt_3=test_prompt,\n",
    "    )\n",
    "\n",
    "print(f'Prompt: \"{test_prompt}\"')\n",
    "print()\n",
    "print('Combined embeddings (what enters joint attention):')\n",
    "print(f'  prompt_embeds shape:         {list(prompt_embeds.shape)}')\n",
    "print(f'  pooled_prompt_embeds shape:  {list(pooled_prompt_embeds.shape)}')\n",
    "print()\n",
    "print('The prompt_embeds are the per-token text embeddings that become')\n",
    "print('text tokens in the joint attention sequence.')\n",
    "print('The pooled_prompt_embeds provide global conditioning via adaLN-Zero')\n",
    "print('(same dual-path pattern as SDXL).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Encode through individual text encoders to verify shapes ---\n",
    "# We also encode individually to confirm the per-encoder shapes.\n",
    "\n",
    "tokenizer_1 = pipe.tokenizer\n",
    "tokenizer_2 = pipe.tokenizer_2\n",
    "tokenizer_3 = pipe.tokenizer_3\n",
    "\n",
    "# CLIP ViT-L\n",
    "if pipe.text_encoder is not None:\n",
    "    tokens_1 = tokenizer_1(\n",
    "        test_prompt, return_tensors='pt', padding='max_length',\n",
    "        max_length=77, truncation=True\n",
    "    ).input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        out_1 = pipe.text_encoder(tokens_1, output_hidden_states=True)\n",
    "    clip_hidden = out_1.hidden_states[-2]  # penultimate layer (standard for SD)\n",
    "    clip_pooled = out_1[1]  # pooled output\n",
    "    print(f'CLIP ViT-L:')\n",
    "    print(f'  Hidden states shape:  {list(clip_hidden.shape)}  (expected [1, 77, 768])')\n",
    "    print(f'  Pooled output shape:  {list(clip_pooled.shape)}')\n",
    "    print()\n",
    "\n",
    "# OpenCLIP ViT-bigG\n",
    "if pipe.text_encoder_2 is not None:\n",
    "    tokens_2 = tokenizer_2(\n",
    "        test_prompt, return_tensors='pt', padding='max_length',\n",
    "        max_length=77, truncation=True\n",
    "    ).input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        out_2 = pipe.text_encoder_2(tokens_2, output_hidden_states=True)\n",
    "    clip2_hidden = out_2.hidden_states[-2]\n",
    "    clip2_pooled = out_2[1]\n",
    "    print(f'OpenCLIP ViT-bigG:')\n",
    "    print(f'  Hidden states shape:  {list(clip2_hidden.shape)}  (expected [1, 77, 1280])')\n",
    "    print(f'  Pooled output shape:  {list(clip2_pooled.shape)}')\n",
    "    print()\n",
    "\n",
    "# T5-XXL\n",
    "if pipe.text_encoder_3 is not None:\n",
    "    tokens_3 = tokenizer_3(\n",
    "        test_prompt, return_tensors='pt', padding='max_length',\n",
    "        max_length=77, truncation=True\n",
    "    ).input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        out_3 = pipe.text_encoder_3(tokens_3)\n",
    "    t5_hidden = out_3.last_hidden_state\n",
    "    print(f'T5-XXL:')\n",
    "    print(f'  Hidden states shape:  {list(t5_hidden.shape)}  (expected [1, 77, 4096])')\n",
    "    print(f'  No pooled output (T5 is encoder-only for embeddings)')\n",
    "    print()\n",
    "\n",
    "print('Shape comparison:')\n",
    "print(f'  CLIP ViT-L:       [1, 77, 768]    ->  768 dims per token')\n",
    "print(f'  OpenCLIP ViT-bigG: [1, 77, 1280]  -> 1280 dims per token')\n",
    "print(f'  T5-XXL:           [1, 77, 4096]   -> 4096 dims per token')\n",
    "print()\n",
    "print(f'T5 embeddings are {4096 // 768}x wider than CLIP ViT-L.')\n",
    "print('More dimensions = more information per token = richer text understanding.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Inspect the MMDiT transformer ---\n",
    "# The denoising network is pipe.transformer (an SD3Transformer2DModel).\n",
    "\n",
    "transformer = pipe.transformer\n",
    "transformer_params = count_parameters(transformer)\n",
    "\n",
    "print('MMDiT Transformer (Denoising Network):')\n",
    "print(f'  Class: {type(transformer).__name__}')\n",
    "print(f'  Parameters: {transformer_params:,} ({transformer_params / 1e9:.2f}B)')\n",
    "print()\n",
    "\n",
    "# Count transformer blocks\n",
    "n_blocks = len(transformer.transformer_blocks)\n",
    "print(f'  Number of MMDiT blocks: {n_blocks}')\n",
    "print()\n",
    "\n",
    "# Parameter breakdown by top-level component\n",
    "print('Parameter breakdown:')\n",
    "groups = {}\n",
    "for name, module in transformer.named_children():\n",
    "    n_params = count_parameters(module)\n",
    "    if n_params > 0:\n",
    "        groups[name] = n_params\n",
    "\n",
    "for group, count in sorted(groups.items(), key=lambda x: -x[1]):\n",
    "    pct = count / transformer_params * 100\n",
    "    print(f'    {group:<35} {count:>14,} ({pct:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Inspect one MMDiT block for modality-specific projections ---\n",
    "# The lesson explained that MMDiT has SEPARATE Q/K/V projections for\n",
    "# text tokens and image tokens. This is the key architectural detail.\n",
    "\n",
    "block_0 = transformer.transformer_blocks[0]\n",
    "print(f'MMDiT Block 0: {type(block_0).__name__}')\n",
    "print()\n",
    "print('Sub-modules (looking for modality-specific projections):')\n",
    "print('-' * 80)\n",
    "for name, module in block_0.named_modules():\n",
    "    if name == '':\n",
    "        continue\n",
    "    n_params = sum(p.numel() for p in module.parameters(recurse=False))\n",
    "    if n_params > 0:\n",
    "        print(f'  {name:<50} {type(module).__name__:<15} ({n_params:,})')\n",
    "\n",
    "block_params = count_parameters(block_0)\n",
    "print(f'\\nBlock 0 total: {block_params:,} parameters')\n",
    "print(f'All {n_blocks} blocks: ~{block_params * n_blocks:,} parameters (estimate)')\n",
    "print()\n",
    "print('Look for modality-specific components:')\n",
    "print('  - Separate attention projections for text vs image (Q/K/V per modality)')\n",
    "print('  - Separate FFN layers for text vs image')\n",
    "print('  - Separate norm layers per modality')\n",
    "print('  - adaLN modulation MLP for timestep conditioning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 7: Compare text encoder vs denoising network parameters ---\n",
    "\n",
    "vae_params = count_parameters(pipe.vae)\n",
    "\n",
    "print('SD3 Medium: Parameter Breakdown')\n",
    "print('=' * 55)\n",
    "print(f'  CLIP ViT-L (text_encoder):     {count_parameters(pipe.text_encoder):>14,}') if pipe.text_encoder else None\n",
    "print(f'  OpenCLIP ViT-bigG (text_enc_2): {count_parameters(pipe.text_encoder_2):>13,}') if pipe.text_encoder_2 else None\n",
    "print(f'  T5-XXL (text_encoder_3):       {count_parameters(pipe.text_encoder_3):>14,}') if pipe.text_encoder_3 else None\n",
    "print(f'  MMDiT transformer:             {transformer_params:>14,}')\n",
    "print(f'  VAE:                           {vae_params:>14,}')\n",
    "print('-' * 55)\n",
    "total = total_encoder_params + transformer_params + vae_params\n",
    "print(f'  Total:                         {total:>14,} ({total / 1e9:.2f}B)')\n",
    "print()\n",
    "print(f'Text encoders:    {total_encoder_params / 1e9:.2f}B ({total_encoder_params / total * 100:.0f}% of total)')\n",
    "print(f'Denoising (MMDiT): {transformer_params / 1e9:.2f}B ({transformer_params / total * 100:.0f}% of total)')\n",
    "print(f'VAE:              {vae_params / 1e6:.0f}M ({vae_params / total * 100:.0f}% of total)')\n",
    "print()\n",
    "print('The text encoders (especially T5-XXL) are a huge fraction of the total model.')\n",
    "print('The field has recognized that text understanding is a bottleneck worth')\n",
    "print('investing parameters in. Understanding the prompt is as important as')\n",
    "print('generating the image.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You loaded SD3 Medium and verified the architecture the lesson described:\n",
    "\n",
    "- **Three text encoders, as predicted.** CLIP ViT-L `[77, 768]`, OpenCLIP ViT-bigG `[77, 1280]`, and T5-XXL `[77, 4096]`. T5's embeddings are 5x wider than CLIP's—more information per token, capturing linguistic structure rather than visual alignment.\n",
    "\n",
    "- **The text encoders are enormous.** T5-XXL alone has ~4.7B parameters. The total text encoder parameter count exceeds the denoising network. This reflects the field's recognition that text understanding is a bottleneck worth investing in.\n",
    "\n",
    "- **Modality-specific components in each MMDiT block.** Inside each block you can see separate attention projections and FFN layers for text vs image tokens. This is the \"shared listening, separate thinking\" pattern from the lesson: shared attention computation, modality-specific projections and processing.\n",
    "\n",
    "- **Pooled embeddings for global conditioning.** The CLIP pooled outputs provide global conditioning via adaLN-Zero (same dual-path pattern as SDXL). T5 provides per-token embeddings only—no pooled summary vector.\n",
    "\n",
    "- **The progression is visible in the numbers.** SD v1.5: one encoder (123M). SDXL: two encoders (~477M). SD3: three encoders (~5B+). Each generation adds richer text understanding.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": "## Exercise 2: Joint Attention Structure `[Guided]`\n\nThe lesson taught that MMDiT replaces cross-attention with **joint self-attention**: concatenate text tokens and image patch tokens into one sequence, then run standard self-attention on the combined sequence. This provides four types of attention simultaneously:\n\n- **Image-to-text** (image tokens read text tokens)—equivalent to cross-attention in the U-Net\n- **Text-to-image** (text tokens read image tokens)—NEW: text representations update based on image content\n- **Image-to-image** (image tokens read image tokens)—equivalent to self-attention in DiT\n- **Text-to-text** (text tokens read text tokens)—NEW: text representations refine each other\n\nIn this exercise, you will:\n1. Encode a prompt and prepare a noisy latent\n2. Verify the concatenated sequence length (text tokens + image tokens)\n3. Inspect the MMDiT block's attention architecture to confirm modality-specific projections\n4. Visualize the **four-quadrant structure** of the joint attention matrix using synthetic data\n\n**Why synthetic data?** The diffusers library does not expose per-head attention weights from the internal `Attention` module during a forward pass. Rather than fighting the implementation, we construct synthetic Q/K vectors matching the model's dimensions to demonstrate what each quadrant of the joint attention matrix *represents*. The quadrant structure is the key insight—it holds regardless of the specific trained weight values.\n\n**Before running, predict:**\n- If the text sequence has 77 tokens and the image has 1024 patch tokens (64x64 latent, patch size 2), what is the joint attention matrix shape?\n- Which quadrant will have the highest attention weights? (Hint: the primary task is spatial coherence in the image.)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 2: Visualize joint attention in an MMDiT block\n",
    "# ============================================================\n",
    "\n",
    "# --- Step 1: Prepare inputs for one denoising step ---\n",
    "# We need: text embeddings (from the encoders), a noisy latent,\n",
    "# and a timestep. We'll run a single forward pass through the\n",
    "# transformer and capture attention weights.\n",
    "\n",
    "prompt = 'a cat sitting on a beach at sunset'\n",
    "\n",
    "# Encode the prompt (reuse from Exercise 1 if still in memory)\n",
    "with torch.no_grad():\n",
    "    (\n",
    "        prompt_embeds,\n",
    "        negative_prompt_embeds,\n",
    "        pooled_prompt_embeds,\n",
    "        negative_pooled_prompt_embeds,\n",
    "    ) = pipe.encode_prompt(\n",
    "        prompt=prompt,\n",
    "        prompt_2=prompt,\n",
    "        prompt_3=prompt,\n",
    "    )\n",
    "\n",
    "print(f'Prompt embeddings shape: {list(prompt_embeds.shape)}')\n",
    "print(f'  This is the text token sequence that enters joint attention.')\n",
    "\n",
    "# Get the expected latent shape from the pipeline\n",
    "# SD3 Medium generates at 1024x1024 by default, but we can use\n",
    "# a smaller size for faster inspection.\n",
    "height, width = 512, 512\n",
    "latent_h = height // pipe.vae_scale_factor\n",
    "latent_w = width // pipe.vae_scale_factor\n",
    "latent_channels = pipe.transformer.config.in_channels\n",
    "\n",
    "print(f'\\nLatent shape: [{latent_channels}, {latent_h}, {latent_w}]')\n",
    "\n",
    "# Compute expected patch count\n",
    "patch_size = pipe.transformer.config.patch_size\n",
    "n_image_tokens = (latent_h // patch_size) * (latent_w // patch_size)\n",
    "n_text_tokens = prompt_embeds.shape[1]\n",
    "n_total_tokens = n_text_tokens + n_image_tokens\n",
    "\n",
    "print(f'Patch size: {patch_size}')\n",
    "print(f'Image patch tokens: ({latent_h}//{patch_size}) * ({latent_w}//{patch_size}) = {n_image_tokens}')\n",
    "print(f'Text tokens: {n_text_tokens}')\n",
    "print(f'Total joint sequence: {n_text_tokens} + {n_image_tokens} = {n_total_tokens}')\n",
    "print(f'Expected attention matrix: [{n_total_tokens}, {n_total_tokens}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Hook into an MMDiT block to capture attention weights ---\n",
    "# We register a forward hook on one block's attention module to\n",
    "# extract the attention weight matrix during a forward pass.\n",
    "\n",
    "# Storage for captured attention weights\n",
    "captured_attention = {}\n",
    "\n",
    "def capture_attention_hook(module, args, kwargs, output):\n",
    "    \"\"\"Hook to capture attention weights from an Attention module.\"\"\"\n",
    "    # We need to run attention manually to get weights.\n",
    "    # The diffusers Attention module does not return weights by default,\n",
    "    # so we compute them from the Q/K projections.\n",
    "    captured_attention['output'] = output\n",
    "    return output\n",
    "\n",
    "# We will use a different approach: run the pipeline for one step\n",
    "# and capture the hidden states by hooking into the block.\n",
    "# Let's capture the joint hidden states entering and exiting a block.\n",
    "\n",
    "block_index = 0  # Inspect the first MMDiT block\n",
    "block = pipe.transformer.transformer_blocks[block_index]\n",
    "\n",
    "# Hook to capture the Q and K after projection (to compute attention weights)\n",
    "captured_qk = {}\n",
    "\n",
    "def capture_qk_hook(module, input, output):\n",
    "    \"\"\"Capture the attention module's input (hidden states) to compute Q/K.\"\"\"\n",
    "    captured_qk['input'] = input\n",
    "    captured_qk['output'] = output\n",
    "\n",
    "hook = block.attn.register_forward_hook(capture_qk_hook)\n",
    "print(f'Hook registered on block {block_index} attention module.')\n",
    "print(f'Block type: {type(block).__name__}')\n",
    "print(f'Attention type: {type(block.attn).__name__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Run one denoising step to trigger the hook ---\n",
    "# We generate with just 1 step to capture the attention patterns.\n",
    "\n",
    "generator = torch.Generator(device='cpu').manual_seed(42)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Run just 2 steps (minimum for the scheduler to work)\n",
    "    result = pipe(\n",
    "        prompt=prompt,\n",
    "        prompt_2=prompt,\n",
    "        prompt_3=prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_inference_steps=2,\n",
    "        guidance_scale=7.0,\n",
    "        generator=generator,\n",
    "        output_type='pil',\n",
    "    )\n",
    "\n",
    "print(f'Generation complete. Captured data from block {block_index}.')\n",
    "print(f'Captured keys: {list(captured_qk.keys())}')\n",
    "\n",
    "# Remove hook\n",
    "hook.remove()\n",
    "print('Hook removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Compute attention weights from the captured Q and K ---\n",
    "# The attention module in diffusers processes the joint QKV internally.\n",
    "# We'll manually compute Q @ K^T to get the attention pattern.\n",
    "#\n",
    "# The MMDiT block has modality-specific Q/K/V projections.\n",
    "# text tokens -> text_W_Q, text_W_K\n",
    "# image tokens -> image_W_Q, image_W_K\n",
    "# Then Q and K are concatenated before attention.\n",
    "#\n",
    "# We can recover the attention pattern by looking at the block's\n",
    "# internal processing. Let's inspect what the attention module received.\n",
    "\n",
    "# Check input shapes\n",
    "if 'input' in captured_qk:\n",
    "    for i, inp in enumerate(captured_qk['input']):\n",
    "        if isinstance(inp, torch.Tensor):\n",
    "            print(f'  Input {i} shape: {list(inp.shape)}')\n",
    "\n",
    "# For visualization, we need to manually compute the attention weights.\n",
    "# Let's do this by running the Q/K projections ourselves on captured inputs.\n",
    "# The joint hidden state entering the attention module contains\n",
    "# both text and image tokens concatenated.\n",
    "\n",
    "# Get the attention module's key components\n",
    "attn = block.attn\n",
    "print(f'\\nAttention module components:')\n",
    "for name, mod in attn.named_modules():\n",
    "    if name == '':\n",
    "        continue\n",
    "    n = sum(p.numel() for p in mod.parameters(recurse=False))\n",
    "    if n > 0:\n",
    "        print(f'  {name:<30} {type(mod).__name__:<15} ({n:,})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "# --- Step 5: Prepare to visualize the four-quadrant structure ---\n# The diffusers Attention module computes attention internally and\n# does not return per-head attention weight matrices. Extracting\n# real weights would require monkey-patching the module's forward\n# method, which is fragile across library versions.\n#\n# Instead, we demonstrate the STRUCTURE of joint attention: what\n# the four quadrants represent, how the matrix dimensions work,\n# and how the attention cost compares to cross-attention. The\n# quadrant structure is a property of the architecture, not of\n# any specific trained weights.\n\n# Create synthetic joint hidden states matching the expected dimensions\nd_model = pipe.transformer.config.joint_attention_dim\nnum_heads = pipe.transformer.config.num_attention_heads\nd_head = d_model // num_heads\n\nprint(f'MMDiT attention config:')\nprint(f'  d_model (joint_attention_dim): {d_model}')\nprint(f'  num_heads: {num_heads}')\nprint(f'  d_head: {d_head}')\nprint(f'  text tokens: {n_text_tokens}')\nprint(f'  image tokens: {n_image_tokens}')\nprint(f'  total tokens: {n_total_tokens}')\nprint()\n\n# For visualization, we use a MUCH smaller token count to make\n# the attention matrix viewable. With 1024 image tokens, the matrix\n# is 1101x1101 which is hard to see patterns in.\n# The quadrant structure is identical at any size.\n\nn_text_vis = 10  # Show first 10 text tokens for clarity\nn_image_vis = 32  # Show 32 image tokens (e.g., a 4x8 patch grid)\nn_total_vis = n_text_vis + n_image_vis\n\nprint(f'Visualization dimensions (downsampled for clarity):')\nprint(f'  Text tokens shown:  {n_text_vis}')\nprint(f'  Image tokens shown: {n_image_vis}')\nprint(f'  Total: {n_total_vis}')\nprint(f'  Attention matrix: [{n_total_vis}, {n_total_vis}]')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "# --- Step 6: Construct a synthetic attention matrix showing quadrant structure ---\n# These are NOT trained attention weights from the model. We construct\n# synthetic Q and K vectors to produce a plausible attention pattern\n# that illustrates the four-quadrant structure of joint attention.\n#\n# The pedagogical point: regardless of what specific values the trained\n# model learns, the joint attention matrix ALWAYS has these four quadrants.\n# The quadrant structure is an architectural property, not a learned one.\n\ntorch.manual_seed(42)\n\n# Create Q and K with separate \"modality\" structure\n# Text tokens: similar to each other (same prompt, coherent meaning)\ntext_base = torch.randn(1, 1, d_head) * 0.5\ntext_Q = text_base + torch.randn(1, n_text_vis, d_head) * 0.3\ntext_K = text_base + torch.randn(1, n_text_vis, d_head) * 0.3\n\n# Image tokens: spatial locality (nearby patches attend to each other)\nimage_Q = torch.randn(1, n_image_vis, d_head) * 0.5\nimage_K = torch.randn(1, n_image_vis, d_head) * 0.5\n\n# Concatenate (joint attention)\njoint_Q = torch.cat([text_Q, image_Q], dim=1)  # [1, 42, d_head]\njoint_K = torch.cat([text_K, image_K], dim=1)  # [1, 42, d_head]\n\n# Compute attention weights: softmax(Q @ K^T / sqrt(d_head))\nimport math\nattn_logits = torch.bmm(joint_Q, joint_K.transpose(1, 2)) / math.sqrt(d_head)\nattn_weights = torch.softmax(attn_logits, dim=-1)  # [1, 42, 42]\n\nprint(f'Joint Q shape: {list(joint_Q.shape)}')\nprint(f'Joint K shape: {list(joint_K.shape)}')\nprint(f'Attention weights shape: {list(attn_weights.shape)}')\nprint(f'  = [{n_total_vis}, {n_total_vis}] (every token attends to every other)')\nprint()\nprint('NOTE: These are synthetic weights for structural demonstration,')\nprint('not trained attention patterns from the model.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "# --- Step 7: Visualize the four-quadrant structure ---\n\nattn_np = attn_weights[0].detach().numpy()\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7),\n                                gridspec_kw={'width_ratios': [1, 1]})\n\n# Left: full attention matrix with quadrant labels\nim = ax1.imshow(attn_np, cmap='viridis', aspect='auto')\nax1.set_title(f'Joint Attention Structure [{n_total_vis} x {n_total_vis}]\\n(synthetic weights, real quadrant structure)', fontsize=11)\nax1.set_xlabel('Key (attending TO)', fontsize=10)\nax1.set_ylabel('Query (attending FROM)', fontsize=10)\n\n# Draw quadrant boundaries\nax1.axhline(y=n_text_vis - 0.5, color='red', linewidth=2, linestyle='--')\nax1.axvline(x=n_text_vis - 0.5, color='red', linewidth=2, linestyle='--')\n\n# Label quadrants\n# Top-left: text-to-text\nax1.text(n_text_vis / 2, n_text_vis / 2, 'Text\\u2192Text',\n         ha='center', va='center', fontsize=11, fontweight='bold',\n         color='white', bbox=dict(boxstyle='round', facecolor='black', alpha=0.6))\n# Top-right: text-to-image\nax1.text(n_text_vis + n_image_vis / 2, n_text_vis / 2, 'Text\\u2192Image\\n(NEW)',\n         ha='center', va='center', fontsize=11, fontweight='bold',\n         color='#22d3ee', bbox=dict(boxstyle='round', facecolor='black', alpha=0.6))\n# Bottom-left: image-to-text\nax1.text(n_text_vis / 2, n_text_vis + n_image_vis / 2, 'Image\\u2192Text\\n(was cross-attn)',\n         ha='center', va='center', fontsize=11, fontweight='bold',\n         color='#fbbf24', bbox=dict(boxstyle='round', facecolor='black', alpha=0.6))\n# Bottom-right: image-to-image\nax1.text(n_text_vis + n_image_vis / 2, n_text_vis + n_image_vis / 2, 'Image\\u2192Image\\n(was self-attn)',\n         ha='center', va='center', fontsize=11, fontweight='bold',\n         color='#34d399', bbox=dict(boxstyle='round', facecolor='black', alpha=0.6))\n\nplt.colorbar(im, ax=ax1, label='Attention weight', shrink=0.8)\n\n# Right: comparison diagram showing cross-attention vs joint attention\nax2.set_xlim(0, 10)\nax2.set_ylim(0, 10)\nax2.set_aspect('equal')\nax2.axis('off')\nax2.set_title('Cross-Attention vs Joint Attention', fontsize=12)\n\n# Cross-attention (left side)\nax2.text(2.5, 9.2, 'Cross-Attention (U-Net)', ha='center', fontsize=10, fontweight='bold')\n# Self-attention box\nrect1 = mpatches.FancyBboxPatch((0.5, 6.5), 4, 2, boxstyle='round,pad=0.2',\n                                  facecolor='#34d399', alpha=0.3, edgecolor='#34d399')\nax2.add_patch(rect1)\nax2.text(2.5, 7.5, 'Self-Attn\\n[256, 256]\\nImage\\u2192Image', ha='center', fontsize=8)\n# Cross-attention box\nrect2 = mpatches.FancyBboxPatch((0.5, 4.0), 4, 2, boxstyle='round,pad=0.2',\n                                  facecolor='#fbbf24', alpha=0.3, edgecolor='#fbbf24')\nax2.add_patch(rect2)\nax2.text(2.5, 5.0, 'Cross-Attn\\n[256, 77]\\nImage\\u2192Text', ha='center', fontsize=8)\nax2.text(2.5, 3.5, 'Two operations', ha='center', fontsize=9, color='#ef4444')\n\n# Joint attention (right side)\nax2.text(7.5, 9.2, 'Joint Attention (MMDiT)', ha='center', fontsize=10, fontweight='bold')\nrect3 = mpatches.FancyBboxPatch((5.5, 4.0), 4, 4.5, boxstyle='round,pad=0.2',\n                                  facecolor='#a78bfa', alpha=0.3, edgecolor='#a78bfa')\nax2.add_patch(rect3)\nax2.text(7.5, 7.2, 'Joint Self-Attn', ha='center', fontsize=9, fontweight='bold')\nax2.text(7.5, 6.3, f'[{n_total_tokens}, {n_total_tokens}]', ha='center', fontsize=8)\nax2.text(7.5, 5.4, 'All four directions:', ha='center', fontsize=8)\nax2.text(7.5, 4.7, 'Img\\u2192Img, Img\\u2192Txt\\nTxt\\u2192Img, Txt\\u2192Txt', ha='center', fontsize=7)\nax2.text(7.5, 3.5, 'One operation', ha='center', fontsize=9, color='#22d3ee')\n\n# Arrow showing simplification\nax2.annotate('', xy=(5.3, 6.0), xytext=(4.7, 6.0),\n             arrowprops=dict(arrowstyle='->', color='white', lw=2))\n\nplt.tight_layout()\nplt.show()\n\nprint('The four quadrants of joint attention (architectural structure):')\nprint(f'  Text\\u2192Text  (top-left):     text tokens refine each other')\nprint(f'  Text\\u2192Image (top-right):    text representations update based on image (NEW)')\nprint(f'  Image\\u2192Text (bottom-left):  image reads text (was cross-attention)')\nprint(f'  Image\\u2192Image (bottom-right): image reads image (was self-attention)')\nprint()\nprint('Cross-attention provided only Image\\u2192Text.')\nprint('Joint attention provides all four in one operation.')\nprint('One room, one conversation.')\nprint()\nprint('The specific weight values above are synthetic, but the quadrant')\nprint('structure is real: it follows directly from concatenating text and')\nprint('image tokens into one sequence before self-attention.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 8: Compute attention cost comparison ---\n",
    "# The lesson compared the computational cost:\n",
    "#   Cross-attention: self-attn [256,256] + cross-attn [256,77] = 85,248\n",
    "#   Joint attention: [333,333] = 110,889\n",
    "#\n",
    "# Let's compute this for SD3's actual dimensions.\n",
    "\n",
    "print('Attention cost comparison (SD3 Medium dimensions):')\n",
    "print('=' * 60)\n",
    "print()\n",
    "\n",
    "# Cross-attention approach (hypothetical)\n",
    "self_attn_cost = n_image_tokens * n_image_tokens\n",
    "cross_attn_cost = n_image_tokens * n_text_tokens\n",
    "total_cross = self_attn_cost + cross_attn_cost\n",
    "print(f'Cross-attention approach (two operations):')\n",
    "print(f'  Self-attention:   {n_image_tokens} x {n_image_tokens} = {self_attn_cost:,}')\n",
    "print(f'  Cross-attention:  {n_image_tokens} x {n_text_tokens} = {cross_attn_cost:,}')\n",
    "print(f'  Total:            {total_cross:,}')\n",
    "print()\n",
    "\n",
    "# Joint attention approach (actual)\n",
    "joint_cost = n_total_tokens * n_total_tokens\n",
    "print(f'Joint attention approach (one operation):')\n",
    "print(f'  Joint:            {n_total_tokens} x {n_total_tokens} = {joint_cost:,}')\n",
    "print()\n",
    "\n",
    "ratio = joint_cost / total_cross\n",
    "print(f'Joint / Cross ratio: {ratio:.2f}x')\n",
    "print(f'Joint attention is ~{(ratio - 1) * 100:.0f}% more expensive in attention compute.')\n",
    "print(f'But it replaces TWO operations with ONE, reducing other overhead.')\n",
    "print(f'And it provides bidirectional text-image interaction, which cross-attention cannot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": "### What Just Happened\n\nYou explored the structure of joint attention in an MMDiT block:\n\n- **The joint sequence is text + image tokens concatenated.** With 77 text tokens and 1024 image tokens (for 512x512), the joint sequence is 1101 tokens. The attention matrix is [1101, 1101]—every token attends to every other token.\n\n- **Four quadrants, four types of attention.** The attention matrix naturally divides into four quadrants: text-to-text, text-to-image, image-to-text, and image-to-image. Cross-attention in the U-Net only provided image-to-text. Joint attention provides all four. This quadrant structure is an architectural property—it holds for any joint attention model, regardless of trained weights.\n\n- **Text-to-image is the key new capability.** In cross-attention, text embeddings are fixed—they never change in response to the image. In joint attention, text tokens can read image tokens and update their representations. \"A crane near a river\" can be disambiguated based on what the image actually contains.\n\n- **Modality-specific projections are visible in the architecture.** Each MMDiT block has separate Q/K/V projection layers for text and image tokens. They \"speak their own language\" but share the attention computation—\"hear each other\" in the same room.\n\n- **Cost is comparable.** Joint attention is moderately more expensive per block than cross-attention + self-attention combined, but it replaces two operations with one and provides richer interaction.\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Exercise 3: SD3 Generation and Flow Matching Steps `[Supported]`\n",
    "\n",
    "The lesson taught that SD3 uses **flow matching** as its training objective—the same straight-line interpolation and velocity prediction you trained with in **Flow Matching** (7.2.2). The practical payoff: good results in 20–30 steps, compared to 50+ for DDPM-based models.\n",
    "\n",
    "In this exercise, you will:\n",
    "1. Generate \"a cat sitting on a beach at sunset\" with SD3 at different step counts\n",
    "2. Compare quality across step counts to observe the flow matching payoff\n",
    "3. Measure generation time at each step count\n",
    "4. Compare the quality-vs-steps tradeoff to what you would expect from a DDPM model\n",
    "\n",
    "Fill in the TODO markers to complete the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 3: Generate with SD3 and compare step counts\n",
    "# ============================================================\n",
    "\n",
    "# --- Step 1: Generate at different step counts ---\n",
    "# The flow matching payoff: SD3 should produce good images at\n",
    "# 20-30 steps. DDPM-based models typically need 50+.\n",
    "\n",
    "prompt = 'a cat sitting on a beach at sunset'\n",
    "height, width = 512, 512  # Use 512x512 for faster generation\n",
    "guidance_scale = 7.0\n",
    "\n",
    "step_counts = [10, 20, 30, 50]\n",
    "images = []\n",
    "titles = []\n",
    "\n",
    "for steps in step_counts:\n",
    "    generator = torch.Generator(device='cpu').manual_seed(42)\n",
    "    start = time.time()\n",
    "\n",
    "    # TODO: Generate an image using pipe() with the parameters above.\n",
    "    # Use: prompt, height, width, num_inference_steps=steps,\n",
    "    #      guidance_scale, generator, output_type='pil'\n",
    "    # Store the result in a variable called 'result'.\n",
    "    raise NotImplementedError(\n",
    "        \"TODO: Call pipe() with the correct arguments to generate an image.\"\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    images.append(result.images[0])\n",
    "    titles.append(f'{steps} steps\\n{elapsed:.1f}s')\n",
    "    print(f'  {steps} steps: {elapsed:.1f}s')\n",
    "\n",
    "print('\\nGeneration complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Display the comparison ---\n",
    "\n",
    "# TODO: Use show_image_row() to display the images with titles.\n",
    "# Add a suptitle like 'SD3 Medium: Quality vs Step Count (flow matching)'\n",
    "# and figsize=(20, 5).\n",
    "raise NotImplementedError(\n",
    "    \"TODO: Call show_image_row(images, titles, suptitle=..., figsize=...)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Analyze the quality-vs-steps tradeoff ---\n",
    "\n",
    "print('Quality vs Steps Analysis:')\n",
    "print('=' * 50)\n",
    "print()\n",
    "print('What you should observe:')\n",
    "print('  10 steps:  Recognizable but may lack fine detail')\n",
    "print('  20 steps:  Good quality - the flow matching sweet spot')\n",
    "print('  30 steps:  High quality - diminishing returns begin')\n",
    "print('  50 steps:  Marginal improvement over 30 steps')\n",
    "print()\n",
    "print('Compare to DDPM-based models (SD v1.5, SDXL):')\n",
    "print('  10 steps:  Poor quality (curved trajectories need more steps)')\n",
    "print('  20 steps:  Mediocre quality')\n",
    "print('  50 steps:  Good quality (the DDPM sweet spot)')\n",
    "print()\n",
    "print('The difference: flow matching produces straight trajectories.')\n",
    "print('Straight paths need fewer ODE solver steps to follow accurately.')\n",
    "print('This is the practical payoff of \"curved vs straight\" from Flow Matching.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Try a complex compositional prompt ---\n",
    "# The lesson taught that T5-XXL provides richer text understanding\n",
    "# than CLIP, especially for compositional descriptions.\n",
    "\n",
    "complex_prompt = 'a red ball to the left of a blue cube, on a wooden table, with a green plant in the background'\n",
    "simple_prompt = 'a sunset over the ocean'\n",
    "\n",
    "prompts_to_try = [\n",
    "    (simple_prompt, 'Simple prompt'),\n",
    "    (complex_prompt, 'Complex compositional'),\n",
    "]\n",
    "\n",
    "comp_images = []\n",
    "comp_titles = []\n",
    "\n",
    "for p, label in prompts_to_try:\n",
    "    generator = torch.Generator(device='cpu').manual_seed(42)\n",
    "\n",
    "    # TODO: Generate an image with 28 steps (SD3's default).\n",
    "    # Use the prompt 'p', height, width, guidance_scale, generator.\n",
    "    raise NotImplementedError(\n",
    "        \"TODO: Generate an image for each prompt with 28 steps.\"\n",
    "    )\n",
    "\n",
    "    comp_images.append(result.images[0])\n",
    "    comp_titles.append(f'{label}\\n\"{p[:40]}...\"' if len(p) > 40 else f'{label}\\n\"{p}\"')\n",
    "\n",
    "show_image_row(\n",
    "    comp_images, comp_titles,\n",
    "    suptitle='SD3 Medium: Simple vs Compositional Prompts (T5-XXL text understanding)',\n",
    "    figsize=(12, 5),\n",
    ")\n",
    "\n",
    "print('T5-XXL helps with compositional prompts that require understanding')\n",
    "print('spatial relationships (\"left of\"), counting, and multiple objects.')\n",
    "print('CLIP alone would struggle with \"a red ball to the left of a blue cube\"')\n",
    "print('because contrastive training does not teach spatial reasoning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The key insight is that SD3's flow matching training objective produces straight trajectories that need fewer ODE solver steps. The pipeline API is straightforward—the only difference from SD v1.5 is the model and the fact that 20–30 steps produce good results instead of 50+.\n",
    "\n",
    "**Step 1: Generate at different step counts**\n",
    "```python\n",
    "result = pipe(\n",
    "    prompt=prompt,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    num_inference_steps=steps,\n",
    "    guidance_scale=guidance_scale,\n",
    "    generator=generator,\n",
    "    output_type='pil',\n",
    ")\n",
    "```\n",
    "\n",
    "**Step 2: Display the comparison**\n",
    "```python\n",
    "show_image_row(\n",
    "    images, titles,\n",
    "    suptitle='SD3 Medium: Quality vs Step Count (flow matching)',\n",
    "    figsize=(20, 5),\n",
    ")\n",
    "```\n",
    "\n",
    "**Step 4: Complex compositional prompt**\n",
    "```python\n",
    "result = pipe(\n",
    "    prompt=p,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    num_inference_steps=28,\n",
    "    guidance_scale=guidance_scale,\n",
    "    generator=generator,\n",
    "    output_type='pil',\n",
    ")\n",
    "```\n",
    "\n",
    "**What to observe:**\n",
    "- At 20–30 steps, SD3 produces images comparable in quality to DDPM models at 50+ steps. This is the practical payoff of flow matching: straight trajectories need fewer solver steps.\n",
    "- The jump from 10 to 20 steps is significant; from 30 to 50 is marginal. This matches the lesson's claim that 20–30 steps is the sweet spot.\n",
    "- Complex compositional prompts benefit from T5-XXL's linguistic understanding. Spatial relationships like \"to the left of\" are handled better than CLIP-only models would manage.\n",
    "\n",
    "**Common mistakes:**\n",
    "- Forgetting to reset the generator seed for each step count. Without a fixed seed, you cannot compare quality at different step counts because the initial noise is different.\n",
    "- Using 1024x1024 resolution, which is slower and may cause OOM on smaller GPUs. 512x512 is sufficient to observe the quality-vs-steps tradeoff.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You generated images with SD3 at different step counts and observed the flow matching payoff:\n",
    "\n",
    "- **20–30 steps is the sweet spot.** SD3 produces good quality at 20 steps and high quality at 30. This is a practical consequence of flow matching's straight trajectories—fewer ODE solver steps are needed to follow a straight path than a curved one.\n",
    "\n",
    "- **Diminishing returns after 30 steps.** Going from 30 to 50 steps provides marginal improvement. The straight trajectories are already well-approximated at 30 steps. Compare to DDPM models where 50 steps is often the minimum for good quality.\n",
    "\n",
    "- **Same training objective you trained with.** SD3's flow matching is the same straight-line interpolation and velocity prediction from your flow matching notebook. The concept is identical; the scale is different.\n",
    "\n",
    "- **T5-XXL helps with complex prompts.** Compositional descriptions with spatial relationships benefit from T5's deep linguistic understanding. CLIP's contrastive training does not teach spatial reasoning or counting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cleanup before Exercise 4 ---\n",
    "# Keep the pipeline loaded for Exercise 4.\n",
    "# Free any intermediate tensors.\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print('Ready for Exercise 4.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## Exercise 4: The Convergence Pipeline Trace `[Independent]`\n",
    "\n",
    "The lesson concluded with the full SD3 pipeline traced end-to-end, every step annotated with the lesson that covered it:\n",
    "\n",
    "```\n",
    " 1. Prompt -> CLIP ViT-L text encoder [77, 768]           (CLIP)\n",
    " 2. Prompt -> OpenCLIP ViT-bigG text encoder [77, 1280]    (SDXL)\n",
    " 3. Prompt -> T5-XXL text encoder [77, 4096]               (this lesson)\n",
    " 4. CLIP pooled embeddings + timestep -> c for adaLN-Zero   (Diffusion Transformers)\n",
    " 5. Per-token embeddings projected -> text tokens [77, d]    (this lesson)\n",
    " 6. Noisy latent z_t [4, 64, 64] -> patchify -> [1024, d]  (Diffusion Transformers)\n",
    " 7. Concatenate: [77 + 1024, d] = [1101, d]                 (this lesson: MMDiT)\n",
    " 8. N MMDiT blocks with joint attention + adaLN-Zero         (this lesson + Diffusion Transformers)\n",
    " 9. Split output -> image tokens [1024, d]                   (this lesson)\n",
    "10. Unpatchify -> [4, 64, 64]                               (Diffusion Transformers)\n",
    "11. Flow matching sampling step                              (Flow Matching)\n",
    "12. Repeat steps 6-11 for ~28 steps                         (Flow Matching: fewer steps needed)\n",
    "13. VAE decode z_0 -> [3, 1024, 1024]                       (From Pixels to Latents)\n",
    "```\n",
    "\n",
    "### Your Task\n",
    "\n",
    "1. **Generate an image with SD3**, capturing intermediate outputs at each stage\n",
    "2. **Trace the pipeline step by step**: text encoding shapes, patchify token count, denoising step count, VAE decode output shape\n",
    "3. **For each step, print which lesson covered the relevant concept**\n",
    "4. **Compare the SD3 pipeline to what you know about the SD v1.5 pipeline**: what changed and what is preserved?\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Text encoding: use `pipe.encode_prompt()` to get prompt embeddings and pooled embeddings\n",
    "- VAE encoding: use `pipe.vae.encode()` on a dummy image to verify latent shapes\n",
    "- Transformer config: `pipe.transformer.config` has `patch_size`, `in_channels`, `num_layers`, `joint_attention_dim`\n",
    "- VAE config: `pipe.vae.config` has `scaling_factor`, `latent_channels`\n",
    "- The pipeline's scheduler: `pipe.scheduler` (check `type(pipe.scheduler).__name__` for the flow matching scheduler)\n",
    "- Use `pipe.vae_scale_factor` for the VAE downsampling factor\n",
    "- To capture a generated image at each denoising step, use the `callback_on_step_end` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 4: Trace the full SD3 pipeline end-to-end\n",
    "# ============================================================\n",
    "#\n",
    "# Trace each stage of the SD3 pipeline, printing shapes and\n",
    "# annotating which lesson covered each concept.\n",
    "#\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print the annotated pipeline trace ---\n",
    "#\n",
    "# After capturing the shapes and configs above, print\n",
    "# the full pipeline trace with lesson annotations.\n",
    "#\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare SD3 pipeline to SD v1.5 pipeline ---\n",
    "#\n",
    "# Print a comparison showing what changed and what\n",
    "# is preserved between the two architectures.\n",
    "#\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The convergence theme is the key insight: every component of SD3 traces to a lesson you completed. The pipeline structure is preserved from SD v1.5—only the components inside each stage have evolved.\n",
    "\n",
    "```python\n",
    "# ============================================================\n",
    "# Part 1: Trace each stage with shapes and annotations\n",
    "# ============================================================\n",
    "\n",
    "prompt = 'a cat sitting on a beach at sunset'\n",
    "height, width = 512, 512\n",
    "\n",
    "print('SD3 Pipeline Trace')\n",
    "print('=' * 70)\n",
    "print()\n",
    "\n",
    "# Stage 1-3: Text Encoding\n",
    "print('STAGE 1-3: TEXT ENCODING')\n",
    "print('-' * 70)\n",
    "with torch.no_grad():\n",
    "    prompt_embeds, neg_embeds, pooled_embeds, neg_pooled = pipe.encode_prompt(\n",
    "        prompt=prompt, prompt_2=prompt, prompt_3=prompt,\n",
    "    )\n",
    "\n",
    "print(f'  1. CLIP ViT-L:       [77, 768]   -> Lesson: CLIP (6.3.3)')\n",
    "print(f'  2. OpenCLIP ViT-bigG: [77, 1280] -> Lesson: SDXL (7.4.1)')\n",
    "print(f'  3. T5-XXL:           [77, 4096]  -> Lesson: SD3 & Flux (this lesson)')\n",
    "print(f'  Combined prompt_embeds: {list(prompt_embeds.shape)}')\n",
    "print(f'  Pooled for adaLN-Zero:  {list(pooled_embeds.shape)}')\n",
    "print()\n",
    "\n",
    "# Stage 4: Global conditioning\n",
    "print('STAGE 4: GLOBAL CONDITIONING')\n",
    "print('-' * 70)\n",
    "print(f'  4. Pooled CLIP + timestep -> adaLN-Zero conditioning')\n",
    "print(f'     Pooled shape: {list(pooled_embeds.shape)}')\n",
    "print(f'     -> Lesson: Diffusion Transformers (7.4.2)')\n",
    "print()\n",
    "\n",
    "# Stage 5: Text token projection\n",
    "print('STAGE 5: TEXT TOKEN PROJECTION')\n",
    "print('-' * 70)\n",
    "d_model = pipe.transformer.config.joint_attention_dim\n",
    "n_text = prompt_embeds.shape[1]\n",
    "print(f'  5. Per-token embeddings -> projected to d={d_model}')\n",
    "print(f'     Text tokens: [{n_text}, {d_model}]')\n",
    "print(f'     -> Lesson: SD3 & Flux (this lesson)')\n",
    "print()\n",
    "\n",
    "# Stage 6: Patchify\n",
    "print('STAGE 6: PATCHIFY')\n",
    "print('-' * 70)\n",
    "vae_scale = pipe.vae_scale_factor\n",
    "latent_h = height // vae_scale\n",
    "latent_w = width // vae_scale\n",
    "latent_c = pipe.transformer.config.in_channels\n",
    "patch_size = pipe.transformer.config.patch_size\n",
    "n_patches = (latent_h // patch_size) * (latent_w // patch_size)\n",
    "print(f'  6. Noisy latent [{latent_c}, {latent_h}, {latent_w}] -> patchify -> [{n_patches}, {d_model}]')\n",
    "print(f'     Patch size: {patch_size}, Patches: ({latent_h}//{patch_size}) x ({latent_w}//{patch_size}) = {n_patches}')\n",
    "print(f'     -> Lesson: Diffusion Transformers (7.4.2)')\n",
    "print()\n",
    "\n",
    "# Stage 7: Concatenate\n",
    "print('STAGE 7: CONCATENATE (JOINT SEQUENCE)')\n",
    "print('-' * 70)\n",
    "n_total = n_text + n_patches\n",
    "print(f'  7. Concatenate: [{n_text} + {n_patches}, {d_model}] = [{n_total}, {d_model}]')\n",
    "print(f'     -> Lesson: SD3 & Flux (this lesson: MMDiT)')\n",
    "print()\n",
    "\n",
    "# Stage 8: MMDiT blocks\n",
    "print('STAGE 8: MMDiT BLOCKS')\n",
    "print('-' * 70)\n",
    "n_blocks = pipe.transformer.config.num_layers\n",
    "print(f'  8. {n_blocks} MMDiT blocks: joint attention + adaLN-Zero')\n",
    "print(f'     Attention matrix: [{n_total}, {n_total}] per block')\n",
    "print(f'     -> Lesson: SD3 & Flux + Diffusion Transformers (7.4.2)')\n",
    "print()\n",
    "\n",
    "# Stage 9-10: Split and unpatchify\n",
    "print('STAGE 9-10: SPLIT AND UNPATCHIFY')\n",
    "print('-' * 70)\n",
    "print(f'  9. Split: image tokens [{n_patches}, {d_model}]')\n",
    "print(f'     -> Lesson: SD3 & Flux (this lesson)')\n",
    "print(f' 10. Unpatchify: [{n_patches}, {d_model}] -> [{latent_c}, {latent_h}, {latent_w}]')\n",
    "print(f'     -> Lesson: Diffusion Transformers (7.4.2)')\n",
    "print()\n",
    "\n",
    "# Stage 11-12: Flow matching sampling\n",
    "print('STAGE 11-12: FLOW MATCHING SAMPLING')\n",
    "print('-' * 70)\n",
    "scheduler_name = type(pipe.scheduler).__name__\n",
    "print(f' 11. Flow matching sampling step (scheduler: {scheduler_name})')\n",
    "print(f'     -> Lesson: Flow Matching (7.2.2)')\n",
    "print(f' 12. Repeat for ~28 steps (fewer steps due to straight trajectories)')\n",
    "print(f'     -> Lesson: Flow Matching (7.2.2)')\n",
    "print()\n",
    "\n",
    "# Stage 13: VAE decode\n",
    "print('STAGE 13: VAE DECODE')\n",
    "print('-' * 70)\n",
    "print(f' 13. VAE decode: [{latent_c}, {latent_h}, {latent_w}] -> [3, {height}, {width}]')\n",
    "print(f'     -> Lesson: From Pixels to Latents (6.3.5)')\n",
    "print()\n",
    "print('=' * 70)\n",
    "print('Every step traces to a lesson you completed.')\n",
    "print('Nothing in this pipeline is unexplained.')\n",
    "```\n",
    "\n",
    "```python\n",
    "# ============================================================\n",
    "# Part 2: Compare SD3 vs SD v1.5 pipeline\n",
    "# ============================================================\n",
    "\n",
    "print('SD3 vs SD v1.5 Pipeline Comparison')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print(f'{\"Component\":<25} {\"SD v1.5\":<25} {\"SD3\":<25}')\n",
    "print('-' * 70)\n",
    "print(f'{\"Text encoders\":<25} {\"1 (CLIP ViT-L)\":<25} {\"3 (CLIP + OpenCLIP + T5)\":<25}')\n",
    "print(f'{\"Text embedding dims\":<25} {\"[77, 768]\":<25} {\"[77, ~combined]\":<25}')\n",
    "print(f'{\"Denoising backbone\":<25} {\"U-Net\":<25} {\"MMDiT (transformer)\":<25}')\n",
    "print(f'{\"Text conditioning\":<25} {\"Cross-attention\":<25} {\"Joint attention\":<25}')\n",
    "print(f'{\"Text flow direction\":<25} {\"Image reads text\":<25} {\"Bidirectional\":<25}')\n",
    "print(f'{\"Timestep conditioning\":<25} {\"AdaGN\":<25} {\"adaLN-Zero\":<25}')\n",
    "print(f'{\"Training objective\":<25} {\"DDPM noise prediction\":<25} {\"Flow matching velocity\":<25}')\n",
    "print(f'{\"Inference steps\":<25} {\"50+\":<25} {\"20-30\":<25}')\n",
    "print(f'{\"VAE\":<25} {\"Same\":<25} {\"Same (improved)\":<25}')\n",
    "print(f'{\"Sampling loop\":<25} {\"Same structure\":<25} {\"Same structure\":<25}')\n",
    "print(f'{\"Output resolution\":<25} {\"512x512\":<25} {\"1024x1024\":<25}')\n",
    "print()\n",
    "print('What CHANGED:')\n",
    "print('  - Architecture: U-Net -> MMDiT (transformer on patch tokens)')\n",
    "print('  - Text conditioning: cross-attention -> joint self-attention')\n",
    "print('  - Text encoders: 1 CLIP -> 3 encoders (CLIP + OpenCLIP + T5-XXL)')\n",
    "print('  - Training objective: DDPM -> flow matching')\n",
    "print()\n",
    "print('What is PRESERVED:')\n",
    "print('  - Latent space generation (VAE encode/decode)')\n",
    "print('  - Iterative denoising loop (still predict and step)')\n",
    "print('  - Classifier-free guidance (still works the same way)')\n",
    "print('  - Pipeline structure: encode text -> denoise latent -> decode to pixels')\n",
    "print()\n",
    "print('The lesson called this \"same pipeline, different denoising network.\"')\n",
    "print('The pipeline structure survived every architectural change.')\n",
    "```\n",
    "\n",
    "**Key observations:**\n",
    "- The pipeline structure is preserved from SD v1.5 to SD3. Text encode, denoise in latent space, VAE decode. The components inside each stage evolved, but the pipeline did not.\n",
    "- Every component of SD3 traces to a specific lesson. Transformers from Series 4. Latent diffusion from Series 6. Flow matching from Module 7.2. DiT/patchify/adaLN-Zero from the previous lesson. T5-XXL and MMDiT from this lesson.\n",
    "- The convergence is literal: SD3 is the combination of concepts taught across 50+ lessons. No single component is new in isolation—the innovation is the combination.\n",
    "\n",
    "**Common mistakes:**\n",
    "- Not distinguishing between what changed (architecture, conditioning mechanism, training objective, text encoders) and what is preserved (VAE, sampling loop structure, guidance). The lesson emphasizes that the pipeline structure survived.\n",
    "- Forgetting that the pooled CLIP embedding provides adaLN-Zero conditioning (global path) while the per-token embeddings provide joint attention conditioning (per-token path). Both paths are needed.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Three encoders, three kinds of understanding.** CLIP ViT-L (123M, visual alignment), OpenCLIP ViT-bigG (354M, richer visual alignment), T5-XXL (4.7B, deep linguistic understanding). T5's embeddings are 5x wider than CLIP's. The text encoders collectively have more parameters than the denoising network—text understanding is worth investing in.\n",
    "\n",
    "2. **Joint attention is one room, one conversation.** Concatenate text tokens and image tokens, run standard self-attention on the combined sequence. Four types of attention in one operation: image-to-text (was cross-attention), text-to-image (NEW), image-to-image (was self-attention), text-to-text (NEW). Simpler than cross-attention (one operation instead of two) and richer (bidirectional).\n",
    "\n",
    "3. **Modality-specific projections, shared attention.** Text and image tokens have separate Q/K/V projections and separate FFN layers. They \"speak their own language\" but \"hear each other\" through shared attention. This is not naive concatenation—each modality maintains its representational identity.\n",
    "\n",
    "4. **Flow matching delivers in practice.** SD3 produces good results at 20–30 steps, compared to 50+ for DDPM-based models. Same straight-line interpolation and velocity prediction you trained with. The concept is identical; the scale is different.\n",
    "\n",
    "5. **Convergence, not revolution.** Every component of the SD3 pipeline traces to a lesson you completed: transformers (Series 4), latent diffusion (Series 6), CLIP (6.3.3), flow matching (7.2.2), patchify and adaLN-Zero (7.4.2), MMDiT and T5-XXL (this lesson). The frontier is not beyond your understanding—it IS your understanding, combined."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}