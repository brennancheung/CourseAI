{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Prompt Engineering\n\nIn this notebook, you'll practice systematic prompt construction—treating prompt design as programming rather than conversation. Each exercise builds a specific prompt engineering skill with immediate, visible feedback.\n\n**What you'll do:**\n- Construct format specifications that constrain model output into bullet points, JSON, and markdown tables—and observe how format tokens anchor the output distribution\n- Write role prompts that bias attention toward specific features of a code snippet, discovering that roles change *what* the model finds, not just *how* it phrases the answer\n- Run a controlled experiment on few-shot example selection, comparing diversity vs quantity vs category bias across multiple trials\n- Design a complete structured prompt for a real task (meeting summary), composing 3+ techniques and evaluating consistency across inputs\n\n**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones—they reveal gaps in your mental model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup — self-contained for Google Colab\n",
    "!pip install -q openai\n",
    "\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "import random\n",
    "from openai import OpenAI\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- API Key Setup ---\n",
    "# Option 1: Set your API key as an environment variable (recommended)\n",
    "#   In Colab: go to the key icon in the left sidebar, add OPENAI_API_KEY\n",
    "# Option 2: Paste it directly (less secure, don't commit this)\n",
    "#   os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "# You can also use any OpenAI-compatible API (e.g., local Ollama, Together AI)\n",
    "# by changing the base_url:\n",
    "#   client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Use a small, cheap model for the exercises\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Reproducible results where possible\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def call_llm(prompt: str, temperature: float = 0.0, max_tokens: int = 300) -> str:\n",
    "    \"\"\"Call the LLM with a single prompt. Returns the response text.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def call_llm_with_system(system_prompt: str, user_prompt: str,\n",
    "                         temperature: float = 0.0, max_tokens: int = 300) -> str:\n",
    "    \"\"\"Call the LLM with a system prompt and user prompt. Returns the response text.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def print_wrapped(text: str, width: int = 80, prefix: str = \"\"):\n",
    "    \"\"\"Print text with word wrapping for readability.\"\"\"\n",
    "    for line in text.split(\"\\n\"):\n",
    "        wrapped = textwrap.fill(line, width=width, initial_indent=prefix,\n",
    "                                subsequent_indent=prefix)\n",
    "        print(wrapped)\n",
    "\n",
    "\n",
    "# Quick test to verify the API is working\n",
    "test = call_llm(\"Say 'API connection successful' and nothing else.\")\n",
    "print(test)\n",
    "print(f\"\\nUsing model: {MODEL}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Shared Data\n\nAll exercises in this notebook work with real text—invoices, code snippets, and reviews. The data is defined here so exercises can share it."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Shared text for Exercise 1: Format Specification ---\n",
    "INVOICE_TEXT = \"\"\"On January 15, 2024, Acme Corporation issued invoice #INV-2024-0892\n",
    "to Globex Industries for consulting services rendered in Q4 2023.\n",
    "The total amount was $12,450.00, with payment terms of Net 30.\n",
    "The invoice covered three line items: Strategic Planning ($5,200),\n",
    "Market Analysis ($4,750), and Implementation Support ($2,500).\n",
    "Contact: billing@acmecorp.com. Tax ID: 82-1234567.\"\"\"\n",
    "\n",
    "\n",
    "# --- Shared code snippet for Exercise 2: Role Prompting ---\n",
    "CODE_SNIPPET = '''def get_user_data(user_id, db_connection):\n",
    "    query = f\"SELECT * FROM users WHERE id = {user_id}\"\n",
    "    result = db_connection.execute(query)\n",
    "    users = []\n",
    "    for row in result:\n",
    "        user = {}\n",
    "        for i in range(len(row)):\n",
    "            user[result.description[i][0]] = row[i]\n",
    "        users.append(user)\n",
    "    data = users[0]\n",
    "    data[\"full_name\"] = data[\"first_name\"] + \" \" + data[\"last_name\"]\n",
    "    data[\"age\"] = 2024 - int(data[\"birth_year\"])\n",
    "    return data\n",
    "'''\n",
    "\n",
    "# Known issues in the code snippet (for reference during Exercise 2):\n",
    "# Security:    SQL injection via f-string, SELECT * exposes all columns\n",
    "# Performance: Iterating all rows when only one is needed, building dicts manually\n",
    "#              instead of using fetchone() or row factories\n",
    "# Style:       No type hints, no docstring, magic number 2024, no error handling,\n",
    "#              index-based column access, mutating the dict in place\n",
    "\n",
    "\n",
    "# --- Shared data for Exercise 3: Few-Shot Example Selection ---\n",
    "# Simple text classification: Tech / Sports / Politics / Entertainment\n",
    "LABELED_EXAMPLES = [\n",
    "    # Tech\n",
    "    (\"Apple released a new M4 chip with improved neural engine performance\", \"Tech\"),\n",
    "    (\"The latest Python 3.13 update includes a new JIT compiler\", \"Tech\"),\n",
    "    (\"OpenAI announced GPT-5 with significantly improved reasoning\", \"Tech\"),\n",
    "    (\"Samsung unveiled a foldable tablet at CES this year\", \"Tech\"),\n",
    "    # Sports\n",
    "    (\"The Lakers won the championship in a thrilling overtime game\", \"Sports\"),\n",
    "    (\"Usain Bolt's world record still stands after fifteen years\", \"Sports\"),\n",
    "    (\"The World Cup final drew over a billion viewers worldwide\", \"Sports\"),\n",
    "    (\"Tennis star broke her ankle during the quarterfinal match\", \"Sports\"),\n",
    "    # Politics\n",
    "    (\"The Senate passed a bipartisan infrastructure bill today\", \"Politics\"),\n",
    "    (\"New trade tariffs were announced affecting imports from Asia\", \"Politics\"),\n",
    "    (\"The Supreme Court ruled on the landmark privacy case\", \"Politics\"),\n",
    "    (\"Election results showed a surprising shift in rural voting patterns\", \"Politics\"),\n",
    "    # Entertainment\n",
    "    (\"The new Dune sequel broke opening weekend box office records\", \"Entertainment\"),\n",
    "    (\"Grammy nominations surprised fans with several indie picks\", \"Entertainment\"),\n",
    "    (\"Netflix reported record subscriber growth after releasing the hit series\", \"Entertainment\"),\n",
    "    (\"Broadway ticket sales reached an all-time high this quarter\", \"Entertainment\"),\n",
    "]\n",
    "\n",
    "# Test examples for classification (held out)\n",
    "TEST_CLASSIFICATION = [\n",
    "    (\"Microsoft acquired a robotics startup for $2 billion\", \"Tech\"),\n",
    "    (\"The marathon runner set a new personal best time\", \"Sports\"),\n",
    "    (\"Congress debated the new healthcare reform proposal\", \"Politics\"),\n",
    "    (\"The animated film won Best Picture at the festival\", \"Entertainment\"),\n",
    "    (\"A new quantum computing breakthrough was published in Nature\", \"Tech\"),\n",
    "    (\"The soccer team qualified for the next round of the tournament\", \"Sports\"),\n",
    "    (\"Diplomatic talks between the two nations resumed after months\", \"Politics\"),\n",
    "    (\"The rock band announced a reunion tour starting in June\", \"Entertainment\"),\n",
    "    (\"The startup raised $50M in Series B for their AI platform\", \"Tech\"),\n",
    "    (\"Olympic committee announced new rules for athlete eligibility\", \"Sports\"),\n",
    "]\n",
    "\n",
    "CATEGORIES = [\"Tech\", \"Sports\", \"Politics\", \"Entertainment\"]\n",
    "\n",
    "print(f\"Invoice text: {len(INVOICE_TEXT)} chars\")\n",
    "print(f\"Code snippet: {len(CODE_SNIPPET.strip().splitlines())} lines\")\n",
    "print(f\"Labeled examples: {len(LABELED_EXAMPLES)} ({len(LABELED_EXAMPLES)//4} per category)\")\n",
    "print(f\"Test examples: {len(TEST_CLASSIFICATION)}\")\n",
    "print(\"\\nData loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 1: Format Specification (Guided)\n\nThe lesson showed that format specification constrains the output distribution—format tokens in the prompt create structural anchors for attention, and autoregressive generation maintains format consistency once the first format token is generated.\n\nIn this exercise, you'll see that claim in action. You'll extract structured data from an invoice paragraph using four prompts:\n1. A conversational prompt (no format specification)\n2. A prompt requesting bullet points\n3. A prompt requesting JSON\n4. A prompt requesting a markdown table\n\nThe first format (bullet points) is fully worked. You'll construct the JSON and markdown table prompts yourself.\n\n**Before running each cell, predict:**\n- What format will the conversational prompt produce? Will it be the same every time?\n- When you specify bullet points, will the model follow the format exactly?\n- For JSON: what happens if you include an example schema vs just saying \"return JSON\"?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Conversational prompt (no format specification) ---\n",
    "# This is the baseline. No structure, just a natural language request.\n",
    "\n",
    "conversational_prompt = f\"\"\"Please extract the key information from the following\n",
    "invoice text and organize it nicely.\n",
    "\n",
    "{INVOICE_TEXT}\"\"\"\n",
    "\n",
    "print(\"CONVERSATIONAL PROMPT (no format spec)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nRunning 3 times with temperature=0.7 to see variation...\\n\")\n",
    "\n",
    "for run in range(3):\n",
    "    result = call_llm(conversational_prompt, temperature=0.7, max_tokens=400)\n",
    "    print(f\"--- Run {run + 1} ---\")\n",
    "    print_wrapped(result)\n",
    "    print()\n",
    "\n",
    "print(\"Notice: Does the format stay consistent across runs?\")\n",
    "print(\"The model might use bullets, paragraphs, bold text, tables...\")\n",
    "print(\"The instruction is clear English. But 'organize it nicely' is a wish, not a constraint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Bullet point format (fully worked) ---\n",
    "# We specify the exact output structure: what fields to extract, in what format.\n",
    "\n",
    "bullet_prompt = f\"\"\"Extract the following information from the invoice text below.\n",
    "Return ONLY a bullet-point list with these exact fields:\n",
    "- Vendor:\n",
    "- Client:\n",
    "- Invoice Number:\n",
    "- Date:\n",
    "- Total Amount:\n",
    "- Payment Terms:\n",
    "- Line Items: (list each with amount)\n",
    "\n",
    "Invoice text:\n",
    "{INVOICE_TEXT}\"\"\"\n",
    "\n",
    "print(\"BULLET POINT FORMAT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nRunning 3 times with temperature=0.7...\\n\")\n",
    "\n",
    "bullet_results = []\n",
    "for run in range(3):\n",
    "    result = call_llm(bullet_prompt, temperature=0.7, max_tokens=400)\n",
    "    bullet_results.append(result)\n",
    "    print(f\"--- Run {run + 1} ---\")\n",
    "    print_wrapped(result)\n",
    "    print()\n",
    "\n",
    "# Check consistency: do all 3 runs have the same structure?\n",
    "all_have_vendor = all(\"Vendor:\" in r for r in bullet_results)\n",
    "all_have_amount = all(\"12,450\" in r or \"12450\" in r for r in bullet_results)\n",
    "print(f\"All runs include 'Vendor:' field: {all_have_vendor}\")\n",
    "print(f\"All runs include correct amount: {all_have_amount}\")\n",
    "print(\"\\nThe format specification created structural anchors. The bullet-point\")\n",
    "print(\"template in the prompt constrains the output — each '- Field:' token\")\n",
    "print(\"creates an attention anchor that the model's generation follows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: JSON format ---\n",
    "# TODO: Write a prompt that extracts the same invoice data as JSON.\n",
    "#\n",
    "# Requirements:\n",
    "# - Include an explicit JSON schema showing the expected structure\n",
    "# - Include the fields: vendor, client, invoice_number, date, total_amount,\n",
    "#   payment_terms, line_items (array of {description, amount})\n",
    "# - Tell the model to return ONLY valid JSON (no markdown fences, no explanation)\n",
    "#\n",
    "# Hint: Showing the schema as an example is more powerful than describing it\n",
    "# in words. The format tokens in the schema become attention anchors.\n",
    "\n",
    "# TODO: Write the json_prompt (5-15 lines)\n",
    "# YOUR CODE HERE\n",
    "json_prompt = f\"\"\"YOUR PROMPT HERE\n",
    "\n",
    "Invoice text:\n",
    "{INVOICE_TEXT}\"\"\"\n",
    "\n",
    "print(\"JSON FORMAT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nRunning 3 times with temperature=0.7...\\n\")\n",
    "\n",
    "json_results = []\n",
    "for run in range(3):\n",
    "    result = call_llm(json_prompt, temperature=0.7, max_tokens=500)\n",
    "    json_results.append(result)\n",
    "    print(f\"--- Run {run + 1} ---\")\n",
    "    print_wrapped(result)\n",
    "    print()\n",
    "\n",
    "# Test: is the output valid JSON?\n",
    "valid_count = 0\n",
    "for i, r in enumerate(json_results):\n",
    "    # Strip markdown code fences if present\n",
    "    cleaned = r.strip()\n",
    "    if cleaned.startswith(\"```\"):\n",
    "        cleaned = cleaned.split(\"\\n\", 1)[-1].rsplit(\"```\", 1)[0].strip()\n",
    "    try:\n",
    "        parsed = json.loads(cleaned)\n",
    "        valid_count += 1\n",
    "        print(f\"Run {i+1}: Valid JSON with keys: {list(parsed.keys())}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Run {i+1}: INVALID JSON — {e}\")\n",
    "\n",
    "print(f\"\\n{valid_count}/3 runs produced valid JSON.\")\n",
    "print(\"The JSON schema in the prompt creates strong format anchors.\")\n",
    "print(\"Once the model generates '{', autoregressive generation\")\n",
    "print(\"constrains all subsequent tokens to be JSON-valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Markdown table format ---\n",
    "# TODO: Write a prompt that extracts the same invoice data as a markdown table.\n",
    "#\n",
    "# Requirements:\n",
    "# - The table should have columns: Field, Value\n",
    "# - Line items should each have their own row\n",
    "# - Show the expected table header in the prompt as a format anchor\n",
    "#\n",
    "# Hint: Including the first row of the table (e.g., the header + separator)\n",
    "# as part of the prompt is a powerful anchoring technique.\n",
    "\n",
    "# TODO: Write the table_prompt (5-15 lines)\n",
    "# YOUR CODE HERE\n",
    "table_prompt = f\"\"\"YOUR PROMPT HERE\n",
    "\n",
    "Invoice text:\n",
    "{INVOICE_TEXT}\"\"\"\n",
    "\n",
    "print(\"MARKDOWN TABLE FORMAT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nRunning 3 times with temperature=0.7...\\n\")\n",
    "\n",
    "table_results = []\n",
    "for run in range(3):\n",
    "    result = call_llm(table_prompt, temperature=0.7, max_tokens=500)\n",
    "    table_results.append(result)\n",
    "    print(f\"--- Run {run + 1} ---\")\n",
    "    print_wrapped(result)\n",
    "    print()\n",
    "\n",
    "# Check: do all runs contain table structure?\n",
    "has_pipes = [(\"|\" in r and \"---\" in r) for r in table_results]\n",
    "print(f\"Runs with table structure (pipes + separators): {sum(has_pipes)}/3\")\n",
    "print(\"\\nThree different formats, same data, same model, same weights.\")\n",
    "print(\"The ONLY difference is the format tokens in the prompt.\")\n",
    "print(\"Format specification is not about being polite — it is about\")\n",
    "print(\"providing structural anchors that constrain the output distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**What just happened:** You extracted the same data from the same text using four different prompts. The conversational prompt produced inconsistent formats. The structured prompts—bullet points, JSON, markdown table—each produced consistent output matching the requested format.\n\nThe lesson explained why: format tokens in the prompt create **structural anchors** for attention. When the model sees `| Field | Value |` in the prompt, its attention mechanism picks up those pipe characters as a strong signal. And once autoregressive generation produces the first format token (a `{` for JSON, a `|` for a table), each subsequent token is constrained to follow the format. The first format token constrains everything after it.\n\nThis is the difference between a wish and a program. \"Organize it nicely\" is a wish—it allows any output format. An explicit schema is a program—it constrains the output space to one specific structure.\n\n<details>\n<summary>Solution</summary>\n\n**Why including a schema is more powerful than describing the format in words:** A JSON schema like `{\"vendor\": \"...\", \"date\": \"...\"}` puts the actual format tokens into the context. The model's attention directly matches these tokens during generation. Describing the format in natural language (\"return a JSON object with a vendor field\") is weaker because the model must translate the description into format tokens—an extra step that introduces variance.\n\n**JSON prompt:**\n```python\njson_prompt = f\"\"\"Extract information from the invoice text below.\nReturn ONLY valid JSON (no markdown, no explanation) matching this exact schema:\n\n{{\n  \"vendor\": \"string\",\n  \"client\": \"string\",\n  \"invoice_number\": \"string\",\n  \"date\": \"string\",\n  \"total_amount\": number,\n  \"payment_terms\": \"string\",\n  \"line_items\": [\n    {{\"description\": \"string\", \"amount\": number}}\n  ]\n}}\n\nInvoice text:\n{INVOICE_TEXT}\"\"\"\n```\n\n**Markdown table prompt:**\n```python\ntable_prompt = f\"\"\"Extract information from the invoice text below into a markdown table.\nUse exactly this format—two columns, Field and Value.\nEach line item gets its own row.\n\nStart your response with this header:\n\n| Field | Value |\n| --- | --- |\n\nInvoice text:\n{INVOICE_TEXT}\"\"\"\n```\n\n**Common mistake:** Not including the actual format tokens in the prompt. Saying \"return a table\" is weaker than showing the table header. The format tokens themselves are the attention anchors.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 2: Role Prompting Effects (Supported)\n\nThe lesson demonstrated that role prompts do not just change the *style* of the response—they change *what the model attends to* in the input. A security-focused role surfaces security issues that a generic review misses entirely.\n\nIn this exercise, you'll test that claim. You have a Python function with known issues spanning three categories: security vulnerabilities, performance problems, and style issues. You'll write prompts with three different roles and compare which issues each role finds.\n\nThe first role (security auditor) is provided. You'll write the performance engineer and code style reviewer roles, then observe how each role biases the model's attention toward different features of the same code.\n\n**Before running, predict:**\n- Will the security auditor find the SQL injection? What about the performance issues?\n- Will the performance engineer mention security at all?\n- What happens when you try a \"combined\" role that covers all three concerns?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Role 1: Security Auditor (provided) ---\n",
    "\n",
    "security_system = \"\"\"You are a senior application security auditor. Your sole focus\n",
    "is identifying security vulnerabilities in code. You look for: injection attacks,\n",
    "authentication/authorization flaws, data exposure risks, and input validation\n",
    "issues. You do NOT comment on code style, performance, or readability — only\n",
    "security. List each vulnerability with a severity rating (Critical/High/Medium/Low).\"\"\"\n",
    "\n",
    "review_user_prompt = f\"\"\"Review this Python function for issues:\\n\\n```python\\n{CODE_SNIPPET}```\"\"\"\n",
    "\n",
    "print(\"ROLE 1: SECURITY AUDITOR\")\n",
    "print(\"=\" * 60)\n",
    "security_result = call_llm_with_system(security_system, review_user_prompt, max_tokens=600)\n",
    "print_wrapped(security_result)\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Note which issues this role found. Does it mention performance?\")\n",
    "print(\"Does it mention style? The role constrains what the model attends to.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Role 2: Performance Engineer ---\n",
    "# TODO: Write a system prompt for a performance engineer.\n",
    "#\n",
    "# Requirements:\n",
    "# - The role should focus ONLY on performance issues (not security, not style)\n",
    "# - Mention what performance concerns to look for: unnecessary iteration,\n",
    "#   memory usage, database query efficiency, etc.\n",
    "# - Ask for specific improvement suggestions\n",
    "#\n",
    "# The key insight: the role tokens bias attention toward performance-related\n",
    "# features of the code. \"Unnecessary iteration\" in the system prompt makes\n",
    "# the for-loop more salient to the attention mechanism.\n",
    "\n",
    "# TODO: Write performance_system (3-6 lines of text)\n",
    "# YOUR CODE HERE\n",
    "performance_system = \"\"\"YOUR SYSTEM PROMPT HERE\"\"\"\n",
    "\n",
    "print(\"ROLE 2: PERFORMANCE ENGINEER\")\n",
    "print(\"=\" * 60)\n",
    "performance_result = call_llm_with_system(performance_system, review_user_prompt, max_tokens=600)\n",
    "print_wrapped(performance_result)\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Compare to the security auditor. Different issues from the same code?\")\n",
    "print(\"The role tokens shifted what the model attended to in the code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Role 3: Code Style Reviewer ---\n",
    "# TODO: Write a system prompt for a code style / readability reviewer.\n",
    "#\n",
    "# Requirements:\n",
    "# - Focus ONLY on readability, naming, documentation, Pythonic idioms\n",
    "# - Do NOT mention security or performance\n",
    "# - Ask for specific style improvements following Python best practices\n",
    "\n",
    "# TODO: Write style_system (3-6 lines of text)\n",
    "# YOUR CODE HERE\n",
    "style_system = \"\"\"YOUR SYSTEM PROMPT HERE\"\"\"\n",
    "\n",
    "print(\"ROLE 3: CODE STYLE REVIEWER\")\n",
    "print(\"=\" * 60)\n",
    "style_result = call_llm_with_system(style_system, review_user_prompt, max_tokens=600)\n",
    "print_wrapped(style_result)\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Three roles, three different sets of issues from the SAME code.\")\n",
    "print(\"The model's weights are identical. The role text in the system prompt\")\n",
    "print(\"biased attention toward different features of the input.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comparison: What did each role find? ---\n",
    "# Let's use the LLM itself to extract a structured comparison.\n",
    "\n",
    "comparison_prompt = f\"\"\"I asked three different reviewers to review the same code.\n",
    "Summarize what each found in a brief list. Use ONLY the issues they actually\n",
    "mentioned — do not add issues they missed.\n",
    "\n",
    "Security Auditor said:\n",
    "{security_result}\n",
    "\n",
    "Performance Engineer said:\n",
    "{performance_result}\n",
    "\n",
    "Code Style Reviewer said:\n",
    "{style_result}\n",
    "\n",
    "Format your response as:\n",
    "SECURITY AUDITOR found: [brief list]\n",
    "PERFORMANCE ENGINEER found: [brief list]\n",
    "STYLE REVIEWER found: [brief list]\n",
    "OVERLAP: [issues mentioned by more than one reviewer, or \"none\"]\"\"\"\n",
    "\n",
    "print(\"COMPARISON ACROSS ROLES\")\n",
    "print(\"=\" * 60)\n",
    "comparison = call_llm(comparison_prompt, max_tokens=500)\n",
    "print_wrapped(comparison)\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nThe code has the SAME issues regardless of who reviews it.\")\n",
    "print(\"But each role surfaces different issues because the role tokens\")\n",
    "print(\"bias attention toward different features of the code.\")\n",
    "print(\"\\nRemember from the lesson: 'SFT teaches format, not knowledge.'\")\n",
    "print(\"Role prompts are the same principle at inference time — they shape\")\n",
    "print(\"HOW the model presents what it already knows, not WHAT it knows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**What just happened:** Three different roles produced three different reviews of the same code. The security auditor found injection vulnerabilities. The performance engineer found inefficient iteration and query patterns. The style reviewer found naming and documentation issues. The overlap was likely minimal—each role focused the model's attention on different features of the input.\n\nThis is not just a style change. The roles caused the model to *find different things*. The role tokens in the system prompt add entries to the K/V cache that bias the attention distribution. When the model's Q vectors process the code, they attend to both the code tokens AND the role tokens. The word \"security\" in the role makes security-related code patterns (like `f\"SELECT...\"`) more salient to the attention mechanism.\n\nBut notice: the role did not give the model any new knowledge about security or performance. It shifted the model's focus within what it already knows. This is exactly the lesson's point: role prompts shape focus, not knowledge.\n\n<details>\n<summary>Solution</summary>\n\n**Why focused roles outperform generic \"review everything\" prompts:** A focused role creates stronger attention bias. If the system prompt contains both \"security\" and \"performance\" and \"style\" tokens, the attention is distributed across all three concerns. A focused role concentrates attention on one concern, making it more likely to find subtle issues in that category.\n\n**Performance Engineer system prompt:**\n```python\nperformance_system = \"\"\"You are a senior performance engineer. Your sole focus is\nidentifying performance bottlenecks and inefficiencies in code. You look for:\nunnecessary iteration, excessive memory allocation, inefficient database queries,\nredundant computation, and missed optimization opportunities. You do NOT comment\non security, code style, or readability—only performance. For each issue,\nexplain the performance impact and suggest a more efficient alternative.\"\"\"\n```\n\n**Code Style Reviewer system prompt:**\n```python\nstyle_system = \"\"\"You are a Python code style reviewer focused on readability and\nPythonic idioms. You look for: missing type hints, missing docstrings, unclear\nvariable names, non-Pythonic patterns (e.g., index-based iteration instead of\nunpacking), magic numbers, and violations of PEP 8. You do NOT comment on\nsecurity or performance—only style and readability. Suggest specific\nimprovements for each issue.\"\"\"\n```\n\n**Common finding:** The security auditor finds SQL injection (Critical) and SELECT * data exposure. The performance engineer finds: iterating all rows when only one is needed, manual dict construction instead of row factories, and possibly the redundant string concatenation. The style reviewer finds: no type hints, no docstring, magic number 2024, no error handling, and index-based column access. Overlap is minimal.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 3: Few-Shot Example Selection (Supported)\n\nThe lesson taught three principles for few-shot example selection: diversity over quantity, format consistency, and difficulty calibration. In this exercise, you'll test the first principle empirically.\n\nYou have a 4-category text classification task (Tech / Sports / Politics / Entertainment) with 16 labeled examples and 10 test examples. You'll compare four example selection strategies:\n\n1. **3 random examples**—baseline\n2. **3 diverse examples**—one from each of three different categories\n3. **3 same-category examples**—all from one category (Tech)\n4. **5 random examples**—more examples, but no diversity guarantee\n\nFor each strategy, you'll run 5 trials (with different random selections where applicable) and measure accuracy. The example selection function and evaluation loop are provided. Your job is to design the example sets and interpret the results.\n\n**Before running, predict:**\n- Will 3 diverse examples outperform 5 random examples? The lesson says diversity > quantity.\n- What will 3 same-category examples do? They provide format consistency but no category diversity.\n- How much variance will you see across the 5 trials for random selection?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper functions (provided) ---\n",
    "\n",
    "def build_classification_prompt(examples: list[tuple[str, str]], test_text: str) -> str:\n",
    "    \"\"\"Build a few-shot classification prompt from labeled examples.\"\"\"\n",
    "    lines = [\"Classify the following text into one of these categories: \"\n",
    "             \"Tech, Sports, Politics, Entertainment.\"]\n",
    "    lines.append(\"Respond with ONLY the category name.\\n\")\n",
    "    for text, label in examples:\n",
    "        lines.append(f'Text: \"{text}\"\\nCategory: {label}\\n')\n",
    "    lines.append(f'Text: \"{test_text}\"\\nCategory:')\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def extract_category(response: str) -> str:\n",
    "    \"\"\"Extract a category from a model response.\"\"\"\n",
    "    response_lower = response.lower().strip()\n",
    "    for cat in CATEGORIES:\n",
    "        if cat.lower() in response_lower:\n",
    "            return cat\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def run_classification_trial(examples: list[tuple[str, str]],\n",
    "                              test_data: list[tuple[str, str]]) -> float:\n",
    "    \"\"\"Run one classification trial. Returns accuracy.\"\"\"\n",
    "    correct = 0\n",
    "    for text, true_label in test_data:\n",
    "        prompt = build_classification_prompt(examples, text)\n",
    "        response = call_llm(prompt)\n",
    "        pred = extract_category(response)\n",
    "        if pred == true_label:\n",
    "            correct += 1\n",
    "    return correct / len(test_data)\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded.\")\n",
    "print(\"\\nSample classification prompt:\")\n",
    "sample = build_classification_prompt(LABELED_EXAMPLES[:2], \"Test headline here\")\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the four example selection strategies ---\n",
    "\n",
    "rng = random.Random(42)\n",
    "\n",
    "# Strategy 1: 3 random examples (5 different random draws)\n",
    "random_3_trials = []\n",
    "for _ in range(5):\n",
    "    selected = rng.sample(LABELED_EXAMPLES, 3)\n",
    "    random_3_trials.append(selected)\n",
    "\n",
    "# Strategy 2: 3 diverse examples — one per category, covering 3 of 4 categories\n",
    "# TODO: Create 5 sets of 3 examples, each covering 3 different categories.\n",
    "# For each set, pick one example from each of 3 categories.\n",
    "#\n",
    "# Hint: You can group LABELED_EXAMPLES by category, then sample one from\n",
    "# each of three different category groups.\n",
    "#\n",
    "# YOUR CODE HERE (5-12 lines)\n",
    "diverse_3_trials = []\n",
    "# Group examples by category\n",
    "by_category = {}\n",
    "for text, label in LABELED_EXAMPLES:\n",
    "    by_category.setdefault(label, []).append((text, label))\n",
    "\n",
    "# TODO: Create 5 diverse sets. For each, pick 3 categories and one example from each.\n",
    "\n",
    "\n",
    "# Strategy 3: 3 same-category examples (all Tech)\n",
    "# TODO: Create 5 sets of 3 examples, all from the Tech category.\n",
    "# Use different Tech examples for each set when possible.\n",
    "#\n",
    "# YOUR CODE HERE (3-6 lines)\n",
    "same_category_trials = []\n",
    "\n",
    "\n",
    "# Strategy 4: 5 random examples (5 different random draws)\n",
    "random_5_trials = []\n",
    "for _ in range(5):\n",
    "    selected = rng.sample(LABELED_EXAMPLES, 5)\n",
    "    random_5_trials.append(selected)\n",
    "\n",
    "\n",
    "print(\"Example sets created.\")\n",
    "print(f\"  Strategy 1 (3 random):        {len(random_3_trials)} trial sets\")\n",
    "print(f\"  Strategy 2 (3 diverse):        {len(diverse_3_trials)} trial sets\")\n",
    "print(f\"  Strategy 3 (3 same-category):  {len(same_category_trials)} trial sets\")\n",
    "print(f\"  Strategy 4 (5 random):         {len(random_5_trials)} trial sets\")\n",
    "print(\"\\nSample diverse set categories:\", [ex[1] for ex in diverse_3_trials[0]] if diverse_3_trials else \"(empty — fill in the TODO)\")\n",
    "print(\"Sample same-category set categories:\", [ex[1] for ex in same_category_trials[0]] if same_category_trials else \"(empty — fill in the TODO)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run all trials ---\n",
    "# This will make 5 strategies x 5 trials x 10 test examples = 200 API calls.\n",
    "# With gpt-4o-mini, this should take about 1-2 minutes.\n",
    "\n",
    "strategies = {\n",
    "    \"3 Random\": random_3_trials,\n",
    "    \"3 Diverse\": diverse_3_trials,\n",
    "    \"3 Same-Cat\": same_category_trials,\n",
    "    \"5 Random\": random_5_trials,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, trials in strategies.items():\n",
    "    print(f\"Running {name}...\", end=\" \")\n",
    "    trial_accs = []\n",
    "    for trial_examples in trials:\n",
    "        acc = run_classification_trial(trial_examples, TEST_CLASSIFICATION)\n",
    "        trial_accs.append(acc)\n",
    "    results[name] = trial_accs\n",
    "    mean_acc = np.mean(trial_accs)\n",
    "    std_acc = np.std(trial_accs)\n",
    "    print(f\"mean={mean_acc:.0%}, std={std_acc:.0%}, trials={[f'{a:.0%}' for a in trial_accs]}\")\n",
    "\n",
    "print(\"\\nAll trials complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize the results ---\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: mean accuracy with error bars\n",
    "strategy_names = list(results.keys())\n",
    "means = [np.mean(results[s]) * 100 for s in strategy_names]\n",
    "stds = [np.std(results[s]) * 100 for s in strategy_names]\n",
    "colors = [\"#f59e0b\", \"#6366f1\", \"#ef4444\", \"#10b981\"]\n",
    "\n",
    "bars = ax1.bar(strategy_names, means, color=colors, edgecolor=\"white\",\n",
    "               linewidth=0.5, width=0.6, yerr=stds, capsize=5,\n",
    "               error_kw={\"color\": \"white\", \"linewidth\": 1.5})\n",
    "for bar, val in zip(bars, means):\n",
    "    ax1.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 3,\n",
    "             f\"{val:.0f}%\", ha=\"center\", va=\"bottom\", fontsize=12,\n",
    "             fontweight=\"bold\", color=\"white\")\n",
    "\n",
    "ax1.set_ylabel(\"Accuracy (%)\", fontsize=11)\n",
    "ax1.set_title(\"Mean Accuracy by Example Selection Strategy\", fontsize=13,\n",
    "              fontweight=\"bold\")\n",
    "ax1.set_ylim(0, 110)\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Right plot: individual trial results (scatter + box)\n",
    "positions = range(len(strategy_names))\n",
    "for i, (name, color) in enumerate(zip(strategy_names, colors)):\n",
    "    trial_accs = [a * 100 for a in results[name]]\n",
    "    # Jittered scatter\n",
    "    jitter = np.random.uniform(-0.15, 0.15, len(trial_accs))\n",
    "    ax2.scatter([i + j for j in jitter], trial_accs, color=color,\n",
    "                s=60, alpha=0.8, edgecolors=\"white\", linewidth=0.5, zorder=3)\n",
    "    # Mean line\n",
    "    ax2.hlines(np.mean(trial_accs), i - 0.3, i + 0.3, color=color,\n",
    "               linewidth=2, zorder=4)\n",
    "\n",
    "ax2.set_xticks(list(positions))\n",
    "ax2.set_xticklabels(strategy_names)\n",
    "ax2.set_ylabel(\"Accuracy (%)\", fontsize=11)\n",
    "ax2.set_title(\"Individual Trial Results (5 trials each)\", fontsize=13,\n",
    "              fontweight=\"bold\")\n",
    "ax2.set_ylim(0, 110)\n",
    "ax2.spines[\"top\"].set_visible(False)\n",
    "ax2.spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.suptitle(\"Example Selection: Diversity vs Quantity\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print interpretation\n",
    "print(\"\\nInterpretation:\")\n",
    "diverse_mean = np.mean(results[\"3 Diverse\"])\n",
    "random5_mean = np.mean(results[\"5 Random\"])\n",
    "samecat_mean = np.mean(results[\"3 Same-Cat\"])\n",
    "random3_mean = np.mean(results[\"3 Random\"])\n",
    "\n",
    "print(f\"  3 Diverse ({diverse_mean:.0%}) vs 5 Random ({random5_mean:.0%}):\")\n",
    "if diverse_mean >= random5_mean:\n",
    "    print(\"    -> Diversity wins over quantity. 3 well-chosen > 5 random.\")\n",
    "else:\n",
    "    print(\"    -> Quantity won this time. But check the variance.\")\n",
    "\n",
    "print(f\"  3 Same-Cat ({samecat_mean:.0%}) vs 3 Diverse ({diverse_mean:.0%}):\")\n",
    "if diverse_mean >= samecat_mean:\n",
    "    print(\"    -> Category diversity matters. Same-category examples\")\n",
    "    print(\"       create K/V patterns biased toward one category.\")\n",
    "else:\n",
    "    print(\"    -> Same-category performed well, but likely only for\")\n",
    "    print(\"       test examples in that category.\")\n",
    "\n",
    "print(f\"\\n  Random 3 variance: {np.std(results['3 Random']):.0%}\")\n",
    "print(f\"  Random 5 variance: {np.std(results['5 Random']):.0%}\")\n",
    "print(\"  Higher variance = more sensitivity to which examples are selected.\")\n",
    "print(\"  This connects to the ICL lesson's ordering sensitivity finding:\")\n",
    "print(\"  when the model's behavior depends on surface features of the prompt,\")\n",
    "print(\"  the CHOICE of examples matters as much as their ORDERING.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**What just happened:** You ran a controlled experiment comparing four example selection strategies on the same classification task. The results should show that:\n\n1. **Diverse examples outperform or match random selection**—even with fewer examples. Diversity creates richer K/V patterns in attention, covering more of the input space.\n2. **Same-category examples create bias**—the model does well on that category but poorly on others, because the attention pattern is dominated by one type of content.\n3. **More random examples help, but not as much as better examples**—5 random may beat 3 random, but 3 diverse often beats both.\n4. **Random selection has high variance**—different random draws produce very different accuracy. This is the same fragility as ordering sensitivity: surface features of the prompt change behavior.\n\nThe lesson's principle: **diversity over quantity, format consistency over more examples.** You can now see the empirical evidence.\n\n<details>\n<summary>Solution</summary>\n\n**Why diverse examples work:** Each example adds K/V entries to the attention context. Diverse examples create K/V patterns that cover the input space—the test input's Q vectors can find a good match regardless of its category. Same-category examples create a narrow K/V pattern that only matches one type of input well.\n\n**Diverse trials:**\n```python\ndiverse_3_trials = []\ncat_list = list(by_category.keys())\nfor trial in range(5):\n    # Pick 3 of 4 categories\n    chosen_cats = rng.sample(cat_list, 3)\n    trial_examples = []\n    for cat in chosen_cats:\n        trial_examples.append(rng.choice(by_category[cat]))\n    diverse_3_trials.append(trial_examples)\n```\n\n**Same-category trials:**\n```python\nsame_category_trials = []\ntech_examples = by_category[\"Tech\"]\nfor trial in range(5):\n    selected = rng.sample(tech_examples, min(3, len(tech_examples)))\n    same_category_trials.append(selected)\n```\n\n**Common finding:** With `gpt-4o-mini` on this simple classification task, diverse examples usually achieve 80-100% accuracy. Same-category examples often get Tech examples right but miss others. The variance tells the story: good selection is more reliable than good luck.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Build a Structured Prompt (Independent)\n",
    "\n",
    "You have now practiced the three core techniques from this lesson: format specification (Exercise 1), role prompting (Exercise 2), and example selection (Exercise 3). In this exercise, you'll compose them into a single structured prompt for a real task.\n",
    "\n",
    "**Your task:** Design a complete prompt that generates a meeting summary from raw meeting notes. Your prompt must use **at least 3 techniques** from the lesson:\n",
    "\n",
    "- Format specification (define the output structure)\n",
    "- Role / system prompt (set the behavioral frame)\n",
    "- Few-shot example (show one input-output pair)\n",
    "- Output constraints (specify length, tone, what to include/exclude)\n",
    "\n",
    "**Specification:**\n",
    "1. Write the structured prompt (system + user, or a single prompt with all components)\n",
    "2. Test it on 3 different sets of meeting notes (provided below)\n",
    "3. Evaluate consistency: does the output follow the same format each time?\n",
    "4. Reflect: which components contributed most to consistency?\n",
    "\n",
    "**No skeleton is provided.** Build the prompt from scratch. Think about which components you need and why. The solution is in the `<details>` block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Three sets of raw meeting notes to test your prompt on ---\n",
    "\n",
    "MEETING_NOTES_1 = \"\"\"Team standup March 3\n",
    "- Sarah: finished auth module, PR is up, needs review from David\n",
    "- David: stuck on database migration, index creation taking too long on prod-size data,\n",
    "  might need to do it in batches. Will pair with Maria tomorrow\n",
    "- Maria: launched A/B test for new onboarding flow, results in 2 weeks.\n",
    "  Also mentioned we need to update the API docs before the partner integration next month\n",
    "- Jake: out sick, sent update via Slack — mobile bug fix is ready for QA\n",
    "- Action items: David to create migration batching plan by Thursday.\n",
    "  Sarah to update API docs. Maria to share A/B test setup with the team.\"\"\"\n",
    "\n",
    "MEETING_NOTES_2 = \"\"\"Product planning meeting — Q2 roadmap review\n",
    "Date: March 5\n",
    "Attendees: Lisa (PM), Tom (eng lead), Priya (design), Ahmed (data)\n",
    "\n",
    "Lisa presented Q2 priorities: 1) self-serve onboarding, 2) analytics dashboard,\n",
    "3) API v2. Budget approved for 2 new hires (1 frontend, 1 data eng).\n",
    "\n",
    "Tom raised concern about API v2 timeline — current architecture won't scale.\n",
    "Needs to do a spike on new approach. 2 weeks.\n",
    "\n",
    "Priya showed mockups for analytics dashboard. Team liked the direction but\n",
    "wants more filtering options. Priya to revise by March 12.\n",
    "\n",
    "Ahmed needs access to production data for analytics work. Lisa to file\n",
    "security review request.\n",
    "\n",
    "Decision: push API v2 start date to April to allow for architecture spike.\n",
    "Decision: hire frontend engineer first, data eng in May.\"\"\"\n",
    "\n",
    "MEETING_NOTES_3 = \"\"\"Incident retro — March 4 outage\n",
    "What happened: deployment at 2:15pm triggered a memory leak in the notification\n",
    "service. Service OOMed at 2:47pm. Users couldn't receive notifications for\n",
    "52 minutes. Rollback completed at 3:39pm.\n",
    "\n",
    "Root cause: new feature added an unbounded in-memory cache for user preferences.\n",
    "No cache eviction policy. Cache grew until OOM.\n",
    "\n",
    "What went well: monitoring caught it quickly (alert at 2:48pm, 1 min after OOM).\n",
    "Rollback process worked smoothly.\n",
    "\n",
    "What went wrong: no load testing on the new feature. Code review didn't catch\n",
    "the missing eviction policy. No memory limits on the container.\n",
    "\n",
    "Action items:\n",
    "- Add memory limits to all containers (ops team, by March 8)\n",
    "- Add load testing to CI pipeline (platform team, by March 15)\n",
    "- Code review checklist update: add \"cache eviction\" check (eng leads, by March 7)\n",
    "- Post-mortem doc to be shared with eng org (Tom, by March 6)\"\"\"\n",
    "\n",
    "all_notes = [\n",
    "    (\"Team Standup\", MEETING_NOTES_1),\n",
    "    (\"Product Planning\", MEETING_NOTES_2),\n",
    "    (\"Incident Retro\", MEETING_NOTES_3),\n",
    "]\n",
    "\n",
    "print(\"Three sets of meeting notes loaded:\")\n",
    "for name, notes in all_notes:\n",
    "    print(f\"  - {name} ({len(notes)} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Design and test your structured prompt.\n",
    "#\n",
    "# 1. Define your prompt (system prompt + user prompt, or a single combined prompt)\n",
    "# 2. Test it on all 3 sets of meeting notes\n",
    "# 3. Print each result\n",
    "# 4. Evaluate: does the format stay consistent across different meeting types?\n",
    "#\n",
    "# Use at least 3 techniques from the lesson:\n",
    "#   - Format specification\n",
    "#   - Role / system prompt\n",
    "#   - Few-shot example OR output constraints\n",
    "#   - (bonus) Any other technique\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reflection ---\n",
    "# After testing your prompt, answer these questions:\n",
    "#\n",
    "# 1. Did the output format stay consistent across all 3 meeting types?\n",
    "#    (standup vs planning vs retro)\n",
    "#\n",
    "# 2. Which component of your prompt contributed MOST to consistency?\n",
    "#    (format spec? role? example?)\n",
    "#\n",
    "# 3. If you removed the format specification but kept everything else,\n",
    "#    what would happen? (Try it if you want.)\n",
    "#\n",
    "# 4. What would you change if the task switched from \"meeting summary\"\n",
    "#    to \"extract only action items with owners and deadlines\"?\n",
    "#    Which components of your prompt would stay, which would change?\n",
    "\n",
    "# Print your reflections here (or just think through them):\n",
    "print(\"Reflection:\")\n",
    "print(\"  1. Format consistency across meeting types: ...\")\n",
    "print(\"  2. Most impactful component: ...\")\n",
    "print(\"  3. Without format spec: ...\")\n",
    "print(\"  4. For a different task: ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n**The key insight:** A structured prompt for meeting summaries needs three things: (1) a role to set the behavioral frame, (2) a format specification to constrain the output, and (3) explicit instructions about what to include and exclude. The few-shot example is optional but powerful—it shows the model the exact contract you expect.\n\n**Why this combination works:** The role (\"executive assistant\") biases attention toward summary-relevant information. The format spec (markdown with specific sections) constrains the output structure. The output constraints (\"no more than 5 bullet points\", \"action items must have owners\") narrow the distribution further. Together, these components create a prompt that produces consistent, useful summaries regardless of the meeting type.\n\n```python\n# System prompt: role + output constraints\nsummary_system = \"\"\"You are an experienced executive assistant who writes concise,\nactionable meeting summaries. You focus on decisions, action items, and key\ndiscussion points. You omit small talk, redundant details, and anything not\nactionable. You always follow the exact format specified by the user.\"\"\"\n\n# User prompt: format specification + one example + the actual notes\ndef build_summary_prompt(meeting_notes: str) -> str:\n    return f\"\"\"Summarize the following meeting notes using this EXACT format:\n\n## Summary\n[1-2 sentence overview of what the meeting was about]\n\n## Key Decisions\n- [decision 1]\n- [decision 2]\n(if no decisions were made, write \"No decisions recorded.\")\n\n## Action Items\n- [ ] [task]—**[owner]**, due [date]\n(every action item MUST have an owner and a date)\n\n## Discussion Highlights\n- [important point 1]\n- [important point 2]\n(max 4 bullet points)\n\n---\n\nMeeting notes:\n{meeting_notes}\"\"\"\n\n\n# Test on all 3\nprint(\"STRUCTURED MEETING SUMMARIES\")\nprint(\"=\" * 60)\nfor name, notes in all_notes:\n    print(f\"\\n{'='*60}\")\n    print(f\"MEETING: {name}\")\n    print(f\"{'='*60}\")\n    prompt = build_summary_prompt(notes)\n    result = call_llm_with_system(summary_system, prompt, max_tokens=600)\n    print_wrapped(result)\n```\n\n**Techniques used:**\n1. **Role prompt** (system): \"experienced executive assistant\" biases toward concise, actionable summaries\n2. **Format specification**: Exact markdown template with sections (Summary, Key Decisions, Action Items, Discussion Highlights)\n3. **Output constraints**: Max 4 bullet points for discussion, action items must have owner and date, \"No decisions\" fallback\n\n**Reflection answers:**\n1. The format should stay consistent—all three summaries follow the same markdown template, even though the meetings are very different types.\n2. The format specification contributes the most to consistency (just like Exercise 1 showed). The markdown template with section headers is the strongest structural anchor.\n3. Without the format spec, the role alone would produce reasonable summaries but in varying formats—sometimes paragraphs, sometimes bullets, sometimes with or without action items.\n4. For \"extract action items only\": keep the role, change the format spec to just an action items list, add a constraint about ignoring non-actionable content. The role is reusable; the format spec is task-specific.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **Format specification is the single most impactful technique.** Explicit output schemas (JSON, markdown, bullet templates) constrain the output distribution more reliably than any other component. Format tokens create structural anchors for attention, and autoregressive generation maintains format consistency. A wish produces variable output. A schema produces consistent output.\n\n2. **Role prompts change what the model attends to, not what it knows.** The security auditor, performance engineer, and style reviewer found different issues in the same code—not because the model gained new knowledge, but because the role tokens biased its attention toward different features of the input. Roles shape focus. They do not add capabilities.\n\n3. **Example diversity matters more than example quantity.** Three diverse examples covering different categories outperform five random examples on classification. Diverse examples create richer K/V patterns for attention, covering more of the input space. Same-category examples create bias. Random selection creates variance.\n\n4. **Prompt engineering is composing techniques deliberately.** The meeting summary exercise combined role prompting, format specification, and output constraints into a single structured prompt. Each component serves a specific function in shaping the attention pattern. The skill is knowing which components to include for a given task—not memorizing templates.\n\n5. **The prompt is a program; attention is the interpreter. Prompt engineering is writing better programs.** Every technique works because of attention. Understanding the mechanism—format tokens anchor attention, role tokens bias it, examples create retrieval patterns—lets you reason about which techniques will help, rather than relying on trial and error."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}