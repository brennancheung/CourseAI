{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ControlNet\n",
    "\n",
    "**Module 7.1, Lesson 1** | CourseAI\n",
    "\n",
    "You built Stable Diffusion from scratch. You know the U-Net encoder-decoder, skip connections, cross-attention, and the frozen-model training pattern. This notebook puts that knowledge to work on a real ControlNet model.\n",
    "\n",
    "**What you will do:**\n",
    "- Inspect a pre-trained ControlNet's architecture and compare its parameter counts to the frozen SD model\n",
    "- Build a zero convolution from scratch and verify that it produces all-zero output at initialization\n",
    "- Trace a forward pass through ControlNet, inspecting feature map shapes at every resolution level\n",
    "- Generate images with and without ControlNet conditioning, and verify that text and spatial conditioning coexist\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "Every concept in this notebook comes from the lesson. The trainable encoder copy, the zero convolution mechanism, the additive connections at each resolution level, and the coexistence of text and spatial conditioning. No new theory hereâ€”just verification with real models and real tensors.\n",
    "\n",
    "**Estimated time:** 30â€“45 minutes (model downloads may take a few minutes on first run).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies, import everything, and configure the environment.\n",
    "\n",
    "**Important:** Set the runtime to GPU before running. In Colab: Runtime â†’ Change runtime type â†’ T4 GPU.\n",
    "\n",
    "The first run will download model weights (~1.5 GB for the ControlNet checkpoint + ~5 GB for the SD v1.5 model). Subsequent runs use cached weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate safetensors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from diffusers import (\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    ControlNetModel,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "from diffusers.models.unets.unet_2d_condition import UNet2DConditionModel\n",
    "from PIL import Image\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('WARNING: No GPU detected. Exercises 3 and 4 will be very slow.')\n",
    "    print('In Colab: Runtime \\u2192 Change runtime type \\u2192 T4 GPU')\n",
    "\n",
    "print('\\nSetup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Inspect the ControlNet Architecture `[Guided]`\n",
    "\n",
    "From the lesson, ControlNet adds spatial conditioning by cloning the U-Net encoder and training only the clone. The frozen SD model stays untouched. The clone's outputs are connected to the frozen decoder via zero convolutions.\n",
    "\n",
    "Let's load a real pre-trained ControlNet (Canny edge variant) and the frozen SD v1.5 U-Net, then inspect what's inside.\n",
    "\n",
    "**Before running, predict:**\n",
    "- The frozen SD U-Net has ~860M parameters. How many parameters does the ControlNet add? (Hint: the lesson said the trainable copy is the encoder half onlyâ€”about 35% of the U-Net.)\n",
    "- Will any of the U-Net's parameters be marked as trainable?\n",
    "- Will the ControlNet's encoder blocks have the same structure as the U-Net's encoder blocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained ControlNet (Canny edge variant) and the SD v1.5 U-Net\n",
    "# This downloads model weights on first run (~1.5 GB for ControlNet, ~3.4 GB for U-Net)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    subfolder=\"unet\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(\"Models loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters in each model\n",
    "\n",
    "def count_params(model):\n",
    "    \"\"\"Count total and trainable parameters in a model.\"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "unet_total, unet_trainable = count_params(unet)\n",
    "cn_total, cn_trainable = count_params(controlnet)\n",
    "\n",
    "print(\"=== Parameter Counts ===\")\n",
    "print(f\"\\nFrozen SD v1.5 U-Net:\")\n",
    "print(f\"  Total parameters:     {unet_total:>12,}\")\n",
    "print(f\"  Trainable parameters: {unet_trainable:>12,}\")\n",
    "\n",
    "print(f\"\\nControlNet (Canny):\")\n",
    "print(f\"  Total parameters:     {cn_total:>12,}\")\n",
    "print(f\"  Trainable parameters: {cn_trainable:>12,}\")\n",
    "\n",
    "print(f\"\\nControlNet as % of U-Net: {cn_total / unet_total * 100:.1f}%\")\n",
    "print(f\"\\nDuring ControlNet training:\")\n",
    "print(f\"  U-Net params frozen:       {unet_total:>12,} (100% of U-Net)\")\n",
    "print(f\"  ControlNet params trained:  {cn_trainable:>12,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the top-level structure of both models\n",
    "# Look for the correspondence: ControlNet's encoder blocks should mirror the U-Net's encoder blocks\n",
    "\n",
    "print(\"=== U-Net Top-Level Modules ===\")\n",
    "for name, module in unet.named_children():\n",
    "    param_count = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"  {name:30s} {param_count:>12,} params\")\n",
    "\n",
    "print(\"\\n=== ControlNet Top-Level Modules ===\")\n",
    "for name, module in controlnet.named_children():\n",
    "    param_count = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"  {name:30s} {param_count:>12,} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# The ControlNet has \"controlnet_down_blocks\" that mirror the U-Net's \"down_blocks\" (encoder).\n# It also has \"controlnet_cond_embedding\" for processing the spatial map input,\n# and \"zero_convs,\" the zero convolution connections.\n#\n# Let's verify the encoder block correspondence.\n\nprint(\"=== Encoder Block Correspondence ===\")\nprint(f\"{'U-Net down_blocks':40s} {'ControlNet down_blocks':40s}\")\nprint(\"-\" * 82)\n\nunet_down = list(unet.down_blocks.named_children())\ncn_down = list(controlnet.down_blocks.named_children())\n\nfor i in range(max(len(unet_down), len(cn_down))):\n    unet_info = \"\"\n    cn_info = \"\"\n    if i < len(unet_down):\n        name, block = unet_down[i]\n        params = sum(p.numel() for p in block.parameters())\n        unet_info = f\"Block {name}: {type(block).__name__} ({params:,})\"\n    if i < len(cn_down):\n        name, block = cn_down[i]\n        params = sum(p.numel() for p in block.parameters())\n        cn_info = f\"Block {name}: {type(block).__name__} ({params:,})\"\n    print(f\"{unet_info:40s} {cn_info:40s}\")\n\nprint(\"\\n=== Zero Convolution Layers ===\")\nfor name, module in controlnet.controlnet_down_blocks.named_children():\n    params = sum(p.numel() for p in module.parameters())\n    # Each zero conv is a small 1x1 convolution\n    print(f\"  controlnet_down_blocks.{name}: {type(module).__name__} ({params:,} params)\")\n\n# Also the mid block zero conv\nmid_zc_params = sum(p.numel() for p in controlnet.controlnet_mid_block.parameters())\nprint(f\"  controlnet_mid_block: ({mid_zc_params:,} params)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You loaded a real pre-trained ControlNet and inspected its architecture alongside the frozen SD v1.5 U-Net.\n",
    "\n",
    "**Key observations:**\n",
    "- The ControlNet adds roughly 35â€“40% of the U-Net's parameter countâ€”not a full duplicate. It contains the encoder half and a mid-block, but no decoder.\n",
    "- The ControlNet's encoder blocks (`down_blocks`) mirror the U-Net's encoder blocks in structureâ€”same block types, same channel dimensions, same number of layers.\n",
    "- The zero convolution layers (`controlnet_down_blocks`) are small 1x1 convolutions that connect the ControlNet's encoder outputs to the frozen decoder. Their parameter count is negligible compared to the encoder copy.\n",
    "- The `controlnet_cond_embedding` module processes the spatial map (Canny edges) into the same format the encoder expects.\n",
    "\n",
    "This matches the lesson's architecture diagram: a trainable copy of the encoder, connected to the frozen decoder via zero convolutions at each resolution level.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Verify the Zero-Initialization Property `[Guided]`\n",
    "\n",
    "From the lesson: a zero convolution is a **1Ã—1 convolution with weights initialized to 0.0 and bias initialized to 0.0**. That is the complete definition. The cleverness is in the *initialization*, not the operation.\n",
    "\n",
    "At initialization:\n",
    "- The zero conv output is exactly zero for any input\n",
    "- Adding zero to the frozen encoder features leaves them unchanged: $e_i + 0 = e_i$\n",
    "- The frozen model's output is identical to vanilla SD\n",
    "\n",
    "This is the **safety guarantee**: connecting a fresh ControlNet changes nothing about the original model.\n",
    "\n",
    "**Before running, predict:**\n",
    "- If you create a `nn.Conv2d(256, 256, 1)` with all weights and bias set to zero, what will the output be for a random input tensor?\n",
    "- If you add that output to a tensor of frozen features, will the frozen features change at all?\n",
    "- After a few gradient updates, will the zero conv still output zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a zero convolution from scratch and verify its properties\n",
    "\n",
    "# Step 1: Create a 1x1 convolution and initialize to zero\n",
    "channels = 256\n",
    "zero_conv = nn.Conv2d(channels, channels, kernel_size=1, bias=True)\n",
    "\n",
    "# Initialize ALL weights and bias to exactly zero\n",
    "nn.init.zeros_(zero_conv.weight)\n",
    "nn.init.zeros_(zero_conv.bias)\n",
    "\n",
    "print(\"=== Zero Convolution at Initialization ===\")\n",
    "print(f\"Weight shape: {zero_conv.weight.shape}\")\n",
    "print(f\"Bias shape:   {zero_conv.bias.shape}\")\n",
    "print(f\"Weight sum:   {zero_conv.weight.sum().item():.6f}  (should be 0.0)\")\n",
    "print(f\"Bias sum:     {zero_conv.bias.sum().item():.6f}  (should be 0.0)\")\n",
    "print(f\"Weight max:   {zero_conv.weight.abs().max().item():.6f}  (should be 0.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Pass a random feature map through the zero conv\n",
    "# This simulates what happens when a freshly-initialized ControlNet encoder\n",
    "# produces features and passes them through a zero convolution.\n",
    "\n",
    "# Random feature map (simulating ControlNet encoder output at some resolution)\n",
    "torch.manual_seed(42)\n",
    "controlnet_features = torch.randn(1, channels, 16, 16)  # batch=1, 256 channels, 16x16 spatial\n",
    "\n",
    "print(f\"ControlNet encoder output (random, simulated):\")\n",
    "print(f\"  Shape: {controlnet_features.shape}\")\n",
    "print(f\"  Mean:  {controlnet_features.mean().item():.4f}\")\n",
    "print(f\"  Std:   {controlnet_features.std().item():.4f}\")\n",
    "print(f\"  Range: [{controlnet_features.min().item():.4f}, {controlnet_features.max().item():.4f}]\")\n",
    "\n",
    "# Pass through the zero convolution\n",
    "with torch.no_grad():\n",
    "    zero_conv_output = zero_conv(controlnet_features)\n",
    "\n",
    "print(f\"\\nAfter zero convolution:\")\n",
    "print(f\"  Shape: {zero_conv_output.shape}\")\n",
    "print(f\"  Mean:  {zero_conv_output.mean().item():.6f}  (should be 0.0)\")\n",
    "print(f\"  Std:   {zero_conv_output.std().item():.6f}  (should be 0.0)\")\n",
    "print(f\"  Max:   {zero_conv_output.abs().max().item():.6f}  (should be 0.0)\")\n",
    "print(f\"  All zeros? {torch.all(zero_conv_output == 0).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Add the zero conv output to \"frozen\" encoder features.\n",
    "# This is what happens at each skip connection: e_i + zero_conv(c_i)\n",
    "\n",
    "# Simulate frozen encoder features at the same resolution\n",
    "torch.manual_seed(99)\n",
    "frozen_features = torch.randn(1, channels, 16, 16)\n",
    "\n",
    "# The additive connection\n",
    "enriched_features = frozen_features + zero_conv_output\n",
    "\n",
    "# Verify: enriched features should be IDENTICAL to frozen features\n",
    "difference = (enriched_features - frozen_features).abs().max().item()\n",
    "\n",
    "print(\"=== Safety Guarantee: Frozen Features Unchanged ===\")\n",
    "print(f\"Frozen encoder feature (e_i):\")\n",
    "print(f\"  Mean: {frozen_features.mean().item():.4f}\")\n",
    "print(f\"  Std:  {frozen_features.std().item():.4f}\")\n",
    "print(f\"\\nEnriched feature (e_i + zero_conv(c_i)):\")\n",
    "print(f\"  Mean: {enriched_features.mean().item():.4f}\")\n",
    "print(f\"  Std:  {enriched_features.std().item():.4f}\")\n",
    "print(f\"\\nMax absolute difference: {difference:.10f}\")\n",
    "print(f\"Features identical? {difference == 0.0}\")\n",
    "print(f\"\\ne_i + 0 = e_i. The frozen model is unchanged. This is the safety guarantee.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Simulate a few gradient updates to show the zero conv learns to produce\n# a small, non-zero signal. The control \"fades in\" gradually.\n\n# Create a simple training scenario: try to make zero conv output a target signal\ntarget_signal = torch.randn(1, channels, 16, 16) * 0.1  # small target\noptimizer = torch.optim.SGD(zero_conv.parameters(), lr=0.01)\n\nprint(\"=== Zero Conv Learns to Produce a Signal ===\")\nprint(f\"{'Step':>6s}  {'Output Mean':>12s}  {'Output Std':>12s}  {'Output Max':>12s}\")\nprint(\"-\" * 50)\n\nfor step in range(6):\n    output = zero_conv(controlnet_features)\n    print(f\"{step:>6d}  {output.mean().item():>12.6f}  {output.std().item():>12.6f}  {output.abs().max().item():>12.6f}\")\n\n    # Simple MSE loss against the target\n    loss = nn.functional.mse_loss(output, target_signal)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(f\"\\nWeight max after training: {zero_conv.weight.abs().max().item():.6f}\")\nprint(f\"Bias max after training:   {zero_conv.bias.abs().max().item():.6f}\")\nprint(f\"\\nThe zero conv started silent (all zeros) and gradually learned to produce\")\nprint(f\"a small signal. Nothing at first, then a whisper, then a clear voice.\")\nprint(f\"Training gradually turns up the volume.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You built a zero convolution from scratch and verified all three properties from the lesson:\n",
    "\n",
    "1. **Zero output at initialization** â€” A 1Ã—1 conv with all-zero weights and bias produces all-zero output, regardless of input. The ControlNet starts silent.\n",
    "\n",
    "2. **Frozen features unchanged** â€” Adding the zero conv output to the frozen encoder features: $e_i + 0 = e_i$. The frozen model's behavior is identical with or without ControlNet connected.\n",
    "\n",
    "3. **Gradual fade-in** â€” After a few gradient updates, the weights drift from zero, producing a small non-zero signal. The control fades in gradually as training progresses.\n",
    "\n",
    "This is the same principle as LoRA's B=0 initialization, applied at the feature level instead of the weight level. Same safety guarantee: ensure the frozen model starts unchanged.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Trace the Forward Pass `[Supported]`\n",
    "\n",
    "From the lesson, the ControlNet forward pass produces features at each resolution level that are added to the frozen encoder's skip connections:\n",
    "\n",
    "```\n",
    "d3 = decoder_block_3(cat(d4, e3 + z3))   # original + control\n",
    "d2 = decoder_block_2(cat(d3, e2 + z2))\n",
    "d1 = decoder_block_1(cat(d2, e1 + z1))\n",
    "```\n",
    "\n",
    "The ControlNet's outputs must have **matching shapes** at every resolution level. Let's verify this with a real model.\n",
    "\n",
    "We will feed a Canny edge map through the ControlNet and inspect the output shapes at each level. The edge map is provided as a pre-computed tensorâ€”no preprocessing needed.\n",
    "\n",
    "**Task:** Fill in the `# TODO` markers. Each is 1â€“2 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic Canny edge map as our spatial conditioning input.\n",
    "# In practice, you would extract edges from a real image (that is next lesson).\n",
    "# Here we use a simple geometric pattern so we can focus on the architecture.\n",
    "\n",
    "def create_synthetic_edge_map(height=512, width=512):\n",
    "    \"\"\"Create a simple synthetic edge map with geometric shapes.\n",
    "    Returns a PIL Image with white edges on black background.\"\"\"\n",
    "    edge_map = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Draw a rectangle\n",
    "    edge_map[100:400, 100] = 255\n",
    "    edge_map[100:400, 400] = 255\n",
    "    edge_map[100, 100:400] = 255\n",
    "    edge_map[400, 100:400] = 255\n",
    "\n",
    "    # Draw a diagonal line\n",
    "    for i in range(200):\n",
    "        x = 150 + i\n",
    "        y = 150 + i\n",
    "        if x < height and y < width:\n",
    "            edge_map[x, y] = 255\n",
    "            # Make the line a bit thicker\n",
    "            if x + 1 < height:\n",
    "                edge_map[x + 1, y] = 255\n",
    "\n",
    "    # Draw a circle (approximate)\n",
    "    cx, cy, r = 300, 300, 60\n",
    "    for angle in np.linspace(0, 2 * np.pi, 360):\n",
    "        x = int(cx + r * np.sin(angle))\n",
    "        y = int(cy + r * np.cos(angle))\n",
    "        if 0 <= x < height and 0 <= y < width:\n",
    "            edge_map[x, y] = 255\n",
    "\n",
    "    # Convert to 3-channel RGB PIL Image (ControlNet expects RGB)\n",
    "    edge_rgb = np.stack([edge_map] * 3, axis=-1)\n",
    "    return Image.fromarray(edge_rgb)\n",
    "\n",
    "edge_map_pil = create_synthetic_edge_map()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(edge_map_pil)\n",
    "plt.title(\"Synthetic edge map (spatial conditioning input)\", fontsize=12)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"This is a simple geometric edge map.\")\n",
    "print(\"In practice, you would extract these from real images using Canny edge detection.\")\n",
    "print(\"For now, we only care about the SHAPES of the feature maps, not the content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs for the ControlNet forward pass.\n",
    "# We need: a noisy latent (z_t), a timestep (t), a text embedding, and the edge map.\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "# Load the text encoder and tokenizer for creating text embeddings\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", subfolder=\"tokenizer\"\n",
    ")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", subfolder=\"text_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "\n",
    "# Move models to device\n",
    "controlnet_device = controlnet.to(device)\n",
    "unet_device = unet.to(device)\n",
    "\n",
    "# Create a text embedding for a simple prompt\n",
    "prompt = \"a house with a garden\"\n",
    "text_input = tokenizer(\n",
    "    prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(device))[0]  # [1, 77, 768]\n",
    "\n",
    "print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
    "print(f\"  (batch=1, sequence_length=77 tokens, embedding_dim=768)\")\n",
    "\n",
    "# Create a random noisy latent (simulating z_t at some timestep)\n",
    "torch.manual_seed(42)\n",
    "noisy_latent = torch.randn(1, 4, 64, 64, device=device, dtype=torch.float16)\n",
    "print(f\"\\nNoisy latent shape: {noisy_latent.shape}\")\n",
    "print(f\"  (batch=1, 4 latent channels, 64x64 spatial)\")\n",
    "\n",
    "# Prepare the edge map as a tensor\n",
    "# ControlNet expects the conditioning image as a [B, 3, H, W] tensor in [0, 1]\n",
    "edge_tensor = torch.from_numpy(np.array(edge_map_pil)).permute(2, 0, 1).unsqueeze(0)\n",
    "edge_tensor = edge_tensor.float() / 255.0\n",
    "edge_tensor = edge_tensor.to(device=device, dtype=torch.float16)\n",
    "print(f\"\\nEdge map tensor shape: {edge_tensor.shape}\")\n",
    "print(f\"  (batch=1, 3 RGB channels, 512x512 spatial)\")\n",
    "\n",
    "# Set a timestep\n",
    "timestep = torch.tensor([500], device=device)  # mid-point of denoising\n",
    "print(f\"\\nTimestep: {timestep.item()} (mid-point of denoising schedule)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ControlNet forward pass and inspect the output shapes.\n",
    "#\n",
    "# The ControlNet returns:\n",
    "#   - down_block_res_samples: features at each encoder resolution level\n",
    "#   - mid_block_res_sample: the bottleneck-level feature\n",
    "#\n",
    "# These are the z_i values from the pseudocode:\n",
    "#   z1 = zero_conv_1(c1), z2 = zero_conv_2(c2), etc.\n",
    "#\n",
    "# They will be ADDED to the frozen encoder's features at matching resolutions.\n",
    "\n",
    "with torch.no_grad():\n",
    "    # TODO: Call controlnet's forward pass.\n",
    "    # The ControlNet takes the same inputs as the U-Net (noisy_latent, timestep,\n",
    "    # text_embeddings) PLUS the conditioning image (edge_tensor).\n",
    "    #\n",
    "    # Call: controlnet(noisy_latent, timestep, encoder_hidden_states=text_embeddings,\n",
    "    #                  controlnet_cond=edge_tensor, return_dict=False)\n",
    "    # This returns a tuple: (down_block_res_samples, mid_block_res_sample)\n",
    "    #\n",
    "    # Hint: unpack into two variables.\n",
    "    \n",
    "\n",
    "\n",
    "# Print the shapes of all ControlNet outputs\n",
    "print(\"=== ControlNet Output Shapes (z_i values) ===\")\n",
    "print(f\"Number of down_block outputs: {len(down_block_res_samples)}\")\n",
    "print()\n",
    "for i, sample in enumerate(down_block_res_samples):\n",
    "    h, w = sample.shape[2], sample.shape[3]\n",
    "    print(f\"  down_block[{i}]: {str(sample.shape):30s}  (spatial: {h}x{w})\")\n",
    "\n",
    "print(f\"\\n  mid_block:     {str(mid_block_res_sample.shape):30s}  (spatial: {mid_block_res_sample.shape[2]}x{mid_block_res_sample.shape[3]})\")\n",
    "print(f\"\\nThese are the zero convolution outputs that get ADDED to the frozen\")\n",
    "print(f\"encoder's skip connections at each resolution level.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the U-Net forward pass WITH the ControlNet outputs.\n",
    "# The U-Net accepts `down_block_additional_residuals` and `mid_block_additional_residual`\n",
    "# which are added to the encoder features at each skip connection.\n",
    "#\n",
    "# This is the e_i + z_i operation from the pseudocode.\n",
    "\n",
    "with torch.no_grad():\n",
    "    # TODO: Call the U-Net with the ControlNet residuals.\n",
    "    # Call: unet(noisy_latent, timestep, encoder_hidden_states=text_embeddings,\n",
    "    #           down_block_additional_residuals=down_block_res_samples,\n",
    "    #           mid_block_additional_residual=mid_block_res_sample)\n",
    "    # Access the .sample attribute of the result to get the noise prediction.\n",
    "    \n",
    "\n",
    "\n",
    "print(f\"U-Net noise prediction shape: {noise_pred_with_cn.shape}\")\n",
    "print(f\"  (batch=1, 4 latent channels, 64x64 spatial)\")\n",
    "print(f\"\\nThis is the predicted noise epsilon_hat, same shape as the noisy latent input.\")\n",
    "print(f\"The decoder received enriched skip connections: e_i + z_i at each resolution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare: run the U-Net WITHOUT ControlNet residuals.\n# This should produce a different noise prediction because the spatial conditioning is absent.\n\nwith torch.no_grad():\n    noise_pred_without_cn = unet(\n        noisy_latent, timestep, encoder_hidden_states=text_embeddings\n    ).sample\n\n# Compare the two predictions\ndiff = (noise_pred_with_cn - noise_pred_without_cn).abs()\n\nprint(\"=== ControlNet's Influence ===\")\nprint(f\"Noise prediction WITH ControlNet:    mean={noise_pred_with_cn.mean().item():.4f}, std={noise_pred_with_cn.std().item():.4f}\")\nprint(f\"Noise prediction WITHOUT ControlNet: mean={noise_pred_without_cn.mean().item():.4f}, std={noise_pred_without_cn.std().item():.4f}\")\nprint(f\"\\nDifference (|with - without|):\")\nprint(f\"  Mean: {diff.mean().item():.4f}\")\nprint(f\"  Max:  {diff.max().item():.4f}\")\nprint(f\"  Std:  {diff.std().item():.4f}\")\nprint(f\"\\nThe predictions are different because the ControlNet adds spatial control\")\nprint(f\"signals at each skip connection. The frozen model's weights are identical\")\nprint(f\"in both cases: the difference comes entirely from the additive z_i terms.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The key insight is that the ControlNet and U-Net communicate through tensor addition at matching resolution levels. The ControlNet produces residuals; the U-Net adds them to its own encoder features.\n",
    "\n",
    "**ControlNet forward pass:**\n",
    "```python\n",
    "down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "    noisy_latent, timestep, encoder_hidden_states=text_embeddings,\n",
    "    controlnet_cond=edge_tensor, return_dict=False,\n",
    ")\n",
    "```\n",
    "\n",
    "The ControlNet takes the same inputs as the U-Net (noisy latent, timestep, text embeddings) **plus** the spatial conditioning image. It returns features at each encoder resolution levelâ€”these are the zero convolution outputs (z_i).\n",
    "\n",
    "**U-Net forward pass with ControlNet residuals:**\n",
    "```python\n",
    "noise_pred_with_cn = unet(\n",
    "    noisy_latent, timestep, encoder_hidden_states=text_embeddings,\n",
    "    down_block_additional_residuals=down_block_res_samples,\n",
    "    mid_block_additional_residual=mid_block_res_sample,\n",
    ").sample\n",
    "```\n",
    "\n",
    "The U-Net's `down_block_additional_residuals` parameter injects the ControlNet's outputs at each skip connection. Internally, this implements `e_i + z_i`â€”the frozen encoder features plus the ControlNet's spatial control signals.\n",
    "\n",
    "**Common mistake:** Forgetting `return_dict=False` on the ControlNet call, which returns a dataclass instead of a tuple.\n",
    "\n",
    "</details>\n",
    "\n",
    "### What Just Happened\n",
    "\n",
    "You traced a real forward pass through ControlNet and the frozen U-Net:\n",
    "\n",
    "1. **ControlNet produces multi-resolution features** â€” One output per encoder resolution level, matching the U-Net's skip connection dimensions exactly. These are the $z_i$ values from the lesson's pseudocode.\n",
    "\n",
    "2. **The U-Net adds them at each skip connection** â€” `down_block_additional_residuals` implements the $e_i + z_i$ operation. The frozen encoder features are enriched with spatial control signals.\n",
    "\n",
    "3. **The ControlNet makes a measurable difference** â€” The noise prediction with ControlNet differs from the prediction without it. The difference comes entirely from the additive residuals; the U-Net's weights are identical in both cases.\n",
    "\n",
    "This is the architecture from the lesson, running with real weights and real tensors.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: ControlNet vs Vanilla SD Comparison `[Independent]`\n",
    "\n",
    "The lesson's central claim: ControlNet adds a **WHERE** dimension (spatial structure) that coexists with the **WHAT** dimension (text conditioning). Same edge map, different text prompts should produce different content following the same structure.\n",
    "\n",
    "**Your task:**\n",
    "1. Build a `StableDiffusionControlNetPipeline` using the models already loaded\n",
    "2. Generate an image **without** ControlNet (vanilla SD) using a prompt and a fixed seed\n",
    "3. Generate an image **with** ControlNet using the same prompt, same seed, and the edge map\n",
    "4. Generate a second image **with** ControlNet using a **different** prompt but the **same** edge map and seed\n",
    "5. Display all three side by side\n",
    "\n",
    "**What to verify:**\n",
    "- The vanilla SD image has no spatial structure matching the edge map\n",
    "- The ControlNet image follows the edge map's structure\n",
    "- Changing the text prompt changes the content/style but preserves the spatial structure\n",
    "\n",
    "**Hints:**\n",
    "- Use `StableDiffusionControlNetPipeline` from diffusers (already imported)\n",
    "- Use `UniPCMultistepScheduler` for fast sampling (~20 steps instead of 1000)\n",
    "- Use `torch.Generator(device=device).manual_seed(seed)` for reproducible results\n",
    "- For vanilla SD (no ControlNet), you can use a `StableDiffusionPipeline` or simply pass a blank conditioning image\n",
    "- Use `num_inference_steps=20` to keep generation fast"
   ]
  },
  {
   "cell_type": "code",
   "source": "# VRAM cleanup: free the standalone models from Exercises 1-3.\n# The pipelines in Exercise 4 will load their own copies of these components.\n# Without this cleanup, two full pipelines + the standalone models would exceed\n# a T4's 16 GB VRAM budget.\n\nimport gc\n\ndel controlnet, unet, controlnet_device, unet_device, text_encoder\ngc.collect()\ntorch.cuda.empty_cache()\n\nif torch.cuda.is_available():\n    allocated = torch.cuda.memory_allocated() / 1e9\n    print(f\"GPU memory after cleanup: {allocated:.2f} GB allocated\")\nelse:\n    print(\"No GPU detected, skipping VRAM report.\")\n\nprint(\"Standalone models deleted. Exercise 4 pipelines will load fresh copies.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "#\n",
    "# Steps:\n",
    "# 1. Build a StableDiffusionControlNetPipeline\n",
    "# 2. Generate without ControlNet (use a blank/zero edge map or a vanilla SD pipeline)\n",
    "# 3. Generate with ControlNet + prompt A\n",
    "# 4. Generate with ControlNet + prompt B (same edge map)\n",
    "# 5. Display all three side by side\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe core insight: ControlNet controls spatial structure (WHERE), text controls content (WHAT). Keeping the edge map fixed while changing the prompt demonstrates their independence.\n\nThe solution loads one pipeline at a time to stay within T4 VRAM limits. Generate with the vanilla pipeline first, delete it, then load the ControlNet pipeline.\n\n```python\nimport gc\nfrom diffusers import StableDiffusionPipeline\n\n# Settings\nseed = 42\nnum_steps = 20\nprompt_a = \"a house with a garden, watercolor painting\"\nprompt_b = \"a futuristic city, cyberpunk neon style\"\n\n# --- Step 1: Vanilla SD (no spatial conditioning) ---\npipe_vanilla = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n    safety_checker=None,\n).to(device)\npipe_vanilla.scheduler = UniPCMultistepScheduler.from_config(pipe_vanilla.scheduler.config)\n\ngenerator = torch.Generator(device=device).manual_seed(seed)\nimg_vanilla = pipe_vanilla(\n    prompt_a, num_inference_steps=num_steps, generator=generator,\n).images[0]\n\n# Free the vanilla pipeline before loading the ControlNet pipeline\ndel pipe_vanilla\ngc.collect()\ntorch.cuda.empty_cache()\n\n# --- Step 2: ControlNet pipeline (load ControlNet + SD together) ---\ncn_model = ControlNetModel.from_pretrained(\n    \"lllyasviel/sd-controlnet-canny\",\n    torch_dtype=torch.float16,\n)\n\npipe_cn = StableDiffusionControlNetPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    controlnet=cn_model,\n    torch_dtype=torch.float16,\n    safety_checker=None,\n).to(device)\npipe_cn.scheduler = UniPCMultistepScheduler.from_config(pipe_cn.scheduler.config)\n\n# ControlNet + prompt A\ngenerator = torch.Generator(device=device).manual_seed(seed)\nimg_cn_a = pipe_cn(\n    prompt_a, image=edge_map_pil, num_inference_steps=num_steps, generator=generator,\n).images[0]\n\n# ControlNet + prompt B (same edge map, same seed)\ngenerator = torch.Generator(device=device).manual_seed(seed)\nimg_cn_b = pipe_cn(\n    prompt_b, image=edge_map_pil, num_inference_steps=num_steps, generator=generator,\n).images[0]\n\n# Free the ControlNet pipeline\ndel pipe_cn, cn_model\ngc.collect()\ntorch.cuda.empty_cache()\n\n# --- Step 3: Display all three side by side ---\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\n\naxes[0].imshow(edge_map_pil)\naxes[0].set_title(\"Edge Map\\n(spatial input)\", fontsize=11)\naxes[0].axis('off')\n\naxes[1].imshow(img_vanilla)\naxes[1].set_title(f\"Vanilla SD\\n\\\"{prompt_a}\\\"\", fontsize=11)\naxes[1].axis('off')\n\naxes[2].imshow(img_cn_a)\naxes[2].set_title(f\"ControlNet + Prompt A\\n\\\"{prompt_a}\\\"\", fontsize=11)\naxes[2].axis('off')\n\naxes[3].imshow(img_cn_b)\naxes[3].set_title(f\"ControlNet + Prompt B\\n\\\"{prompt_b}\\\"\", fontsize=11)\naxes[3].axis('off')\n\nplt.suptitle(\"Spatial conditioning (WHERE) coexists with text conditioning (WHAT)\", fontsize=13, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"Observations:\")\nprint(\"- Vanilla SD: no spatial structure matching the edge map\")\nprint(\"- ControlNet + Prompt A: spatial structure follows the edges, content matches prompt A\")\nprint(\"- ControlNet + Prompt B: SAME spatial structure, DIFFERENT content matches prompt B\")\nprint(\"\")\nprint(\"Timestep says WHEN. Text says WHAT. ControlNet says WHERE.\")\nprint(\"Three conditioning signals, three mechanisms, all coexisting.\")\n```\n\n**Notes:**\n- We use `safety_checker=None` to avoid downloading the safety checker model (saves memory on Colab).\n- `UniPCMultistepScheduler` allows fast sampling in ~20 steps instead of the 1000 steps of DDPM. This is an improved sampler; the architecture lesson does not change.\n- The same seed ensures the initial noise is identical across all three runs, isolating the effect of ControlNet.\n- The solution loads and deletes pipelines sequentially to stay within a T4's 16 GB VRAM budget. The vanilla pipeline is deleted before the ControlNet pipeline is loaded.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **ControlNet adds ~35â€“40% of the U-Net's parameters, not 100%.** The trainable copy is the encoder half only. The frozen decoder runs once, receiving enriched skip connections from both the frozen encoder and the ControlNet copy.\n",
    "\n",
    "2. **Zero convolutions are standard 1Ã—1 convs with zero initialization.** The output is exactly zero before training. Adding zero to the frozen features leaves them unchanged: $e_i + 0 = e_i$. The control signal fades in gradually as training progresses.\n",
    "\n",
    "3. **ControlNet produces multi-resolution features that match the frozen encoder's shapes exactly.** Each output is added at the corresponding skip connection. The only change to the forward pass is $e_i + z_i$ instead of $e_i$.\n",
    "\n",
    "4. **Text and spatial conditioning coexist.** Same edge map + different prompts = same structure, different content. Timestep says WHEN. Text says WHAT. ControlNet says WHERE. Three conditioning signals, three mechanisms, all active simultaneously.\n",
    "\n",
    "5. **The frozen model is never modified.** Disconnect the ControlNet and the output is vanilla SD, bit-for-bit identical. This is the same safety principle as LoRA's B=0 initialization, applied at the feature level."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}