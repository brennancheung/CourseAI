{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# GPU Training\n\n**Module 2.3, Lesson 2** | CourseAI\n\nIn this notebook you will move your MNIST training loop to GPU, measure the speedup, add device-aware checkpointing, and use mixed precision to squeeze even more speed out of the hardware.\n\n**What you'll do:**\n- Move the MNIST model and training loop to GPU, time it, and compare to CPU\n- Add device-aware checkpointing â€” save during GPU training, load on CPU with `map_location`\n- Add mixed precision (`autocast` + `GradScaler`) to the GPU training loop and compare speed\n- Write a complete, portable training script with device detection, GPU, mixed precision, and device-aware checkpoints\n\n**For each exercise, PREDICT the output before running the cell.**\n\n**Estimated time:** 20-30 minutes.\n\n**Requirements:** This notebook needs a GPU. In Colab: Runtime -> Change runtime type -> T4 GPU.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to import everything and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n# Reproducible results\ntorch.manual_seed(42)\n\n# For nice plots\nplt.style.use('dark_background')\nplt.rcParams['figure.figsize'] = [10, 4]\n\n# Verify GPU is available\nprint(f'CUDA available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'GPU memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\nelse:\n    print('WARNING: No GPU detected. Go to Runtime â†’ Change runtime type â†’ T4 GPU.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Setup: Model and Data\n",
    "\n",
    "The same MNISTClassifier and data loading you have used in previous lessons. We define them once and reuse across all exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=False\n",
    ")\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "print(f'Parameters: {sum(p.numel() for p in MNISTClassifier().parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch. Returns average loss.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    return running_loss / n_batches\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set. Returns (loss, accuracy).\"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return running_loss / n_batches, 100.0 * correct / total\n",
    "\n",
    "print('Helper functions defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 1: Move to GPU and Time It (Supported)\n\nThe simplest GPU training experiment: take the training loop you already know and add three lines of device placement. Then time both CPU and GPU training to measure the speedup.\n\n**Before running, predict:** How much faster will GPU training be for this small MNIST model? 2x? 5x? 10x? Will it always be faster?\n\n**Task:**\n1. Train on **CPU** for 3 epochs and record the wall-clock time\n2. Train on **GPU** for 3 epochs and record the wall-clock time\n3. Compare the two times\n\nThe CPU version is provided. Fill in the GPU version â€” you need to add device placement."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CPU Training (provided) =====\n",
    "print('=== Training on CPU ===')\n",
    "\n",
    "cpu_device = torch.device('cpu')\n",
    "cpu_model = MNISTClassifier().to(cpu_device)\n",
    "cpu_optimizer = optim.Adam(cpu_model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "cpu_start = time.time()\n",
    "\n",
    "for epoch in range(3):\n",
    "    train_loss = train_one_epoch(cpu_model, train_loader, cpu_optimizer, criterion, cpu_device)\n",
    "    val_loss, val_acc = evaluate(cpu_model, test_loader, cpu_device)\n",
    "    print(f'Epoch {epoch+1}/3 | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "cpu_time = time.time() - cpu_start\n",
    "print(f'\\nCPU training time: {cpu_time:.1f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== GPU Training (fill in the blanks) =====\n",
    "print('=== Training on GPU ===')\n",
    "\n",
    "# Step 1: Create the device\n",
    "gpu_device = ____  # FILL IN: torch.device('cuda')\n",
    "\n",
    "# Step 2: Create model and move to GPU\n",
    "gpu_model = ____  # FILL IN: MNISTClassifier().to(gpu_device)\n",
    "gpu_optimizer = optim.Adam(gpu_model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "gpu_start = time.time()\n",
    "\n",
    "for epoch in range(3):\n",
    "    # Note: train_one_epoch already moves batches to the device â€”\n",
    "    # check the helper function above to see how\n",
    "    train_loss = train_one_epoch(gpu_model, train_loader, gpu_optimizer, criterion, gpu_device)\n",
    "    val_loss, val_acc = evaluate(gpu_model, test_loader, gpu_device)\n",
    "    print(f'Epoch {epoch+1}/3 | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "# Sync GPU before stopping timer (GPU operations are async)\n",
    "torch.cuda.synchronize()\n",
    "gpu_time = time.time() - gpu_start\n",
    "print(f'\\nGPU training time: {gpu_time:.1f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the results\n",
    "speedup = cpu_time / gpu_time\n",
    "\n",
    "print('=' * 50)\n",
    "print(f'{\"Device\":<10} {\"Time\":>10} {\"Speedup\":>10}')\n",
    "print('-' * 50)\n",
    "print(f'{\"CPU\":<10} {cpu_time:>9.1f}s {\"1.0x\":>10}')\n",
    "print(f'{\"GPU\":<10} {gpu_time:>9.1f}s {speedup:>9.1f}x')\n",
    "print('=' * 50)\n",
    "\n",
    "if speedup > 1:\n",
    "    print(f'\\nGPU is {speedup:.1f}x faster.')\n",
    "else:\n",
    "    print(f'\\nGPU is slower! Transfer overhead dominates for this model/data size.')\n",
    "    print('This is expected for small models â€” GPU shines with larger models.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\n**The key insight:** Moving to GPU requires exactly two things â€” place the model on the device once, and place each batch on the device in the loop. Our helper function already handles the batch placement, so you only need to set up the device and model.\n\n```python\ngpu_device = torch.device('cuda')\ngpu_model = MNISTClassifier().to(gpu_device)\n```\n\n**Key points:**\n- The model moves to GPU once with `.to(gpu_device)`.\n- Each batch of data moves to GPU inside the training loop â€” our `train_one_epoch` helper already does this with `images.to(device)` and `labels.to(device)`.\n- `torch.cuda.synchronize()` ensures all GPU operations finish before we stop the timer. Without it, the timer stops while the GPU is still computing.\n- For MNIST (235K parameters), you should see a 3-5x speedup on a T4 GPU. Larger models see larger speedups.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 2: Device-Aware Checkpointing (Supported)\n\nYou learned checkpointing in Saving, Loading, and Checkpoints. Now your tensors are on GPU. The challenge: save a checkpoint during GPU training, then load it as if you were on a CPU-only machine.\n\n**Before running, predict:** If you save a model trained on GPU, then try to load it on a CPU-only machine without `map_location`, what happens?\n\n**Task:**\n1. Train on GPU for 5 epochs, saving a checkpoint at the end\n2. Load the checkpoint using `map_location` to force everything to CPU\n3. Verify the loaded model produces the same predictions as the GPU model (on CPU)\n\n**Hints:**\n- Save a checkpoint dict with keys: `model_state_dict`, `optimizer_state_dict`, `epoch`, `loss`\n- Use `map_location=torch.device('cpu')` when loading to simulate a CPU-only machine\n- Use `torch.allclose()` to verify outputs match"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# Train on GPU for 5 epochs\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_2 = MNISTClassifier().to(device)\n",
    "optimizer_2 = optim.Adam(model_2.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f'Training on {device} for 5 epochs...')\n",
    "for epoch in range(5):\n",
    "    train_loss = train_one_epoch(model_2, train_loader, optimizer_2, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model_2, test_loader, device)\n",
    "    print(f'Epoch {epoch+1}/5 | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "# TODO: Save a checkpoint to 'saved_models/gpu_checkpoint.pth'\n",
    "# The checkpoint dict should have keys:\n",
    "#   'model_state_dict', 'optimizer_state_dict', 'epoch', 'loss'\n",
    "\n",
    "\n",
    "\n",
    "print(f'\\nCheckpoint saved. File size: {os.path.getsize(\"saved_models/gpu_checkpoint.pth\") / 1024:.1f} KB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint on CPU (simulating a machine with no GPU)\n",
    "cpu_device = torch.device('cpu')\n",
    "\n",
    "# TODO: Load the checkpoint with map_location=cpu_device\n",
    "# checkpoint = torch.load(...)\n",
    "\n",
    "\n",
    "# Create a new model on CPU and load the state dict\n",
    "loaded_model = MNISTClassifier().to(cpu_device)\n",
    "\n",
    "# TODO: Load the model state dict from the checkpoint\n",
    "# loaded_model.load_state_dict(...)\n",
    "\n",
    "\n",
    "print(f'Loaded from epoch {checkpoint[\"epoch\"] + 1}, loss: {checkpoint[\"loss\"]:.4f}')\n",
    "print(f'Model is on: {next(loaded_model.parameters()).device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: run both models on the same test batch and compare\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "\n",
    "# GPU model prediction\n",
    "model_2.eval()\n",
    "with torch.no_grad():\n",
    "    gpu_outputs = model_2(test_images.to(device)).cpu()\n",
    "\n",
    "# CPU model prediction\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    cpu_outputs = loaded_model(test_images.to(cpu_device))\n",
    "\n",
    "# Compare\n",
    "match = torch.allclose(gpu_outputs, cpu_outputs, atol=1e-5)\n",
    "print(f'Predictions match: {match}')\n",
    "\n",
    "if match:\n",
    "    print('Device-aware checkpointing works!')\n",
    "    print('You saved on GPU and loaded on CPU â€” the checkpoint is portable.')\n",
    "else:\n",
    "    print('Predictions do NOT match. Check your map_location setting.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\n**The key insight:** `map_location` remaps every tensor to the specified device during loading. Without it, `torch.load()` puts tensors back on `cuda:0` â€” crashing on CPU-only machines.\n\n**Saving the checkpoint:**\n```python\ncheckpoint = {\n    'model_state_dict': model_2.state_dict(),\n    'optimizer_state_dict': optimizer_2.state_dict(),\n    'epoch': 4,\n    'loss': val_loss,\n}\ntorch.save(checkpoint, 'saved_models/gpu_checkpoint.pth')\n```\n\n**Loading with map_location:**\n```python\ncheckpoint = torch.load(\n    'saved_models/gpu_checkpoint.pth',\n    map_location=cpu_device,\n    weights_only=False\n)\nloaded_model.load_state_dict(checkpoint['model_state_dict'])\n```\n\n**Why `weights_only=False`:** Our checkpoint dict contains non-tensor metadata (epoch number, loss value). `weights_only=True` would reject these. For pure state_dicts with no metadata, use `weights_only=True`.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 3: Add Mixed Precision (Supported)\n\nMixed precision uses float16 for the forward pass (where values are large) and float32 for gradient accumulation (where values can be tiny). PyTorch handles this automatically with two tools:\n\n- **`torch.amp.autocast(device_type='cuda')`** â€” wraps the forward pass, choosing float16 where safe\n- **`torch.amp.GradScaler()`** â€” scales the loss up before backward (to prevent float16 underflow), then scales gradients back down before the optimizer step\n\n**Before running, predict:** Will mixed precision noticeably speed up this small MNIST model? Will accuracy change?\n\n**Task:**\n1. Train on GPU for 3 epochs **without** mixed precision â€” time it\n2. Train on GPU for 3 epochs **with** mixed precision â€” time it\n3. Compare speed and final accuracy\n\nThe non-mixed-precision version is provided. Add mixed precision to the second version.\n\n**Hints:**\n- Create `scaler = torch.amp.GradScaler()` before the loop\n- Wrap forward + loss in `with torch.amp.autocast(device_type='cuda'):`\n- Replace `loss.backward()` with `scaler.scale(loss).backward()`\n- Replace `optimizer.step()` with `scaler.step(optimizer)`\n- Add `scaler.update()` after the step"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Baseline: GPU without mixed precision (provided) =====\n",
    "print('=== GPU Training (float32) ===')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_fp32 = MNISTClassifier().to(device)\n",
    "optimizer_fp32 = optim.Adam(model_fp32.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "fp32_start = time.time()\n",
    "\n",
    "for epoch in range(3):\n",
    "    train_loss = train_one_epoch(model_fp32, train_loader, optimizer_fp32, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model_fp32, test_loader, device)\n",
    "    print(f'Epoch {epoch+1}/3 | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "fp32_time = time.time() - fp32_start\n",
    "fp32_acc = val_acc\n",
    "print(f'\\nfloat32 time: {fp32_time:.1f}s | Final acc: {fp32_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Mixed Precision: GPU with autocast + GradScaler =====\n",
    "print('=== GPU Training (mixed precision) ===')\n",
    "\n",
    "model_amp = MNISTClassifier().to(device)\n",
    "optimizer_amp = optim.Adam(model_amp.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO: Create a GradScaler\n",
    "# scaler = ...\n",
    "\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "amp_start = time.time()\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_amp.train()\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # TODO: Wrap the forward pass and loss computation in autocast\n",
    "        # with torch.amp.autocast(device_type='cuda'):\n",
    "        outputs = model_amp(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # TODO: Replace the standard backward/step with scaled versions\n",
    "        optimizer_amp.zero_grad()\n",
    "        loss.backward()       # Replace with: scaler.scale(loss).backward()\n",
    "        optimizer_amp.step()  # Replace with: scaler.step(optimizer_amp)\n",
    "        # Add: scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = running_loss / n_batches\n",
    "    val_loss, val_acc = evaluate(model_amp, test_loader, device)\n",
    "    print(f'Epoch {epoch+1}/3 | Train Loss: {avg_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "amp_time = time.time() - amp_start\n",
    "amp_acc = val_acc\n",
    "print(f'\\nMixed precision time: {amp_time:.1f}s | Final acc: {amp_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "amp_speedup = fp32_time / amp_time\n",
    "\n",
    "print('=' * 55)\n",
    "print(f'{\"Method\":<20} {\"Time\":>10} {\"Accuracy\":>10} {\"Speedup\":>10}')\n",
    "print('-' * 55)\n",
    "print(f'{\"GPU (float32)\":<20} {fp32_time:>9.1f}s {fp32_acc:>9.2f}% {\"1.0x\":>10}')\n",
    "print(f'{\"GPU (mixed prec.)\":<20} {amp_time:>9.1f}s {amp_acc:>9.2f}% {amp_speedup:>9.1f}x')\n",
    "print('=' * 55)\n",
    "\n",
    "acc_diff = abs(amp_acc - fp32_acc)\n",
    "print(f'\\nAccuracy difference: {acc_diff:.2f} percentage points')\n",
    "\n",
    "if acc_diff < 0.5:\n",
    "    print('Accuracy is essentially the same â€” mixed precision is a free speedup.')\n",
    "else:\n",
    "    print('Accuracy differs â€” check your implementation.')\n",
    "\n",
    "if amp_speedup > 1:\n",
    "    print(f'Mixed precision is {amp_speedup:.1f}x faster.')\n",
    "else:\n",
    "    print('Mixed precision is not faster for this model size â€” speedup is more noticeable with larger models.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\n**The key insight:** Mixed precision changes only four lines in your training loop. `autocast` picks float16 where safe (forward pass), and `GradScaler` prevents the tiny float16 gradients from underflowing to zero.\n\n```python\n# Create GradScaler\nscaler = torch.amp.GradScaler()\n\n# Inside the training loop:\nfor images, labels in train_loader:\n    images, labels = images.to(device), labels.to(device)\n\n    # Wrap forward + loss in autocast\n    with torch.amp.autocast(device_type='cuda'):\n        outputs = model_amp(images)\n        loss = criterion(outputs, labels)\n\n    # Scaled backward + step\n    optimizer_amp.zero_grad()\n    scaler.scale(loss).backward()\n    scaler.step(optimizer_amp)\n    scaler.update()\n```\n\n**Four lines changed:**\n1. `scaler = torch.amp.GradScaler()` â€” creates the scaler\n2. `with torch.amp.autocast(device_type='cuda'):` â€” wraps forward pass in auto-casting\n3. `scaler.scale(loss).backward()` â€” scales loss up to prevent float16 underflow\n4. `scaler.step(optimizer_amp)` + `scaler.update()` â€” unscales gradients, steps optimizer, adjusts scale\n\n**On MNIST:** The speedup may be modest (1.0-1.3x) because the model is small. On larger models (millions of parameters), mixed precision typically gives 1.5-3x speedup and uses ~50% less GPU memory.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 4: Complete Portable Training Script (Independent)\n\nPut everything together: write a complete training script from scratch that:\n\n1. **Detects the device** â€” GPU if available, CPU if not\n2. **Trains on GPU** with the device-aware training loop\n3. **Uses mixed precision** if on GPU (skip if on CPU)\n4. **Checkpoints** with device portability (`map_location`)\n5. **Tracks the best model** by validation accuracy\n6. **Times the training** and reports results\n\nThis is the production-ready pattern you carry forward to every future project. Write it from memory â€” the building blocks are all in the exercises above.\n\n**Specifications:**\n- Train for 10 epochs\n- Save best model checkpoint to `'saved_models/portable_best.pth'`\n- Print timing, device, and final accuracy when done\n- The script should work correctly on both GPU and CPU machines"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Write the complete portable training script.\n",
    "#\n",
    "# Structure:\n",
    "#   1. Device detection\n",
    "#   2. Data loading (reuse train_loader and test_loader)\n",
    "#   3. Model + optimizer + criterion\n",
    "#   4. Mixed precision setup (only if GPU)\n",
    "#   5. Training loop with:\n",
    "#      - Device-aware batch placement\n",
    "#      - Mixed precision forward/backward (if GPU)\n",
    "#      - Validation after each epoch\n",
    "#      - Best model checkpointing\n",
    "#   6. Report: device, time, best accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: load the best checkpoint on CPU and evaluate\n",
    "cpu_device = torch.device('cpu')\n",
    "verify_model = MNISTClassifier().to(cpu_device)\n",
    "\n",
    "checkpoint = torch.load(\n",
    "    'saved_models/portable_best.pth',\n",
    "    map_location=cpu_device,\n",
    "    weights_only=False\n",
    ")\n",
    "verify_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "_, verify_acc = evaluate(verify_model, test_loader, cpu_device)\n",
    "print(f'Loaded checkpoint from epoch {checkpoint[\"epoch\"] + 1}')\n",
    "print(f'Accuracy on CPU: {verify_acc:.2f}%')\n",
    "print('\\nPortable checkpoint verified â€” saved on GPU, loaded on CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\n**The key insight:** This is the portable pattern â€” detect the device once, conditionally enable mixed precision, and use `map_location` for checkpoint portability. One script runs everywhere.\n\n```python\nimport time\nimport os\n\nos.makedirs('saved_models', exist_ok=True)\n\n# 1. Device detection\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nuse_amp = device.type == 'cuda'\nprint(f'Training on: {device} | Mixed precision: {use_amp}')\n\n# 2. Model, optimizer, criterion\nmodel = MNISTClassifier().to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n# 3. Mixed precision setup (only if GPU)\nscaler = torch.amp.GradScaler() if use_amp else None\n\n# 4. Training loop\nnum_epochs = 10\nbest_acc = 0.0\n\nstart_time = time.time()\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    n_batches = 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        if use_amp:\n            with torch.amp.autocast(device_type='cuda'):\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n            optimizer.zero_grad()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        running_loss += loss.item()\n        n_batches += 1\n\n    avg_loss = running_loss / n_batches\n    val_loss, val_acc = evaluate(model, test_loader, device)\n\n    # Checkpoint best model\n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': avg_loss,\n            'accuracy': val_acc,\n        }, 'saved_models/portable_best.pth')\n\n    print(f'Epoch {epoch+1:2d}/{num_epochs} | '\n          f'Loss: {avg_loss:.4f} | Acc: {val_acc:.2f}%'\n          f'{\" <- best\" if val_acc >= best_acc else \"\"}')\n\nif device.type == 'cuda':\n    torch.cuda.synchronize()\nelapsed = time.time() - start_time\n\nprint(f'\\nTraining complete on {device}')\nprint(f'Time: {elapsed:.1f}s')\nprint(f'Best accuracy: {best_acc:.2f}%')\n```\n\n**The portable pattern:**\n1. Detect device once at the top\n2. Set `use_amp = device.type == 'cuda'` â€” mixed precision only on GPU\n3. Create GradScaler conditionally\n4. Branch the forward/backward in the inner loop based on `use_amp`\n5. Always use `map_location=device` when loading checkpoints\n6. `torch.cuda.synchronize()` before timing (only if on GPU)\n\nThis exact pattern works on any machine â€” GPU or CPU â€” with no code changes.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **GPU training = same loop + 3 lines of device placement.** Move the model once, move each batch inside the loop.\n2. **`torch.cuda.synchronize()`** before timing â€” GPU operations are asynchronous.\n3. **`map_location=device`** when loading checkpoints makes them portable across machines.\n4. **Mixed precision** (`autocast` + `GradScaler`) uses float16 for speed and float32 for precision. 4 lines of change.\n5. **GPU wins at scale.** Small models may not benefit. Larger models see 3-10x speedups.\n6. **The portable pattern** detects the device and conditionally uses mixed precision â€” one script runs everywhere.\n\n**Clean up** (optional): delete the `saved_models/` directory when you are done experimenting."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: clean up saved files\n",
    "# import shutil\n",
    "# shutil.rmtree('saved_models', ignore_errors=True)\n",
    "# print('Cleaned up saved_models/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}