{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samplers and Efficiency\n",
    "\n",
    "**Module 6.4, Lesson 2** | CourseAI\n",
    "\n",
    "In the lesson, you learned that the model predicts noise and the sampler decides what to do with that prediction. DDPM takes 1000 tiny steps. DDIM predicts xâ‚€ and leaps. DPM-Solver reads the road ahead. All use the same trained modelâ€”no retraining required.\n",
    "\n",
    "**What you will do:**\n",
    "- Swap samplers on the same pre-trained model and compare quality and speed across DDPM, DDIM, and DPM-Solver\n",
    "- Verify that DDIM is deterministic (same seed = same image) while DDPM is stochastic\n",
    "- Explore the quality-vs-steps curve for DPM-Solver and find the sweet spot\n",
    "- Inspect DDIM intermediate latents at each step and observe the coarse-to-fine trajectory\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "This is a STRETCH notebook. The exercises build understanding of sampler mechanisms using `diffusers` schedulersâ€”not implementing samplers from scratch. The focus is on comparing behavior, inspecting intermediate states, and verifying the \"same model, different sampler\" insight.\n",
    "\n",
    "**Estimated time:** 30â€“45 minutes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies and import everything."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q diffusers transformers accelerate\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float16 if device.type == 'cuda' else torch.float32\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB')\n",
    "\n",
    "print('\\nSetup complete.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Helpers\n",
    "\n",
    "Load the Stable Diffusion pipeline once and define helper functions for generating images and displaying results. All exercises share the same model weights."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from diffusers import StableDiffusionPipeline, DDPMScheduler, DDIMScheduler, DPMSolverMultistepScheduler\n",
    "\n",
    "model_id = 'stable-diffusion-v1-5/stable-diffusion-v1-5'\n",
    "\n",
    "# Load the pipeline once. We will swap the scheduler for each exercise.\n",
    "print('Loading Stable Diffusion pipeline...')\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=dtype,\n",
    "    safety_checker=None,\n",
    "    requires_safety_checker=False,\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "print(f'Pipeline loaded on {device}.')\n",
    "\n",
    "# Save the scheduler config so we can create fresh schedulers from it.\n",
    "scheduler_config = pipe.scheduler.config\n",
    "\n",
    "\n",
    "def latent_to_pil(pipe, latent):\n",
    "    \"\"\"Decode a latent tensor to a PIL image using the pipeline's VAE.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        image = pipe.vae.decode(latent / pipe.vae.config.scaling_factor).sample\n",
    "    image = image.detach().cpu().float().squeeze(0).permute(1, 2, 0).numpy()\n",
    "    image = np.clip((image + 1.0) / 2.0, 0.0, 1.0)\n",
    "    return Image.fromarray((image * 255).astype(np.uint8))\n",
    "\n",
    "\n",
    "def show_images(images, titles, figsize=None):\n",
    "    \"\"\"Display a list of PIL images side by side.\"\"\"\n",
    "    n = len(images)\n",
    "    if figsize is None:\n",
    "        figsize = (5 * n, 5)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(np.array(img))\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print('Helpers defined.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Same Model, Different Sampler [Guided]\n",
    "\n",
    "The lesson's central claim: the same trained model produces comparable results at 1000, 50, and 20 steps with different samplers. No retraining. Same weights. Different walkers.\n",
    "\n",
    "In this exercise, you will generate an image with:\n",
    "1. **DDPMScheduler** at 1000 steps (the original algorithm from Module 6.2)\n",
    "2. **DDIMScheduler** at 50 steps (predict-xâ‚€-then-jump, 20Ã— fewer steps)\n",
    "3. **DPMSolverMultistepScheduler** at 20 steps (higher-order solver, 50Ã— fewer steps)\n",
    "\n",
    "All three use the **exact same U-Net weights**, the **same prompt**, and the **same starting noise** (same seed). The only variable is the scheduler.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Will the 20-step DPM-Solver image look significantly worse than the 1000-step DDPM image?\n",
    "- How much faster will DPM-Solver at 20 steps be compared to DDPM at 1000 steps?\n",
    "- Will the three images look identical, or will there be visible differences?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "prompt = \"a cat sitting on a beach at sunset\"\n",
    "guidance_scale = 7.5\n",
    "seed = 42\n",
    "\n",
    "# Define the three configurations: (scheduler class, num_steps, label)\n",
    "configs = [\n",
    "    (DDPMScheduler, 1000, 'DDPM (1000 steps)'),\n",
    "    (DDIMScheduler, 50, 'DDIM (50 steps)'),\n",
    "    (DPMSolverMultistepScheduler, 20, 'DPM-Solver (20 steps)'),\n",
    "]\n",
    "\n",
    "images = []\n",
    "timings = []\n",
    "\n",
    "for scheduler_cls, num_steps, label in configs:\n",
    "    print(f'\\n--- {label} ---')\n",
    "\n",
    "    # Swap the scheduler. One line. No retraining.\n",
    "    pipe.scheduler = scheduler_cls.from_config(scheduler_config)\n",
    "    print(f'Scheduler: {pipe.scheduler.__class__.__name__}')\n",
    "\n",
    "    # Generate with the same seed for fair comparison.\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    start = time.time()\n",
    "\n",
    "    result = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=num_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    timings.append(elapsed)\n",
    "    images.append(result.images[0])\n",
    "\n",
    "    print(f'Time: {elapsed:.1f}s')\n",
    "    print(f'U-Net forward passes: {num_steps * 2} (steps Ã— 2 for CFG)')\n",
    "\n",
    "# Display all three side by side\n",
    "titles = [\n",
    "    f'DDPM (1000 steps)\\n{timings[0]:.1f}s',\n",
    "    f'DDIM (50 steps)\\n{timings[1]:.1f}s',\n",
    "    f'DPM-Solver (20 steps)\\n{timings[2]:.1f}s',\n",
    "]\n",
    "show_images(images, titles, figsize=(15, 5))\n",
    "\n",
    "# Timing comparison\n",
    "print(f'\\n=== Timing Summary ===')\n",
    "print(f'DDPM (1000 steps):      {timings[0]:>6.1f}s  (baseline)')\n",
    "print(f'DDIM (50 steps):        {timings[1]:>6.1f}s  ({timings[0]/timings[1]:.0f}Ã— speedup)')\n",
    "print(f'DPM-Solver (20 steps):  {timings[2]:>6.1f}s  ({timings[0]/timings[2]:.0f}Ã— speedup)')\n",
    "print()\n",
    "print('Same model. Same weights. Same prompt. Same starting noise.')\n",
    "print('The ONLY difference: how the noise prediction was used to take steps.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You generated three images with the **exact same model weights**, the same prompt, and the same starting noise. The only variable was the scheduler:\n",
    "\n",
    "- **DDPM at 1000 steps:** The original algorithm from Module 6.2. Takes a tiny step at each of the 1000 timesteps, adding fresh noise at every step. Slow but faithful to the original formulation.\n",
    "- **DDIM at 50 steps:** Predicts xâ‚€ from the noise prediction, then leaps to a distant timestep using the closed-form formula. 20Ã— fewer steps, comparable quality. Deterministic (no noise injected per step).\n",
    "- **DPM-Solver at 20 steps:** A higher-order ODE solver that evaluates the model at multiple points to estimate trajectory curvature, enabling even larger steps. 50Ã— fewer steps than DDPM.\n",
    "\n",
    "The images are not identicalâ€”different samplers follow different paths through the noise-to-data space. But the **quality** is comparable, despite the massive difference in step counts and compute time.\n",
    "\n",
    "The key insight: swapping `pipe.scheduler` is one line of code. No retraining. No weight changes. The modelâ€™s job (predict noise) never changes. The samplerâ€™s job (decide how to step) is the only variable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 2: DDIM Determinism vs DDPM Stochasticity [Guided]\n\nThe lesson explained that DDIM with Ïƒ=0 is fully deterministic: same starting noise = same image, every time. DDPM is stochastic: it injects fresh random noise at every step, so different random states produce different images.\n\nIn this exercise, you will:\n1. Generate 3 images with **DDIM** using the same seed before each run. Verify they are pixel-identical.\n2. Generate 3 images with **DDPM** using a single seed (set once, not re-seeded between runs). The generator's internal state advances after each run, producing different images.\n\n**Before running, predict:**\n- Will DDIM with the same seed produce the exact same image every time? (Think about the Ïƒ=0 determinism from the lesson.)\n- Will DDPM produce different images when the generator state has advanced between runs? (Think about the fresh noise z injected at every stepâ€”the Ïƒ_t Â· z term in the DDPM reverse formula.)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "prompt = \"a lighthouse on a rocky cliff\"\n",
    "seed = 123\n",
    "num_runs = 3\n",
    "\n",
    "# ---- Part 1: DDIM (deterministic) ----\n",
    "print('=== DDIM (50 steps, Ïƒ=0) ===')\n",
    "pipe.scheduler = DDIMScheduler.from_config(scheduler_config)\n",
    "\n",
    "ddim_images = []\n",
    "for i in range(num_runs):\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    result = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.5,\n",
    "        generator=generator,\n",
    "    )\n",
    "    ddim_images.append(result.images[0])\n",
    "    print(f'  Run {i+1} complete')\n",
    "\n",
    "# Check pixel-level identity\n",
    "ddim_arrays = [np.array(img) for img in ddim_images]\n",
    "ddim_1_vs_2 = np.array_equal(ddim_arrays[0], ddim_arrays[1])\n",
    "ddim_1_vs_3 = np.array_equal(ddim_arrays[0], ddim_arrays[2])\n",
    "print(f'\\nDDIM: Run 1 == Run 2? {ddim_1_vs_2}')\n",
    "print(f'DDIM: Run 1 == Run 3? {ddim_1_vs_3}')\n",
    "if ddim_1_vs_2 and ddim_1_vs_3:\n",
    "    print('All 3 DDIM images are PIXEL-IDENTICAL. Same seed = same image, guaranteed.')\n",
    "\n",
    "show_images(\n",
    "    ddim_images,\n",
    "    [f'DDIM Run {i+1}' for i in range(num_runs)],\n",
    "    figsize=(15, 5),\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ---- Part 2: DDPM (stochastic) ----\nprint('=== DDPM (50 steps, stochastic) ===')\n# Note: DDPM was designed for 1000 steps, but we use 50 here for speed.\n# Quality will be lower, but the stochasticity point still holds.\n# We also use fewer steps so the exercise runs in a reasonable time.\n# The quality difference you see is about step count (DDPM outside its\n# designed range), NOT about stochastic vs deterministic per se.\npipe.scheduler = DDPMScheduler.from_config(scheduler_config)\n\nddpm_images = []\n\n# Seed the generator ONCE. Each pipeline call consumes random state\n# (for z_T AND for per-step noise injection). After the first run,\n# the generator's internal state has advanced, so the second and third\n# runs draw DIFFERENT random numbersâ€”producing different images.\ngenerator = torch.Generator(device=device).manual_seed(seed)\n\nfor i in range(num_runs):\n    # Do NOT re-seed between runs! The whole point is that DDPM's\n    # stochastic noise injection means different generator states\n    # produce different images.\n    result = pipe(\n        prompt,\n        num_inference_steps=50,\n        guidance_scale=7.5,\n        generator=generator,\n    )\n    ddpm_images.append(result.images[0])\n    print(f'  Run {i+1} complete')\n\n# Check pixel-level identity\nddpm_arrays = [np.array(img) for img in ddpm_images]\nddpm_1_vs_2 = np.array_equal(ddpm_arrays[0], ddpm_arrays[1])\nddpm_1_vs_3 = np.array_equal(ddpm_arrays[0], ddpm_arrays[2])\nprint(f'\\nDDPM: Run 1 == Run 2? {ddpm_1_vs_2}')\nprint(f'DDPM: Run 1 == Run 3? {ddpm_1_vs_3}')\n\n# Compute difference magnitude\ndiff_1_2 = np.abs(ddpm_arrays[0].astype(float) - ddpm_arrays[1].astype(float))\nprint(f'Mean pixel difference (Run 1 vs Run 2): {diff_1_2.mean():.2f} / 255')\nprint(f'Max pixel difference (Run 1 vs Run 2): {diff_1_2.max():.0f} / 255')\n\nshow_images(\n    ddpm_images,\n    [f'DDPM Run {i+1}' for i in range(num_runs)],\n    figsize=(15, 5),\n)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### What Just Happened\n\nYou verified two fundamental properties of the samplers:\n\n- **DDIM is deterministic** (Ïƒ=0): all three runs with the same seed produced pixel-identical images. The predict-xâ‚€-then-jump mechanism uses no random noise during sampling. Given the same starting noise z_T, the trajectory to z_0 is fully determined by the model's predictions at each step. This is why DDIM is valuable for reproducibility, A/B testing, and interpolation in z_T space.\n\n- **DDPM is stochastic**: the three runs produced different images even though they started from the same generator seed. Why? Because the generator was seeded once and its internal state advanced after each run. DDPM injects fresh random noise at every stepâ€”the Ïƒ_t Â· z term in the reverse formula. Each pipeline call consumed dozens of random draws (one per step for the noise injection, plus one for z_T). By the second run, the generator was in a different state, producing a different z_T and different per-step noise. The result: three distinct images from the same starting seed.\n\n- **Note on quality:** DDPM was designed for 1000 steps. At 50 steps (used here for speed), quality is lower. That quality difference is about step count, not about stochastic vs deterministic. The stochasticity claim is about *diversity*â€”different images from different random statesâ€”which holds regardless of step count.\n\nThe temperature analogy from **Sampling and Generation** applies directly: DDIM is temperature=0 (deterministic, always picks the \"most likely\" path). DDPM is temperature>0 (stochastic, explores diverse paths). Same model, same destination, but DDPM wanders while DDIM strides.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Step Count Exploration [Supported]\n",
    "\n",
    "The lesson claimed that advanced samplers have a **sweet spot**: too few steps degrades quality, but adding steps beyond the sweet spot yields diminishing returns. DPM-Solver at 200 steps is not meaningfully better than at 25.\n",
    "\n",
    "Your task: generate images at **5, 10, 20, 50, 100, and 200 steps** with DPM-Solver, display them in a grid, and time each generation. Then answer:\n",
    "- At what step count does quality become acceptable?\n",
    "- At what step count does increasing steps stop improving quality noticeably?\n",
    "- Plot generation time vs step countâ€”is the relationship linear?\n",
    "\n",
    "**Hints:**\n",
    "- Use `DPMSolverMultistepScheduler` for all runs\n",
    "- Use the same seed for fair comparison\n",
    "- Store both images and timings in lists\n",
    "- Use `matplotlib` to create a time-vs-steps plot"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "prompt = \"a cozy cabin in a snowy forest at night, warm light from windows\"\nseed = 77\nguidance_scale = 7.5\n\nstep_counts = [5, 10, 20, 50, 100, 200]\nstep_images = []\nstep_timings = []\n\n# TODO: Loop over step_counts. For each step count:\n#   1. Set the scheduler to DPMSolverMultistepScheduler\n#   2. Create a generator with the same seed\n#   3. Time the generation with pipe()\n#   4. Append the image and timing to the lists\n#\n# Look at Exercise 1 for the pattern of setting a scheduler and calling pipe().\n\nfor n in step_counts:\n    pipe.scheduler = DPMSolverMultistepScheduler.from_config(scheduler_config)\n    generator = torch.Generator(device=device).manual_seed(seed)\n\n    start = time.time()\n    # YOUR CODE HERE: call pipe() to generate the image\n    result = None\n    elapsed = time.time() - start\n\n    if result is not None:\n        step_images.append(result.images[0])\n        step_timings.append(elapsed)\n        print(f'{n:>3d} steps: {elapsed:.1f}s')\n    else:\n        print(f'{n:>3d} steps: TODO - fill in the pipe() call above')\n        break",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Display the image grid (only runs if images were generated)\nif len(step_images) == len(step_counts):\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    for ax, img, n, t in zip(axes.flat, step_images, step_counts, step_timings):\n        ax.imshow(np.array(img))\n        ax.set_title(f'{n} steps ({t:.1f}s)', fontsize=11)\n        ax.axis('off')\n    plt.suptitle(f'DPM-Solver: Quality vs Step Count\\n\"{prompt}\"', fontsize=13)\n    plt.tight_layout()\n    plt.show()\n\n    # YOUR CODE HERE: Create a time-vs-steps plot.\n    # Plot step_counts (x-axis) vs step_timings (y-axis).\n    # Add axis labels and a title. Add a grid for readability.\n    plt.figure(figsize=(8, 4))\n    # YOUR CODE HERE\n    plt.tight_layout()\n    plt.show()\n\n    print(f'\\nTime at 5 steps:   {step_timings[0]:.1f}s')\n    print(f'Time at 200 steps: {step_timings[-1]:.1f}s')\n    print(f'Ratio (200/5): {step_timings[-1] / step_timings[0]:.1f}x')\nelse:\n    print('Fill in the TODOs in the cell above first.')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe key insight: quality vs steps is **highly nonlinear** for advanced samplers. DPM-Solver achieves good quality by ~20 steps, and adding more steps beyond that yields diminishing returns. This is because the higher-order solver accounts for trajectory curvature, allowing large accurate steps.\n\n**Cell 1 â€” Generation loop (replace `result = None`):**\n```python\nresult = pipe(\n    prompt,\n    num_inference_steps=n,\n    guidance_scale=guidance_scale,\n    generator=generator,\n)\n```\n\n**Cell 2 â€” Time-vs-steps plot (replace `# YOUR CODE HERE`):**\n```python\nplt.plot(step_counts, step_timings, 'o-', color='cyan', linewidth=2, markersize=8)\nplt.xlabel('Number of Steps')\nplt.ylabel('Generation Time (s)')\nplt.title('DPM-Solver: Generation Time vs Step Count')\nplt.grid(True, alpha=0.3)\nfor n, t in zip(step_counts, step_timings):\n    plt.annotate(f'{t:.1f}s', (n, t), textcoords='offset points',\n                 xytext=(0, 10), ha='center', fontsize=9)\n```\n\n**What to observe:**\n- 5 steps: likely garbled or very blurry. Even DPM-Solver cannot follow a 1000-timestep trajectory accurately in 5 jumps.\n- 10 steps: recognizable but missing fine details. The coarse structure (layout, colors) is established but textures are mushy.\n- 20 steps: good quality. The sweet spot for DPM-Solver. This is why 20â€“30 is the recommended range.\n- 50â€“200 steps: nearly indistinguishable from 20 steps. Diminishing returns.\n\nGeneration time scales approximately linearly with step count because each step requires 2 U-Net forward passes (CFG). The fixed overhead (CLIP encoding, VAE decoding, pipeline setup) is small relative to the denoising loop.\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Inspect DDIM Intermediates [Independent]\n",
    "\n",
    "The lesson's trajectory perspective says that sampling traces a path from noise to data. DDIM follows a smooth, deterministic trajectory. DDPM follows a jittery, stochastic path. At each step, the latent z_t contains progressively more structure.\n",
    "\n",
    "Your task:\n",
    "1. Generate an image with **DDIM at 10 steps**, extracting and storing the **intermediate latent z_t at every step** (10 intermediates total)\n",
    "2. **VAE-decode each intermediate latent** to visualize the coarse-to-fine progression\n",
    "3. Generate an image with **DDPM at 10 steps**, extracting the same intermediates\n",
    "4. Display both sets of intermediates side by side\n",
    "5. **Reflection:** Why does DDIM at 10 steps produce recognizable intermediates while DDPM at 10 steps looks much worse?\n",
    "\n",
    "**Key tips:**\n",
    "- Use `pipe()` with `output_type='latent'` to get the final latent (but you need intermediates at EVERY step, not just the final one)\n",
    "- To capture intermediates, use the `callback_on_step_end` parameter. The callback receives `(pipe, step_index, timestep, callback_kwargs)` and the current latent is in `callback_kwargs['latents']`\n",
    "- Or, run the denoising loop manually (like the reference notebook Exercise 3) and store z_t at each step\n",
    "- Use the `latent_to_pil()` helper to decode latents to images\n",
    "- Use the same prompt and seed for both DDIM and DDPM\n",
    "\n",
    "**Expected output:** Two rows of 10 images. The DDIM row should show a smooth progression from noise to a recognizable image. The DDPM row should show a noisier, less coherent progression (because DDPM was designed for 1000 steps, not 10)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "#\n",
    "# 1. Generate with DDIM at 10 steps, capturing the intermediate latent at every step\n",
    "# 2. VAE-decode each intermediate to a PIL image\n",
    "# 3. Generate with DDPM at 10 steps, capturing intermediates the same way\n",
    "# 4. Display both rows\n",
    "# 5. Answer the reflection question in a print() statement\n",
    "#\n",
    "# Hint for capturing intermediates with a callback:\n",
    "#\n",
    "# intermediates = []\n",
    "# def capture_latent(pipe, step_index, timestep, callback_kwargs):\n",
    "#     intermediates.append(callback_kwargs['latents'].clone())\n",
    "#     return callback_kwargs\n",
    "#\n",
    "# result = pipe(\n",
    "#     prompt, num_inference_steps=10, ...,\n",
    "#     callback_on_step_end=capture_latent,\n",
    "# )\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The key insight: DDIM at 10 steps shows a **smooth coarse-to-fine progression** because its predict-xâ‚€-then-jump mechanism is designed for arbitrary step sizes. Each step makes a large, accurate leap along the trajectory. DDPM at 10 steps shows **a much less coherent progression** because its reverse formula assumes adjacent timestepsâ€”applying it with gaps of 100 timesteps produces accumulating errors.\n",
    "\n",
    "```python\n",
    "prompt = \"a mountain landscape with a river\"\n",
    "seed = 42\n",
    "num_steps = 10\n",
    "\n",
    "# ---- DDIM intermediates ----\n",
    "pipe.scheduler = DDIMScheduler.from_config(scheduler_config)\n",
    "\n",
    "ddim_intermediates = []\n",
    "def capture_ddim(pipe, step_index, timestep, callback_kwargs):\n",
    "    ddim_intermediates.append(callback_kwargs['latents'].clone())\n",
    "    return callback_kwargs\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "ddim_result = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps=num_steps,\n",
    "    guidance_scale=7.5,\n",
    "    generator=generator,\n",
    "    callback_on_step_end=capture_ddim,\n",
    ")\n",
    "# The callback fires AFTER each step, so ddim_intermediates[0] is z after step 1,\n",
    "# and ddim_intermediates[-1] is z_0 (the final latent).\n",
    "\n",
    "# ---- DDPM intermediates ----\n",
    "pipe.scheduler = DDPMScheduler.from_config(scheduler_config)\n",
    "\n",
    "ddpm_intermediates = []\n",
    "def capture_ddpm(pipe, step_index, timestep, callback_kwargs):\n",
    "    ddpm_intermediates.append(callback_kwargs['latents'].clone())\n",
    "    return callback_kwargs\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "ddpm_result = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps=num_steps,\n",
    "    guidance_scale=7.5,\n",
    "    generator=generator,\n",
    "    callback_on_step_end=capture_ddpm,\n",
    ")\n",
    "\n",
    "# ---- Decode and display ----\n",
    "print(f'DDIM intermediates captured: {len(ddim_intermediates)}')\n",
    "print(f'DDPM intermediates captured: {len(ddpm_intermediates)}')\n",
    "\n",
    "# Decode each intermediate latent\n",
    "ddim_decoded = [latent_to_pil(pipe, z) for z in ddim_intermediates]\n",
    "ddpm_decoded = [latent_to_pil(pipe, z) for z in ddpm_intermediates]\n",
    "\n",
    "# Display two rows: DDIM on top, DDPM on bottom\n",
    "fig, axes = plt.subplots(2, num_steps, figsize=(num_steps * 2.5, 5))\n",
    "\n",
    "for i in range(num_steps):\n",
    "    axes[0, i].imshow(np.array(ddim_decoded[i]))\n",
    "    axes[0, i].set_title(f'Step {i+1}', fontsize=8)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    axes[1, i].imshow(np.array(ddpm_decoded[i]))\n",
    "    axes[1, i].set_title(f'Step {i+1}', fontsize=8)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('DDIM', fontsize=11, rotation=0, labelpad=40)\n",
    "axes[1, 0].set_ylabel('DDPM', fontsize=11, rotation=0, labelpad=40)\n",
    "\n",
    "plt.suptitle(f'Intermediate Latents: DDIM vs DDPM at {num_steps} Steps', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print('Reflection: Why does DDIM at 10 steps produce recognizable intermediates')\n",
    "print('while DDPM at 10 steps looks much worse?')\n",
    "print()\n",
    "print('DDIM uses the predict-x0-then-jump mechanism: at each step, it predicts')\n",
    "print('the clean image x0, then uses the closed-form formula to leap to the next')\n",
    "print('timestep. This formula works with alpha_bar (cumulative signal fraction),')\n",
    "print('not alpha (single-step), so it handles large jumps accurately.')\n",
    "print()\n",
    "print('DDPM uses the reverse step formula calibrated for ADJACENT timesteps.')\n",
    "print('At 10 steps with 1000 total timesteps, each \"step\" skips ~100 timesteps.')\n",
    "print('The DDPM coefficients (alpha_t, beta_t) are calibrated for tiny transitions,')\n",
    "print('not 100-timestep jumps. Errors accumulate at each step, compounding into')\n",
    "print('visible artifacts.')\n",
    "print()\n",
    "print('Same model, same starting noise, same 10 steps. But DDIM\\'s formula was')\n",
    "print('DESIGNED for arbitrary step sizes. DDPM\\'s was not.')\n",
    "```\n",
    "\n",
    "**Common mistakes:**\n",
    "- Forgetting to `.clone()` the latents in the callback. Without cloning, all entries point to the same tensor (the final z_0).\n",
    "- Using `output_type='latent'` instead of a callback. That only gives you the final latent, not intermediates.\n",
    "- Not using the same seed for both DDIM and DDPM, making the comparison unfair.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The model predicts noise. The sampler decides what to do with that prediction.** DDPM takes tiny adjacent steps (1000 steps, stochastic). DDIM predicts xâ‚€ and leaps using the closed-form formula (50 steps, deterministic). DPM-Solver reads trajectory curvature with multiple evaluations (20 steps, current standard). All use the exact same trained model.\n",
    "\n",
    "2. **No retraining. Same weights. Different walkers.** Swapping `pipe.scheduler` is one line of code. The modelâ€™s job (predict noise) never changes. The samplerâ€™s job (decide how to step) is the only variable. You verified this by generating comparable images with three different schedulers.\n",
    "\n",
    "3. **DDIM is deterministic; DDPM is stochastic.** Same seed with DDIM produces pixel-identical images every time (Ïƒ=0, no per-step noise injection). DDPM injects fresh noise at every step, producing diverse but non-reproducible results. This is the temperature analogy from Sampling and Generation.\n",
    "\n",
    "4. **Quality vs steps is highly nonlinear for advanced samplers.** DPM-Solver achieves good quality at ~20 steps. Adding more steps beyond the sweet spot yields diminishing returns. Use the recommended range: DPM-Solver++ at 20â€“30 steps as your default.\n",
    "\n",
    "5. **DDIMâ€™s predict-and-leap mechanism handles large step sizes; DDPMâ€™s adjacent-step formula does not.** The intermediate inspection exercise showed this concretely: DDIM at 10 steps produces a smooth coarse-to-fine trajectory, while DDPM at 10 steps produces accumulating artifacts because its formula was calibrated for tiny transitions."
   ]
  }
 ]
}