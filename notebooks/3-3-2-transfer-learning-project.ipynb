{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Transfer Learning on Flowers\n",
    "\n",
    "**Module 3.3, Lesson 2 (Capstone)** | CourseAI\n",
    "\n",
    "This is the Series 3 capstone project. You will combine everything you have learned about CNNs into a single practitioner workflow:\n",
    "\n",
    "1. **Explore** the dataset (Oxford Flowers, 8 species)\n",
    "2. **Feature extraction** — freeze a pretrained ResNet-18 backbone, train a new head\n",
    "3. **Grad-CAM validation** — check if the model focuses on flowers or shortcuts\n",
    "4. **Fine-tuning** — unfreeze layer4 with a differential learning rate, compare\n",
    "5. **Final comparison** — accuracy table + Grad-CAM heatmaps side by side\n",
    "\n",
    "No new concepts. Every technique here was taught in a prior lesson. The challenge is putting them together.\n",
    "\n",
    "**Estimated time:** 30–45 minutes on a Colab GPU (T4).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies and import everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 1: Explore the Data\n",
    "\n",
    "We use **Oxford Flowers102**, a dataset of flower photographs from 102 species. To keep training fast and the task manageable, we filter to **8 visually distinct species** with roughly 50–80 images each.\n",
    "\n",
    "This is a realistic small-dataset scenario: enough data for transfer learning, far too little to train from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Oxford Flowers102\n",
    "# This downloads ~350MB the first time\n",
    "from torchvision.datasets import Flowers102\n",
    "\n",
    "# Download both train and test splits\n",
    "raw_train = Flowers102(root='./data', split='train', download=True)\n",
    "raw_val = Flowers102(root='./data', split='val', download=True)\n",
    "raw_test = Flowers102(root='./data', split='test', download=True)\n",
    "\n",
    "print(f'Train: {len(raw_train)} images')\n",
    "print(f'Val:   {len(raw_val)} images')\n",
    "print(f'Test:  {len(raw_test)} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# The Flowers102 labels are 0-indexed (0 to 101).\n# We'll pick 8 visually distinct species and remap labels to 0-7.\n#\n# Selected classes (these have enough samples and are visually distinct):\n# Label names are chosen for display purposes and may not match the original\n# dataset's label file exactly. The important thing is visual distinctness\n# between classes, not the precise species name.\n\nSELECTED_CLASSES = [1, 10, 17, 28, 51, 63, 70, 82]  # Original Flowers102 label indices\nCLASS_NAMES = [\n    'Pink Primrose',     # class 1\n    'Globe Thistle',     # class 10\n    'Purple Coneflower', # class 17\n    'Stemless Gentian',  # class 28\n    'Wild Pansy',        # class 51\n    'Black-eyed Susan',  # class 63\n    'Bird of Paradise',  # class 70\n    'Clematis',          # class 82\n]\n\n# Create mapping from original labels to new 0-7 labels\nlabel_map = {orig: new for new, orig in enumerate(SELECTED_CLASSES)}\nNUM_CLASSES = len(SELECTED_CLASSES)\n\nprint(f'Selected {NUM_CLASSES} classes:')\nfor i, name in enumerate(CLASS_NAMES):\n    print(f'  {i}: {name} (original label {SELECTED_CLASSES[i]})')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter datasets to only include selected classes\n",
    "\n",
    "def filter_dataset(dataset, selected_classes, label_map):\n",
    "    \"\"\"Return indices of samples belonging to selected classes.\"\"\"\n",
    "    indices = []\n",
    "    mapped_labels = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if label in selected_classes:\n",
    "            indices.append(i)\n",
    "            mapped_labels.append(label_map[label])\n",
    "    return indices, mapped_labels\n",
    "\n",
    "print('Filtering datasets (this takes a moment)...')\n",
    "\n",
    "# Combine train + val for our training set (Flowers102 train split is small)\n",
    "train_indices_raw, train_labels_raw = filter_dataset(raw_train, SELECTED_CLASSES, label_map)\n",
    "val_indices_raw, val_labels_raw = filter_dataset(raw_val, SELECTED_CLASSES, label_map)\n",
    "test_indices, test_labels = filter_dataset(raw_test, SELECTED_CLASSES, label_map)\n",
    "\n",
    "# Combine train+val for training, use test for evaluation\n",
    "# (Flowers102 train split has only 10 images per class — too few alone)\n",
    "train_indices = train_indices_raw + val_indices_raw\n",
    "train_labels = train_labels_raw + val_labels_raw\n",
    "\n",
    "print(f'\\nFiltered dataset sizes:')\n",
    "print(f'  Train: {len(train_indices)} images')\n",
    "print(f'  Test:  {len(test_indices)} images')\n",
    "\n",
    "# Class distribution\n",
    "print(f'\\nTrain class distribution:')\n",
    "train_counts = Counter(train_labels)\n",
    "for cls_idx in range(NUM_CLASSES):\n",
    "    count = train_counts.get(cls_idx, 0)\n",
    "    print(f'  {CLASS_NAMES[cls_idx]}: {count} images')\n",
    "\n",
    "print(f'\\nTest class distribution:')\n",
    "test_counts = Counter(test_labels)\n",
    "for cls_idx in range(NUM_CLASSES):\n",
    "    count = test_counts.get(cls_idx, 0)\n",
    "    print(f'  {CLASS_NAMES[cls_idx]}: {count} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "# Training: augmentation to help with small dataset\n",
    "# Test: just resize and normalize (no augmentation)\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "print('Transforms defined.')\n",
    "print('Training augmentation: RandomResizedCrop, RandomHorizontalFlip, ColorJitter')\n",
    "print('Test: CenterCrop only (no augmentation)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset wrapper that applies our label mapping and transforms\n",
    "\n",
    "class FilteredFlowersDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, indices, labels, transform):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.indices = indices\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, _ = self.base_dataset[self.indices[idx]]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# We need to handle the fact that train_indices spans two datasets\n",
    "# (raw_train and raw_val). Build a combined approach.\n",
    "\n",
    "class CombinedFilteredDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Combines samples from multiple base datasets with pre-computed indices and labels.\"\"\"\n",
    "    def __init__(self, datasets_with_indices, labels, transform):\n",
    "        # datasets_with_indices: list of (dataset, [indices])\n",
    "        self.items = []  # (dataset, original_index)\n",
    "        for dataset, indices in datasets_with_indices:\n",
    "            for idx in indices:\n",
    "                self.items.append((dataset, idx))\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        assert len(self.items) == len(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dataset, orig_idx = self.items[idx]\n",
    "        img, _ = dataset[orig_idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "train_dataset = CombinedFilteredDataset(\n",
    "    [(raw_train, train_indices_raw), (raw_val, val_indices_raw)],\n",
    "    train_labels,\n",
    "    train_transform,\n",
    ")\n",
    "\n",
    "test_dataset = FilteredFlowersDataset(\n",
    "    raw_test, test_indices, test_labels, test_transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'Train dataset: {len(train_dataset)} images')\n",
    "print(f'Test dataset:  {len(test_dataset)} images')\n",
    "print(f'Train batches: {len(train_loader)}')\n",
    "print(f'Test batches:  {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from each class\n",
    "\n",
    "def unnormalize(tensor):\n",
    "    \"\"\"Reverse ImageNet normalization for display.\"\"\"\n",
    "    mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n",
    "    std = torch.tensor(IMAGENET_STD).view(3, 1, 1)\n",
    "    return (tensor.cpu() * std + mean).clamp(0, 1)\n",
    "\n",
    "# Show 2 images per class\n",
    "fig, axes = plt.subplots(2, NUM_CLASSES, figsize=(20, 6))\n",
    "\n",
    "# Collect samples per class\n",
    "class_samples = {i: [] for i in range(NUM_CLASSES)}\n",
    "for idx in range(len(test_dataset)):\n",
    "    img, label = test_dataset[idx]\n",
    "    if len(class_samples[label]) < 2:\n",
    "        class_samples[label].append(img)\n",
    "    if all(len(v) >= 2 for v in class_samples.values()):\n",
    "        break\n",
    "\n",
    "for cls_idx in range(NUM_CLASSES):\n",
    "    for row in range(2):\n",
    "        if row < len(class_samples[cls_idx]):\n",
    "            img = unnormalize(class_samples[cls_idx][row])\n",
    "            axes[row, cls_idx].imshow(img.permute(1, 2, 0).numpy())\n",
    "        axes[row, cls_idx].axis('off')\n",
    "        if row == 0:\n",
    "            axes[row, cls_idx].set_title(CLASS_NAMES[cls_idx], fontsize=9)\n",
    "\n",
    "fig.suptitle('Sample Images from Each Class', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to Notice\n",
    "\n",
    "- Are the classes visually distinct? Can you tell them apart yourself?\n",
    "- Look at the backgrounds — are they varied or consistent within a class? Consistent backgrounds could become shortcuts.\n",
    "- This is a **small** dataset. Transfer learning is the only viable strategy here.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Feature Extraction\n",
    "\n",
    "The practitioner workflow says: **start with the simplest strategy**. Feature extraction is simpler than fine-tuning:\n",
    "\n",
    "1. Load pretrained ResNet-18\n",
    "2. Freeze all backbone parameters\n",
    "3. Replace the classification head for 8 classes\n",
    "4. Train only the head\n",
    "\n",
    "You did this on CIFAR-10 in the Transfer Learning lesson. Now do it on flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_feature_extraction_model(num_classes):\n    \"\"\"Load pretrained ResNet-18 and set up for feature extraction.\n\n    Steps:\n        1. Load pretrained ResNet-18\n        2. Freeze ALL backbone parameters (requires_grad = False)\n        3. Replace model.fc with a new Linear layer for num_classes\n\n    Returns:\n        model with frozen backbone and trainable classification head\n    \"\"\"\n    model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n\n    # TODO: Freeze all backbone parameters\n    # Hint: iterate over model.parameters() and set requires_grad = False\n    # Expected: every parameter in the model should have requires_grad == False after this\n    # (1-2 lines)\n\n\n    # TODO: Replace the classification head for num_classes\n    # Hint: the original head is model.fc — check model.fc.in_features for the input size\n    # Expected: model.fc should be a new nn.Linear(in_features, num_classes)\n    # (1-2 lines)\n\n\n    return model\n\nfe_model = create_feature_extraction_model(NUM_CLASSES).to(device)\n\n# Verify: only the fc layer should be trainable\ntrainable = sum(p.numel() for p in fe_model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in fe_model.parameters())\nfc_in = fe_model.fc.in_features\nprint(f'Trainable parameters: {trainable:,} / {total:,} ({trainable/total:.1%})')\nprint(f'Only training the classification head ({fc_in} -> {NUM_CLASSES})')\n\n# Sanity check — if this fails, revisit your TODO implementation\nassert trainable < total, \"All parameters are trainable — did you forget to freeze the backbone?\"\nassert trainable > 0, \"No trainable parameters — did you freeze the new fc layer too?\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training utilities (provided)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    \"\"\"Train for one epoch. Returns (loss, accuracy).\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    \"\"\"Evaluate on a dataset. Returns (loss, accuracy).\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, optimizer, num_epochs=15):\n",
    "    \"\"\"Full training loop with logging. Returns history dict.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "\n",
    "        print(f'Epoch {epoch+1:2d}/{num_epochs}  '\n",
    "              f'Train Loss: {train_loss:.4f}  Train Acc: {train_acc:.1%}  '\n",
    "              f'Test Loss: {test_loss:.4f}  Test Acc: {test_acc:.1%}')\n",
    "\n",
    "    return history\n",
    "\n",
    "print('Training utilities loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the feature extraction model\n",
    "# Only the fc layer is trainable, so we pass only fc parameters to the optimizer.\n",
    "\n",
    "fe_optimizer = optim.Adam(fe_model.fc.parameters(), lr=1e-3)\n",
    "\n",
    "print('Training feature extraction model...')\n",
    "print('=' * 70)\n",
    "fe_history = train_model(fe_model, train_loader, test_loader, fe_optimizer, num_epochs=15)\n",
    "print('=' * 70)\n",
    "print(f'\\nFinal test accuracy: {fe_history[\"test_acc\"][-1]:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "\n",
    "def plot_training_curves(history, title='Training Curves'):\n",
    "    \"\"\"Plot loss and accuracy curves.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Train')\n",
    "    ax1.plot(epochs, history['test_loss'], 'r-', label='Test')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2.plot(epochs, history['train_acc'], 'b-', label='Train')\n",
    "    ax2.plot(epochs, history['test_acc'], 'r-', label='Test')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_curves(fe_history, 'Feature Extraction Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint: Feature Extraction Results\n",
    "\n",
    "Before moving on, note your results:\n",
    "- What accuracy did you achieve?\n",
    "- Is there a large gap between train and test accuracy (overfitting)?\n",
    "- Training should be fast because only the fc layer is being updated.\n",
    "\n",
    "**But accuracy alone is not enough.** Time for the most important step.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Grad-CAM Validation\n",
    "\n",
    "High accuracy is step two of the practitioner workflow. The real work starts here.\n",
    "\n",
    "**Question:** Is the model right for the right reasons? Does it focus on the flowers, or on something else (background, pot, image borders)?\n",
    "\n",
    "You implemented Grad-CAM from scratch in Visualizing Features. The utility below is provided so you can focus on **interpretation**, not reimplementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM utility (provided)\n",
    "\n",
    "def grad_cam(model, img_tensor, target_class=None):\n",
    "    \"\"\"Compute Grad-CAM for a given image and target class.\n",
    "\n",
    "    Args:\n",
    "        model: the model (in eval mode)\n",
    "        img_tensor: preprocessed image [1, 3, 224, 224] on device\n",
    "        target_class: int class index. If None, uses predicted class.\n",
    "\n",
    "    Returns:\n",
    "        cam: numpy array [224, 224], values in [0, 1]\n",
    "        predicted_class: the class index used\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    stored = {}\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        stored['activations'] = output\n",
    "\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        stored['gradients'] = grad_output[0]\n",
    "\n",
    "    fhook = model.layer4.register_forward_hook(forward_hook)\n",
    "    bhook = model.layer4.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    output = model(img_tensor)\n",
    "\n",
    "    if target_class is None:\n",
    "        target_class = output.argmax(dim=1).item()\n",
    "\n",
    "    model.zero_grad()\n",
    "    output[0, target_class].backward()\n",
    "\n",
    "    gradients = stored['gradients']   # [1, 512, 7, 7]\n",
    "    activations = stored['activations']  # [1, 512, 7, 7]\n",
    "\n",
    "    weights = gradients.mean(dim=[2, 3])  # [1, 512]\n",
    "    cam = (weights.unsqueeze(-1).unsqueeze(-1) * activations).sum(dim=1, keepdim=True)\n",
    "    cam = F.relu(cam)\n",
    "    cam = F.interpolate(cam, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    cam = cam.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    if cam.max() > 0:\n",
    "        cam = cam / cam.max()\n",
    "\n",
    "    fhook.remove()\n",
    "    bhook.remove()\n",
    "\n",
    "    return cam, target_class\n",
    "\n",
    "\n",
    "def show_grad_cam_overlay(img_tensor, cam, class_name, ax=None):\n",
    "    \"\"\"Overlay Grad-CAM heatmap on the image.\"\"\"\n",
    "    img_np = unnormalize(img_tensor.squeeze(0)).permute(1, 2, 0).numpy()\n",
    "    heatmap = plt.cm.jet(cam)[:, :, :3]\n",
    "    overlay = 0.5 * img_np + 0.5 * heatmap\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "    ax.imshow(overlay)\n",
    "    ax.set_title(f'Grad-CAM: {class_name}', fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "print('Grad-CAM utilities loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run Grad-CAM on correctly classified test images\n",
    "#\n",
    "# For each class, find 2 correctly classified images and show:\n",
    "# - Original image\n",
    "# - Grad-CAM overlay\n",
    "#\n",
    "# Ask yourself: does the model focus on the flower, or on something else?\n",
    "\n",
    "# Collect correctly classified samples (2 per class)\n",
    "fe_model.eval()\n",
    "correct_samples = {i: [] for i in range(NUM_CLASSES)}\n",
    "\n",
    "for idx in range(len(test_dataset)):\n",
    "    img, label = test_dataset[idx]\n",
    "    img_batch = img.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = fe_model(img_batch).argmax(dim=1).item()\n",
    "\n",
    "    if pred == label and len(correct_samples[label]) < 2:\n",
    "        correct_samples[label].append((img_batch, label))\n",
    "\n",
    "    if all(len(v) >= 2 for v in correct_samples.values()):\n",
    "        break\n",
    "\n",
    "# Display Grad-CAM for each class\n",
    "fig, axes = plt.subplots(NUM_CLASSES, 4, figsize=(16, 4 * NUM_CLASSES))\n",
    "\n",
    "for cls_idx in range(NUM_CLASSES):\n",
    "    for sample_idx in range(min(2, len(correct_samples[cls_idx]))):\n",
    "        img_batch, label = correct_samples[cls_idx][sample_idx]\n",
    "\n",
    "        # Original\n",
    "        col_offset = sample_idx * 2\n",
    "        img_display = unnormalize(img_batch.squeeze(0)).permute(1, 2, 0).numpy()\n",
    "        axes[cls_idx, col_offset].imshow(img_display)\n",
    "        axes[cls_idx, col_offset].set_title(f'{CLASS_NAMES[cls_idx]}', fontsize=9)\n",
    "        axes[cls_idx, col_offset].axis('off')\n",
    "\n",
    "        # Grad-CAM\n",
    "        cam, _ = grad_cam(fe_model, img_batch, target_class=label)\n",
    "        show_grad_cam_overlay(img_batch, cam, CLASS_NAMES[cls_idx], ax=axes[cls_idx, col_offset + 1])\n",
    "\n",
    "fig.suptitle('Feature Extraction Model: Grad-CAM Validation', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Your Grad-CAM Results\n",
    "\n",
    "Look at each heatmap carefully:\n",
    "\n",
    "- **Good signs:** Heatmap highlights the flower petals, center, or distinctive shape\n",
    "- **Warning signs:** Heatmap highlights background, image borders, or non-flower regions\n",
    "- **Ambiguous:** Heatmap includes flower + some surrounding context (leaves, stem) — this can be legitimate\n",
    "\n",
    "Remember the husky/wolf example from Visualizing Features. Your flower model might have its own version of this.\n",
    "\n",
    "**Key question for each heatmap:** *\"If I showed this to someone who does not know ML, would they agree the model is focusing on the right thing?\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Optional: The Other Two Tools\n\nGrad-CAM answers \"what mattered for this prediction?\" But the **three questions, three tools** framework from Visualizing Features gives you two more lenses:\n\n1. **Filter visualization** (conv1 weights) — What patterns does the first layer detect?\n2. **Activation maps** (layer4 output) — What does the network see at the deepest level?\n\nSince you froze the backbone for feature extraction, conv1 filters should be **identical** to the pretrained ImageNet filters. This is a concrete confirmation of what \"frozen\" means — the early features were not touched.\n\nRun the cell below to check. This is optional but reinforces the full visualization toolkit.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Optional: Visualize conv1 filters and layer4 activation maps\n# from the feature extraction model\n\n# --- 1. Conv1 filter visualization ---\n# Since the backbone was frozen, these should be identical to ImageNet pretrained filters.\nfilters = fe_model.conv1.weight.data.cpu().clone()\n\n# Normalize each filter to [0, 1] for display\nfilters_min = filters.flatten(1).min(dim=1).values[:, None, None, None]\nfilters_max = filters.flatten(1).max(dim=1).values[:, None, None, None]\nfilters_norm = (filters - filters_min) / (filters_max - filters_min + 1e-8)\n\nfig, axes = plt.subplots(4, 16, figsize=(16, 4))\nfor i in range(min(64, filters_norm.shape[0])):\n    row, col = i // 16, i % 16\n    axes[row, col].imshow(filters_norm[i].permute(1, 2, 0).numpy())\n    axes[row, col].axis('off')\nfig.suptitle('Conv1 Filters (frozen — should match ImageNet pretrained)', fontsize=13)\nplt.tight_layout()\nplt.show()\n\n# --- 2. Layer4 activation maps ---\n# Capture layer4 activations on a sample flower image using a forward hook.\n# Use a correctly classified sample from Phase 3\nsample_img = list(correct_samples.values())[0][0][0]\n\nstored_acts = {}\nhook = fe_model.layer4.register_forward_hook(lambda m, inp, out: stored_acts.update({'act': out}))\nfe_model.eval()\nwith torch.no_grad():\n    fe_model(sample_img)\nhook.remove()\n\nacts = stored_acts['act'].squeeze(0).cpu()  # [512, 7, 7]\n\n# Show the first 16 activation maps\nfig, axes = plt.subplots(2, 8, figsize=(16, 4))\nfor i in range(16):\n    row, col = i // 8, i % 8\n    axes[row, col].imshow(acts[i].numpy(), cmap='viridis')\n    axes[row, col].axis('off')\n    axes[row, col].set_title(f'ch {i}', fontsize=8)\nfig.suptitle('Layer4 Activation Maps (feature extraction model)', fontsize=13)\nplt.tight_layout()\nplt.show()\n\nprint('Conv1 filters are frozen ImageNet features — edge and color detectors.')\nprint('Layer4 activations show what high-level patterns the model detected in this flower.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Phase 4: Fine-Tuning\n\nNow that you have feature extraction results and Grad-CAM heatmaps, try fine-tuning to see if adapting the backbone helps.\n\nFeature extraction freezes the entire backbone. Fine-tuning goes one step further: **unfreeze the last residual stage (layer4)** so the backbone can adapt its high-level features to the flower domain.\n\nThe key technique: **differential learning rates**. The unfrozen backbone layers get a much lower learning rate than the classification head. This prevents destroying the pretrained features.\n\nYou saw this pattern in Transfer Learning. Now apply it to your own data and compare to the feature extraction baseline — both accuracy **and** Grad-CAM focus."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Create a fine-tuning model\n#\n# Start from a fresh pretrained ResNet-18 (not the already-trained fe_model).\n# Three steps: freeze everything, then selectively unfreeze layer4, then replace the head.\n\ndef create_finetuning_model(num_classes):\n    \"\"\"Load pretrained ResNet-18 set up for fine-tuning with layer4 unfrozen.\n\n    Steps:\n        1. Load pretrained ResNet-18\n        2. Freeze ALL parameters\n        3. Unfreeze layer4 parameters (set requires_grad = True)\n        4. Replace model.fc for num_classes (new layers are trainable by default)\n\n    Returns:\n        model with layer4 + fc trainable, everything else frozen\n    \"\"\"\n    model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n\n    # TODO: Freeze all parameters (same as feature extraction)\n    # (1-2 lines)\n\n\n    # TODO: Unfreeze layer4 parameters\n    # Hint: iterate over model.layer4.parameters() and set requires_grad = True\n    # (1-2 lines)\n\n\n    # TODO: Replace the classification head for num_classes\n    # (same as feature extraction — 1-2 lines)\n\n\n    return model\n\nft_model = create_finetuning_model(NUM_CLASSES).to(device)\n\n# Verify: layer4 + fc should be trainable\ntrainable = sum(p.numel() for p in ft_model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in ft_model.parameters())\nlayer4_params = sum(p.numel() for p in ft_model.layer4.parameters())\nfc_params = sum(p.numel() for p in ft_model.fc.parameters())\nprint(f'Trainable parameters: {trainable:,} / {total:,} ({trainable/total:.1%})')\nprint(f'Trainable layers: layer4 ({layer4_params:,}) + fc ({fc_params:,})')\n\n# Sanity check\nassert trainable > fc_params, \"Only fc is trainable — did you forget to unfreeze layer4?\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Set up optimizer with differential learning rates\n#\n# Create an Adam optimizer with TWO parameter groups:\n#   1. layer4 parameters → low learning rate (1e-4) to adapt pretrained features gently\n#   2. fc parameters → higher learning rate (1e-3) to learn the new classification head\n#\n# Hint: pass a list of dicts to optim.Adam:\n#   optim.Adam([\n#       {'params': ..., 'lr': ...},\n#       {'params': ..., 'lr': ...},\n#   ])\n#\n# You did this in Transfer Learning — same pattern, your own code.\n\n# TODO: Create the optimizer with differential learning rates\n# (1-4 lines)\nft_optimizer = None  # Replace this\n\n\n# Verify (this will fail if ft_optimizer is still None)\nassert ft_optimizer is not None, \"Create the optimizer above\"\nprint('Optimizer configured with differential learning rates:')\nfor i, group in enumerate(ft_optimizer.param_groups):\n    print(f'  Group {i}: lr={group[\"lr\"]}, {sum(p.numel() for p in group[\"params\"]):,} parameters')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the fine-tuning model\n",
    "print('Training fine-tuning model...')\n",
    "print('=' * 70)\n",
    "ft_history = train_model(ft_model, train_loader, test_loader, ft_optimizer, num_epochs=15)\n",
    "print('=' * 70)\n",
    "print(f'\\nFinal test accuracy: {ft_history[\"test_acc\"][-1]:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(ft_history, 'Fine-Tuning Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint: Fine-Tuning vs Feature Extraction\n",
    "\n",
    "Compare the two approaches:\n",
    "- Did fine-tuning improve test accuracy over feature extraction?\n",
    "- Was there more overfitting (larger train/test accuracy gap)?\n",
    "- Was the improvement (if any) worth the added complexity?\n",
    "\n",
    "On a small dataset like this, fine-tuning may or may not help. That is a realistic outcome, not a failure.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Final Comparison\n",
    "\n",
    "Build the complete picture: accuracy numbers **and** Grad-CAM heatmaps for both approaches, side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy comparison table\n",
    "\n",
    "fe_final_acc = fe_history['test_acc'][-1]\n",
    "ft_final_acc = ft_history['test_acc'][-1]\n",
    "\n",
    "print('=' * 50)\n",
    "print('ACCURACY COMPARISON')\n",
    "print('=' * 50)\n",
    "print(f'{\"Approach\":<25} {\"Test Accuracy\":<15}')\n",
    "print('-' * 40)\n",
    "print(f'{\"Feature Extraction\":<25} {fe_final_acc:<15.1%}')\n",
    "print(f'{\"Fine-Tuning (layer4)\":<25} {ft_final_acc:<15.1%}')\n",
    "print('-' * 40)\n",
    "diff = ft_final_acc - fe_final_acc\n",
    "direction = 'improvement' if diff > 0 else 'decrease'\n",
    "print(f'Difference: {abs(diff):.1%} {direction}')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side training curves\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_fe = range(1, len(fe_history['test_acc']) + 1)\n",
    "epochs_ft = range(1, len(ft_history['test_acc']) + 1)\n",
    "\n",
    "ax1.plot(epochs_fe, fe_history['test_acc'], 'b-', label='Feature Extraction', linewidth=2)\n",
    "ax1.plot(epochs_ft, ft_history['test_acc'], 'r-', label='Fine-Tuning', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Test Accuracy')\n",
    "ax1.set_title('Test Accuracy Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "ax2.plot(epochs_fe, fe_history['test_loss'], 'b-', label='Feature Extraction', linewidth=2)\n",
    "ax2.plot(epochs_ft, ft_history['test_loss'], 'r-', label='Fine-Tuning', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Test Loss')\n",
    "ax2.set_title('Test Loss Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Side-by-side Grad-CAM comparison\n",
    "#\n",
    "# For each class, show the same test image with:\n",
    "# - Original image\n",
    "# - Grad-CAM from feature extraction model\n",
    "# - Grad-CAM from fine-tuning model\n",
    "#\n",
    "# Look for differences in spatial focus between the two approaches.\n",
    "\n",
    "# Collect one test image per class\n",
    "comparison_images = {}\n",
    "for idx in range(len(test_dataset)):\n",
    "    img, label = test_dataset[idx]\n",
    "    if label not in comparison_images:\n",
    "        comparison_images[label] = img.unsqueeze(0).to(device)\n",
    "    if len(comparison_images) == NUM_CLASSES:\n",
    "        break\n",
    "\n",
    "fig, axes = plt.subplots(NUM_CLASSES, 3, figsize=(14, 4 * NUM_CLASSES))\n",
    "\n",
    "for cls_idx in range(NUM_CLASSES):\n",
    "    img_batch = comparison_images[cls_idx]\n",
    "\n",
    "    # Original\n",
    "    img_display = unnormalize(img_batch.squeeze(0)).permute(1, 2, 0).numpy()\n",
    "    axes[cls_idx, 0].imshow(img_display)\n",
    "    axes[cls_idx, 0].set_title(f'{CLASS_NAMES[cls_idx]}', fontsize=10)\n",
    "    axes[cls_idx, 0].axis('off')\n",
    "\n",
    "    # Grad-CAM: feature extraction model\n",
    "    cam_fe, _ = grad_cam(fe_model, img_batch, target_class=cls_idx)\n",
    "    show_grad_cam_overlay(img_batch, cam_fe, 'Feature Extraction', ax=axes[cls_idx, 1])\n",
    "\n",
    "    # Grad-CAM: fine-tuning model\n",
    "    cam_ft, _ = grad_cam(ft_model, img_batch, target_class=cls_idx)\n",
    "    show_grad_cam_overlay(img_batch, cam_ft, 'Fine-Tuning', ax=axes[cls_idx, 2])\n",
    "\n",
    "fig.suptitle('Grad-CAM Comparison: Feature Extraction vs Fine-Tuning', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the Comparison\n",
    "\n",
    "Compare the Grad-CAM heatmaps for the two approaches:\n",
    "\n",
    "- **Did fine-tuning change what the model focuses on?** Often, fine-tuning produces tighter, more object-focused heatmaps because layer4 adapts to the specific task.\n",
    "- **Are there classes where one approach focuses on the flower and the other does not?** This is the clearest evidence of the value of fine-tuning (or the danger of it).\n",
    "- **Are there any classes where both approaches focus on something other than the flower?** That would suggest a dataset bias — something about the images that correlates with the label beyond the flower itself.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You just completed the full practitioner workflow:\n",
    "\n",
    "| Step | What You Did |\n",
    "|------|--------------|\n",
    "| **1. Explore** | Understood the dataset: 8 flower species, ~50–80 images each |\n",
    "| **2. Feature Extraction** | Froze ResNet-18 backbone, trained classification head |\n",
    "| **3. Grad-CAM Validation** | Checked if the model focuses on flowers, not shortcuts |\n",
    "| **4. Fine-Tuning** | Unfroze layer4 with differential LR, compared to baseline |\n",
    "| **5. Comparison** | Accuracy table + Grad-CAM heatmaps side by side |\n",
    "\n",
    "The most important step was **not** the one that maximized accuracy. It was the Grad-CAM validation — the step where you checked whether the model learned the right features.\n",
    "\n",
    "**Correct prediction does not mean correct reasoning.** You now have the tools to check.\n",
    "\n",
    "---\n",
    "\n",
    "### Series 3 Complete\n",
    "\n",
    "You started Series 3 asking \"what is a convolution?\" You ended it by fine-tuning a pretrained CNN on a custom dataset and using Grad-CAM to verify the model's reasoning.\n",
    "\n",
    "The practical superpower you built: not just \"can I get high accuracy?\" but **\"can I understand what my model learned and trust its reasoning?\"**\n",
    "\n",
    "Next up: **Series 4 — LLMs and Transformers**. A different architecture, a different data modality, but the same practitioner mindset: understand the model, do not just use it."
   ]
  }
 ]
}