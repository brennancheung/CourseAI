{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project: Transfer Learning on Flowers\n",
        "\n",
        "**Module 3.3, Lesson 2 (Capstone)** | CourseAI\n",
        "\n",
        "This is the Series 3 capstone project. You will combine everything you have learned about CNNs into a single practitioner workflow:\n",
        "\n",
        "1. **Explore** the dataset (Oxford Flowers, 8 species)\n",
        "2. **Feature extraction** \u2014 freeze a pretrained ResNet-18 backbone, train a new head\n",
        "3. **Grad-CAM validation** \u2014 check if the model focuses on flowers or shortcuts\n",
        "4. **Fine-tuning** \u2014 unfreeze layer4 with a differential learning rate, compare\n",
        "5. **Final comparison** \u2014 accuracy table + Grad-CAM heatmaps side by side\n",
        "\n",
        "No new concepts. Every technique here was taught in a prior lesson. The challenge is putting them together.\n",
        "\n",
        "**Estimated time:** 30\u201345 minutes on a Colab GPU (T4).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run this cell to install dependencies and import everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet18_Weights\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import copy\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "if device.type == 'cuda':\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 1: Explore the Data\n",
        "\n",
        "We use **Oxford Flowers102**, a dataset of flower photographs from 102 species. To keep training fast and the task manageable, we filter to **8 visually distinct species** with roughly 50\u201380 images each.\n",
        "\n",
        "This is a realistic small-dataset scenario: enough data for transfer learning, far too little to train from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Oxford Flowers102\n",
        "# This downloads ~350MB the first time\n",
        "from torchvision.datasets import Flowers102\n",
        "\n",
        "# Download both train and test splits\n",
        "raw_train = Flowers102(root='./data', split='train', download=True)\n",
        "raw_val = Flowers102(root='./data', split='val', download=True)\n",
        "raw_test = Flowers102(root='./data', split='test', download=True)\n",
        "\n",
        "print(f'Train: {len(raw_train)} images')\n",
        "print(f'Val:   {len(raw_val)} images')\n",
        "print(f'Test:  {len(raw_test)} images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The Flowers102 labels are 0-indexed (0 to 101).\n",
        "# We'll pick 8 visually distinct species and remap labels to 0-7.\n",
        "#\n",
        "# Selected classes (these have enough samples and are visually distinct):\n",
        "# Label names are approximate (the dataset doesn't include official names in torchvision,\n",
        "# but these correspond to visually distinct flower types).\n",
        "\n",
        "SELECTED_CLASSES = [1, 10, 17, 28, 51, 63, 70, 82]  # Original Flowers102 label indices\n",
        "CLASS_NAMES = [\n",
        "    'Pink Primrose',     # class 1\n",
        "    'Globe Thistle',     # class 10\n",
        "    'Purple Coneflower', # class 17\n",
        "    'Stemless Gentian',  # class 28\n",
        "    'Wild Pansy',        # class 51\n",
        "    'Black-eyed Susan',  # class 63\n",
        "    'Bird of Paradise',  # class 70\n",
        "    'Clematis',          # class 82\n",
        "]\n",
        "\n",
        "# Create mapping from original labels to new 0-7 labels\n",
        "label_map = {orig: new for new, orig in enumerate(SELECTED_CLASSES)}\n",
        "NUM_CLASSES = len(SELECTED_CLASSES)\n",
        "\n",
        "print(f'Selected {NUM_CLASSES} classes:')\n",
        "for i, name in enumerate(CLASS_NAMES):\n",
        "    print(f'  {i}: {name} (original label {SELECTED_CLASSES[i]})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter datasets to only include selected classes\n",
        "\n",
        "def filter_dataset(dataset, selected_classes, label_map):\n",
        "    \"\"\"Return indices of samples belonging to selected classes.\"\"\"\n",
        "    indices = []\n",
        "    mapped_labels = []\n",
        "    for i in range(len(dataset)):\n",
        "        _, label = dataset[i]\n",
        "        if label in selected_classes:\n",
        "            indices.append(i)\n",
        "            mapped_labels.append(label_map[label])\n",
        "    return indices, mapped_labels\n",
        "\n",
        "print('Filtering datasets (this takes a moment)...')\n",
        "\n",
        "# Combine train + val for our training set (Flowers102 train split is small)\n",
        "train_indices_raw, train_labels_raw = filter_dataset(raw_train, SELECTED_CLASSES, label_map)\n",
        "val_indices_raw, val_labels_raw = filter_dataset(raw_val, SELECTED_CLASSES, label_map)\n",
        "test_indices, test_labels = filter_dataset(raw_test, SELECTED_CLASSES, label_map)\n",
        "\n",
        "# Combine train+val for training, use test for evaluation\n",
        "# (Flowers102 train split has only 10 images per class â€” too few alone)\n",
        "train_indices = train_indices_raw + val_indices_raw\n",
        "train_labels = train_labels_raw + val_labels_raw\n",
        "\n",
        "print(f'\\nFiltered dataset sizes:')\n",
        "print(f'  Train: {len(train_indices)} images')\n",
        "print(f'  Test:  {len(test_indices)} images')\n",
        "\n",
        "# Class distribution\n",
        "print(f'\\nTrain class distribution:')\n",
        "train_counts = Counter(train_labels)\n",
        "for cls_idx in range(NUM_CLASSES):\n",
        "    count = train_counts.get(cls_idx, 0)\n",
        "    print(f'  {CLASS_NAMES[cls_idx]}: {count} images')\n",
        "\n",
        "print(f'\\nTest class distribution:')\n",
        "test_counts = Counter(test_labels)\n",
        "for cls_idx in range(NUM_CLASSES):\n",
        "    count = test_counts.get(cls_idx, 0)\n",
        "    print(f'  {CLASS_NAMES[cls_idx]}: {count} images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transforms\n",
        "# Training: augmentation to help with small dataset\n",
        "# Test: just resize and normalize (no augmentation)\n",
        "\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "])\n",
        "\n",
        "print('Transforms defined.')\n",
        "print('Training augmentation: RandomResizedCrop, RandomHorizontalFlip, ColorJitter')\n",
        "print('Test: CenterCrop only (no augmentation)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom dataset wrapper that applies our label mapping and transforms\n",
        "\n",
        "class FilteredFlowersDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_dataset, indices, labels, transform):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.indices = indices\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, _ = self.base_dataset[self.indices[idx]]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# We need to handle the fact that train_indices spans two datasets\n",
        "# (raw_train and raw_val). Build a combined approach.\n",
        "\n",
        "class CombinedFilteredDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Combines samples from multiple base datasets with pre-computed indices and labels.\"\"\"\n",
        "    def __init__(self, datasets_with_indices, labels, transform):\n",
        "        # datasets_with_indices: list of (dataset, [indices])\n",
        "        self.items = []  # (dataset, original_index)\n",
        "        for dataset, indices in datasets_with_indices:\n",
        "            for idx in indices:\n",
        "                self.items.append((dataset, idx))\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        assert len(self.items) == len(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dataset, orig_idx = self.items[idx]\n",
        "        img, _ = dataset[orig_idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "\n",
        "train_dataset = CombinedFilteredDataset(\n",
        "    [(raw_train, train_indices_raw), (raw_val, val_indices_raw)],\n",
        "    train_labels,\n",
        "    train_transform,\n",
        ")\n",
        "\n",
        "test_dataset = FilteredFlowersDataset(\n",
        "    raw_test, test_indices, test_labels, test_transform,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f'Train dataset: {len(train_dataset)} images')\n",
        "print(f'Test dataset:  {len(test_dataset)} images')\n",
        "print(f'Train batches: {len(train_loader)}')\n",
        "print(f'Test batches:  {len(test_loader)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample images from each class\n",
        "\n",
        "def unnormalize(tensor):\n",
        "    \"\"\"Reverse ImageNet normalization for display.\"\"\"\n",
        "    mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n",
        "    std = torch.tensor(IMAGENET_STD).view(3, 1, 1)\n",
        "    return (tensor.cpu() * std + mean).clamp(0, 1)\n",
        "\n",
        "# Show 2 images per class\n",
        "fig, axes = plt.subplots(2, NUM_CLASSES, figsize=(20, 6))\n",
        "\n",
        "# Collect samples per class\n",
        "class_samples = {i: [] for i in range(NUM_CLASSES)}\n",
        "for idx in range(len(test_dataset)):\n",
        "    img, label = test_dataset[idx]\n",
        "    if len(class_samples[label]) < 2:\n",
        "        class_samples[label].append(img)\n",
        "    if all(len(v) >= 2 for v in class_samples.values()):\n",
        "        break\n",
        "\n",
        "for cls_idx in range(NUM_CLASSES):\n",
        "    for row in range(2):\n",
        "        if row < len(class_samples[cls_idx]):\n",
        "            img = unnormalize(class_samples[cls_idx][row])\n",
        "            axes[row, cls_idx].imshow(img.permute(1, 2, 0).numpy())\n",
        "        axes[row, cls_idx].axis('off')\n",
        "        if row == 0:\n",
        "            axes[row, cls_idx].set_title(CLASS_NAMES[cls_idx], fontsize=9)\n",
        "\n",
        "fig.suptitle('Sample Images from Each Class', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What to Notice\n",
        "\n",
        "- Are the classes visually distinct? Can you tell them apart yourself?\n",
        "- Look at the backgrounds \u2014 are they varied or consistent within a class? Consistent backgrounds could become shortcuts.\n",
        "- This is a **small** dataset. Transfer learning is the only viable strategy here.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 2: Feature Extraction\n",
        "\n",
        "The practitioner workflow says: **start with the simplest strategy**. Feature extraction is simpler than fine-tuning:\n",
        "\n",
        "1. Load pretrained ResNet-18\n",
        "2. Freeze all backbone parameters\n",
        "3. Replace the classification head for 8 classes\n",
        "4. Train only the head\n",
        "\n",
        "You did this on CIFAR-10 in the Transfer Learning lesson. Now do it on flowers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_feature_extraction_model(num_classes):\n",
        "    \"\"\"Load pretrained ResNet-18 and set up for feature extraction.\"\"\"\n",
        "    model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    # TODO: Freeze all backbone parameters\n",
        "    # Hint: iterate over model.parameters() and set requires_grad = False\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # TODO: Replace the classification head (model.fc) for num_classes\n",
        "    # Hint: model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "fe_model = create_feature_extraction_model(NUM_CLASSES).to(device)\n",
        "\n",
        "# Verify: only the fc layer should be trainable\n",
        "trainable = sum(p.numel() for p in fe_model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in fe_model.parameters())\n",
        "print(f'Trainable parameters: {trainable:,} / {total:,} ({trainable/total:.1%})')\n",
        "print(f'Only training the classification head ({in_features} -> {NUM_CLASSES})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training utilities (provided)\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    \"\"\"Train for one epoch. Returns (loss, accuracy).\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    \"\"\"Evaluate on a dataset. Returns (loss, accuracy).\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, test_loader, optimizer, num_epochs=15):\n",
        "    \"\"\"Full training loop with logging. Returns history dict.\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1:2d}/{num_epochs}  '\n",
        "              f'Train Loss: {train_loss:.4f}  Train Acc: {train_acc:.1%}  '\n",
        "              f'Test Loss: {test_loss:.4f}  Test Acc: {test_acc:.1%}')\n",
        "\n",
        "    return history\n",
        "\n",
        "print('Training utilities loaded.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Train the feature extraction model\n",
        "# Only the fc layer is trainable, so we pass only fc parameters to the optimizer.\n",
        "\n",
        "fe_optimizer = optim.Adam(fe_model.fc.parameters(), lr=1e-3)\n",
        "\n",
        "print('Training feature extraction model...')\n",
        "print('=' * 70)\n",
        "fe_history = train_model(fe_model, train_loader, test_loader, fe_optimizer, num_epochs=15)\n",
        "print('=' * 70)\n",
        "print(f'\\nFinal test accuracy: {fe_history[\"test_acc\"][-1]:.1%}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "\n",
        "def plot_training_curves(history, title='Training Curves'):\n",
        "    \"\"\"Plot loss and accuracy curves.\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    ax1.plot(epochs, history['train_loss'], 'b-', label='Train')\n",
        "    ax1.plot(epochs, history['test_loss'], 'r-', label='Test')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    ax2.plot(epochs, history['train_acc'], 'b-', label='Train')\n",
        "    ax2.plot(epochs, history['test_acc'], 'r-', label='Test')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.set_title('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_ylim(0, 1.05)\n",
        "\n",
        "    fig.suptitle(title, fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_curves(fe_history, 'Feature Extraction Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Checkpoint: Feature Extraction Results\n",
        "\n",
        "Before moving on, note your results:\n",
        "- What accuracy did you achieve?\n",
        "- Is there a large gap between train and test accuracy (overfitting)?\n",
        "- Training should be fast because only the fc layer is being updated.\n",
        "\n",
        "**But accuracy alone is not enough.** Time for the most important step.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 3: Grad-CAM Validation\n",
        "\n",
        "High accuracy is step two of the practitioner workflow. The real work starts here.\n",
        "\n",
        "**Question:** Is the model right for the right reasons? Does it focus on the flowers, or on something else (background, pot, image borders)?\n",
        "\n",
        "You implemented Grad-CAM from scratch in Visualizing Features. The utility below is provided so you can focus on **interpretation**, not reimplementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grad-CAM utility (provided)\n",
        "\n",
        "def grad_cam(model, img_tensor, target_class=None):\n",
        "    \"\"\"Compute Grad-CAM for a given image and target class.\n",
        "\n",
        "    Args:\n",
        "        model: the model (in eval mode)\n",
        "        img_tensor: preprocessed image [1, 3, 224, 224] on device\n",
        "        target_class: int class index. If None, uses predicted class.\n",
        "\n",
        "    Returns:\n",
        "        cam: numpy array [224, 224], values in [0, 1]\n",
        "        predicted_class: the class index used\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    stored = {}\n",
        "\n",
        "    def forward_hook(module, input, output):\n",
        "        stored['activations'] = output\n",
        "\n",
        "    def backward_hook(module, grad_input, grad_output):\n",
        "        stored['gradients'] = grad_output[0]\n",
        "\n",
        "    fhook = model.layer4.register_forward_hook(forward_hook)\n",
        "    bhook = model.layer4.register_full_backward_hook(backward_hook)\n",
        "\n",
        "    output = model(img_tensor)\n",
        "\n",
        "    if target_class is None:\n",
        "        target_class = output.argmax(dim=1).item()\n",
        "\n",
        "    model.zero_grad()\n",
        "    output[0, target_class].backward()\n",
        "\n",
        "    gradients = stored['gradients']   # [1, 512, 7, 7]\n",
        "    activations = stored['activations']  # [1, 512, 7, 7]\n",
        "\n",
        "    weights = gradients.mean(dim=[2, 3])  # [1, 512]\n",
        "    cam = (weights.unsqueeze(-1).unsqueeze(-1) * activations).sum(dim=1, keepdim=True)\n",
        "    cam = F.relu(cam)\n",
        "    cam = F.interpolate(cam, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "    cam = cam.squeeze().detach().cpu().numpy()\n",
        "\n",
        "    if cam.max() > 0:\n",
        "        cam = cam / cam.max()\n",
        "\n",
        "    fhook.remove()\n",
        "    bhook.remove()\n",
        "\n",
        "    return cam, target_class\n",
        "\n",
        "\n",
        "def show_grad_cam_overlay(img_tensor, cam, class_name, ax=None):\n",
        "    \"\"\"Overlay Grad-CAM heatmap on the image.\"\"\"\n",
        "    img_np = unnormalize(img_tensor.squeeze(0)).permute(1, 2, 0).numpy()\n",
        "    heatmap = plt.cm.jet(cam)[:, :, :3]\n",
        "    overlay = 0.5 * img_np + 0.5 * heatmap\n",
        "    overlay = np.clip(overlay, 0, 1)\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "\n",
        "    ax.imshow(overlay)\n",
        "    ax.set_title(f'Grad-CAM: {class_name}', fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "print('Grad-CAM utilities loaded.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Run Grad-CAM on correctly classified test images\n",
        "#\n",
        "# For each class, find 2 correctly classified images and show:\n",
        "# - Original image\n",
        "# - Grad-CAM overlay\n",
        "#\n",
        "# Ask yourself: does the model focus on the flower, or on something else?\n",
        "\n",
        "# Collect correctly classified samples (2 per class)\n",
        "fe_model.eval()\n",
        "correct_samples = {i: [] for i in range(NUM_CLASSES)}\n",
        "\n",
        "for idx in range(len(test_dataset)):\n",
        "    img, label = test_dataset[idx]\n",
        "    img_batch = img.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = fe_model(img_batch).argmax(dim=1).item()\n",
        "\n",
        "    if pred == label and len(correct_samples[label]) < 2:\n",
        "        correct_samples[label].append((img_batch, label))\n",
        "\n",
        "    if all(len(v) >= 2 for v in correct_samples.values()):\n",
        "        break\n",
        "\n",
        "# Display Grad-CAM for each class\n",
        "fig, axes = plt.subplots(NUM_CLASSES, 4, figsize=(16, 4 * NUM_CLASSES))\n",
        "\n",
        "for cls_idx in range(NUM_CLASSES):\n",
        "    for sample_idx in range(min(2, len(correct_samples[cls_idx]))):\n",
        "        img_batch, label = correct_samples[cls_idx][sample_idx]\n",
        "\n",
        "        # Original\n",
        "        col_offset = sample_idx * 2\n",
        "        img_display = unnormalize(img_batch.squeeze(0)).permute(1, 2, 0).numpy()\n",
        "        axes[cls_idx, col_offset].imshow(img_display)\n",
        "        axes[cls_idx, col_offset].set_title(f'{CLASS_NAMES[cls_idx]}', fontsize=9)\n",
        "        axes[cls_idx, col_offset].axis('off')\n",
        "\n",
        "        # Grad-CAM\n",
        "        cam, _ = grad_cam(fe_model, img_batch, target_class=label)\n",
        "        show_grad_cam_overlay(img_batch, cam, CLASS_NAMES[cls_idx], ax=axes[cls_idx, col_offset + 1])\n",
        "\n",
        "fig.suptitle('Feature Extraction Model: Grad-CAM Validation', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpret Your Grad-CAM Results\n",
        "\n",
        "Look at each heatmap carefully:\n",
        "\n",
        "- **Good signs:** Heatmap highlights the flower petals, center, or distinctive shape\n",
        "- **Warning signs:** Heatmap highlights background, image borders, or non-flower regions\n",
        "- **Ambiguous:** Heatmap includes flower + some surrounding context (leaves, stem) \u2014 this can be legitimate\n",
        "\n",
        "Remember the husky/wolf example from Visualizing Features. Your flower model might have its own version of this.\n",
        "\n",
        "**Key question for each heatmap:** *\"If I showed this to someone who does not know ML, would they agree the model is focusing on the right thing?\"*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 4: Fine-Tuning\n",
        "\n",
        "Feature extraction freezes the entire backbone. Fine-tuning goes one step further: **unfreeze the last residual stage (layer4)** so the backbone can adapt its high-level features to the flower domain.\n",
        "\n",
        "The key technique: **differential learning rates**. The unfrozen backbone layers get a much lower learning rate than the classification head. This prevents destroying the pretrained features.\n",
        "\n",
        "You saw this pattern in Transfer Learning. Now apply it to your own data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a fine-tuning model\n",
        "#\n",
        "# Start from a fresh pretrained ResNet-18 (not the already-trained fe_model).\n",
        "# Freeze everything, then selectively unfreeze layer4.\n",
        "\n",
        "def create_finetuning_model(num_classes):\n",
        "    \"\"\"Load pretrained ResNet-18 set up for fine-tuning with layer4 unfrozen.\"\"\"\n",
        "    model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    # Step 1: Freeze all parameters\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Step 2: Unfreeze layer4\n",
        "    # TODO: Set requires_grad = True for all parameters in model.layer4\n",
        "    for param in model.layer4.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # Step 3: Replace the classification head (new head is trainable by default)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "ft_model = create_finetuning_model(NUM_CLASSES).to(device)\n",
        "\n",
        "# Verify: layer4 + fc should be trainable\n",
        "trainable = sum(p.numel() for p in ft_model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in ft_model.parameters())\n",
        "print(f'Trainable parameters: {trainable:,} / {total:,} ({trainable/total:.1%})')\n",
        "print(f'Trainable layers: layer4 ({sum(p.numel() for p in ft_model.layer4.parameters()):,}) + fc ({sum(p.numel() for p in ft_model.fc.parameters()):,})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Set up optimizer with differential learning rates\n",
        "#\n",
        "# Two parameter groups:\n",
        "# 1. layer4 parameters: low learning rate (1e-4)\n",
        "# 2. fc parameters: higher learning rate (1e-3)\n",
        "#\n",
        "# This prevents destroying pretrained features in layer4\n",
        "# while allowing the head to learn quickly.\n",
        "\n",
        "ft_optimizer = optim.Adam([\n",
        "    {'params': ft_model.layer4.parameters(), 'lr': 1e-4},   # low LR for backbone\n",
        "    {'params': ft_model.fc.parameters(), 'lr': 1e-3},       # higher LR for head\n",
        "])\n",
        "\n",
        "print('Optimizer configured with differential learning rates:')\n",
        "print(f'  layer4: lr=1e-4 (adapt pretrained features gently)')\n",
        "print(f'  fc:     lr=1e-3 (learn new classification head)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the fine-tuning model\n",
        "print('Training fine-tuning model...')\n",
        "print('=' * 70)\n",
        "ft_history = train_model(ft_model, train_loader, test_loader, ft_optimizer, num_epochs=15)\n",
        "print('=' * 70)\n",
        "print(f'\\nFinal test accuracy: {ft_history[\"test_acc\"][-1]:.1%}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_training_curves(ft_history, 'Fine-Tuning Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Checkpoint: Fine-Tuning vs Feature Extraction\n",
        "\n",
        "Compare the two approaches:\n",
        "- Did fine-tuning improve test accuracy over feature extraction?\n",
        "- Was there more overfitting (larger train/test accuracy gap)?\n",
        "- Was the improvement (if any) worth the added complexity?\n",
        "\n",
        "On a small dataset like this, fine-tuning may or may not help. That is a realistic outcome, not a failure.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 5: Final Comparison\n",
        "\n",
        "Build the complete picture: accuracy numbers **and** Grad-CAM heatmaps for both approaches, side by side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Accuracy comparison table\n",
        "\n",
        "fe_final_acc = fe_history['test_acc'][-1]\n",
        "ft_final_acc = ft_history['test_acc'][-1]\n",
        "\n",
        "print('=' * 50)\n",
        "print('ACCURACY COMPARISON')\n",
        "print('=' * 50)\n",
        "print(f'{\"Approach\":<25} {\"Test Accuracy\":<15}')\n",
        "print('-' * 40)\n",
        "print(f'{\"Feature Extraction\":<25} {fe_final_acc:<15.1%}')\n",
        "print(f'{\"Fine-Tuning (layer4)\":<25} {ft_final_acc:<15.1%}')\n",
        "print('-' * 40)\n",
        "diff = ft_final_acc - fe_final_acc\n",
        "direction = 'improvement' if diff > 0 else 'decrease'\n",
        "print(f'Difference: {abs(diff):.1%} {direction}')\n",
        "print('=' * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Side-by-side training curves\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "epochs_fe = range(1, len(fe_history['test_acc']) + 1)\n",
        "epochs_ft = range(1, len(ft_history['test_acc']) + 1)\n",
        "\n",
        "ax1.plot(epochs_fe, fe_history['test_acc'], 'b-', label='Feature Extraction', linewidth=2)\n",
        "ax1.plot(epochs_ft, ft_history['test_acc'], 'r-', label='Fine-Tuning', linewidth=2)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Test Accuracy')\n",
        "ax1.set_title('Test Accuracy Comparison')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim(0, 1.05)\n",
        "\n",
        "ax2.plot(epochs_fe, fe_history['test_loss'], 'b-', label='Feature Extraction', linewidth=2)\n",
        "ax2.plot(epochs_ft, ft_history['test_loss'], 'r-', label='Fine-Tuning', linewidth=2)\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Test Loss')\n",
        "ax2.set_title('Test Loss Comparison')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Side-by-side Grad-CAM comparison\n",
        "#\n",
        "# For each class, show the same test image with:\n",
        "# - Original image\n",
        "# - Grad-CAM from feature extraction model\n",
        "# - Grad-CAM from fine-tuning model\n",
        "#\n",
        "# Look for differences in spatial focus between the two approaches.\n",
        "\n",
        "# Collect one test image per class\n",
        "comparison_images = {}\n",
        "for idx in range(len(test_dataset)):\n",
        "    img, label = test_dataset[idx]\n",
        "    if label not in comparison_images:\n",
        "        comparison_images[label] = img.unsqueeze(0).to(device)\n",
        "    if len(comparison_images) == NUM_CLASSES:\n",
        "        break\n",
        "\n",
        "fig, axes = plt.subplots(NUM_CLASSES, 3, figsize=(14, 4 * NUM_CLASSES))\n",
        "\n",
        "for cls_idx in range(NUM_CLASSES):\n",
        "    img_batch = comparison_images[cls_idx]\n",
        "\n",
        "    # Original\n",
        "    img_display = unnormalize(img_batch.squeeze(0)).permute(1, 2, 0).numpy()\n",
        "    axes[cls_idx, 0].imshow(img_display)\n",
        "    axes[cls_idx, 0].set_title(f'{CLASS_NAMES[cls_idx]}', fontsize=10)\n",
        "    axes[cls_idx, 0].axis('off')\n",
        "\n",
        "    # Grad-CAM: feature extraction model\n",
        "    cam_fe, _ = grad_cam(fe_model, img_batch, target_class=cls_idx)\n",
        "    show_grad_cam_overlay(img_batch, cam_fe, 'Feature Extraction', ax=axes[cls_idx, 1])\n",
        "\n",
        "    # Grad-CAM: fine-tuning model\n",
        "    cam_ft, _ = grad_cam(ft_model, img_batch, target_class=cls_idx)\n",
        "    show_grad_cam_overlay(img_batch, cam_ft, 'Fine-Tuning', ax=axes[cls_idx, 2])\n",
        "\n",
        "fig.suptitle('Grad-CAM Comparison: Feature Extraction vs Fine-Tuning', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpret the Comparison\n",
        "\n",
        "Compare the Grad-CAM heatmaps for the two approaches:\n",
        "\n",
        "- **Did fine-tuning change what the model focuses on?** Often, fine-tuning produces tighter, more object-focused heatmaps because layer4 adapts to the specific task.\n",
        "- **Are there classes where one approach focuses on the flower and the other does not?** This is the clearest evidence of the value of fine-tuning (or the danger of it).\n",
        "- **Are there any classes where both approaches focus on something other than the flower?** That would suggest a dataset bias \u2014 something about the images that correlates with the label beyond the flower itself.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "You just completed the full practitioner workflow:\n",
        "\n",
        "| Step | What You Did |\n",
        "|------|--------------|\n",
        "| **1. Explore** | Understood the dataset: 8 flower species, ~50\u201380 images each |\n",
        "| **2. Feature Extraction** | Froze ResNet-18 backbone, trained classification head |\n",
        "| **3. Grad-CAM Validation** | Checked if the model focuses on flowers, not shortcuts |\n",
        "| **4. Fine-Tuning** | Unfroze layer4 with differential LR, compared to baseline |\n",
        "| **5. Comparison** | Accuracy table + Grad-CAM heatmaps side by side |\n",
        "\n",
        "The most important step was **not** the one that maximized accuracy. It was the Grad-CAM validation \u2014 the step where you checked whether the model learned the right features.\n",
        "\n",
        "**Correct prediction does not mean correct reasoning.** You now have the tools to check.\n",
        "\n",
        "---\n",
        "\n",
        "### Series 3 Complete\n",
        "\n",
        "You started Series 3 asking \"what is a convolution?\" You ended it by fine-tuning a pretrained CNN on a custom dataset and using Grad-CAM to verify the model's reasoning.\n",
        "\n",
        "The practical superpower you built: not just \"can I get high accuracy?\" but **\"can I understand what my model learned and trust its reasoning?\"**\n",
        "\n",
        "Next up: **Series 4 \u2014 LLMs and Transformers**. A different architecture, a different data modality, but the same practitioner mindset: understand the model, do not just use it."
      ]
    }
  ]
}
