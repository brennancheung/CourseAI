{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "## Module 3.2, Lesson 3 — Feature Extraction and Fine-Tuning a Pretrained ResNet\n",
    "\n",
    "In this notebook you will:\n",
    "1. Set up a **small dataset** (3 classes from CIFAR-10, limited to ~500 training images per class)\n",
    "2. Try **training from scratch** — see it overfit and fail\n",
    "3. Load a **pretrained ResNet-18** via `torchvision.models`\n",
    "4. Do **feature extraction** — freeze the backbone, replace the head, train\n",
    "5. Do **fine-tuning** — unfreeze the last stage with differential learning rates\n",
    "6. **Compare all three approaches** side by side\n",
    "\n",
    "The key insight: same architecture, same data — the difference is the starting point.\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:** ResNets and Skip Connections lesson, PyTorch training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a Small Dataset\n",
    "\n",
    "To make transfer learning's benefit obvious, we need a **small dataset** where training from scratch will overfit.\n",
    "\n",
    "We take CIFAR-10 but keep only 3 classes (cat, dog, horse) and limit to 500 training images per class. This simulates a realistic scenario: you have a niche classification task and limited data.\n",
    "\n",
    "**Important:** We resize images to 224x224 (the size pretrained ResNet expects) and normalize using ImageNet statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes to keep: cat (3), dog (5), horse (7)\n",
    "SELECTED_CLASSES = [3, 5, 7]\n",
    "CLASS_NAMES = ['cat', 'dog', 'horse']\n",
    "NUM_CLASSES = len(SELECTED_CLASSES)\n",
    "SAMPLES_PER_CLASS = 500  # small dataset!\n",
    "\n",
    "# ImageNet normalization (required for pretrained models)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "You already know `Compose`, `ToTensor`, and `Normalize` from the Datasets and DataLoaders lesson. For small datasets, we add random augmentations to prevent overfitting:\n",
    "\n",
    "- **RandomResizedCrop(224)** — random crop and resize, adds position/scale variation\n",
    "- **RandomHorizontalFlip()** — mirrors the image, effectively doubling the dataset\n",
    "- **ColorJitter** — varies brightness and contrast, making the model robust to lighting\n",
    "\n",
    "Validation transforms are deterministic (always the same crop, no random flips)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transforms: augmentation + ImageNet normalization\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Validation transforms: deterministic (no random augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_small_dataset(dataset, selected_classes, samples_per_class):\n",
    "    \"\"\"Filter a dataset to keep only selected classes with limited samples.\n",
    "    \n",
    "    Returns indices for the subset and a mapping from original labels to new labels (0, 1, 2, ...).\n",
    "    \"\"\"\n",
    "    # Build label mapping: original_class -> new_class (0, 1, 2, ...)\n",
    "    label_map = {orig: new for new, orig in enumerate(selected_classes)}\n",
    "    \n",
    "    # Collect indices for each selected class\n",
    "    class_indices = {c: [] for c in selected_classes}\n",
    "    targets = np.array(dataset.targets)\n",
    "    \n",
    "    for cls in selected_classes:\n",
    "        all_indices = np.where(targets == cls)[0]\n",
    "        # Take at most samples_per_class\n",
    "        chosen = all_indices[:samples_per_class]\n",
    "        class_indices[cls] = chosen.tolist()\n",
    "    \n",
    "    # Flatten all indices\n",
    "    all_chosen = []\n",
    "    for cls in selected_classes:\n",
    "        all_chosen.extend(class_indices[cls])\n",
    "    \n",
    "    return all_chosen, label_map\n",
    "\n",
    "\n",
    "class RemappedSubset(torch.utils.data.Dataset):\n",
    "    \"\"\"A subset that remaps labels to 0, 1, 2, ...\"\"\"\n",
    "    def __init__(self, dataset, indices, label_map):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.label_map = label_map\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[self.indices[idx]]\n",
    "        return img, self.label_map[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CIFAR-10\n",
    "full_train = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=train_transform\n",
    ")\n",
    "full_test = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=val_transform\n",
    ")\n",
    "\n",
    "# Create small training set (500 per class = 1500 total)\n",
    "train_indices, label_map = create_small_dataset(full_train, SELECTED_CLASSES, SAMPLES_PER_CLASS)\n",
    "train_dataset = RemappedSubset(full_train, train_indices, label_map)\n",
    "\n",
    "# Create test set (all available test images for selected classes)\n",
    "test_indices, _ = create_small_dataset(full_test, SELECTED_CLASSES, samples_per_class=10000)\n",
    "test_dataset = RemappedSubset(full_test, test_indices, label_map)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')\n",
    "print(f'Classes: {CLASS_NAMES}')\n",
    "print(f'Label mapping: {label_map}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Some Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataset, n=8):\n",
    "    \"\"\"Display a grid of images from the dataset.\"\"\"\n",
    "    fig, axes = plt.subplots(1, n, figsize=(2 * n, 2.5))\n",
    "    for i in range(n):\n",
    "        img, label = dataset[i]\n",
    "        # Undo normalization for display\n",
    "        img = img.clone()\n",
    "        for c in range(3):\n",
    "            img[c] = img[c] * IMAGENET_STD[c] + IMAGENET_MEAN[c]\n",
    "        img = img.clamp(0, 1)\n",
    "        axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
    "        axes[i].set_title(CLASS_NAMES[label], fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_images(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Loop (Provided)\n",
    "\n",
    "This training function works for all three approaches. It tracks training loss, training accuracy, and test accuracy per epoch.\n",
    "\n",
    "**Note on `nn.CrossEntropyLoss`:** This combines log-softmax and negative log-likelihood into one operation. It takes raw logits (no softmax needed) and is the standard loss for multi-class classification. You have used `nn.MSELoss` before — `CrossEntropyLoss` is the classification equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, optimizer, epochs=15, label='Model'):\n",
    "    \"\"\"Train a model and return training history.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        train_loss = running_loss / total\n",
    "        train_acc = 100.0 * correct / total\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        test_acc = 100.0 * correct / total\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        print(f'[{label}] Epoch {epoch+1:2d}/{epochs} | '\n",
    "              f'Train Loss: {train_loss:.4f} | '\n",
    "              f'Train Acc: {train_acc:.1f}% | '\n",
    "              f'Test Acc: {test_acc:.1f}%')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Approach 1: Training from Scratch (Baseline)\n",
    "\n",
    "First, train a ResNet-18 from random initialization on our small dataset. This is the baseline — we expect it to overfit badly because 1500 images are nowhere near enough to learn 11M parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-18 from scratch (random initialization)\n",
    "scratch_model = models.resnet18(weights=None)  # No pretrained weights\n",
    "scratch_model.fc = nn.Linear(512, NUM_CLASSES)  # Replace 1000-class head with 3-class head\n",
    "\n",
    "scratch_optimizer = optim.Adam(scratch_model.parameters(), lr=1e-3)\n",
    "\n",
    "print(f'Parameters: {sum(p.numel() for p in scratch_model.parameters()):,}')\n",
    "print(f'Training images: {len(train_dataset)}')\n",
    "print(f'Ratio: {sum(p.numel() for p in scratch_model.parameters()) / len(train_dataset):.0f} params per image')\n",
    "print(f'\\nThis is a recipe for overfitting!\\n')\n",
    "\n",
    "scratch_history = train_model(\n",
    "    scratch_model, train_loader, test_loader,\n",
    "    optimizer=scratch_optimizer, epochs=15, label='Scratch'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. YOUR TURN: Approach 2 — Feature Extraction\n",
    "\n",
    "Now use a **pretrained** ResNet-18. The three steps from the lesson:\n",
    "\n",
    "1. **Load pretrained:** `models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)`\n",
    "2. **Freeze all parameters:** `for param in model.parameters(): param.requires_grad = False`\n",
    "3. **Replace the head:** `model.fc = nn.Linear(512, num_classes)` — new layer is trainable by default\n",
    "\n",
    "Then create an optimizer that only updates `model.fc.parameters()`.\n",
    "\n",
    "Fill in the `TODO` sections below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Load a pretrained ResNet-18\n",
    "# Hint: models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "fe_model = ...  # YOUR CODE HERE\n",
    "\n",
    "# TODO 2: Freeze ALL parameters in the model\n",
    "# Hint: loop over model.parameters() and set requires_grad = False\n",
    "\n",
    "...  # YOUR CODE HERE\n",
    "\n",
    "# TODO 3: Replace the classification head\n",
    "# The original head is model.fc = Linear(512, 1000)\n",
    "# Replace it with Linear(512, NUM_CLASSES)\n",
    "# The new layer's parameters will have requires_grad=True by default\n",
    "\n",
    "...  # YOUR CODE HERE\n",
    "\n",
    "# Verify: count trainable vs frozen parameters\n",
    "trainable = sum(p.numel() for p in fe_model.parameters() if p.requires_grad)\n",
    "frozen = sum(p.numel() for p in fe_model.parameters() if not p.requires_grad)\n",
    "print(f'Trainable parameters: {trainable:,}')\n",
    "print(f'Frozen parameters: {frozen:,}')\n",
    "print(f'Total: {trainable + frozen:,}')\n",
    "print(f'\\nOnly {trainable / (trainable + frozen) * 100:.2f}% of parameters are trainable!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4: Create an optimizer that only updates the head's parameters\n",
    "# Hint: optim.Adam(fe_model.fc.parameters(), lr=1e-3)\n",
    "# We only pass model.fc.parameters() because everything else is frozen\n",
    "\n",
    "fe_optimizer = ...  # YOUR CODE HERE\n",
    "\n",
    "# Train\n",
    "fe_history = train_model(\n",
    "    fe_model, train_loader, test_loader,\n",
    "    optimizer=fe_optimizer, epochs=15, label='Feature Extraction'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "Compare the feature extraction results to training from scratch:\n",
    "- **Training accuracy:** Feature extraction should have lower training accuracy (it cannot overfit as easily because only the tiny head is being trained)\n",
    "- **Test accuracy:** Feature extraction should have much HIGHER test accuracy — the pretrained features generalize\n",
    "- **Training speed:** Feature extraction is faster because only the head's gradients are computed\n",
    "\n",
    "This is the same architecture, the same data, the same training loop. The only difference is that the backbone started from ImageNet weights instead of random initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. YOUR TURN: Approach 3 — Fine-Tuning\n",
    "\n",
    "Feature extraction keeps the backbone completely frozen. **Fine-tuning** goes one step further: unfreeze some pretrained layers and train them with a lower learning rate.\n",
    "\n",
    "The key skill: **differential learning rates** via parameter groups.\n",
    "\n",
    "Steps:\n",
    "1. Load a fresh pretrained model (do not reuse the feature extraction model — its head is already trained)\n",
    "2. Freeze all parameters\n",
    "3. Replace the head\n",
    "4. Unfreeze `model.layer4` (the last residual stage)\n",
    "5. Create an optimizer with two parameter groups:\n",
    "   - `model.fc.parameters()` with lr=1e-3 (new head, learn fast)\n",
    "   - `model.layer4.parameters()` with lr=1e-5 (pretrained, learn slowly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5: Load a fresh pretrained ResNet-18\n",
    "\n",
    "ft_model = ...  # YOUR CODE HERE\n",
    "\n",
    "# TODO 6: Freeze all parameters\n",
    "\n",
    "...  # YOUR CODE HERE\n",
    "\n",
    "# TODO 7: Replace the head\n",
    "\n",
    "...  # YOUR CODE HERE\n",
    "\n",
    "# TODO 8: Unfreeze layer4 (the last residual stage)\n",
    "# Hint: for param in ft_model.layer4.parameters(): param.requires_grad = True\n",
    "\n",
    "...  # YOUR CODE HERE\n",
    "\n",
    "# Verify\n",
    "trainable = sum(p.numel() for p in ft_model.parameters() if p.requires_grad)\n",
    "frozen = sum(p.numel() for p in ft_model.parameters() if not p.requires_grad)\n",
    "print(f'Trainable parameters: {trainable:,} (head + layer4)')\n",
    "print(f'Frozen parameters: {frozen:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 9: Create optimizer with differential learning rates\n",
    "# Two parameter groups:\n",
    "#   1. model.fc.parameters() with lr=1e-3  (new head: learn fast)\n",
    "#   2. model.layer4.parameters() with lr=1e-5  (pretrained: learn slow)\n",
    "#\n",
    "# Hint:\n",
    "# optimizer = optim.Adam([\n",
    "#     {'params': ft_model.fc.parameters(), 'lr': 1e-3},\n",
    "#     {'params': ft_model.layer4.parameters(), 'lr': 1e-5},\n",
    "# ])\n",
    "\n",
    "ft_optimizer = ...  # YOUR CODE HERE\n",
    "\n",
    "# Train\n",
    "ft_history = train_model(\n",
    "    ft_model, train_loader, test_loader,\n",
    "    optimizer=ft_optimizer, epochs=15, label='Fine-Tuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare All Three Approaches\n",
    "\n",
    "Now let's see all three approaches side by side. The comparison should make it viscerally clear why transfer learning is the default approach for practical deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n",
    "epochs_range = range(1, 16)\n",
    "\n",
    "colors = {\n",
    "    'scratch': '#ef4444',      # red\n",
    "    'feature_ext': '#3b82f6',  # blue\n",
    "    'fine_tune': '#22c55e',    # green\n",
    "}\n",
    "\n",
    "# Training Loss\n",
    "axes[0].plot(epochs_range, scratch_history['train_loss'], 'o-', label='From Scratch', color=colors['scratch'], markersize=4)\n",
    "axes[0].plot(epochs_range, fe_history['train_loss'], 's-', label='Feature Extraction', color=colors['feature_ext'], markersize=4)\n",
    "axes[0].plot(epochs_range, ft_history['train_loss'], '^-', label='Fine-Tuning', color=colors['fine_tune'], markersize=4)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training Accuracy\n",
    "axes[1].plot(epochs_range, scratch_history['train_acc'], 'o-', label='From Scratch', color=colors['scratch'], markersize=4)\n",
    "axes[1].plot(epochs_range, fe_history['train_acc'], 's-', label='Feature Extraction', color=colors['feature_ext'], markersize=4)\n",
    "axes[1].plot(epochs_range, ft_history['train_acc'], '^-', label='Fine-Tuning', color=colors['fine_tune'], markersize=4)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training Accuracy')\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Test Accuracy (the one that matters!)\n",
    "axes[2].plot(epochs_range, scratch_history['test_acc'], 'o-', label='From Scratch', color=colors['scratch'], markersize=4)\n",
    "axes[2].plot(epochs_range, fe_history['test_acc'], 's-', label='Feature Extraction', color=colors['feature_ext'], markersize=4)\n",
    "axes[2].plot(epochs_range, ft_history['test_acc'], '^-', label='Fine-Tuning', color=colors['fine_tune'], markersize=4)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Accuracy (%)')\n",
    "axes[2].set_title('Test Accuracy (what matters!)')\n",
    "axes[2].legend(fontsize=8)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print('\\n' + '=' * 70)\n",
    "print('COMPARISON SUMMARY')\n",
    "print('=' * 70)\n",
    "print(f'{\"\":25s} {\"From Scratch\":>15s} {\"Feature Ext\":>15s} {\"Fine-Tune\":>15s}')\n",
    "print('-' * 70)\n",
    "print(f'{\"Final train acc\":25s} {scratch_history[\"train_acc\"][-1]:>14.1f}% {fe_history[\"train_acc\"][-1]:>14.1f}% {ft_history[\"train_acc\"][-1]:>14.1f}%')\n",
    "print(f'{\"Final test acc\":25s} {scratch_history[\"test_acc\"][-1]:>14.1f}% {fe_history[\"test_acc\"][-1]:>14.1f}% {ft_history[\"test_acc\"][-1]:>14.1f}%')\n",
    "print(f'{\"Best test acc\":25s} {max(scratch_history[\"test_acc\"]):>14.1f}% {max(fe_history[\"test_acc\"]):>14.1f}% {max(ft_history[\"test_acc\"]):>14.1f}%')\n",
    "\n",
    "gap = scratch_history['train_acc'][-1] - scratch_history['test_acc'][-1]\n",
    "print(f'{\"Train-test gap (scratch)\":25s} {gap:>14.1f}%')\n",
    "print('-' * 70)\n",
    "print()\n",
    "print('Key observations:')\n",
    "print('  1. From scratch: high train acc, low test acc = overfitting')\n",
    "print('  2. Feature extraction: strong test acc with minimal training')\n",
    "print('  3. Fine-tuning: may improve slightly over feature extraction')\n",
    "print()\n",
    "print('Same architecture. Same data. The difference is the STARTING POINT.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inspect What Changed\n",
    "\n",
    "Let's verify our understanding by checking which parameters were updated in each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model structure to see what we worked with\n",
    "pretrained = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "print('ResNet-18 top-level modules:')\n",
    "print('-' * 40)\n",
    "for name, module in pretrained.named_children():\n",
    "    num_params = sum(p.numel() for p in module.parameters())\n",
    "    print(f'  {name:12s} — {num_params:>10,} params')\n",
    "\n",
    "total = sum(p.numel() for p in pretrained.parameters())\n",
    "fc_params = sum(p.numel() for p in pretrained.fc.parameters())\n",
    "layer4_params = sum(p.numel() for p in pretrained.layer4.parameters())\n",
    "\n",
    "print(f'\\nTotal: {total:,}')\n",
    "print(f'\\nFeature extraction trained: fc = {fc_params:,} params ({fc_params/total*100:.2f}%)')\n",
    "print(f'Fine-tuning trained: fc + layer4 = {fc_params + layer4_params:,} params ({(fc_params + layer4_params)/total*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reflection\n",
    "\n",
    "Before moving on, consider:\n",
    "\n",
    "1. **The overfitting gap:** Training from scratch shows a huge gap between training accuracy (~99%) and test accuracy (~40-60%). This is exactly what you predicted from the Overfitting and Regularization lesson. 11M parameters + 1500 images = memorization, not learning.\n",
    "\n",
    "2. **Feature extraction is surprisingly effective:** With only ~1,500 trainable parameters (the fc layer), feature extraction achieves strong test accuracy. The pretrained backbone converts raw pixels into meaningful 512-dimensional feature vectors, and a simple linear classifier on top is enough.\n",
    "\n",
    "3. **Fine-tuning is a refinement, not a revolution:** Fine-tuning may improve over feature extraction by a few percentage points. The real value is when your domain is significantly different from ImageNet, and the later layers need adaptation.\n",
    "\n",
    "4. **The practical takeaway:** Always start with feature extraction. It is fast, hard to mess up, and gives a strong baseline. Only add fine-tuning if you need more accuracy and have enough data.\n",
    "\n",
    "5. **Transfer learning changes the economics of deep learning:** You no longer need millions of images and weeks of GPU time. A pretrained model + a few hundred labeled images + an afternoon = a working classifier.\n",
    "\n",
    "---\n",
    "\n",
    "**Module 3.2 complete!** You now understand CNN architectures from LeNet through ResNet and how to put them to practical use with transfer learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
