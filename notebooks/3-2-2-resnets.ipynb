{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNets and Skip Connections\n",
    "## Module 3.2, Lesson 2 — Implement a ResNet Block and Train on CIFAR-10\n",
    "\n",
    "In this notebook you will:\n",
    "1. Load CIFAR-10 data\n",
    "2. See a **plain deep CNN** baseline (no skip connections) — provided\n",
    "3. **Implement a ResidualBlock** class (your code)\n",
    "4. **Assemble a small ResNet** from your blocks (your code)\n",
    "5. Train both networks and compare\n",
    "\n",
    "The key observation: the ResNet trains successfully at depths where the plain network degrades.\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:** Architecture Evolution lesson, nn.Module lesson, MNIST CNN Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load CIFAR-10\n",
    "\n",
    "CIFAR-10 has 60,000 32x32 RGB images in 10 classes (airplane, car, bird, cat, deer, dog, frog, horse, ship, truck).\n",
    "\n",
    "We apply basic normalization — no data augmentation (keeping it simple for comparison)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 channel-wise mean and std (standard values)\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')\n",
    "print(f'Input shape: {train_dataset[0][0].shape}')  # [3, 32, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plain Deep CNN (Baseline — No Skip Connections)\n",
    "\n",
    "This is a deep CNN that stacks many conv layers *without* skip connections. It follows the VGG philosophy: repeated Conv-BN-ReLU blocks. We use 20 conv layers total — deep enough to see degradation effects.\n",
    "\n",
    "**This code is provided.** Read it to understand the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainBlock(nn.Module):\n",
    "    \"\"\"A plain block: two 3x3 convs with BN and ReLU. No skip connection.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class PlainDeepCNN(nn.Module):\n",
    "    \"\"\"A plain deep CNN with no skip connections.\n",
    "    \n",
    "    Architecture:\n",
    "    - Initial conv: 3 -> 16 channels\n",
    "    - Stage 1: n blocks at 16 channels (32x32 spatial)\n",
    "    - Stage 2: n blocks at 32 channels (16x16 spatial) \n",
    "    - Stage 3: n blocks at 64 channels (8x8 spatial)\n",
    "    - Global average pooling -> FC(10)\n",
    "    \n",
    "    Total conv layers = 1 + 2*n*3 = 1 + 6n\n",
    "    With n=3: 19 conv layers. With n=5: 31 conv layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_blocks=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        # Build stages\n",
    "        self.stage1 = self._make_stage(16, 16, n_blocks, stride=1)\n",
    "        self.stage2 = self._make_stage(16, 32, n_blocks, stride=2)\n",
    "        self.stage3 = self._make_stage(32, 64, n_blocks, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "\n",
    "    def _make_stage(self, in_channels, out_channels, n_blocks, stride):\n",
    "        blocks = [PlainBlock(in_channels, out_channels, stride=stride)]\n",
    "        for _ in range(1, n_blocks):\n",
    "            blocks.append(PlainBlock(out_channels, out_channels, stride=1))\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.stage1(out)\n",
    "        out = self.stage2(out)\n",
    "        out = self.stage3(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Test it\n",
    "plain_net = PlainDeepCNN(n_blocks=3)\n",
    "test_input = torch.randn(2, 3, 32, 32)\n",
    "test_output = plain_net(test_input)\n",
    "print(f'PlainDeepCNN output shape: {test_output.shape}')  # [2, 10]\n",
    "print(f'Total parameters: {sum(p.numel() for p in plain_net.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. YOUR TURN: Implement a ResidualBlock\n",
    "\n",
    "Now implement the residual version. The structure is almost identical to `PlainBlock`, but you add a **skip connection** that adds the input to the output.\n",
    "\n",
    "Remember the pattern from the lesson:\n",
    "1. Conv(3x3) → BN → ReLU\n",
    "2. Conv(3x3) → BN\n",
    "3. **Add the shortcut (input x)**\n",
    "4. ReLU\n",
    "\n",
    "**Important:** When `stride > 1` or `in_channels != out_channels`, you need a **projection shortcut** (1x1 conv) to match dimensions. Otherwise, use the identity shortcut (just add x directly).\n",
    "\n",
    "Fill in the `TODO` sections below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"A residual block: two 3x3 convs with BN, ReLU, and a skip connection.\n",
    "    \n",
    "    When stride > 1 or in_channels != out_channels, uses a projection shortcut\n",
    "    (1x1 conv + BN) to match dimensions.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO 1: Define the two conv layers with batch norm\n",
    "        # conv1: 3x3 conv from in_channels to out_channels with the given stride\n",
    "        # bn1: BatchNorm2d for out_channels\n",
    "        # conv2: 3x3 conv from out_channels to out_channels with stride=1\n",
    "        # bn2: BatchNorm2d for out_channels\n",
    "        # Hint: use bias=False when using batch norm (BN has its own bias)\n",
    "        # Hint: use padding=1 for 3x3 convs to preserve spatial size\n",
    "        \n",
    "        self.conv1 = ...  # YOUR CODE HERE\n",
    "        self.bn1 = ...    # YOUR CODE HERE\n",
    "        self.conv2 = ...  # YOUR CODE HERE\n",
    "        self.bn2 = ...    # YOUR CODE HERE\n",
    "        \n",
    "        # TODO 2: Define the shortcut connection\n",
    "        # If in_channels == out_channels AND stride == 1: identity (no-op)\n",
    "        # Otherwise: 1x1 conv + BN to match dimensions\n",
    "        # Hint: nn.Sequential() creates an empty no-op module\n",
    "        # Hint: nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "        \n",
    "        self.shortcut = ...  # YOUR CODE HERE\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO 3: Implement the forward pass\n",
    "        # 1. Save the identity (input) for the shortcut\n",
    "        # 2. Pass through conv1 -> bn1 -> relu\n",
    "        # 3. Pass through conv2 -> bn2 (NO relu yet!)\n",
    "        # 4. Add the shortcut to the output\n",
    "        # 5. Apply relu AFTER the addition\n",
    "        \n",
    "        ...  # YOUR CODE HERE\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your ResidualBlock\n",
    "\n",
    "Run these tests to verify your implementation. All three should print the expected output shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Identity shortcut (same channels, stride=1)\n",
    "block1 = ResidualBlock(16, 16, stride=1)\n",
    "x1 = torch.randn(2, 16, 32, 32)\n",
    "out1 = block1(x1)\n",
    "print(f'Identity shortcut: {x1.shape} -> {out1.shape}')  # Expected: [2, 16, 32, 32]\n",
    "assert out1.shape == (2, 16, 32, 32), f'Expected (2, 16, 32, 32), got {out1.shape}'\n",
    "\n",
    "# Test 2: Projection shortcut (different channels)\n",
    "block2 = ResidualBlock(16, 32, stride=1)\n",
    "x2 = torch.randn(2, 16, 32, 32)\n",
    "out2 = block2(x2)\n",
    "print(f'Channel change: {x2.shape} -> {out2.shape}')  # Expected: [2, 32, 32, 32]\n",
    "assert out2.shape == (2, 32, 32, 32), f'Expected (2, 32, 32, 32), got {out2.shape}'\n",
    "\n",
    "# Test 3: Projection shortcut (different channels + stride)\n",
    "block3 = ResidualBlock(16, 32, stride=2)\n",
    "x3 = torch.randn(2, 16, 32, 32)\n",
    "out3 = block3(x3)\n",
    "print(f'Stride + channels: {x3.shape} -> {out3.shape}')  # Expected: [2, 32, 16, 16]\n",
    "assert out3.shape == (2, 32, 16, 16), f'Expected (2, 32, 16, 16), got {out3.shape}'\n",
    "\n",
    "print('\\nAll tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. YOUR TURN: Assemble a Small ResNet\n",
    "\n",
    "Now build a full ResNet using your `ResidualBlock`. The architecture mirrors `PlainDeepCNN` but uses residual blocks instead of plain blocks:\n",
    "\n",
    "- Initial conv: 3 → 16 channels\n",
    "- Stage 1: `n_blocks` residual blocks at 16 channels (32x32)\n",
    "- Stage 2: `n_blocks` residual blocks at 32 channels (16x16)\n",
    "- Stage 3: `n_blocks` residual blocks at 64 channels (8x8)\n",
    "- Global average pooling → FC(10)\n",
    "\n",
    "Fill in the `TODO` sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResNet(nn.Module):\n",
    "    \"\"\"A small ResNet for CIFAR-10.\n",
    "    \n",
    "    Same overall structure as PlainDeepCNN, but uses ResidualBlock instead of PlainBlock.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_blocks=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO 4: Define the initial conv layer and batch norm\n",
    "        # conv1: 3x3 conv from 3 channels (RGB) to 16 channels\n",
    "        # bn1: BatchNorm2d for 16 channels\n",
    "        \n",
    "        self.conv1 = ...  # YOUR CODE HERE\n",
    "        self.bn1 = ...    # YOUR CODE HERE\n",
    "        \n",
    "        # TODO 5: Build the three stages using _make_stage\n",
    "        # Stage 1: 16 -> 16 channels, stride=1 (spatial stays 32x32)\n",
    "        # Stage 2: 16 -> 32 channels, stride=2 (spatial becomes 16x16)\n",
    "        # Stage 3: 32 -> 64 channels, stride=2 (spatial becomes 8x8)\n",
    "        \n",
    "        self.stage1 = ...  # YOUR CODE HERE\n",
    "        self.stage2 = ...  # YOUR CODE HERE\n",
    "        self.stage3 = ...  # YOUR CODE HERE\n",
    "        \n",
    "        # Global average pooling and classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "    \n",
    "    def _make_stage(self, in_channels, out_channels, n_blocks, stride):\n",
    "        \"\"\"Create a stage of residual blocks.\n",
    "        \n",
    "        The first block uses the given stride (may downsample).\n",
    "        Remaining blocks use stride=1.\n",
    "        \"\"\"\n",
    "        # TODO 6: Build a list of ResidualBlock instances\n",
    "        # First block: ResidualBlock(in_channels, out_channels, stride=stride)\n",
    "        # Remaining blocks: ResidualBlock(out_channels, out_channels, stride=1)\n",
    "        # Return as nn.Sequential\n",
    "        \n",
    "        ...  # YOUR CODE HERE\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO 7: Implement forward pass\n",
    "        # 1. Initial conv -> bn -> relu\n",
    "        # 2. Stage 1 -> Stage 2 -> Stage 3\n",
    "        # 3. Global average pooling\n",
    "        # 4. Flatten and FC\n",
    "        \n",
    "        ...  # YOUR CODE HERE\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = SimpleResNet(n_blocks=3)\n",
    "test_input = torch.randn(2, 3, 32, 32)\n",
    "test_output = resnet(test_input)\n",
    "print(f'SimpleResNet output shape: {test_output.shape}')  # Expected: [2, 10]\n",
    "assert test_output.shape == (2, 10), f'Expected (2, 10), got {test_output.shape}'\n",
    "print(f'Total parameters: {sum(p.numel() for p in resnet.parameters()):,}')\n",
    "print('\\nResNet test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop (Provided)\n",
    "\n",
    "This training function works for both the plain network and the ResNet. It tracks training loss, training accuracy, and test accuracy per epoch.\n",
    "\n",
    "**Note:** We use `nn.CrossEntropyLoss`, which takes raw logits (no softmax needed) and computes the cross-entropy loss for multi-class classification. It is the standard loss for classification tasks in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, epochs=15, lr=0.1):\n",
    "    \"\"\"Train a model and return training history.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    \n",
    "    # Learning rate schedule: reduce by 10x at epoch 8 and 12\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8, 12], gamma=0.1)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': [],\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss = running_loss / total\n",
    "        train_acc = 100.0 * correct / total\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        test_acc = 100.0 * correct / total\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1:2d}/{epochs} | '\n",
    "              f'Train Loss: {train_loss:.4f} | '\n",
    "              f'Train Acc: {train_acc:.1f}% | '\n",
    "              f'Test Acc: {test_acc:.1f}%')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Both Models\n",
    "\n",
    "We train the plain network and the ResNet with the same hyperparameters (learning rate, optimizer, epochs) so the comparison is fair.\n",
    "\n",
    "**Expected runtime:** ~2–3 minutes per model on a Colab GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use n_blocks=5 for a deeper comparison (31 conv layers)\n",
    "N_BLOCKS = 5\n",
    "EPOCHS = 15\n",
    "\n",
    "print('=' * 60)\n",
    "print(f'Training Plain Deep CNN ({1 + 6*N_BLOCKS} conv layers, no skip connections)')\n",
    "print('=' * 60)\n",
    "plain_model = PlainDeepCNN(n_blocks=N_BLOCKS)\n",
    "plain_params = sum(p.numel() for p in plain_model.parameters())\n",
    "print(f'Parameters: {plain_params:,}\\n')\n",
    "\n",
    "start = time.time()\n",
    "plain_history = train_model(plain_model, train_loader, test_loader, epochs=EPOCHS)\n",
    "plain_time = time.time() - start\n",
    "print(f'\\nTraining time: {plain_time:.1f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 60)\n",
    "print(f'Training ResNet ({1 + 6*N_BLOCKS} conv layers, WITH skip connections)')\n",
    "print('=' * 60)\n",
    "resnet_model = SimpleResNet(n_blocks=N_BLOCKS)\n",
    "resnet_params = sum(p.numel() for p in resnet_model.parameters())\n",
    "print(f'Parameters: {resnet_params:,}\\n')\n",
    "\n",
    "start = time.time()\n",
    "resnet_history = train_model(resnet_model, train_loader, test_loader, epochs=EPOCHS)\n",
    "resnet_time = time.time() - start\n",
    "print(f'\\nTraining time: {resnet_time:.1f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Results\n",
    "\n",
    "Plot training loss and accuracy curves side by side. The key observation is in the **training accuracy** — if the plain network degrades, its training accuracy will plateau lower than the ResNet's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "# Training loss\n",
    "axes[0].plot(epochs_range, plain_history['train_loss'], 'o-', label='Plain CNN', color='#f59e0b')\n",
    "axes[0].plot(epochs_range, resnet_history['train_loss'], 's-', label='ResNet', color='#22c55e')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training accuracy\n",
    "axes[1].plot(epochs_range, plain_history['train_acc'], 'o-', label='Plain CNN', color='#f59e0b')\n",
    "axes[1].plot(epochs_range, resnet_history['train_acc'], 's-', label='ResNet', color='#22c55e')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Test accuracy\n",
    "axes[2].plot(epochs_range, plain_history['test_acc'], 'o-', label='Plain CNN', color='#f59e0b')\n",
    "axes[2].plot(epochs_range, resnet_history['test_acc'], 's-', label='ResNet', color='#22c55e')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Accuracy (%)')\n",
    "axes[2].set_title('Test Accuracy')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print('\\n' + '=' * 60)\n",
    "print('COMPARISON SUMMARY')\n",
    "print('=' * 60)\n",
    "print(f'{\"\":20s} {\"Plain CNN\":>15s} {\"ResNet\":>15s}')\n",
    "print('-' * 60)\n",
    "print(f'{\"Conv layers\":20s} {1 + 6*N_BLOCKS:>15d} {1 + 6*N_BLOCKS:>15d}')\n",
    "print(f'{\"Parameters\":20s} {plain_params:>15,} {resnet_params:>15,}')\n",
    "print(f'{\"Final train acc\":20s} {plain_history[\"train_acc\"][-1]:>14.1f}% {resnet_history[\"train_acc\"][-1]:>14.1f}%')\n",
    "print(f'{\"Final test acc\":20s} {plain_history[\"test_acc\"][-1]:>14.1f}% {resnet_history[\"test_acc\"][-1]:>14.1f}%')\n",
    "print(f'{\"Training time\":20s} {plain_time:>14.1f}s {resnet_time:>14.1f}s')\n",
    "print('-' * 60)\n",
    "\n",
    "acc_diff = resnet_history['test_acc'][-1] - plain_history['test_acc'][-1]\n",
    "print(f'\\nResNet advantage: {acc_diff:+.1f}% test accuracy')\n",
    "if acc_diff > 0:\n",
    "    print('The skip connections helped! The ResNet trained better at this depth.')\n",
    "print()\n",
    "print('Look at the training accuracy curves above.')\n",
    "print('If the plain network plateaus lower, that is the degradation problem in action.')\n",
    "print('The ResNet avoids degradation because each block defaults to identity.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reflection\n",
    "\n",
    "Before moving on, consider:\n",
    "\n",
    "1. **Did the ResNet achieve higher training accuracy than the plain network?** If so, that is the degradation problem — the plain network could not even fit the training data as well, despite having the same number of parameters.\n",
    "\n",
    "2. **The only difference between PlainBlock and ResidualBlock is one line:** `out = out + identity`. One line of code, and the network can train at much greater depth.\n",
    "\n",
    "3. **Think about the mental model:** A residual block starts from identity and learns to deviate. Making \"do nothing\" the easiest path, not the hardest. This is why every major architecture after 2015 uses skip connections.\n",
    "\n",
    "---\n",
    "\n",
    "**Next lesson:** Transfer Learning — use a pretrained ResNet and adapt it to new tasks in minutes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
