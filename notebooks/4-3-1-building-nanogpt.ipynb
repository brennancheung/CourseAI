{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building nanoGPT\n",
    "\n",
    "In this notebook, you'll build a complete GPT language model from scratch in PyTorch.\n",
    "\n",
    "**What you'll do:**\n",
    "- Build token embeddings + positional encoding and verify output shapes\n",
    "- Implement a single self-attention head with causal masking\n",
    "- Assemble multi-head attention using the batched reshape trick\n",
    "- Wire together the full transformer block (attention + FFN + layer norms + residuals)\n",
    "- Compose everything into a complete GPT model class, verify the parameter count matches GPT-2 (~124M), and generate text\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones \u2014 they reveal gaps in your mental model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tiktoken\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configuration: all hyperparameters in one place\n",
    "# ============================================================\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 50257     # GPT-2 BPE vocabulary\n",
    "    block_size: int = 1024      # Maximum context length\n",
    "    n_layer: int = 12           # Number of transformer blocks\n",
    "    n_head: int = 12            # Number of attention heads\n",
    "    n_embd: int = 768           # Embedding dimension (d_model)\n",
    "    dropout: float = 0.0        # Dropout rate (0 for exercises \u2014 deterministic output)\n",
    "    bias: bool = False          # Use bias in Linear layers?\n",
    "\n",
    "# Tiny debug config \u2014 fast iteration, same shapes\n",
    "debug_config = GPTConfig(\n",
    "    vocab_size=256,\n",
    "    block_size=64,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    n_embd=128,\n",
    ")\n",
    "\n",
    "print(f\"Debug config: {debug_config}\")\n",
    "print(f\"Head size: {debug_config.n_embd // debug_config.n_head}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Token Embedding + Positional Encoding (Guided)\n",
    "\n",
    "The bottom of the GPT architecture: turn token IDs into vectors, then add positional information. Two `nn.Embedding` layers, one addition.\n",
    "\n",
    "The token embedding maps each token ID to a learned vector of size `n_embd`. The position embedding maps each position (0, 1, 2, ...) to a learned vector of the same size. Adding them gives a representation that encodes both *what* the token is and *where* it sits.\n",
    "\n",
    "**Before running, predict:**\n",
    "- If input `idx` has shape `(B=2, T=16)`, what shape will `tok_emb` have?\n",
    "- What shape will `pos_emb` have? (Hint: positions are shared across all batches)\n",
    "- What shape will `x` (the sum) have? How does broadcasting work here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token embedding + positional encoding\n",
    "wte = nn.Embedding(debug_config.vocab_size, debug_config.n_embd)  # token embedding\n",
    "wpe = nn.Embedding(debug_config.block_size, debug_config.n_embd)  # position embedding\n",
    "\n",
    "# Simulate a batch of token IDs\n",
    "B, T = 2, 16\n",
    "idx = torch.randint(0, debug_config.vocab_size, (B, T))  # (B, T)\n",
    "\n",
    "# Token embeddings\n",
    "tok_emb = wte(idx)                          # (B, T) -> (B, T, n_embd)\n",
    "\n",
    "# Position embeddings \u2014 same positions for every batch element\n",
    "pos = torch.arange(0, T)                    # (T,)\n",
    "pos_emb = wpe(pos)                          # (T,) -> (T, n_embd)\n",
    "\n",
    "# Add them \u2014 broadcasting adds (T, n_embd) to each batch element\n",
    "x = tok_emb + pos_emb                       # (B, T, n_embd)\n",
    "\n",
    "print(f\"Input idx shape:       {idx.shape}\")        # (2, 16)\n",
    "print(f\"Token embed shape:     {tok_emb.shape}\")    # (2, 16, 128)\n",
    "print(f\"Position embed shape:  {pos_emb.shape}\")    # (16, 128)\n",
    "print(f\"Combined x shape:      {x.shape}\")          # (2, 16, 128)\n",
    "print()\n",
    "print(f\"Token embed params:    {wte.weight.shape[0]} tokens x {wte.weight.shape[1]} dims = {wte.weight.numel():,}\")\n",
    "print(f\"Position embed params: {wpe.weight.shape[0]} positions x {wpe.weight.shape[1]} dims = {wpe.weight.numel():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:**\n",
    "- `wte(idx)` looks up a learned vector for each token ID: shape goes from `(B, T)` to `(B, T, n_embd)`\n",
    "- `wpe(pos)` looks up a learned vector for each position 0..T-1: shape is `(T, n_embd)`\n",
    "- Addition broadcasts `(T, n_embd)` across the batch dimension, producing `(B, T, n_embd)`\n",
    "- Every token now carries both its *identity* (from `wte`) and its *position* (from `wpe`)\n",
    "\n",
    "This is the entry point of the model. Everything downstream operates on these `(B, T, n_embd)` vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Single Self-Attention Head (Guided)\n",
    "\n",
    "The core operation: compute attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V, with a causal mask that prevents attending to future positions.\n",
    "\n",
    "You traced this formula by hand in Module 4.2. Now see it in code. Every line maps to a step you already know:\n",
    "- Three linear projections create Q, K, V (the three \"lenses\" on the input)\n",
    "- Scaled dot-product gives attention scores\n",
    "- The causal mask sets future positions to -inf\n",
    "- Softmax converts scores to weights\n",
    "- Weighted sum of V produces the output\n",
    "\n",
    "**Before running, predict:**\n",
    "- If input is `(B=2, T=16, n_embd=128)` and `head_size=32`, what shape are Q, K, V?\n",
    "- What shape are the attention scores (`q @ k.transpose(-2, -1)`)?\n",
    "- What does the causal mask look like for T=4? (Draw the 4x4 grid: which entries are 0 vs 1?)\n",
    "- What is the final output shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"Single head of self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, config, head_size):\n",
    "        super().__init__()\n",
    "        # Q, K, V projections \u2014 three \"lenses\" on the same input\n",
    "        self.query = nn.Linear(config.n_embd, head_size, bias=config.bias)\n",
    "        self.key   = nn.Linear(config.n_embd, head_size, bias=config.bias)\n",
    "        self.value = nn.Linear(config.n_embd, head_size, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # Causal mask \u2014 registered as a buffer (not a parameter)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape                       # (batch, seq_len, n_embd)\n",
    "\n",
    "        q = self.query(x)                       # (B, T, head_size)\n",
    "        k = self.key(x)                         # (B, T, head_size)\n",
    "        v = self.value(x)                       # (B, T, head_size)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scale = k.shape[-1] ** -0.5\n",
    "        scores = q @ k.transpose(-2, -1) * scale   # (B, T, T)\n",
    "\n",
    "        # Causal mask: set future positions to -inf\n",
    "        scores = scores.masked_fill(\n",
    "            self.mask[:T, :T] == 0, float('-inf')\n",
    "        )                                        # (B, T, T)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)     # (B, T, T)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        out = weights @ v                       # (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ---- Verify ----\n",
    "head = Head(debug_config, head_size=32)\n",
    "x = torch.randn(2, 16, 128)  # (B=2, T=16, n_embd=128)\n",
    "out = head(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")     # (2, 16, 128)\n",
    "print(f\"Output shape: {out.shape}\")   # (2, 16, 32)\n",
    "assert out.shape == (2, 16, 32), \"Head output shape wrong!\"\n",
    "print(\"\\nShape check passed!\")\n",
    "\n",
    "# Inspect the causal mask for a tiny sequence\n",
    "print(f\"\\nCausal mask (first 4x4):\")\n",
    "print(head.mask[:4, :4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:**\n",
    "- Three `nn.Linear` projections created Q, K, V, each with shape `(B, T, head_size)`\n",
    "- `q @ k.transpose(-2, -1)` computed raw attention scores: `(B, T, head_size) @ (B, head_size, T) = (B, T, T)`\n",
    "- The `1/sqrt(d_k)` scale prevents scores from growing large, which would push softmax into saturation\n",
    "- `masked_fill` set above-diagonal entries to `-inf`, which softmax converts to 0 \u2014 this is the causal mask\n",
    "- `weights @ v` produced the weighted sum: `(B, T, T) @ (B, T, head_size) = (B, T, head_size)`\n",
    "- `register_buffer` stores the mask as a non-trainable tensor that moves to GPU with the model\n",
    "\n",
    "The input was `(B, T, n_embd)` and the output is `(B, T, head_size)`. The dimension shrank from 128 to 32 \u2014 this is one head's \"slice\" of the full representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Multi-Head Attention (Supported)\n",
    "\n",
    "Multiple heads run in parallel with dimension splitting: `d_k = d_model / h`. The lesson showed two approaches:\n",
    "1. **Explicit loop**: run `h` independent `Head` modules, concatenate\n",
    "2. **Batched reshape**: one projection produces all heads, reshape splits them\n",
    "\n",
    "The batched approach is what production implementations use. A single `nn.Linear` produces Q, K, V for **all** heads at once. Then a reshape operation IS the dimension splitting.\n",
    "\n",
    "**Your task:** Fill in the TODOs to complete `CausalSelfAttention`. Each TODO is 1-3 lines. The shape comments tell you exactly what each operation should produce.\n",
    "\n",
    "<details>\n",
    "<summary>\ud83d\udca1 Solution</summary>\n",
    "\n",
    "The key insight is that the reshape from `(B, T, n_embd)` to `(B, n_head, T, head_size)` IS the dimension splitting. A single large projection creates all heads at once, and `.view().transpose()` separates them.\n",
    "\n",
    "```python\n",
    "# TODO 1: Split qkv into q, k, v \u2014 each gets n_embd dimensions\n",
    "q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "\n",
    "# TODO 2: Reshape into (B, n_head, T, head_size)\n",
    "q = q.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
    "k = k.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
    "v = v.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
    "\n",
    "# TODO 3: Compute attention scores and apply causal mask\n",
    "scale = head_size ** -0.5\n",
    "scores = q @ k.transpose(-2, -1) * scale\n",
    "scores = scores.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "weights = self.attn_dropout(weights)\n",
    "\n",
    "# TODO 4: Combine heads back\n",
    "out = weights @ v\n",
    "out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "```\n",
    "\n",
    "Common mistake: swapping `n_head` and `T` in the view/transpose. The result has the correct final shape but completely wrong values \u2014 shape correctness is not enough.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention with batched computation (no loop).\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # Q, K, V for ALL heads in one projection\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)  # W_O\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                 .view(1, 1, config.block_size, config.block_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape                                         # (B, T, n_embd)\n",
    "        head_size = C // self.n_head\n",
    "\n",
    "        # Single projection produces Q, K, V for all heads\n",
    "        qkv = self.c_attn(x)                                      # (B, T, 3*n_embd)\n",
    "\n",
    "        # TODO 1: Split qkv into q, k, v using .split()\n",
    "        # Each should have shape (B, T, n_embd)\n",
    "        # Hint: split along dim=2, each chunk has self.n_embd elements\n",
    "        q, k, v = None, None, None  # <-- REPLACE THIS LINE\n",
    "\n",
    "        # TODO 2: Reshape each into (B, n_head, T, head_size)\n",
    "        # Step 1: .view(B, T, self.n_head, head_size)\n",
    "        # Step 2: .transpose(1, 2) to move n_head before T\n",
    "        # Apply to q, k, and v\n",
    "        pass  # <-- REPLACE WITH THREE LINES (one per tensor)\n",
    "\n",
    "        # TODO 3: Compute scaled dot-product attention with causal mask\n",
    "        # - Compute scale factor (1/sqrt(head_size))\n",
    "        # - scores = q @ k.transpose(-2, -1) * scale       -> (B, nh, T, T)\n",
    "        # - Apply causal mask with masked_fill\n",
    "        # - Softmax over last dimension\n",
    "        # - Apply self.attn_dropout\n",
    "        weights = None  # <-- REPLACE THIS (multiple lines)\n",
    "\n",
    "        # TODO 4: Weighted sum of values and recombine heads\n",
    "        # - out = weights @ v                                -> (B, nh, T, hs)\n",
    "        # - Transpose back: .transpose(1, 2)                -> (B, T, nh, hs)\n",
    "        # - Reshape: .contiguous().view(B, T, C)            -> (B, T, n_embd)\n",
    "        out = None  # <-- REPLACE THIS\n",
    "\n",
    "        # Output projection (W_O)\n",
    "        out = self.c_proj(out)                                     # (B, T, n_embd)\n",
    "        out = self.resid_dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ---- Verify ----\n",
    "mha = CausalSelfAttention(debug_config)\n",
    "x = torch.randn(2, 16, 128)  # (B=2, T=16, n_embd=128)\n",
    "out = mha(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")     # (2, 16, 128)\n",
    "print(f\"Output shape: {out.shape}\")   # (2, 16, 128)\n",
    "assert out.shape == (2, 16, 128), \"MHA output shape wrong!\"\n",
    "print(\"\\nShape check passed! Multi-head attention preserves shape.\")\n",
    "print(f\"\\nParameters in c_attn: {mha.c_attn.weight.shape} = {mha.c_attn.weight.numel():,}\")\n",
    "print(f\"Parameters in c_proj: {mha.c_proj.weight.shape} = {mha.c_proj.weight.numel():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:**\n",
    "- A single `nn.Linear(n_embd, 3*n_embd)` produced Q, K, V for all heads at once\n",
    "- `.split()` divided the output into three equal chunks\n",
    "- `.view().transpose()` reshaped from `(B, T, n_embd)` to `(B, n_head, T, head_size)` \u2014 this IS the dimension splitting\n",
    "- Attention scores `(B, n_head, T, T)` were computed for all heads simultaneously \u2014 no Python loop\n",
    "- `.transpose().contiguous().view()` merged heads back to `(B, T, n_embd)`\n",
    "- The output projection `c_proj` (W_O) mixed information across heads\n",
    "\n",
    "Input shape = output shape = `(B, T, n_embd)`. Multi-head attention reads from the context and writes back to the same representation space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Transformer Block (Supported)\n",
    "\n",
    "The transformer block formula from The Transformer Block:\n",
    "- `x' = x + MHA(LN(x))`\n",
    "- `out = x' + FFN(LN(x'))`\n",
    "\n",
    "Pre-norm ordering (layer norm before the sub-layer), two residual connections. Attention reads from the context, FFN processes each token independently.\n",
    "\n",
    "**Your task:** Fill in the TODOs to complete both `FeedForward` and `Block`. The FeedForward uses the 4x expansion factor: `n_embd -> 4*n_embd -> n_embd` with GELU activation.\n",
    "\n",
    "<details>\n",
    "<summary>\ud83d\udca1 Solution</summary>\n",
    "\n",
    "The FeedForward network is two linear layers with GELU in between. The 4x expansion factor gives the network more capacity to transform each token's representation independently.\n",
    "\n",
    "```python\n",
    "# FeedForward.forward:\n",
    "x = self.c_fc(x)        # (B, T, 4*n_embd)\n",
    "x = self.gelu(x)        # (B, T, 4*n_embd)\n",
    "x = self.c_proj(x)      # (B, T, n_embd)\n",
    "x = self.dropout(x)\n",
    "return x\n",
    "\n",
    "# Block.forward:\n",
    "x = x + self.attn(self.ln_1(x))   # residual + MHA(LN(x))\n",
    "x = x + self.ffn(self.ln_2(x))    # residual + FFN(LN(x))\n",
    "return x\n",
    "```\n",
    "\n",
    "Key insight: the Block's forward method IS the formula. `x +` is the residual connection, `self.ln_1(x)` is the pre-norm, and `self.attn(...)` is the sub-layer. The block preserves shape because both residual additions stay in `(B, T, n_embd)`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu   = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):                        # (B, T, n_embd)\n",
    "        # TODO: Pass x through c_fc, gelu, c_proj, dropout\n",
    "        # Shape journey: (B,T,n_embd) -> (B,T,4*n_embd) -> (B,T,4*n_embd) -> (B,T,n_embd)\n",
    "        pass  # <-- REPLACE WITH 4 LINES\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: MHA + FFN with residual connections and layer norm.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.ffn  = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):                        # (B, T, n_embd)\n",
    "        # TODO: Implement the block formula\n",
    "        # x' = x + MHA(LN(x))\n",
    "        # out = x' + FFN(LN(x'))\n",
    "        # Hint: each line is a residual add: x = x + sublayer(layernorm(x))\n",
    "        pass  # <-- REPLACE WITH 2 LINES + return\n",
    "\n",
    "\n",
    "# ---- Verify ----\n",
    "block = Block(debug_config)\n",
    "x = torch.randn(2, 16, 128)  # (B, T, n_embd)\n",
    "out = block(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")     # (2, 16, 128)\n",
    "print(f\"Output shape: {out.shape}\")   # (2, 16, 128)\n",
    "assert out.shape == x.shape, \"Block must preserve shape!\"\n",
    "print(\"\\nShape check passed! Block preserves shape exactly.\")\n",
    "print(\"This is what makes stacking possible \u2014 12 identical blocks,\")\n",
    "print(\"each reading from and writing to the same residual stream.\")\n",
    "\n",
    "# Count parameters in one block\n",
    "block_params = sum(p.numel() for p in block.parameters())\n",
    "print(f\"\\nParameters in one block: {block_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:**\n",
    "- `FeedForward` expands from `n_embd` to `4*n_embd`, applies GELU, then projects back to `n_embd`. This is the \"writer\" \u2014 it processes each token independently after attention has gathered context.\n",
    "- `Block.forward` IS the formula: `x = x + sublayer(LN(x))` for both attention and FFN.\n",
    "- Pre-norm ordering means layer norm comes *before* each sub-layer, not after. This is the modern standard (GPT-2 and later).\n",
    "- The block preserves shape: `(B, T, n_embd)` in, `(B, T, n_embd)` out. This is what makes stacking 12 identical blocks possible.\n",
    "- The FFN holds about two-thirds of the block's parameters, despite being conceptually simpler than attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Complete GPT Model (Independent)\n",
    "\n",
    "Now wire everything together into a complete GPT model class. The GPT class needs:\n",
    "\n",
    "1. **`__init__`**: Create token embeddings (`wte`), position embeddings (`wpe`), dropout, a list of `Block` modules, a final `LayerNorm`, and an output projection (`lm_head`). Apply weight tying between `wte` and `lm_head`.\n",
    "\n",
    "2. **`forward(idx, targets=None)`**: Token IDs in, logits out.\n",
    "   - Look up token embeddings: `(B, T)` \u2192 `(B, T, n_embd)`\n",
    "   - Add position embeddings (positions 0..T-1)\n",
    "   - Apply dropout\n",
    "   - Pass through all blocks\n",
    "   - Final layer norm\n",
    "   - Output projection to vocab size: `(B, T, n_embd)` \u2192 `(B, T, vocab_size)`\n",
    "   - If targets provided, compute cross-entropy loss\n",
    "   - Return `(logits, loss)`\n",
    "\n",
    "3. **`generate(idx, max_new_tokens, temperature=1.0)`**: Autoregressive text generation.\n",
    "   - For each new token: crop to block_size, forward pass, take last position's logits, apply temperature, sample, append\n",
    "\n",
    "**Configuration reminder:** Use `GPTConfig` for all hyperparameters. The `nn.ModuleDict` pattern from the lesson organizes sub-modules cleanly.\n",
    "\n",
    "**Verification targets:**\n",
    "- With `debug_config`: model should accept `(B, T)` input and produce `(B, T, vocab_size)` logits\n",
    "- With full `GPTConfig()`: parameter count should be ~124.4M\n",
    "- `generate()` should produce valid token IDs (gibberish is expected \u2014 the model is untrained)\n",
    "\n",
    "<details>\n",
    "<summary>\ud83d\udca1 Solution</summary>\n",
    "\n",
    "The GPT class is an assembly job \u2014 every component was built in the previous exercises. The key decisions are:\n",
    "\n",
    "1. **Weight tying**: `self.transformer.wte.weight = self.lm_head.weight` makes the embedding and output projection share the same matrix. Embedding maps token ID \u2192 vector; output projection maps vector \u2192 token scores. Same mapping, opposite direction. This saves ~38M parameters.\n",
    "\n",
    "2. **Weight initialization**: Normal distribution with \u03c3=0.02 for all weights. Residual projections (named `c_proj`) get scaled by `1/sqrt(2*n_layer)` to prevent activations from growing with depth.\n",
    "\n",
    "3. **Generate method**: `@torch.no_grad()` disables gradient tracking for speed. The crop to `block_size` handles sequences longer than the context window.\n",
    "\n",
    "```python\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"Complete GPT language model.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # Weight tying\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        # Scaled init for residual projections\n",
    "        for name, p in self.named_parameters():\n",
    "            if name.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / (2 * config.n_layer) ** 0.5)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.block_size, \\\n",
    "            f\"Sequence length {T} exceeds block_size {self.config.block_size}\"\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos = torch.arange(0, T, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1)\n",
    "            )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        return idx\n",
    "```\n",
    "\n",
    "Common mistakes:\n",
    "- Forgetting weight tying (parameter count will be ~38M too high)\n",
    "- Not using `torch.no_grad()` in generate (wastes memory on gradient tracking)\n",
    "- Forgetting to crop `idx` to `block_size` in generate (crashes on long sequences)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# YOUR TASK: Implement the complete GPT model\n",
    "# ============================================================\n",
    "\n",
    "# Write your GPT class here.\n",
    "# It needs: __init__, _init_weights, forward, generate\n",
    "# See the specification above for exact requirements.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Verify with debug config ----\n",
    "model = GPT(debug_config)\n",
    "\n",
    "# Test forward pass\n",
    "idx = torch.randint(0, debug_config.vocab_size, (2, 16))  # (B=2, T=16)\n",
    "logits, loss = model(idx)\n",
    "\n",
    "print(f\"Input shape:  {idx.shape}\")        # (2, 16)\n",
    "print(f\"Logits shape: {logits.shape}\")     # (2, 16, 256)\n",
    "assert logits.shape == (2, 16, debug_config.vocab_size), \"Forward pass shape wrong!\"\n",
    "print(\"Forward pass shape check passed!\")\n",
    "\n",
    "# Test with targets (loss computation)\n",
    "targets = torch.randint(0, debug_config.vocab_size, (2, 16))\n",
    "logits, loss = model(idx, targets=targets)\n",
    "print(f\"\\nLoss: {loss.item():.4f}\")\n",
    "print(f\"Expected ~ln(vocab_size) = ln({debug_config.vocab_size}) = {torch.log(torch.tensor(float(debug_config.vocab_size))).item():.4f}\")\n",
    "print(\"(Random model, uniform predictions => loss should be close to ln(vocab_size))\")\n",
    "\n",
    "# Test generate\n",
    "prompt = torch.zeros((1, 1), dtype=torch.long)  # single token\n",
    "generated = model.generate(prompt, max_new_tokens=10)\n",
    "print(f\"\\nGenerated token IDs: {generated[0].tolist()}\")\n",
    "print(f\"Generated shape: {generated.shape}\")  # (1, 11)\n",
    "print(\"\\nDebug config checks passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Verify with full GPT-2 config ----\n",
    "print(\"Building GPT-2 small (124M parameters)...\")\n",
    "print(\"(This may take a few seconds)\\n\")\n",
    "\n",
    "full_model = GPT(GPTConfig())\n",
    "\n",
    "# Count total parameters\n",
    "total = sum(p.numel() for p in full_model.parameters())\n",
    "print(f\"Total parameters: {total:,}\")\n",
    "\n",
    "# Count per component\n",
    "print(\"\\nPer component:\")\n",
    "for name, module in full_model.transformer.named_children():\n",
    "    n = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"  {name}: {n:,}\")\n",
    "\n",
    "lm_head_params = sum(p.numel() for p in full_model.lm_head.parameters())\n",
    "print(f\"  lm_head: {lm_head_params:,} (weight-tied with wte)\")\n",
    "\n",
    "# The parameter count should be ~124M\n",
    "# (exact number depends on whether biases are used)\n",
    "print(f\"\\nParameter count matches GPT-2 small: \", end=\"\")\n",
    "if 123_000_000 < total < 125_000_000:\n",
    "    print(\"YES\")\n",
    "else:\n",
    "    print(f\"NO (expected ~124M, got {total:,})\")\n",
    "    print(\"Check: weight tying? bias setting? dimensions?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Generate text from the untrained model ----\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Move model to device\n",
    "full_model = full_model.to(device)\n",
    "full_model.eval()\n",
    "\n",
    "prompt_text = \"The meaning of life is\"\n",
    "prompt_ids = enc.encode(prompt_text)\n",
    "idx = torch.tensor([prompt_ids], device=device)  # (1, T)\n",
    "\n",
    "print(f\"Prompt: \\\"{prompt_text}\\\"\")\n",
    "print(f\"Prompt token IDs: {prompt_ids}\")\n",
    "print()\n",
    "\n",
    "# Generate with different temperatures\n",
    "for temp in [0.5, 1.0, 2.0]:\n",
    "    torch.manual_seed(42)\n",
    "    output = full_model.generate(idx, max_new_tokens=30, temperature=temp)\n",
    "    text = enc.decode(output[0].tolist())\n",
    "    print(f\"Temperature {temp}: {text}\")\n",
    "\n",
    "print()\n",
    "print(\"The output is gibberish \u2014 and that's correct!\")\n",
    "print(\"The architecture works. Every weight is random.\")\n",
    "print(\"In the next lesson, you train it on real text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Five PyTorch operations build the entire GPT model.** `nn.Linear`, `nn.Embedding`, `nn.LayerNorm`, `nn.GELU`, `nn.Dropout`. Every one is familiar from Series 2. The complexity is in the assembly, not the parts.\n",
    "\n",
    "2. **Bottom-up assembly: Head \u2192 MHA \u2192 FFN \u2192 Block \u2192 GPT.** Each class is small (5\u201315 lines), testable independently, and maps 1:1 to a concept from Module 4.2. The build order mirrors the learning order.\n",
    "\n",
    "3. **Parameter count = architecture verification.** If your model has ~124.4M parameters, the dimensions, projections, and weight tying are correct. One number confirms the entire structure.\n",
    "\n",
    "4. **An untrained model generating gibberish is a success.** It means the architecture, shapes, masking, and generation loop all work. The model is correct \u2014 it just needs to learn from data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}