{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging and Visualization\n",
    "\n",
    "**Module 2.2, Lesson 3** | CourseAI\n",
    "\n",
    "This notebook gives you hands-on practice with PyTorch debugging and visualization tools:\n",
    "\n",
    "1. **torchinfo** \u2014 Inspect model architecture and verify parameter counts\n",
    "2. **Gradient monitoring** \u2014 Write a function to track gradient norms per layer\n",
    "3. **TensorBoard** \u2014 Log training metrics and compare runs visually\n",
    "4. **Debugging a broken script** \u2014 Apply the debugging checklist to find real bugs\n",
    "\n",
    "These are the tools you reach for when something goes wrong. Practice them now so they\u2019re second nature when you need them.\n",
    "\n",
    "**Estimated time:** 30\u201345 minutes on Colab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run these cells to install dependencies and import everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "\n",
    "# TensorBoard\n",
    "%load_ext tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# For nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Model and Data (from MNIST Project)\n",
    "\n",
    "This is the same model and data loading code from lesson 2-2-2. Run these cells so you have a working model to debug and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=False\n",
    ")\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')\n",
    "print(f'Image shape: {train_dataset[0][0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    \"\"\"The same MNIST model from your MNIST Project.\n",
    "    \n",
    "    Architecture:\n",
    "        Flatten (784) -> Linear(784, 128) -> ReLU\n",
    "        -> Linear(128, 64) -> ReLU -> Linear(64, 10)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MNISTClassifier().to(device)\n",
    "print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Inspect with torchinfo (Guided)\n",
    "\n",
    "Run `torchinfo.summary()` on your MNIST model. Verify the parameter count matches your manual calculation from the MNIST Project.\n",
    "\n",
    "Recall the manual calculation:\n",
    "- `fc1`: 784 \u00d7 128 + 128 = 100,480\n",
    "- `fc2`: 128 \u00d7 64 + 64 = 8,256\n",
    "- `fc3`: 64 \u00d7 10 + 10 = 650\n",
    "- **Total**: 109,386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run torchinfo summary on MNISTClassifier\n",
    "# input_size matches a single MNIST batch: (batch_size, channels, height, width)\n",
    "summary(model, input_size=(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: manual calculation vs torchinfo\n",
    "manual_fc1 = 784 * 128 + 128\n",
    "manual_fc2 = 128 * 64 + 64\n",
    "manual_fc3 = 64 * 10 + 10\n",
    "manual_total = manual_fc1 + manual_fc2 + manual_fc3\n",
    "\n",
    "actual_total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f'Manual calculation:')\n",
    "print(f'  fc1: {manual_fc1:>10,}')\n",
    "print(f'  fc2: {manual_fc2:>10,}')\n",
    "print(f'  fc3: {manual_fc3:>10,}')\n",
    "print(f'  Total: {manual_total:>8,}')\n",
    "print(f'')\n",
    "print(f'Actual total:  {actual_total:,}')\n",
    "print(f'Match: {manual_total == actual_total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to notice\n",
    "\n",
    "- torchinfo shows **output shape** at each layer \u2014 this is how you catch dimension bugs\n",
    "- It shows **parameter count** per layer \u2014 use this to verify your architecture\n",
    "- The `Flatten` layer has 0 parameters (it just reshapes)\n",
    "- Most parameters live in `fc1` because its input is 784-dimensional\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Find a Shape Bug with torchinfo (Guided)\n",
    "\n",
    "Introduce a shape bug by removing `nn.Flatten()` from the model. Run torchinfo and identify the problem. Then fix it.\n",
    "\n",
    "This simulates a common debugging scenario: the model crashes with a cryptic shape error, and you need to figure out where the mismatch happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrokenModel(nn.Module):\n",
    "    \"\"\"Same as MNISTClassifier but missing nn.Flatten().\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Bug: no self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bug: x goes straight into fc1 without flattening\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "broken_model = BrokenModel().to(device)\n",
    "\n",
    "# Try running torchinfo on the broken model.\n",
    "# It will show an error or unexpected shapes — that's the point.\n",
    "try:\n",
    "    summary(broken_model, input_size=(1, 1, 28, 28))\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')\n",
    "    print()\n",
    "    print('The model crashed because fc1 expects input of size 784,')\n",
    "    print('but received a 4D tensor of shape [1, 1, 28, 28].')\n",
    "    print('The fix: add nn.Flatten() before the first Linear layer.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedModel(nn.Module):\n",
    "    \"\"\"BrokenModel with the Flatten fix applied.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()  # Fix: add flatten\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)  # Fix: flatten before fc1\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "fixed_model = FixedModel().to(device)\n",
    "\n",
    "# Now torchinfo should work\n",
    "print('Fixed model:')\n",
    "summary(fixed_model, input_size=(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway\n",
    "\n",
    "When you hit a shape error:\n",
    "1. Run `torchinfo.summary()` \u2014 it shows the output shape at every layer\n",
    "2. Find the layer where the shape goes wrong\n",
    "3. The fix is usually a missing reshape, flatten, or wrong dimension argument\n",
    "\n",
    "This is faster than reading PyTorch\u2019s error messages, which often point to the *symptom* (wrong size at a Linear layer) rather than the *cause* (missing Flatten).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Gradient Norm Monitoring (Supported)\n",
    "\n",
    "Write a `log_gradient_norms()` function that iterates over `model.named_parameters()` and prints `grad.norm()` for each parameter.\n",
    "\n",
    "Then compare gradient norms between:\n",
    "- A **healthy model** (default PyTorch initialization)\n",
    "- A **poorly-initialized model** (weights set to 100.0)\n",
    "\n",
    "This shows you what gradient health looks like \u2014 and what problems look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gradient_norms(model):\n",
    "    \"\"\"Print the gradient norm for each named parameter.\n",
    "    \n",
    "    Call this after loss.backward() to inspect gradient magnitudes.\n",
    "    Healthy gradients are typically in the range 0.01 to 10.0.\n",
    "    Very large (>100) or very small (<1e-6) gradients signal problems.\n",
    "    \"\"\"\n",
    "    print(f'{\"Layer\":<30} {\"Grad Norm\":>12}')\n",
    "    print('-' * 44)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            print(f'{name:<30} {grad_norm:>12.6f}')\n",
    "        else:\n",
    "            print(f'{name:<30} {\"No grad\":>12}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Healthy model (default initialization) ---\n",
    "healthy_model = MNISTClassifier().to(device)\n",
    "\n",
    "# One forward + backward pass\n",
    "sample_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "sample_target = torch.tensor([3]).to(device)\n",
    "\n",
    "output = healthy_model(sample_input)\n",
    "loss = nn.CrossEntropyLoss()(output, sample_target)\n",
    "loss.backward()\n",
    "\n",
    "print('HEALTHY MODEL (default init)')\n",
    "print('=' * 44)\n",
    "log_gradient_norms(healthy_model)\n",
    "print(f'\\nLoss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Poorly initialized model (weights set to 100.0) ---\n",
    "bad_model = MNISTClassifier().to(device)\n",
    "\n",
    "# Set all weights to an extreme value\n",
    "with torch.no_grad():\n",
    "    for param in bad_model.parameters():\n",
    "        param.fill_(100.0)\n",
    "\n",
    "# One forward + backward pass\n",
    "output = bad_model(sample_input)\n",
    "loss = nn.CrossEntropyLoss()(output, sample_target)\n",
    "loss.backward()\n",
    "\n",
    "print('POORLY INITIALIZED MODEL (all weights = 100.0)')\n",
    "print('=' * 44)\n",
    "log_gradient_norms(bad_model)\n",
    "print(f'\\nLoss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to notice\n",
    "\n",
    "- **Healthy model**: Gradient norms are moderate (roughly 0.01 to 10). The loss is a reasonable number (around 2\u20133 for random predictions on 10 classes).\n",
    "- **Bad model**: Gradient norms are either enormous (exploding) or zero (saturated ReLU). The loss may be NaN or extremely large.\n",
    "- This is exactly the kind of signal that tells you \"something is wrong with initialization\" before you waste time training.\n",
    "\n",
    "**Rule of thumb:** If gradient norms vary by more than 3\u20134 orders of magnitude across layers, or if any are exactly 0.0, investigate before training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: TensorBoard Logging (Supported)\n",
    "\n",
    "Add TensorBoard logging to your MNIST training loop. Train for 10 epochs, then open TensorBoard to examine the loss and accuracy curves.\n",
    "\n",
    "TensorBoard gives you a persistent, interactive dashboard \u2014 much better than printing numbers in a cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, loader):\n",
    "    \"\"\"Compute accuracy on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with TensorBoard logging\n",
    "tb_model = MNISTClassifier().to(device)\n",
    "optimizer = optim.Adam(tb_model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create a TensorBoard writer\n",
    "writer = SummaryWriter('runs/mnist')\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    tb_model.train()\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = tb_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = running_loss / n_batches\n",
    "    test_acc = evaluate_accuracy(tb_model, test_loader)\n",
    "\n",
    "    # Log to TensorBoard\n",
    "    writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/test', test_acc, epoch)\n",
    "\n",
    "    print(f'Epoch {epoch+1:2d}/{num_epochs}  Loss: {avg_loss:.4f}  Test Acc: {test_acc:.2%}')\n",
    "\n",
    "writer.close()\n",
    "print('\\nTraining complete. TensorBoard logs written to runs/mnist/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open TensorBoard inline\n",
    "# You should see Loss/train decreasing and Accuracy/test increasing.\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to notice in TensorBoard\n",
    "\n",
    "- **Loss/train** should decrease smoothly over epochs\n",
    "- **Accuracy/test** should increase and plateau\n",
    "- If loss is noisy or increasing, something is wrong (learning rate too high, bug in the loop)\n",
    "- TensorBoard keeps data across runs \u2014 useful for comparing experiments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Learning Rate Comparison (Supported)\n",
    "\n",
    "Train 3 runs with different learning rates: 0.001, 0.01, and 0.1. Compare them in TensorBoard.\n",
    "\n",
    "Identify which learning rate is too high, too low, and just right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "num_epochs = 10\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f'\\n{\"=\" * 50}')\n",
    "    print(f'Training with lr={lr}')\n",
    "    print(f'{\"=\" * 50}')\n",
    "\n",
    "    lr_model = MNISTClassifier().to(device)\n",
    "    optimizer = optim.Adam(lr_model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Each run gets its own TensorBoard log directory\n",
    "    writer = SummaryWriter(f'runs/lr_{lr}')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        lr_model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = lr_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = running_loss / n_batches\n",
    "        test_acc = evaluate_accuracy(lr_model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/test', test_acc, epoch)\n",
    "\n",
    "        print(f'  Epoch {epoch+1:2d}/{num_epochs}  Loss: {avg_loss:.4f}  Test Acc: {test_acc:.2%}')\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "print('\\nAll runs complete. Open TensorBoard to compare.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open TensorBoard to compare all runs side by side\n",
    "# You'll see runs/lr_0.001, runs/lr_0.01, runs/lr_0.1 as separate curves.\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to look for\n",
    "\n",
    "- **lr=0.001**: Loss decreases steadily but slowly. Accuracy climbs gradually. This is the *too low* rate \u2014 it works but wastes compute.\n",
    "- **lr=0.01**: Loss drops faster. Accuracy ramps up quickly. Likely the *just right* rate for this model.\n",
    "- **lr=0.1**: Loss may spike, oscillate, or fail to converge. Accuracy may be erratic or stuck. This is the *too high* rate.\n",
    "\n",
    "The exact behavior depends on the optimizer (Adam is more forgiving than SGD), but the pattern holds. Being able to **see** these curves side by side is why TensorBoard exists.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Debug a Broken Training Script (Independent)\n",
    "\n",
    "The script below has **3 intentional bugs**. Your job: use the debugging checklist to find and fix all three.\n",
    "\n",
    "The bugs:\n",
    "1. A **shape error** \u2014 the model processes input incorrectly\n",
    "2. A **missing `model.eval()`** \u2014 evaluation runs in training mode\n",
    "3. A **subtle data loading bug** \u2014 the training DataLoader is misconfigured\n",
    "\n",
    "**Approach:**\n",
    "- Read the script carefully\n",
    "- Try running it \u2014 one bug will crash immediately\n",
    "- Fix the crash, then look for the two silent bugs (they won\u2019t crash, but they hurt performance)\n",
    "- Use `torchinfo.summary()` and print statements to help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BUGGY TRAINING SCRIPT =====\n",
    "# There are 3 bugs in this script. Find and fix them all.\n",
    "#\n",
    "# Bug hints (read these AFTER you've tried finding them yourself):\n",
    "#   1. The model is missing a critical layer for handling image input\n",
    "#   2. The evaluation function doesn't switch the model to eval mode\n",
    "#   3. The training DataLoader has a configuration that hurts learning\n",
    "\n",
    "\n",
    "class BuggyClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # BUG 1: No nn.Flatten() — fc1 will receive a 4D tensor instead of 784\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # BUG 1: x is [batch, 1, 28, 28] but fc1 expects [batch, 784]\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def buggy_evaluate(model, loader):\n",
    "    \"\"\"Evaluate accuracy on a dataset.\"\"\"\n",
    "    # BUG 2: Missing model.eval() — dropout/batchnorm would behave incorrectly\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def buggy_train():\n",
    "    buggy_model = BuggyClassifier().to(device)\n",
    "    optimizer = optim.Adam(buggy_model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # BUG 3: shuffle=False for training data\n",
    "    # The model sees the same order every epoch, which can hurt generalization\n",
    "    # and cause the loss to follow a predictable pattern rather than converging smoothly\n",
    "    buggy_train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=64, shuffle=False\n",
    "    )\n",
    "\n",
    "    for epoch in range(5):\n",
    "        buggy_model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for images, labels in buggy_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = buggy_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = running_loss / n_batches\n",
    "        acc = buggy_evaluate(buggy_model, test_loader)\n",
    "        print(f'Epoch {epoch+1}/5  Loss: {avg_loss:.4f}  Test Acc: {acc:.2%}')\n",
    "\n",
    "\n",
    "# This will crash — start debugging here\n",
    "buggy_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix the bugs below\n",
    "\n",
    "Copy the buggy code into the cell below and fix all three bugs. Then run it to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== YOUR FIXED VERSION =====\n",
    "# Copy the buggy code here and fix all three bugs:\n",
    "#   1. Add nn.Flatten() to the model\n",
    "#   2. Add model.eval() in the evaluate function\n",
    "#   3. Set shuffle=True in the training DataLoader\n",
    "\n",
    "\n",
    "class FixedClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()       # FIX 1: add flatten\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)               # FIX 1: flatten input\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def fixed_evaluate(model, loader):\n",
    "    model.eval()                           # FIX 2: switch to eval mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def fixed_train():\n",
    "    fixed_model = FixedClassifier().to(device)\n",
    "    optimizer = optim.Adam(fixed_model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # FIX 3: shuffle=True for training data\n",
    "    fixed_train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=64, shuffle=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(5):\n",
    "        fixed_model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for images, labels in fixed_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = fixed_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = running_loss / n_batches\n",
    "        acc = fixed_evaluate(fixed_model, test_loader)\n",
    "        print(f'Epoch {epoch+1}/5  Loss: {avg_loss:.4f}  Test Acc: {acc:.2%}')\n",
    "\n",
    "\n",
    "fixed_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bug debrief\n",
    "\n",
    "| Bug | Type | How to catch it |\n",
    "|-----|------|----------------|\n",
    "| Missing `nn.Flatten()` | Shape error | Crashes immediately. `torchinfo.summary()` would show the dimension mismatch. |\n",
    "| Missing `model.eval()` | Silent correctness bug | Doesn\u2019t crash. Affects models with dropout or batch norm. Check your evaluation function template. |\n",
    "| `shuffle=False` in training | Silent performance bug | Doesn\u2019t crash. Model still trains, but converges slower and generalizes worse. Always shuffle training data. |\n",
    "\n",
    "The shape error is easy \u2014 it crashes. The dangerous bugs are the silent ones. That\u2019s why you need a **debugging checklist** that you run through systematically, not just when things crash.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **`torchinfo.summary()`** is your first tool when a model crashes or you want to verify architecture. It shows shapes and parameter counts at every layer.\n",
    "\n",
    "2. **Gradient norms** tell you if training is healthy before you waste time on epochs. Extreme values (too large or zero) mean something is wrong with initialization or architecture.\n",
    "\n",
    "3. **TensorBoard** replaces print statements with persistent, interactive dashboards. Use separate run directories to compare experiments (different learning rates, architectures, etc.).\n",
    "\n",
    "4. **The most dangerous bugs are silent.** Missing `model.eval()`, wrong `shuffle` settings, and subtle data issues won\u2019t crash your code \u2014 they just produce worse results without explanation. A systematic debugging checklist catches these.\n",
    "\n",
    "5. **Use these tools proactively**, not just when something breaks. Run `torchinfo` on every new model. Check gradient norms after the first batch. Log everything to TensorBoard. The 30 seconds of setup saves hours of confusion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}