{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Debugging and Visualization\n\nThis notebook gives you hands-on practice with PyTorch debugging and visualization tools:\n\n1. **torchinfo** â€” Inspect model architecture and verify parameter counts\n2. **Gradient monitoring** â€” Write a function to track gradient norms per layer\n3. **TensorBoard** â€” Log training metrics and compare runs visually\n4. **Debugging a broken script** â€” Apply the debugging checklist to find real bugs\n\nThese are the tools you reach for when something goes wrong. Practice them now so they're second nature when you need them.\n\n**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones â€” they reveal gaps in your mental model.\n\n**Estimated time:** 30â€“45 minutes on Colab.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run these cells to install dependencies and import everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom torchinfo import summary\n\n# TensorBoard\n%load_ext tensorboard\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Reproducibility\ntorch.manual_seed(42)\n\n# Use GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n# For nice plots\nplt.style.use('dark_background')\nplt.rcParams['figure.figsize'] = [10, 4]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Model and Data (from MNIST Project)\n",
    "\n",
    "This is the same model and data loading code from lesson 2-2-2. Run these cells so you have a working model to debug and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=False\n",
    ")\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')\n",
    "print(f'Image shape: {train_dataset[0][0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    \"\"\"The same MNIST model from your MNIST Project.\n",
    "    \n",
    "    Architecture:\n",
    "        Flatten (784) -> Linear(784, 128) -> ReLU\n",
    "        -> Linear(128, 64) -> ReLU -> Linear(64, 10)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MNISTClassifier().to(device)\n",
    "print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 1: Inspect with torchinfo (Guided)\n\nRun `torchinfo.summary()` on your MNIST model. Verify the parameter count matches your manual calculation from the MNIST Project.\n\nRecall the manual calculation:\n- `fc1`: 784 x 128 + 128 = 100,480\n- `fc2`: 128 x 64 + 64 = 8,256\n- `fc3`: 64 x 10 + 10 = 650\n- **Total**: 109,386\n\n**Before running, predict:** What output shape will torchinfo show after the Flatten layer? After fc1? Where will most of the parameters be concentrated?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run torchinfo summary on MNISTClassifier\n",
    "# input_size matches a single MNIST batch: (batch_size, channels, height, width)\n",
    "summary(model, input_size=(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: manual calculation vs torchinfo\n",
    "manual_fc1 = 784 * 128 + 128\n",
    "manual_fc2 = 128 * 64 + 64\n",
    "manual_fc3 = 64 * 10 + 10\n",
    "manual_total = manual_fc1 + manual_fc2 + manual_fc3\n",
    "\n",
    "actual_total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f'Manual calculation:')\n",
    "print(f'  fc1: {manual_fc1:>10,}')\n",
    "print(f'  fc2: {manual_fc2:>10,}')\n",
    "print(f'  fc3: {manual_fc3:>10,}')\n",
    "print(f'  Total: {manual_total:>8,}')\n",
    "print(f'')\n",
    "print(f'Actual total:  {actual_total:,}')\n",
    "print(f'Match: {manual_total == actual_total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to notice\n",
    "\n",
    "- torchinfo shows **output shape** at each layer â€” this is how you catch dimension bugs\n",
    "- It shows **parameter count** per layer â€” use this to verify your architecture\n",
    "- The `Flatten` layer has 0 parameters (it just reshapes)\n",
    "- Most parameters live in `fc1` because its input is 784-dimensional\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 2: Find a Shape Bug with torchinfo (Guided)\n\nIntroduce a shape bug by removing `nn.Flatten()` from the model. Run torchinfo and identify the problem. Then fix it.\n\nThis simulates a common debugging scenario: the model crashes with a cryptic shape error, and you need to figure out where the mismatch happens.\n\n**Before running, predict:** If you feed a `[1, 1, 28, 28]` tensor directly into `nn.Linear(784, 128)` without flattening, what error will you get? What shape does the Linear layer actually receive?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrokenModel(nn.Module):\n",
    "    \"\"\"Same as MNISTClassifier but missing nn.Flatten().\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Bug: no self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bug: x goes straight into fc1 without flattening\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "broken_model = BrokenModel().to(device)\n",
    "\n",
    "# Try running torchinfo on the broken model.\n",
    "# It will show an error or unexpected shapes â€” that's the point.\n",
    "try:\n",
    "    summary(broken_model, input_size=(1, 1, 28, 28))\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')\n",
    "    print()\n",
    "    print('The model crashed because fc1 expects input of size 784,')\n",
    "    print('but received a 4D tensor of shape [1, 1, 28, 28].')\n",
    "    print('The fix: add nn.Flatten() before the first Linear layer.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedModel(nn.Module):\n",
    "    \"\"\"BrokenModel with the Flatten fix applied.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()  # Fix: add flatten\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)  # Fix: flatten before fc1\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "fixed_model = FixedModel().to(device)\n",
    "\n",
    "# Now torchinfo should work\n",
    "print('Fixed model:')\n",
    "summary(fixed_model, input_size=(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway\n",
    "\n",
    "When you hit a shape error:\n",
    "1. Run `torchinfo.summary()` â€” it shows the output shape at every layer\n",
    "2. Find the layer where the shape goes wrong\n",
    "3. The fix is usually a missing reshape, flatten, or wrong dimension argument\n",
    "\n",
    "This is faster than reading PyTorchâ€™s error messages, which often point to the *symptom* (wrong size at a Linear layer) rather than the *cause* (missing Flatten).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 3: Gradient Norm Monitoring (Supported)\n\nWrite a `log_gradient_norms()` function that iterates over `model.named_parameters()` and prints `grad.norm()` for each parameter.\n\nThen compare gradient norms between:\n- A **healthy model** (default PyTorch initialization)\n- A **poorly-initialized model** (weights set to 100.0)\n\nThis shows you what gradient health looks like â€” and what problems look like.\n\n<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe key insight is that `model.named_parameters()` gives you access to every learnable tensor and its name. After calling `loss.backward()`, each parameter's `.grad` attribute contains the gradient. The L2 norm of that gradient tells you the magnitude of the update signal.\n\n```python\ndef log_gradient_norms(model):\n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            grad_norm = param.grad.norm().item()\n            print(f'{name:<30} {grad_norm:>12.6f}')\n```\n\nHealthy gradients are typically in the 0.01 to 10.0 range. If you see norms above 100 (exploding) or exactly 0.0 (dead neurons/saturated activations), something is wrong with your initialization or architecture.\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gradient_norms(model):\n",
    "    \"\"\"Print the gradient norm for each named parameter.\n",
    "    \n",
    "    Call this after loss.backward() to inspect gradient magnitudes.\n",
    "    Healthy gradients are typically in the range 0.01 to 10.0.\n",
    "    Very large (>100) or very small (<1e-6) gradients signal problems.\n",
    "    \"\"\"\n",
    "    print(f'{\"Layer\":<30} {\"Grad Norm\":>12}')\n",
    "    print('-' * 44)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            print(f'{name:<30} {grad_norm:>12.6f}')\n",
    "        else:\n",
    "            print(f'{name:<30} {\"No grad\":>12}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Healthy model (default initialization) ---\n",
    "healthy_model = MNISTClassifier().to(device)\n",
    "\n",
    "# One forward + backward pass\n",
    "sample_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "sample_target = torch.tensor([3]).to(device)\n",
    "\n",
    "output = healthy_model(sample_input)\n",
    "loss = nn.CrossEntropyLoss()(output, sample_target)\n",
    "loss.backward()\n",
    "\n",
    "print('HEALTHY MODEL (default init)')\n",
    "print('=' * 44)\n",
    "log_gradient_norms(healthy_model)\n",
    "print(f'\\nLoss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Poorly initialized model (weights set to 100.0) ---\n",
    "bad_model = MNISTClassifier().to(device)\n",
    "\n",
    "# Set all weights to an extreme value\n",
    "with torch.no_grad():\n",
    "    for param in bad_model.parameters():\n",
    "        param.fill_(100.0)\n",
    "\n",
    "# One forward + backward pass\n",
    "output = bad_model(sample_input)\n",
    "loss = nn.CrossEntropyLoss()(output, sample_target)\n",
    "loss.backward()\n",
    "\n",
    "print('POORLY INITIALIZED MODEL (all weights = 100.0)')\n",
    "print('=' * 44)\n",
    "log_gradient_norms(bad_model)\n",
    "print(f'\\nLoss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to notice\n",
    "\n",
    "- **Healthy model**: Gradient norms are moderate (roughly 0.01 to 10). The loss is a reasonable number (around 2â€“3 for random predictions on 10 classes).\n",
    "- **Bad model**: Gradient norms are either enormous (exploding) or zero (saturated ReLU). The loss may be NaN or extremely large.\n",
    "- This is exactly the kind of signal that tells you \"something is wrong with initialization\" before you waste time training.\n",
    "\n",
    "**Rule of thumb:** If gradient norms vary by more than 3â€“4 orders of magnitude across layers, or if any are exactly 0.0, investigate before training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 4: TensorBoard Logging (Supported)\n\nAdd TensorBoard logging to your MNIST training loop. Train for 10 epochs, then open TensorBoard to examine the loss and accuracy curves.\n\nTensorBoard gives you a persistent, interactive dashboard â€” much better than printing numbers in a cell.\n\n<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe key insight is that `SummaryWriter` creates a log directory, and `writer.add_scalar(tag, value, step)` logs a single data point. Use separate tags for different metrics (e.g., `'Loss/train'`, `'Accuracy/test'`) and the epoch number as the step.\n\n```python\nwriter = SummaryWriter('runs/mnist')\n\nfor epoch in range(num_epochs):\n    # ... training loop ...\n    writer.add_scalar('Loss/train', avg_loss, epoch)\n    writer.add_scalar('Accuracy/test', test_acc, epoch)\n\nwriter.close()\n```\n\nAlways call `writer.close()` when done. Each experiment should use a separate log directory (e.g., `runs/mnist_v1`, `runs/mnist_v2`) so you can compare them in TensorBoard.\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, loader):\n",
    "    \"\"\"Compute accuracy on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with TensorBoard logging\n",
    "tb_model = MNISTClassifier().to(device)\n",
    "optimizer = optim.Adam(tb_model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create a TensorBoard writer\n",
    "writer = SummaryWriter('runs/mnist')\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    tb_model.train()\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = tb_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = running_loss / n_batches\n",
    "    test_acc = evaluate_accuracy(tb_model, test_loader)\n",
    "\n",
    "    # Log to TensorBoard\n",
    "    writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/test', test_acc, epoch)\n",
    "\n",
    "    print(f'Epoch {epoch+1:2d}/{num_epochs}  Loss: {avg_loss:.4f}  Test Acc: {test_acc:.2%}')\n",
    "\n",
    "writer.close()\n",
    "print('\\nTraining complete. TensorBoard logs written to runs/mnist/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open TensorBoard inline\n",
    "# You should see Loss/train decreasing and Accuracy/test increasing.\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to notice in TensorBoard\n",
    "\n",
    "- **Loss/train** should decrease smoothly over epochs\n",
    "- **Accuracy/test** should increase and plateau\n",
    "- If loss is noisy or increasing, something is wrong (learning rate too high, bug in the loop)\n",
    "- TensorBoard keeps data across runs â€” useful for comparing experiments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 5: Learning Rate Comparison (Supported)\n\nTrain 3 runs with different learning rates: 0.001, 0.01, and 0.1. Compare them in TensorBoard.\n\nIdentify which learning rate is too high, too low, and just right.\n\n<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe key insight is using separate `SummaryWriter` directories for each run so TensorBoard can overlay them. The pattern is `SummaryWriter(f'runs/lr_{lr}')`.\n\n```python\nfor lr in [0.001, 0.01, 0.1]:\n    writer = SummaryWriter(f'runs/lr_{lr}')\n    model = MNISTClassifier().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    # ... same training loop, logging with writer ...\n    writer.close()\n```\n\nYou should see: lr=0.001 converges slowly but steadily, lr=0.01 converges quickly and cleanly, lr=0.1 may oscillate or fail to converge. Adam is more forgiving than SGD, so even lr=0.1 might work â€” but the loss curve will be noisier.\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "num_epochs = 10\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f'\\n{\"=\" * 50}')\n",
    "    print(f'Training with lr={lr}')\n",
    "    print(f'{\"=\" * 50}')\n",
    "\n",
    "    lr_model = MNISTClassifier().to(device)\n",
    "    optimizer = optim.Adam(lr_model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Each run gets its own TensorBoard log directory\n",
    "    writer = SummaryWriter(f'runs/lr_{lr}')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        lr_model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = lr_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = running_loss / n_batches\n",
    "        test_acc = evaluate_accuracy(lr_model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/test', test_acc, epoch)\n",
    "\n",
    "        print(f'  Epoch {epoch+1:2d}/{num_epochs}  Loss: {avg_loss:.4f}  Test Acc: {test_acc:.2%}')\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "print('\\nAll runs complete. Open TensorBoard to compare.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open TensorBoard to compare all runs side by side\n",
    "# You'll see runs/lr_0.001, runs/lr_0.01, runs/lr_0.1 as separate curves.\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to look for\n",
    "\n",
    "- **lr=0.001**: Loss decreases steadily but slowly. Accuracy climbs gradually. This is the *too low* rate â€” it works but wastes compute.\n",
    "- **lr=0.01**: Loss drops faster. Accuracy ramps up quickly. Likely the *just right* rate for this model.\n",
    "- **lr=0.1**: Loss may spike, oscillate, or fail to converge. Accuracy may be erratic or stuck. This is the *too high* rate.\n",
    "\n",
    "The exact behavior depends on the optimizer (Adam is more forgiving than SGD), but the pattern holds. Being able to **see** these curves side by side is why TensorBoard exists.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 6: Debug a Broken Training Script (Independent)\n\nThe script below has **3 intentional bugs**. Your job: use the debugging checklist to find and fix all three.\n\nThe bugs:\n1. A **shape error** â€” the model processes input incorrectly\n2. A **missing `model.eval()`** â€” evaluation runs in training mode\n3. A **subtle data loading bug** â€” the training DataLoader is misconfigured\n\n**Approach:**\n- Read the script carefully\n- Try running it â€” one bug will crash immediately\n- Fix the crash, then look for the two silent bugs (they won't crash, but they hurt performance)\n- Use `torchinfo.summary()` and print statements to help\n\n<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe three bugs target different categories of errors you'll encounter in practice:\n\n**Bug 1 (Shape error):** The model is missing `nn.Flatten()`. The input is `[batch, 1, 28, 28]` but `nn.Linear(784, 128)` expects `[batch, 784]`. Fix: add `self.flatten = nn.Flatten()` and call it first in `forward()`. This one crashes immediately, making it the easiest to find.\n\n**Bug 2 (Silent correctness):** `buggy_evaluate()` never calls `model.eval()`. This affects models with Dropout or BatchNorm because they behave differently during training vs inference. Fix: add `model.eval()` at the top of the evaluation function.\n\n**Bug 3 (Silent performance):** The training DataLoader uses `shuffle=False`. The model sees data in the same order every epoch, which can cause it to learn ordering patterns instead of generalizing. Fix: set `shuffle=True`.\n\n```python\n# Bug 1 fix\nself.flatten = nn.Flatten()\n# in forward: x = self.flatten(x)\n\n# Bug 2 fix\ndef fixed_evaluate(model, loader):\n    model.eval()  # Add this line\n    ...\n\n# Bug 3 fix\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n```\n\nThe shape error is easy because it crashes. The dangerous bugs are the silent ones â€” that's why systematic checklists matter.\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BUGGY TRAINING SCRIPT =====\n",
    "# There are 3 bugs in this script. Find and fix them all.\n",
    "#\n",
    "# Bug hints (read these AFTER you've tried finding them yourself):\n",
    "#   1. The model is missing a critical layer for handling image input\n",
    "#   2. The evaluation function doesn't switch the model to eval mode\n",
    "#   3. The training DataLoader has a configuration that hurts learning\n",
    "\n",
    "\n",
    "class BuggyClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # BUG 1: No nn.Flatten() â€” fc1 will receive a 4D tensor instead of 784\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # BUG 1: x is [batch, 1, 28, 28] but fc1 expects [batch, 784]\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def buggy_evaluate(model, loader):\n",
    "    \"\"\"Evaluate accuracy on a dataset.\"\"\"\n",
    "    # BUG 2: Missing model.eval() â€” dropout/batchnorm would behave incorrectly\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def buggy_train():\n",
    "    buggy_model = BuggyClassifier().to(device)\n",
    "    optimizer = optim.Adam(buggy_model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # BUG 3: shuffle=False for training data\n",
    "    # The model sees the same order every epoch, which can hurt generalization\n",
    "    # and cause the loss to follow a predictable pattern rather than converging smoothly\n",
    "    buggy_train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=64, shuffle=False\n",
    "    )\n",
    "\n",
    "    for epoch in range(5):\n",
    "        buggy_model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for images, labels in buggy_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = buggy_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = running_loss / n_batches\n",
    "        acc = buggy_evaluate(buggy_model, test_loader)\n",
    "        print(f'Epoch {epoch+1}/5  Loss: {avg_loss:.4f}  Test Acc: {acc:.2%}')\n",
    "\n",
    "\n",
    "# This will crash â€” start debugging here\n",
    "buggy_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix the bugs below\n",
    "\n",
    "Copy the buggy code into the cell below and fix all three bugs. Then run it to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== YOUR FIXED VERSION =====\n",
    "# Copy the buggy code here and fix all three bugs:\n",
    "#   1. Add nn.Flatten() to the model\n",
    "#   2. Add model.eval() in the evaluate function\n",
    "#   3. Set shuffle=True in the training DataLoader\n",
    "\n",
    "\n",
    "class FixedClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()       # FIX 1: add flatten\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)               # FIX 1: flatten input\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def fixed_evaluate(model, loader):\n",
    "    model.eval()                           # FIX 2: switch to eval mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def fixed_train():\n",
    "    fixed_model = FixedClassifier().to(device)\n",
    "    optimizer = optim.Adam(fixed_model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # FIX 3: shuffle=True for training data\n",
    "    fixed_train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=64, shuffle=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(5):\n",
    "        fixed_model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for images, labels in fixed_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = fixed_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = running_loss / n_batches\n",
    "        acc = fixed_evaluate(fixed_model, test_loader)\n",
    "        print(f'Epoch {epoch+1}/5  Loss: {avg_loss:.4f}  Test Acc: {acc:.2%}')\n",
    "\n",
    "\n",
    "fixed_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bug debrief\n",
    "\n",
    "| Bug | Type | How to catch it |\n",
    "|-----|------|----------------|\n",
    "| Missing `nn.Flatten()` | Shape error | Crashes immediately. `torchinfo.summary()` would show the dimension mismatch. |\n",
    "| Missing `model.eval()` | Silent correctness bug | Doesnâ€™t crash. Affects models with dropout or batch norm. Check your evaluation function template. |\n",
    "| `shuffle=False` in training | Silent performance bug | Doesnâ€™t crash. Model still trains, but converges slower and generalizes worse. Always shuffle training data. |\n",
    "\n",
    "The shape error is easy â€” it crashes. The dangerous bugs are the silent ones. Thatâ€™s why you need a **debugging checklist** that you run through systematically, not just when things crash.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **`torchinfo.summary()`** is your first tool when a model crashes or you want to verify architecture. It shows shapes and parameter counts at every layer.\n",
    "\n",
    "2. **Gradient norms** tell you if training is healthy before you waste time on epochs. Extreme values (too large or zero) mean something is wrong with initialization or architecture.\n",
    "\n",
    "3. **TensorBoard** replaces print statements with persistent, interactive dashboards. Use separate run directories to compare experiments (different learning rates, architectures, etc.).\n",
    "\n",
    "4. **The most dangerous bugs are silent.** Missing `model.eval()`, wrong `shuffle` settings, and subtle data issues wonâ€™t crash your code â€” they just produce worse results without explanation. A systematic debugging checklist catches these.\n",
    "\n",
    "5. **Use these tools proactively**, not just when something breaks. Run `torchinfo` on every new model. Check gradient norms after the first batch. Log everything to TensorBoard. The 30 seconds of setup saves hours of confusion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}