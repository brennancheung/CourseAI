{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDXL\n",
    "\n",
    "**Module 7.4, Lesson 1** | CourseAI\n",
    "\n",
    "SDXL is the U-Net pushed to its practical ceiling. Every improvement is about what goes IN to the U-Net (dual text encoders), what goes AROUND it (refiner model), or what goes ALONGSIDE it (micro-conditioning). The architecture itself is the same species as SD v1.5.\n",
    "\n",
    "**What you will do:**\n",
    "- Load the SDXL base pipeline and inspect its dual text encoders—verify the tensor shapes [77, 768] and [77, 1280] that concatenate to [77, 2048]\n",
    "- Generate with SDXL at 1024×1024 and compare to SD v1.5 at 512×512—see the quality jump for yourself\n",
    "- Explore micro-conditioning by varying original_size and crop_top_left—observe how training metadata steers generation quality and composition\n",
    "- Build the base + refiner two-stage pipeline and compare base-only vs base+refiner output at different handoff points\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "Every concept in this notebook comes from the lesson. Dual text encoders, micro-conditioning, the refiner as img2img with a specialist. No new theory—just hands-on verification of what you just read.\n",
    "\n",
    "**Estimated time:** 40–60 minutes. All exercises use pre-trained models (no training). Requires a GPU runtime with sufficient VRAM (~7 GB for SDXL base in float16)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies and configure the environment.\n",
    "\n",
    "**Important:** Switch to a GPU runtime in Colab (Runtime > Change runtime type > T4 GPU). SDXL requires a GPU with at least 7 GB VRAM in float16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import (\n",
    "    StableDiffusionXLPipeline,\n",
    "    StableDiffusionXLImg2ImgPipeline,\n",
    "    DPMSolverMultistepScheduler,\n",
    ")\n",
    "from IPython.display import display\n",
    "\n",
    "# Reproducible results\n",
    "SEED = 42\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [14, 5]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(f'Device: {device}')\n",
    "print(f'Dtype: {dtype}')\n",
    "if device.type == 'cpu':\n",
    "    print('WARNING: No GPU detected. SDXL will be extremely slow on CPU.')\n",
    "    print('Switch to a GPU runtime: Runtime > Change runtime type > T4 GPU')\n",
    "print()\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Helpers\n",
    "\n",
    "Utility functions for timing generation and displaying image comparisons. Run this cell now—these are used across all four exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sdxl(\n",
    "    pipe,\n",
    "    prompt,\n",
    "    num_inference_steps=30,\n",
    "    guidance_scale=7.5,\n",
    "    seed=SEED,\n",
    "    original_size=None,\n",
    "    crops_coords_top_left=None,\n",
    "    target_size=None,\n",
    "    output_type=\"pil\",\n",
    "    denoising_end=None,\n",
    "):\n",
    "    \"\"\"Generate an image with SDXL and return (image, elapsed_seconds).\n",
    "\n",
    "    Supports micro-conditioning kwargs and denoising_end for refiner handoff.\n",
    "    \"\"\"\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    kwargs = dict(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator,\n",
    "        output_type=output_type,\n",
    "    )\n",
    "    if original_size is not None:\n",
    "        kwargs[\"original_size\"] = original_size\n",
    "    if crops_coords_top_left is not None:\n",
    "        kwargs[\"crops_coords_top_left\"] = crops_coords_top_left\n",
    "    if target_size is not None:\n",
    "        kwargs[\"target_size\"] = target_size\n",
    "    if denoising_end is not None:\n",
    "        kwargs[\"denoising_end\"] = denoising_end\n",
    "\n",
    "    start = time.time()\n",
    "    result = pipe(**kwargs)\n",
    "    elapsed = time.time() - start\n",
    "    return result.images[0], elapsed\n",
    "\n",
    "\n",
    "def show_image_row(images, titles, suptitle=None, figsize=None):\n",
    "    \"\"\"Display a row of PIL images with titles.\"\"\"\n",
    "    n = len(images)\n",
    "    fig_w = figsize[0] if figsize else max(5 * n, 12)\n",
    "    fig_h = figsize[1] if figsize else 5\n",
    "    fig, axes = plt.subplots(1, n, figsize=(fig_w, fig_h))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle, fontsize=13, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def free_memory(*pipelines):\n",
    "    \"\"\"Delete pipelines and free GPU memory.\"\"\"\n",
    "    for p in pipelines:\n",
    "        del p\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Memory freed.\")\n",
    "\n",
    "\n",
    "print('Helpers defined: generate_sdxl, show_image_row, free_memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: SDXL Pipeline Inspection `[Guided]`\n",
    "\n",
    "The lesson taught that SDXL uses two text encoders instead of one:\n",
    "- **CLIP ViT-L/14** (SD v1.5's original encoder): produces [77, 768] embeddings\n",
    "- **OpenCLIP ViT-bigG/14** (SDXL's addition): produces [77, 1280] embeddings\n",
    "\n",
    "The two outputs are concatenated along the embedding dimension: [77, 768] + [77, 1280] = [77, 2048]. This combined tensor becomes the K/V source for cross-attention. One cross-attention path, wider input.\n",
    "\n",
    "Let's verify this by loading the SDXL pipeline and inspecting the encoders directly.\n",
    "\n",
    "**Before running, predict:**\n",
    "- SDXL has two text encoders. What class will each one be? (Hint: one is the standard CLIP text model, the other is an OpenCLIP variant.)\n",
    "- How many parameters will the larger encoder have compared to the smaller one?\n",
    "- What will the combined embedding shape be after concatenation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1: Load SDXL and inspect the dual text encoders\n",
    "# ============================================================\n",
    "\n",
    "# --- Step 1: Load the SDXL base pipeline ---\n",
    "print(\"Loading SDXL base pipeline...\")\n",
    "print(\"(This downloads ~6.5 GB on first run. Subsequent runs use the cache.)\")\n",
    "print()\n",
    "\n",
    "pipe_sdxl = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=dtype,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "print(\"SDXL base loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Inspect the two text encoders ---\n",
    "\n",
    "text_encoder_1 = pipe_sdxl.text_encoder\n",
    "text_encoder_2 = pipe_sdxl.text_encoder_2\n",
    "\n",
    "# Class names\n",
    "print(\"=== Text Encoder 1 (CLIP ViT-L) ===\")\n",
    "print(f\"  Class: {text_encoder_1.__class__.__name__}\")\n",
    "params_1 = sum(p.numel() for p in text_encoder_1.parameters())\n",
    "print(f\"  Parameters: {params_1:,} ({params_1 / 1e6:.1f}M)\")\n",
    "print()\n",
    "\n",
    "print(\"=== Text Encoder 2 (OpenCLIP ViT-bigG) ===\")\n",
    "print(f\"  Class: {text_encoder_2.__class__.__name__}\")\n",
    "params_2 = sum(p.numel() for p in text_encoder_2.parameters())\n",
    "print(f\"  Parameters: {params_2:,} ({params_2 / 1e6:.1f}M)\")\n",
    "print()\n",
    "\n",
    "print(f\"Encoder 2 is {params_2 / params_1:.1f}x larger than Encoder 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Step 3: Run a prompt through both encoders and check output shapes ---\n\nprompt = \"a cat sitting on a beach at sunset\"\n\n# Tokenize and encode with both encoders\ntokenizer_1 = pipe_sdxl.tokenizer\ntokenizer_2 = pipe_sdxl.tokenizer_2\n\n# Encoder 1: CLIP ViT-L\ntokens_1 = tokenizer_1(\n    prompt,\n    padding=\"max_length\",\n    max_length=tokenizer_1.model_max_length,\n    truncation=True,\n    return_tensors=\"pt\",\n).input_ids.to(device)\n\nwith torch.no_grad():\n    output_1 = text_encoder_1(tokens_1)\n    hidden_1 = output_1.last_hidden_state  # per-token embeddings\n\n# Encoder 2: OpenCLIP ViT-bigG\ntokens_2 = tokenizer_2(\n    prompt,\n    padding=\"max_length\",\n    max_length=tokenizer_2.model_max_length,\n    truncation=True,\n    return_tensors=\"pt\",\n).input_ids.to(device)\n\nwith torch.no_grad():\n    # output_hidden_states=True is needed to access the penultimate layer\n    output_2 = text_encoder_2(tokens_2, output_hidden_states=True)\n    hidden_2 = output_2.hidden_states[-2]  # penultimate layer (SDXL uses this)\n    pooled = output_2.text_embeds  # pooled output for global conditioning\n\nprint(f'Prompt: \"{prompt}\"')\nprint()\nprint(f\"Encoder 1 (CLIP ViT-L) output shape:       {list(hidden_1.shape)}\")\nprint(f\"Encoder 2 (OpenCLIP ViT-bigG) output shape: {list(hidden_2.shape)}\")\nprint()\n\n# Concatenate along the embedding dimension (dim=2)\ncombined = torch.cat([hidden_1, hidden_2], dim=-1)\nprint(f\"Concatenated embedding shape:                {list(combined.shape)}\")\nprint()\nprint(f\"Pooled embedding shape (global conditioning): {list(pooled.shape)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You loaded the SDXL base pipeline and inspected its dual text encoders. Here is what you should have observed:\n",
    "\n",
    "- **Text Encoder 1** is `CLIPTextModel`—the same CLIP ViT-L/14 from SD v1.5. It produces **[1, 77, 768]** embeddings. ~123M parameters. This is the encoder you already know.\n",
    "\n",
    "- **Text Encoder 2** is `CLIPTextModelWithProjection`—an OpenCLIP ViT-bigG/14 variant. It produces **[1, 77, 1280]** embeddings. ~695M parameters (about 5.6x larger). This is SDXL's addition.\n",
    "\n",
    "- **The concatenated embedding is [1, 77, 2048].** 768 + 1280 = 2048 dimensions per token. This is the K/V source for cross-attention. The cross-attention mechanism is unchanged from SD v1.5—it just reads from a wider embedding.\n",
    "\n",
    "- **The pooled embedding is [1, 1280].** A single vector summarizing the entire prompt, from Encoder 2. This is concatenated with the timestep embedding and injected through adaptive group normalization—the same global conditioning pathway as the timestep.\n",
    "\n",
    "This confirms the lesson's tensor shape trace:\n",
    "```\n",
    "CLIP ViT-L/14:     [77, 768]   ← SD v1.5's original encoder\n",
    "OpenCLIP ViT-bigG:  [77, 1280]  ← SDXL's addition\n",
    "Concatenated:       [77, 2048]  ← One embedding per token, richer\n",
    "```\n",
    "\n",
    "Two encoders, one cross-attention path. Not decoupled attention (like IP-Adapter)—concatenation before the bottleneck.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: SDXL Base Generation and Comparison `[Guided]`\n",
    "\n",
    "The lesson showed two descriptions of the same prompt at SD v1.5 vs SDXL quality. Now you will see it for real.\n",
    "\n",
    "We will generate \"a cat sitting on a beach at sunset\"—the same prompt from the SD v1.5 pipeline lesson—with SDXL at 1024×1024. Then we will vary `guidance_scale` to explore the richer conditioning.\n",
    "\n",
    "From the lesson: SDXL's dual text encoders provide better text understanding. The hypothesis is that with richer conditioning, you need *less* amplification from classifier-free guidance. SD v1.5 typically uses guidance_scale ~7.5. SDXL should work well at lower values.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Will the optimal guidance_scale for SDXL be higher or lower than SD v1.5's typical 7.5? Why? (Think about what guidance_scale does: it amplifies the difference between conditioned and unconditioned predictions. If the conditioned prediction is already better...)\n",
    "- At guidance_scale=10, will SDXL show oversaturation artifacts like SD v1.5 often does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 2: Generate with SDXL and compare guidance scales\n",
    "# ============================================================\n",
    "\n",
    "PROMPT = \"a cat sitting on a beach at sunset\"\n",
    "\n",
    "# Use DPM-Solver++ for faster generation (Level 1 acceleration—free speedup)\n",
    "pipe_sdxl.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "    pipe_sdxl.scheduler.config\n",
    ")\n",
    "\n",
    "# --- Generate at three guidance scales ---\n",
    "guidance_scales = [5.0, 7.5, 10.0]\n",
    "images = []\n",
    "times = []\n",
    "\n",
    "for gs in guidance_scales:\n",
    "    print(f\"Generating: guidance_scale={gs}...\")\n",
    "    img, elapsed = generate_sdxl(\n",
    "        pipe_sdxl,\n",
    "        PROMPT,\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=gs,\n",
    "    )\n",
    "    images.append(img)\n",
    "    times.append(elapsed)\n",
    "    print(f\"  Done in {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare all three side by side ---\n",
    "show_image_row(\n",
    "    images,\n",
    "    [\n",
    "        f\"guidance_scale=5.0\\n{times[0]:.1f}s\",\n",
    "        f\"guidance_scale=7.5\\n{times[1]:.1f}s\",\n",
    "        f\"guidance_scale=10.0\\n{times[2]:.1f}s\",\n",
    "    ],\n",
    "    suptitle=f'SDXL at 1024\\u00d71024: \"{PROMPT}\"',\n",
    "    figsize=(18, 6),\n",
    ")\n",
    "\n",
    "print(\"Compare across guidance scales:\")\n",
    "print(f\"  guidance_scale=5.0:  Clean, well-balanced. SDXL's typical sweet spot.\")\n",
    "print(f\"  guidance_scale=7.5:  Slightly more vivid. SD v1.5's default.\")\n",
    "print(f\"  guidance_scale=10.0: May show oversaturation or high-contrast artifacts.\")\n",
    "print()\n",
    "print(\"The dual text encoders provide richer conditioning,\")\n",
    "print(\"so less CFG amplification is needed. SDXL typically works\")\n",
    "print(\"best at guidance_scale ~5-7, lower than SD v1.5's ~7.5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You generated the same prompt that you traced through SD v1.5 in the pipeline lesson, now with SDXL at 1024×1024. Here is what to observe:\n",
    "\n",
    "- **The quality jump is dramatic.** Even at the same guidance_scale, SDXL produces sharper details, better text following, and more coherent composition. This comes from the combination of dual text encoders, higher resolution, and the larger U-Net.\n",
    "\n",
    "- **guidance_scale=5.0 often looks the best for SDXL.** The dual text encoders provide richer conditioning—the conditioned prediction is already more accurate. CFG amplifies the *difference* between conditioned and unconditioned. When the conditioned prediction is better, less amplification is needed to steer generation. SD v1.5 needed guidance_scale ~7.5 to compensate for its single encoder's limitations.\n",
    "\n",
    "- **guidance_scale=10.0 may show oversaturation.** Higher guidance pushes the prediction further from the unconditioned baseline. With SDXL's already-strong conditioning, this can overshoot—producing overly vivid colors, extreme contrast, or unnatural lighting. The same effect happens with SD v1.5 at high guidance, but SDXL reaches the oversaturation threshold sooner.\n",
    "\n",
    "- **This confirms the lesson's insight.** Better text encoders → better conditioned predictions → less need for CFG amplification. The connection between conditioning quality and optimal guidance scale is not just theoretical—you can see it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Micro-Conditioning Exploration `[Supported]`\n",
    "\n",
    "From the lesson: micro-conditioning tells the model about each training image's context. Three additional numbers are fed alongside the timestep:\n",
    "- **original_size**: the resolution of the training image before any resizing\n",
    "- **crops_coords_top_left**: where the crop was taken from\n",
    "- **target_size**: the resolution the model should generate\n",
    "\n",
    "At inference, setting `original_size=(1024, 1024)` and `crops_coords_top_left=(0, 0)` tells the model: \"generate as if the original image was high-resolution and well-centered.\"\n",
    "\n",
    "But what happens if you change these values? The model learned what low-resolution originals and off-center crops look like during training. We can exploit this to see micro-conditioning in action.\n",
    "\n",
    "Your task: generate the same prompt with three different micro-conditioning configurations and compare the outputs.\n",
    "\n",
    "Fill in the TODO markers to complete the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 3: Micro-conditioning exploration\n",
    "# ============================================================\n",
    "\n",
    "PROMPT = \"a cat sitting on a beach at sunset\"\n",
    "\n",
    "# --- Configuration 1: Default (best quality) ---\n",
    "# original_size=(1024, 1024), crops_coords_top_left=(0, 0)\n",
    "# This is the default: \"generate as if the original was high-res and well-framed.\"\n",
    "print(\"Generating: default micro-conditioning (high-quality)...\")\n",
    "img_default, time_default = generate_sdxl(\n",
    "    pipe_sdxl,\n",
    "    PROMPT,\n",
    "    num_inference_steps=30,\n",
    "    guidance_scale=5.0,\n",
    "    original_size=(1024, 1024),\n",
    "    crops_coords_top_left=(0, 0),\n",
    "    target_size=(1024, 1024),\n",
    ")\n",
    "print(f\"  Done in {time_default:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration 2: Simulate a low-resolution original ---\n",
    "# Setting original_size=(256, 256) tells the model:\n",
    "# \"generate as if the training image was a 256x256 thumbnail.\"\n",
    "# The model learned that 256x256 originals tend to be softer and less detailed.\n",
    "\n",
    "# TODO: Generate with original_size=(256, 256), crops_coords_top_left=(0, 0),\n",
    "#       target_size=(1024, 1024). Use the same prompt, steps, and guidance_scale\n",
    "#       as Configuration 1.\n",
    "# Hint: Call generate_sdxl with the appropriate micro-conditioning kwargs.\n",
    "raise NotImplementedError(\n",
    "    \"TODO: Generate with original_size=(256, 256). See the hint above.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration 3: Simulate an off-center crop ---\n",
    "# Setting crops_coords_top_left=(512, 512) tells the model:\n",
    "# \"generate as if this was cropped from the center of a much larger image.\"\n",
    "# The model learned that off-center crops often have unusual compositions.\n",
    "\n",
    "# TODO: Generate with original_size=(1024, 1024), crops_coords_top_left=(512, 512),\n",
    "#       target_size=(1024, 1024). Same prompt, steps, and guidance_scale.\n",
    "raise NotImplementedError(\n",
    "    \"TODO: Generate with crops_coords_top_left=(512, 512). See the hint above.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare all three side by side ---\n",
    "show_image_row(\n",
    "    [img_default, img_lowres, img_crop],\n",
    "    [\n",
    "        \"Default\\norig=(1024,1024)\\ncrop=(0,0)\",\n",
    "        \"Low-res original\\norig=(256,256)\\ncrop=(0,0)\",\n",
    "        \"Off-center crop\\norig=(1024,1024)\\ncrop=(512,512)\",\n",
    "    ],\n",
    "    suptitle=f'Micro-Conditioning Comparison: \"{PROMPT}\"',\n",
    "    figsize=(18, 6),\n",
    ")\n",
    "\n",
    "print(\"Compare the three outputs:\")\n",
    "print(\"  Default (left):    Best quality. High-res original, well-centered.\")\n",
    "print(\"  Low-res (middle):  Should appear softer or less detailed.\")\n",
    "print(\"                     The model generates 'as if' the original was 256x256.\")\n",
    "print(\"  Off-center (right): May show shifted composition or cropping artifacts.\")\n",
    "print(\"                     The model generates 'as if' this was a center crop.\")\n",
    "print()\n",
    "print(\"Micro-conditioning is not just metadata—it actively steers generation.\")\n",
    "print(\"The model learned to separate content from quality during training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The key insight is that micro-conditioning values are not suggestions—they are conditioning inputs that the model learned to respond to during training. Setting `original_size=(256, 256)` does not resize anything. It tells the model \"generate with the characteristics you learned from 256x256 training images\" (typically softer, less detailed). Setting `crops_coords_top_left=(512, 512)` tells the model \"generate with the characteristics you learned from center-cropped training images\" (potentially shifted composition).\n",
    "\n",
    "```python\n",
    "# Configuration 2: Low-resolution original\n",
    "print(\"Generating: low-resolution original micro-conditioning...\")\n",
    "img_lowres, time_lowres = generate_sdxl(\n",
    "    pipe_sdxl,\n",
    "    PROMPT,\n",
    "    num_inference_steps=30,\n",
    "    guidance_scale=5.0,\n",
    "    original_size=(256, 256),\n",
    "    crops_coords_top_left=(0, 0),\n",
    "    target_size=(1024, 1024),\n",
    ")\n",
    "print(f\"  Done in {time_lowres:.2f}s\")\n",
    "\n",
    "# Configuration 3: Off-center crop\n",
    "print(\"Generating: off-center crop micro-conditioning...\")\n",
    "img_crop, time_crop = generate_sdxl(\n",
    "    pipe_sdxl,\n",
    "    PROMPT,\n",
    "    num_inference_steps=30,\n",
    "    guidance_scale=5.0,\n",
    "    original_size=(1024, 1024),\n",
    "    crops_coords_top_left=(512, 512),\n",
    "    target_size=(1024, 1024),\n",
    ")\n",
    "print(f\"  Done in {time_crop:.2f}s\")\n",
    "```\n",
    "\n",
    "**Common mistakes:**\n",
    "- Confusing `original_size` with actually resizing the output. The output is always 1024x1024. `original_size` is a conditioning signal, not a resize parameter.\n",
    "- Forgetting `target_size=(1024, 1024)`. Without it, the pipeline uses the default, which is usually correct—but being explicit ensures the experiment is controlled.\n",
    "- Expecting dramatic visual differences. The effect can be subtle, especially for `crops_coords_top_left`. The model learned statistical tendencies, not absolute rules.\n",
    "\n",
    "</details>\n",
    "\n",
    "### What Just Happened\n",
    "\n",
    "You explored micro-conditioning by changing the metadata values the model uses as conditioning inputs:\n",
    "\n",
    "- **Default (original_size=1024, crop=(0,0))** produces the highest quality. This tells the model: \"the original training image was high-resolution and well-framed.\" The model generates accordingly.\n",
    "\n",
    "- **Low-resolution original (original_size=256)** may produce softer or less detailed output. The model learned during training that 256x256 images tend to have less fine detail. At inference, you are asking it to generate with those characteristics—not literally at 256x256, but with the quality profile it associates with small originals.\n",
    "\n",
    "- **Off-center crop (crop=(512,512))** may shift the composition. The model learned that training crops taken from off-center positions have different statistical properties (subjects at edges, asymmetric compositions). The effect varies by prompt—some prompts are more sensitive than others.\n",
    "\n",
    "- **The effects may be subtle.** Micro-conditioning represents statistical tendencies learned from millions of training images, not deterministic rules. The difference between `original_size=(1024, 1024)` and `original_size=(256, 256)` is a shift in the quality distribution, not a binary switch. This is exactly how adaptive norm conditioning works—continuous influence, not discrete control.\n",
    "\n",
    "- **This confirms the lesson's insight.** Without micro-conditioning, SDXL would have to either throw away low-resolution training data or accept that the model learns to produce artifacts. Micro-conditioning separates content from quality, letting the model train on diverse data while generating at the quality level you request.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Base + Refiner Pipeline `[Independent]`\n",
    "\n",
    "From the lesson: the SDXL refiner is a second U-Net, fine-tuned for the low-noise timesteps where fine detail matters. The two-model pipeline works like this:\n",
    "\n",
    "1. **Base model** denoises from t=T (pure noise) to t=t_switch—handles composition, structure, color\n",
    "2. **Refiner model** denoises from t=t_switch to t=0—handles fine detail, texture, sharpness\n",
    "\n",
    "This is the img2img mechanism you already know, applied with a specialized second model. The base model produces a partially denoised latent at t_switch. The refiner takes that latent and completes the denoising.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "1. **Free the base-only pipeline** from the previous exercises to make VRAM for the refiner\n",
    "2. **Load both the base pipeline and the refiner pipeline** (`StableDiffusionXLImg2ImgPipeline` for the refiner)\n",
    "3. **Generate base-only** at 40 steps as the comparison baseline\n",
    "4. **Generate base+refiner** with the base using `denoising_end` to stop early and the refiner using `denoising_start` to continue from there\n",
    "5. **Vary the handoff point (t_switch):** try giving the refiner 10%, 20%, and 40% of the denoising steps\n",
    "6. **Compare** base-only vs base+refiner outputs. Look for differences in fine detail, texture, and sharpness.\n",
    "7. **Bonus:** time the base-only and base+refiner pipelines to quantify the compute tradeoff\n",
    "\n",
    "### Hints\n",
    "\n",
    "- The refiner model is at `\"stabilityai/stable-diffusion-xl-refiner-1.0\"`\n",
    "- Use `StableDiffusionXLImg2ImgPipeline.from_pretrained(...)` for the refiner\n",
    "- The base pipeline's `denoising_end` parameter controls where it stops (e.g., `denoising_end=0.8` means stop at 80% completion)\n",
    "- The refiner's `denoising_start` parameter controls where it picks up (e.g., `denoising_start=0.8` means start at 80% completion)\n",
    "- The base must output latents, not images: use `output_type=\"latent\"` for the base when handing off to the refiner\n",
    "- Pass the base's latent output as the `image` argument to the refiner\n",
    "- Both pipelines should use the same prompt and seed for a fair comparison\n",
    "- On T4 GPUs, you may need to load the refiner with `torch_dtype=torch.float16` and `variant=\"fp16\"` to fit in memory. Consider using `.enable_model_cpu_offload()` instead of `.to(device)` for the refiner to save VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 4: Base + Refiner two-stage pipeline\n",
    "# ============================================================\n",
    "#\n",
    "# Free the pipeline from previous exercises first.\n",
    "# Then load both base and refiner, generate, and compare.\n",
    "#\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Your comparison visualization ---\n",
    "#\n",
    "# Use show_image_row() to display base-only vs base+refiner results.\n",
    "# Try multiple handoff points (denoising_end/denoising_start of 0.9, 0.8, 0.6)\n",
    "# and compare the fine detail differences.\n",
    "#\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The refiner is img2img with a specialized model. The base model generates the composition and structure (high-noise timesteps). The refiner polishes fine detail (low-noise timesteps). The `denoising_end` and `denoising_start` parameters control where the handoff happens.\n",
    "\n",
    "```python\n",
    "# --- Step 1: Free memory from previous exercises ---\n",
    "free_memory(pipe_sdxl)\n",
    "\n",
    "# --- Step 2: Load both base and refiner ---\n",
    "print(\"Loading SDXL base...\")\n",
    "base = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    ")\n",
    "base.enable_model_cpu_offload()\n",
    "\n",
    "print(\"Loading SDXL refiner...\")\n",
    "refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    ")\n",
    "refiner.enable_model_cpu_offload()\n",
    "print(\"Both models loaded.\")\n",
    "\n",
    "PROMPT = \"a cat sitting on a beach at sunset\"\n",
    "N_STEPS = 40\n",
    "\n",
    "# --- Step 3: Base-only generation (all 40 steps) ---\n",
    "print(\"\\nGenerating: base-only (40 steps)...\")\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(SEED)\n",
    "start = time.time()\n",
    "img_base_only = base(\n",
    "    prompt=PROMPT,\n",
    "    num_inference_steps=N_STEPS,\n",
    "    guidance_scale=5.0,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "time_base_only = time.time() - start\n",
    "print(f\"  Done in {time_base_only:.2f}s\")\n",
    "\n",
    "# --- Step 4: Base + Refiner at different handoff points ---\n",
    "handoff_points = [0.9, 0.8, 0.6]  # refiner gets 10%, 20%, 40% of steps\n",
    "refiner_images = []\n",
    "refiner_times = []\n",
    "\n",
    "for handoff in handoff_points:\n",
    "    pct = int((1 - handoff) * 100)\n",
    "    print(f\"\\nGenerating: base+refiner (refiner gets {pct}% of steps)...\")\n",
    "\n",
    "    # Base: denoise from noise to the handoff point\n",
    "    generator = torch.Generator(device=\"cpu\").manual_seed(SEED)\n",
    "    start = time.time()\n",
    "    latent = base(\n",
    "        prompt=PROMPT,\n",
    "        num_inference_steps=N_STEPS,\n",
    "        guidance_scale=5.0,\n",
    "        denoising_end=handoff,\n",
    "        output_type=\"latent\",\n",
    "        generator=generator,\n",
    "    ).images\n",
    "\n",
    "    # Refiner: pick up from the handoff point and denoise to completion\n",
    "    generator = torch.Generator(device=\"cpu\").manual_seed(SEED)\n",
    "    img_refined = refiner(\n",
    "        prompt=PROMPT,\n",
    "        num_inference_steps=N_STEPS,\n",
    "        guidance_scale=5.0,\n",
    "        denoising_start=handoff,\n",
    "        image=latent,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    refiner_images.append(img_refined)\n",
    "    refiner_times.append(elapsed)\n",
    "    print(f\"  Done in {elapsed:.2f}s\")\n",
    "\n",
    "# --- Step 5: Compare all results ---\n",
    "all_images = [img_base_only] + refiner_images\n",
    "all_titles = [\n",
    "    f\"Base only\\n{N_STEPS} steps | {time_base_only:.1f}s\",\n",
    "] + [\n",
    "    f\"Base+Refiner\\nrefiner gets {int((1-h)*100)}% | {t:.1f}s\"\n",
    "    for h, t in zip(handoff_points, refiner_times)\n",
    "]\n",
    "\n",
    "show_image_row(\n",
    "    all_images,\n",
    "    all_titles,\n",
    "    suptitle=f'Base vs Base+Refiner: \"{PROMPT}\"',\n",
    "    figsize=(22, 6),\n",
    ")\n",
    "\n",
    "print(\"Timing comparison:\")\n",
    "print(f\"  Base only:              {time_base_only:.2f}s\")\n",
    "for h, t in zip(handoff_points, refiner_times):\n",
    "    pct = int((1 - h) * 100)\n",
    "    print(f\"  Base+Refiner ({pct}%):     {t:.2f}s\")\n",
    "```\n",
    "\n",
    "**Key observations:**\n",
    "- The refiner adds polish, not fundamentally different content. Look at textures (fur, sand) and edges (cat outline, wave crests). The composition should be the same.\n",
    "- Giving the refiner too many steps (e.g., 40%) can sometimes *hurt* quality, because the refiner was trained for the low-noise regime and may struggle with higher-noise inputs.\n",
    "- The compute cost scales linearly: base+refiner takes roughly the time of base-only plus the refiner's steps. Two models, two forward-pass sets.\n",
    "- `enable_model_cpu_offload()` keeps models in CPU memory until needed, then loads them to GPU on demand. This trades speed for VRAM—necessary on T4 GPUs where loading both models simultaneously would exceed memory.\n",
    "\n",
    "**Common mistakes:**\n",
    "- Forgetting `output_type=\"latent\"` on the base when handing off to the refiner. Without this, the base decodes to pixels and you cannot pass it to the refiner.\n",
    "- Using different seeds for base and refiner. The seeds should match for reproducible comparison.\n",
    "- Not passing the prompt to the refiner. The refiner uses text conditioning during its denoising pass—without it, the detail polishing is semantically unguided.\n",
    "- Setting `denoising_start` to a different value than `denoising_end`. These must match for a clean handoff.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Dual text encoders are real and inspectable.** CLIP ViT-L produces [77, 768]. OpenCLIP ViT-bigG produces [77, 1280]. Concatenated: [77, 2048]. One cross-attention path, wider K/V source. The mechanism is unchanged from SD v1.5—the input is just richer.\n",
    "\n",
    "2. **Richer conditioning means lower optimal guidance.** SDXL works best at guidance_scale ~5–7, lower than SD v1.5's ~7.5. The dual encoders provide better conditioned predictions, so less CFG amplification is needed.\n",
    "\n",
    "3. **Micro-conditioning is not metadata—it is an active conditioning signal.** Setting `original_size=(256, 256)` tells the model to generate with the quality characteristics it learned from low-resolution training images. The model separates content from quality.\n",
    "\n",
    "4. **The refiner is img2img with a specialist.** Same mechanism you know from Img2img & Inpainting: take a partially denoised latent, complete the denoising with a specialized model. The base handles composition; the refiner polishes details. Optional but effective.\n",
    "\n",
    "5. **Every SDXL improvement is about what goes IN, AROUND, or ALONGSIDE the U-Net.** Dual encoders (in), refiner (around), micro-conditioning (alongside). The U-Net backbone itself is the same species as SD v1.5, just larger. The next lesson asks: what if you replaced it entirely?"
   ]
  }
 ]
}