{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Datasets and DataLoaders\n\nIn this notebook, you'll learn how PyTorch organizes data for training. Instead of manually slicing tensors into batches, you'll use `Dataset` and `DataLoader` â€” the standard abstraction that every PyTorch project uses.\n\n**What you'll do:**\n- Build a custom `Dataset` class from scratch\n- Load a real dataset (MNIST) with torchvision\n- Integrate `DataLoader` into a training loop\n- Experiment with batch sizes and observe the effects\n- Write a complete data pipeline for a CSV dataset\n\n**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones â€” they reveal gaps in your mental model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport io\nimport csv\n\n# Reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# For nice plots\nplt.style.use('dark_background')\nplt.rcParams['figure.figsize'] = [10, 4]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 1: Custom Dataset (Guided)\n\nA PyTorch `Dataset` is any class that implements three methods:\n- `__init__`: Generate or load data\n- `__len__`: Return the number of samples\n- `__getitem__`: Return a single sample by index\n\nYou'll create a `LinearDataset` that generates data from `y = 2x + 1 + noise`.\nThen wrap it in a `DataLoader` and iterate through one epoch.\n\n**Before running, predict:** If you create a `DataLoader` with `batch_size=16` over a dataset of 200 samples, how many batches will you get per epoch? Will the last batch be the same size as the others?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDataset(Dataset):\n",
    "    \"\"\"Dataset for y = 2x + 1 + noise.\"\"\"\n",
    "\n",
    "    def __init__(self, n_samples=200, noise_std=0.5):\n",
    "        torch.manual_seed(42)\n",
    "        self.x = torch.randn(n_samples, 1)\n",
    "        self.y = 2 * self.x + 1 + torch.randn(n_samples, 1) * noise_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = LinearDataset(n_samples=200)\n",
    "print(f'Dataset size: {len(dataset)}')\n",
    "print(f'Single sample: x={dataset[0][0].item():.4f}, y={dataset[0][1].item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap in a DataLoader\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Iterate one epoch and print batch shapes\n",
    "print(f'Number of batches per epoch: {len(loader)}')\n",
    "print(f'Batch size: 16, Dataset size: {len(dataset)}\\n')\n",
    "\n",
    "for i, (x_batch, y_batch) in enumerate(loader):\n",
    "    print(f'Batch {i+1:2d}: x shape = {x_batch.shape}, y shape = {y_batch.shape}')\n",
    "\n",
    "print(f'\\nTotal batches iterated: {i + 1}')\n",
    "print(f'Last batch size: {x_batch.shape[0]} (may be smaller if dataset size is not divisible by batch_size)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:**\n",
    "- `DataLoader` automatically split our 200 samples into batches of 16\n",
    "- Each batch is a tuple of `(x_batch, y_batch)` tensors\n",
    "- `shuffle=True` randomizes the order each epoch\n",
    "- The last batch may be smaller if `len(dataset) % batch_size != 0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 2: Loading MNIST (Guided)\n\nPyTorch's `torchvision.datasets` provides pre-built `Dataset` classes for common datasets. You don't need to write `__len__` or `__getitem__` â€” it's already done.\n\nLoad MNIST, apply standard transforms, and inspect what comes out.\n\n**Before running, predict:** After applying `ToTensor()` (scales to [0, 1]) and then `Normalize((0.1307,), (0.3081,))`, will the pixel values still be in [0, 1]? What range do you expect?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST with standard transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                          # PIL Image -> Tensor, scales to [0, 1]\n",
    "    transforms.Normalize((0.1307,), (0.3081,))      # Normalize with MNIST mean and std\n",
    "])\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f'Dataset size: {len(mnist_train)}')\n",
    "print(f'Single sample type: image={type(mnist_train[0][0])}, label={type(mnist_train[0][1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader and inspect one batch\n",
    "mnist_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "\n",
    "# Grab one batch\n",
    "images, labels = next(iter(mnist_loader))\n",
    "\n",
    "print(f'Batch image shape: {images.shape}')     # [64, 1, 28, 28]\n",
    "print(f'Batch label shape: {labels.shape}')      # [64]\n",
    "print(f'Image dtype: {images.dtype}')\n",
    "print(f'Label dtype: {labels.dtype}')\n",
    "print(f'Value range: min={images.min():.4f}, max={images.max():.4f}')\n",
    "print(f'\\nAfter normalization, values are centered around 0 (not [0, 1]).')\n",
    "print(f'Mean of batch: {images.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few samples\n",
    "fig, axes = plt.subplots(1, 8, figsize=(14, 2))\n",
    "for i in range(8):\n",
    "    axes[i].imshow(images[i].squeeze(), cmap='gray')\n",
    "    axes[i].set_title(f'Label: {labels[i].item()}')\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('MNIST Batch Samples', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:**\n",
    "- `ToTensor()` converted PIL images to float tensors in `[0, 1]`\n",
    "- `Normalize((0.1307,), (0.3081,))` shifted the distribution to roughly `[-0.4, 2.8]`\n",
    "- The DataLoader collated 64 individual `(image, label)` pairs into batched tensors\n",
    "- Image shape is `[batch, channels, height, width]` â€” PyTorch's standard format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 3: DataLoader in a Training Loop (Supported)\n\nNow integrate the `DataLoader` into a real training loop. Instead of passing the entire dataset as one tensor, you iterate over batches.\n\nThe template below trains `nn.Linear(1, 1)` on your `LinearDataset`. Fill in the parts marked `TODO`.\n\n<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe key insight is that the inner loop iterates over batches from the DataLoader, and each batch gets its own forward-backward-update cycle. This is different from Exercise 3 in the training loop notebook where the entire dataset was one tensor.\n\n```python\nfor x_batch, y_batch in train_loader:\n    predictions = model(x_batch)\n    loss = criterion(predictions, y_batch)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\nThe DataLoader handles all the batching and shuffling. Your training loop code is the same 5-step pattern regardless of how the data is organized.\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "train_dataset = LinearDataset(n_samples=200, noise_std=0.5)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = nn.Linear(1, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 20\n",
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:  # <-- DataLoader handles batching\n",
    "        # TODO: Forward pass\n",
    "        predictions = model(x_batch)\n",
    "\n",
    "        # TODO: Compute loss\n",
    "        loss = criterion(predictions, y_batch)\n",
    "\n",
    "        # TODO: Backward pass and update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = running_loss / n_batches\n",
    "    epoch_losses.append(avg_loss)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1:2d}/{n_epochs} | Avg Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Check learned parameters\n",
    "w = model.weight.item()\n",
    "b = model.bias.item()\n",
    "print(f'\\nLearned: y = {w:.4f}x + {b:.4f}')\n",
    "print(f'Target:  y = 2.0000x + 1.0000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot(range(1, n_epochs + 1), epoch_losses, 'o-', linewidth=2, markersize=4)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss (MSE)')\n",
    "plt.title('Training Loss with DataLoader')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key difference from raw tensor training:**\n",
    "- The inner `for x_batch, y_batch in train_loader` replaces manual slicing\n",
    "- Each epoch sees the data in a different random order (because `shuffle=True`)\n",
    "- The model updates multiple times per epoch (once per batch), not once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 4: Batch Size Experiments (Supported)\n\nHow does batch size affect training? You'll train the same model 4 times with different batch sizes:\n- **Batch size 1** (stochastic gradient descent)\n- **Batch size 32** (mini-batch â€” the default for most projects)\n- **Batch size 256**\n- **Full batch** (the entire dataset as one batch)\n\nFor each, record:\n1. How many iterations (gradient updates) per epoch\n2. The loss curve over 20 epochs\n\n<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe insight is that batch size controls a fundamental trade-off: smaller batches mean noisier gradient estimates but more parameter updates per epoch, while larger batches mean cleaner gradients but fewer updates.\n\n```python\nfor bs in batch_sizes:\n    loader = DataLoader(dataset, batch_size=bs, shuffle=True)\n    # Same training loop as before â€” only the DataLoader changes\n```\n\nWith batch_size=1 you get 200 updates per epoch (one per sample), while full-batch gives you 1 update per epoch. Mini-batch (32) is the standard compromise: enough noise for exploration, enough signal for stable progress.\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dataset = LinearDataset(n_samples=200, noise_std=0.5)\n",
    "\n",
    "batch_sizes = [1, 32, 256, len(experiment_dataset)]  # SGD, mini-batch, large-batch, full-batch\n",
    "results = {}\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    # Fresh model for each experiment\n",
    "    torch.manual_seed(0)\n",
    "    model = nn.Linear(1, 1)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    loader = DataLoader(experiment_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "    epoch_losses = []\n",
    "    iters_per_epoch = len(loader)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for x_batch, y_batch in loader:\n",
    "            predictions = model(x_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        epoch_losses.append(running_loss / n_batches)\n",
    "\n",
    "    label = f'bs={bs}' if bs < len(experiment_dataset) else 'full-batch'\n",
    "    results[label] = {\n",
    "        'losses': epoch_losses,\n",
    "        'iters_per_epoch': iters_per_epoch,\n",
    "        'final_loss': epoch_losses[-1],\n",
    "    }\n",
    "\n",
    "    print(f'Batch size {bs:>4d} | Iters/epoch: {iters_per_epoch:>4d} | Final loss: {epoch_losses[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all loss curves on one plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "for label, data in results.items():\n",
    "    axes[0].plot(range(1, 21), data['losses'], 'o-', label=label, linewidth=2, markersize=3)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Average Loss (MSE)')\n",
    "axes[0].set_title('Loss Curves by Batch Size')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Iterations per epoch comparison\n",
    "labels = list(results.keys())\n",
    "iters = [results[l]['iters_per_epoch'] for l in labels]\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(labels)))\n",
    "bars = axes[1].bar(labels, iters, color=colors, edgecolor='white', linewidth=0.5)\n",
    "axes[1].set_ylabel('Iterations per Epoch')\n",
    "axes[1].set_title('Gradient Updates per Epoch')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "for bar, val in zip(bars, iters):\n",
    "    axes[1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,\n",
    "                 str(val), ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you should observe:**\n",
    "\n",
    "| Batch Size | Iters/Epoch | Noise in Gradient | Convergence |\n",
    "|------------|-------------|-------------------|-------------|\n",
    "| 1 (SGD) | 200 | Very high | Fast early, noisy |\n",
    "| 32 | ~7 | Moderate | Good balance |\n",
    "| 256 | 1 | Low | Smooth but slow |\n",
    "| Full batch | 1 | None | Smoothest, can undershoot |\n",
    "\n",
    "- **Small batches** = noisy gradients = irregular loss curves, but more updates per epoch\n",
    "- **Large batches** = clean gradients = smooth loss curves, but fewer updates per epoch\n",
    "- **Mini-batch (32)** is the standard trade-off: enough noise for exploration, enough signal for progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 5: Custom Dataset for CSV Data (Independent)\n\nReal data often comes from CSV files. Here you'll:\n1. Create a small CSV dataset (inline using `io.StringIO`)\n2. Write a custom `Dataset` that reads it and converts to tensors\n3. Train a model with `DataLoader`\n\nThe dataset has 3 feature columns and 1 target column.\n\n<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe key insight is that a custom Dataset just needs to parse the data in `__init__` and return tensors from `__getitem__`. The CSV parsing is standard Python â€” the PyTorch-specific part is converting to tensors and implementing the three required methods.\n\n```python\nclass CSVDataset(Dataset):\n    def __init__(self, csv_string):\n        reader = csv.reader(io.StringIO(csv_string))\n        header = next(reader)  # skip header\n        rows = [[float(val) for val in row] for row in reader]\n        data = torch.tensor(rows, dtype=torch.float32)\n        self.features = data[:, :-1]  # all columns except last\n        self.targets = data[:, -1:]   # last column\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        return self.features[idx], self.targets[idx]\n```\n\nOnce you have the Dataset, the DataLoader and training loop are identical to what you've already written. The Dataset abstraction separates \"how to load one sample\" from \"how to batch and iterate.\"\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small CSV dataset inline\n",
    "# Target = 0.5 * feature_1 + 1.2 * feature_2 - 0.8 * feature_3 + 3.0 + noise\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 300\n",
    "features = np.random.randn(n_samples, 3)\n",
    "target = 0.5 * features[:, 0] + 1.2 * features[:, 1] - 0.8 * features[:, 2] + 3.0\n",
    "target += np.random.randn(n_samples) * 0.3  # noise\n",
    "\n",
    "# Build CSV string\n",
    "csv_buffer = io.StringIO()\n",
    "writer = csv.writer(csv_buffer)\n",
    "writer.writerow(['feature_1', 'feature_2', 'feature_3', 'target'])\n",
    "for i in range(n_samples):\n",
    "    writer.writerow([f'{features[i, 0]:.4f}', f'{features[i, 1]:.4f}',\n",
    "                     f'{features[i, 2]:.4f}', f'{target[i]:.4f}'])\n",
    "\n",
    "csv_string = csv_buffer.getvalue()\n",
    "print('First 5 lines of CSV:')\n",
    "print('\\n'.join(csv_string.strip().split('\\n')[:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    \"\"\"Custom Dataset that reads from a CSV string.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_string):\n",
    "        reader = csv.reader(io.StringIO(csv_string))\n",
    "        header = next(reader)  # skip header\n",
    "\n",
    "        rows = []\n",
    "        for row in reader:\n",
    "            rows.append([float(val) for val in row])\n",
    "\n",
    "        data = torch.tensor(rows, dtype=torch.float32)\n",
    "        self.features = data[:, :-1]  # all columns except last\n",
    "        self.targets = data[:, -1:]   # last column\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "# Create dataset and verify\n",
    "csv_dataset = CSVDataset(csv_string)\n",
    "print(f'Dataset size: {len(csv_dataset)}')\n",
    "print(f'Feature shape: {csv_dataset[0][0].shape}')\n",
    "print(f'Target shape:  {csv_dataset[0][1].shape}')\n",
    "\n",
    "# Create DataLoader\n",
    "csv_loader = DataLoader(csv_dataset, batch_size=32, shuffle=True)\n",
    "x_batch, y_batch = next(iter(csv_loader))\n",
    "print(f'\\nBatch feature shape: {x_batch.shape}')  # [32, 3]\n",
    "print(f'Batch target shape:  {y_batch.shape}')     # [32, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a small model on the CSV data\n",
    "torch.manual_seed(42)\n",
    "model = nn.Linear(3, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs = 30\n",
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for x_batch, y_batch in csv_loader:\n",
    "        predictions = model(x_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = running_loss / n_batches\n",
    "    epoch_losses.append(avg_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1:2d}/{n_epochs} | Avg Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Check learned parameters vs true\n",
    "w = model.weight.data.squeeze().tolist()\n",
    "b = model.bias.item()\n",
    "print(f'\\nLearned weights: [{w[0]:.4f}, {w[1]:.4f}, {w[2]:.4f}], bias: {b:.4f}')\n",
    "print(f'True weights:    [0.5000, 1.2000, -0.8000], bias: 3.0000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot(range(1, n_epochs + 1), epoch_losses, 'o-', linewidth=2, markersize=4)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss (MSE)')\n",
    "plt.title('Training on CSV Dataset')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **`Dataset`** defines *what* your data is: implement `__len__` and `__getitem__`\n2. **`DataLoader`** defines *how* to serve it: batching, shuffling, parallelism\n3. **The training loop iterates over `DataLoader`**, not raw tensors â€” this is the PyTorch standard\n4. **Batch size** controls the noise/speed trade-off: small batches = noisy but frequent updates, large batches = smooth but few updates\n5. **`torchvision.datasets`** provides ready-made `Dataset` classes for common benchmarks (MNIST, CIFAR, ImageNet, etc.)\n6. **The same `Dataset`/`DataLoader` pattern works** whether your data is synthetic, from a CSV, from images on disk, or from a database"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}