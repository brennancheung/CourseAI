{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Flow Matching\n\n**Module 7.2, Lesson 2** | CourseAI\n\nYou know the theory from the lesson: flow matching replaces curved diffusion trajectories with straight lines, uses velocity prediction instead of noise prediction, and produces a simpler training objective. This notebook makes that concrete.\n\n**What you will do:**\n- Implement both DDPM and flow matching interpolation in 1D, plot the paths and velocity profiles\n- Apply Euler's method on curved vs straight 2D trajectories and see why straight paths need fewer steps\n- Train a flow matching model on a 2D distribution (two-moons) and generate samples via Euler ODE solving\n- Compare flow matching vs DDPM head-to-head: sample quality at varying step counts (1, 5, 10, 20, 50)\n\n**For each exercise, PREDICT the output before running the cell.**\n\nEvery concept in this notebook comes from the lesson. Straight-line interpolation, constant velocity, velocity prediction, the connection to Euler's method. No new theory—just hands-on practice with the math and models.\n\n**Estimated time:** 30–45 minutes. Exercises 1–2 are pure math (no training). Exercises 3–4 train small MLPs on 2D data (~1–2 minutes on CPU)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\nRun this cell to import everything and configure the environment.\n\nNo GPU required for this notebook. Everything runs on CPU. The models in Exercises 3–4 are tiny MLPs trained on 2D point distributions."
  },
  {
   "cell_type": "code",
   "source": "!pip install -q torch numpy matplotlib scikit-learn",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print('Setup complete. No GPU needed for this notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Shared Helpers\n\nA small MLP for velocity prediction (flow matching) and noise prediction (DDPM), plus data generation utilities. The same architecture works for both—the only difference is the training target.\n\nRun this cell now. It defines everything needed for Exercises 3 and 4."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Shared: MLP for 2D generative models (flow matching + DDPM)\n",
    "# ============================================================\n",
    "# The same architecture predicts either velocity (flow matching)\n",
    "# or noise (DDPM). The network takes (x_t, t) and outputs a 2D vector.\n",
    "# What that vector *means* depends on the training objective.\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    \"\"\"MLP that takes (x_t, t) and outputs a 2D vector.\n",
    "    \n",
    "    For flow matching: output = predicted velocity v_theta(x_t, t)\n",
    "    For DDPM: output = predicted noise epsilon_theta(x_t, t)\n",
    "    Same architecture, different training target.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # Sinusoidal time embedding (simple but effective)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        # Main network\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2 + hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        \"\"\"Predict a 2D vector from (x_t, t).\n",
    "        \n",
    "        x_t: (batch, 2) -- noisy/interpolated point\n",
    "        t: (batch, 1) or (batch,) -- timestep in [0, 1]\n",
    "        \"\"\"\n",
    "        if t.dim() == 1:\n",
    "            t = t.unsqueeze(-1)\n",
    "        t_emb = self.time_mlp(t)\n",
    "        inp = torch.cat([x_t, t_emb], dim=-1)\n",
    "        return self.net(inp)\n",
    "\n",
    "\n",
    "def sample_two_moons(n, noise=0.06):\n",
    "    \"\"\"Sample from the two-moons distribution.\n",
    "    \n",
    "    Returns a (n, 2) tensor of 2D points.\n",
    "    The distribution has two crescent-shaped clusters.\n",
    "    \"\"\"\n",
    "    data, _ = make_moons(n_samples=n, noise=noise)\n",
    "    # Center and scale\n",
    "    data = (data - data.mean(axis=0)) * 2.0\n",
    "    return torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def make_cosine_schedule(T=100):\n",
    "    \"\"\"Cosine noise schedule for DDPM.\n",
    "    \n",
    "    Returns alpha_bars: (T,) tensor where alpha_bars[t] is the\n",
    "    cumulative signal retention at timestep t.\n",
    "    \"\"\"\n",
    "    steps = torch.arange(T + 1, dtype=torch.float32) / T\n",
    "    f = torch.cos((steps + 0.008) / 1.008 * (np.pi / 2)) ** 2\n",
    "    alpha_bars = f[1:] / f[0]  # Normalize so alpha_bar[0] is close to 1\n",
    "    alpha_bars = alpha_bars.clamp(min=1e-5, max=1.0 - 1e-5)\n",
    "    return alpha_bars\n",
    "\n",
    "\n",
    "print('Shared helpers defined.')\n",
    "print('- ToyModel: MLP that takes (x_t, t) -> 2D vector')\n",
    "print('- sample_two_moons: 2D two-moons distribution')\n",
    "print('- make_cosine_schedule: cosine noise schedule for DDPM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Flow Matching vs DDPM Interpolation `[Guided]`\n",
    "\n",
    "From the lesson: DDPM uses $x_t = \\sqrt{\\bar\\alpha_t}\\, x_0 + \\sqrt{1-\\bar\\alpha_t}\\, \\epsilon$ with nonlinear coefficients. Flow matching uses $x_t = (1-t)\\, x_0 + t\\, \\epsilon$ with linear coefficients. The DDPM path curves; the flow matching path is a straight line.\n",
    "\n",
    "We will implement both interpolation schemes in 1D:\n",
    "- Start at $x_0 = 5.0$ (data point) and interpolate toward $\\epsilon = -2.0$ (noise sample)\n",
    "- Plot the interpolation path $x_t$ vs time for both methods\n",
    "- Plot the \"velocity\" (derivative $dx_t/dt$) along each path\n",
    "\n",
    "**Before running, predict:**\n",
    "- Which path will be a straight line on the plot? Which will curve?\n",
    "- For the flow matching path, will the velocity be constant or varying?\n",
    "- For the DDPM path, at what point in time will the velocity change the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1: Compare DDPM vs Flow Matching interpolation in 1D\n",
    "# ============================================================\n",
    "\n",
    "x_0 = 5.0   # Data point\n",
    "eps = -2.0   # Noise sample\n",
    "\n",
    "# Time grid (0 to 1)\n",
    "t = torch.linspace(0, 1, 200)\n",
    "\n",
    "# --- Flow Matching interpolation ---\n",
    "# x_t = (1 - t) * x_0 + t * epsilon\n",
    "# This is a straight line from x_0 (at t=0) to epsilon (at t=1)\n",
    "fm_path = (1 - t) * x_0 + t * eps\n",
    "\n",
    "# Flow matching velocity: dx/dt = epsilon - x_0 (constant!)\n",
    "fm_velocity = torch.full_like(t, eps - x_0)  # -7.0 everywhere\n",
    "\n",
    "# --- DDPM interpolation (with cosine schedule) ---\n",
    "# x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
    "# The coefficients are nonlinear functions of t\n",
    "T_steps = 200\n",
    "alpha_bars_1d = make_cosine_schedule(T_steps)\n",
    "\n",
    "# Map t in [0, 1] to the discrete schedule\n",
    "# t=0 -> alpha_bar near 1 (clean), t=1 -> alpha_bar near 0 (noisy)\n",
    "indices = (t * (T_steps - 1)).long().clamp(0, T_steps - 1)\n",
    "ab = alpha_bars_1d[indices]\n",
    "\n",
    "ddpm_path = torch.sqrt(ab) * x_0 + torch.sqrt(1 - ab) * eps\n",
    "\n",
    "# DDPM \"velocity\" (numerical derivative)\n",
    "ddpm_velocity = torch.zeros_like(t)\n",
    "dt = t[1] - t[0]\n",
    "ddpm_velocity[1:-1] = (ddpm_path[2:] - ddpm_path[:-2]) / (2 * dt)\n",
    "ddpm_velocity[0] = (ddpm_path[1] - ddpm_path[0]) / dt\n",
    "ddpm_velocity[-1] = (ddpm_path[-1] - ddpm_path[-2]) / dt\n",
    "\n",
    "print(f'Data point x_0 = {x_0}, noise sample epsilon = {eps}')\n",
    "print(f'Flow matching velocity = epsilon - x_0 = {eps - x_0} (constant)')\n",
    "print(f'DDPM velocity varies from {ddpm_velocity[1]:.2f} to {ddpm_velocity[-2]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot: interpolation paths and velocity profiles side by side\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# --- Left panel: Interpolation paths ---\nax = axes[0]\nax.plot(t.numpy(), fm_path.numpy(), color='#34d399', linewidth=2.5, label='Flow Matching: $x_t = (1-t)\\,x_0 + t\\,\\epsilon$')\nax.plot(t.numpy(), ddpm_path.numpy(), color='#f59e0b', linewidth=2.5, label='DDPM: $x_t = \\sqrt{\\\\bar\\\\alpha_t}\\,x_0 + \\sqrt{1-\\\\bar\\\\alpha_t}\\,\\epsilon$')\nax.axhline(x_0, color='white', linestyle=':', alpha=0.3, label=f'$x_0$ = {x_0}')\nax.axhline(eps, color='white', linestyle=':', alpha=0.3, label=f'$\\epsilon$ = {eps}')\nax.set_xlabel('Time $t$', fontsize=12)\nax.set_ylabel('$x_t$', fontsize=12)\nax.set_title('Interpolation Path: Data to Noise', fontsize=13)\nax.legend(fontsize=9)\nax.grid(alpha=0.2)\n\n# --- Right panel: Velocity profiles ---\nax = axes[1]\nax.plot(t.numpy(), fm_velocity.numpy(), color='#34d399', linewidth=2.5, label='Flow Matching velocity (constant)')\nax.plot(t.numpy(), ddpm_velocity.numpy(), color='#f59e0b', linewidth=2.5, label='DDPM velocity (varies with $t$)')\nax.axhline(0, color='white', linewidth=0.5, alpha=0.3)\nax.set_xlabel('Time $t$', fontsize=12)\nax.set_ylabel('$dx_t / dt$', fontsize=12)\nax.set_title('Velocity Along the Path', fontsize=13)\nax.legend(fontsize=9)\nax.grid(alpha=0.2)\n\nplt.tight_layout()\nplt.show()\n\nprint('Observations:')\nprint('- Left panel: The flow matching path (green) is a perfect straight line.')\nprint('  The DDPM path (amber) curves\\u2014it stays near x_0 for a while, then')\nprint('  drops rapidly toward epsilon. This is because the cosine schedule')\nprint('  keeps alpha_bar near 1 (mostly signal) at early timesteps.')\nprint()\nprint('- Right panel: The flow matching velocity is constant (-7.0 everywhere).')\nprint('  The DDPM velocity changes dramatically\\u2014slow at first, fast in the middle,')\nprint('  then slow again. This CHANGING velocity is what creates curvature.')\nprint('  A network trying to predict DDPM velocity would need to learn a complex')\nprint('  time-dependent function. Flow matching velocity is trivial: one number.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### What Just Happened\n\nYou implemented both interpolation schemes and saw the core difference:\n\n- **Flow matching path is a straight line.** The linear coefficients $(1-t)$ and $t$ produce a path from $x_0$ to $\\epsilon$ with zero curvature. The velocity is constant: $v = \\epsilon - x_0 = -7.0$ at every timestep.\n\n- **DDPM path curves.** The nonlinear coefficients $\\sqrt{\\bar\\alpha_t}$ and $\\sqrt{1-\\bar\\alpha_t}$ from the cosine schedule create a path that lingers near $x_0$ (the schedule keeps $\\bar\\alpha_t$ near 1 for a while), then drops rapidly toward $\\epsilon$. The velocity changes dramatically with time.\n\n- **Changing velocity = curvature.** The DDPM path has a velocity that varies from slow to fast to slow. This is exactly what creates curvature in the trajectory. An ODE solver following this path must adjust to the changing velocity at every step. On the flow matching path, the velocity never changes—there is nothing to adjust to.\n\nThis is the interpolation difference from the lesson: same idea (interpolate between data and noise), simpler coefficients (linear instead of nonlinear), straight path instead of curved.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 2: Euler Steps on Curved vs Straight Paths `[Guided]`\n\nFrom the lesson: Euler's method computes the direction at the current point and takes a step. On a curved path, the trajectory bends after each step, causing the Euler approximation to drift off course. On a straight path, Euler's extrapolation is exact—the true trajectory IS a straight line, so linear extrapolation is perfect.\n\nWe will create two 2D ODEs:\n1. A **curved** trajectory (a spiral) where the velocity field changes direction\n2. A **straight** trajectory (a line) where the velocity is constant\n\nThen apply Euler's method with $N=5$ steps and $N=1$ step to both, and compare to the true solution.\n\n**Before running, predict:**\n- With $N=5$ Euler steps on the straight path, how large will the error be?\n- With $N=1$ Euler step on the straight path, will the error increase compared to $N=5$?\n- On the curved path, which will have more error: $N=5$ or $N=1$?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 2: Euler's method on curved vs straight trajectories\n",
    "# ============================================================\n",
    "\n",
    "# --- Define two ODE systems ---\n",
    "#\n",
    "# Curved ODE: dx/dt = A * x where A is a rotation matrix\n",
    "# This produces a spiral trajectory (the velocity direction rotates).\n",
    "#\n",
    "# Straight ODE: dx/dt = v (constant velocity)\n",
    "# This produces a straight-line trajectory.\n",
    "\n",
    "# Starting point (same for both)\n",
    "x_start = torch.tensor([2.0, 0.0])\n",
    "\n",
    "# --- Curved ODE: spiral ---\n",
    "# dx/dt = A * x, where A produces rotation + slight contraction\n",
    "theta_rate = 3.0  # Rotation speed (radians per unit time)\n",
    "contraction = -0.5  # Slight inward pull\n",
    "\n",
    "def curved_velocity(x, t):\n",
    "    \"\"\"Velocity field for the curved (spiral) ODE.\"\"\"\n",
    "    # Rotation + contraction matrix\n",
    "    A = torch.tensor([\n",
    "        [contraction, -theta_rate],\n",
    "        [theta_rate, contraction]\n",
    "    ])\n",
    "    return A @ x\n",
    "\n",
    "# Analytical solution for the curved ODE: x(t) = exp(A*t) * x_start\n",
    "# For rotation matrix: x(t) = exp(contraction*t) * R(theta_rate*t) * x_start\n",
    "def curved_exact(t_val):\n",
    "    \"\"\"Exact solution of the spiral ODE at time t.\"\"\"\n",
    "    scale = np.exp(contraction * t_val)\n",
    "    angle = theta_rate * t_val\n",
    "    R = torch.tensor([\n",
    "        [np.cos(angle), -np.sin(angle)],\n",
    "        [np.sin(angle), np.cos(angle)]\n",
    "    ])\n",
    "    return scale * (R @ x_start)\n",
    "\n",
    "# --- Straight ODE: constant velocity ---\n",
    "target_point = torch.tensor([-1.0, 2.0])\n",
    "straight_v = target_point - x_start  # Constant velocity\n",
    "\n",
    "def straight_velocity(x, t):\n",
    "    \"\"\"Velocity field for the straight-line ODE.\"\"\"\n",
    "    return straight_v\n",
    "\n",
    "def straight_exact(t_val):\n",
    "    \"\"\"Exact solution of the straight-line ODE at time t.\"\"\"\n",
    "    return x_start + t_val * straight_v\n",
    "\n",
    "# --- Euler's method ---\n",
    "def euler_solve(velocity_fn, x_init, T_end, n_steps):\n",
    "    \"\"\"Solve an ODE using Euler's method.\n",
    "    \n",
    "    Returns the trajectory as a list of points.\n",
    "    \"\"\"\n",
    "    dt = T_end / n_steps\n",
    "    trajectory = [x_init.clone()]\n",
    "    x = x_init.clone()\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        t = i * dt\n",
    "        v = velocity_fn(x, t)\n",
    "        x = x + dt * v\n",
    "        trajectory.append(x.clone())\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "\n",
    "T_end = 1.0  # Integrate from t=0 to t=1\n",
    "\n",
    "# Exact solutions (dense, for plotting)\n",
    "t_dense = np.linspace(0, T_end, 200)\n",
    "curved_true = torch.stack([curved_exact(t) for t in t_dense])\n",
    "straight_true = torch.stack([straight_exact(t) for t in t_dense])\n",
    "\n",
    "# Euler solutions at different step counts\n",
    "step_counts = [5, 1]\n",
    "curved_eulers = {n: euler_solve(curved_velocity, x_start, T_end, n) for n in step_counts}\n",
    "straight_eulers = {n: euler_solve(straight_velocity, x_start, T_end, n) for n in step_counts}\n",
    "\n",
    "# Compute endpoint errors\n",
    "curved_true_end = curved_exact(T_end)\n",
    "straight_true_end = straight_exact(T_end)\n",
    "\n",
    "print('Endpoint errors:')\n",
    "print(f'  True curved endpoint:   [{curved_true_end[0]:.4f}, {curved_true_end[1]:.4f}]')\n",
    "print(f'  True straight endpoint: [{straight_true_end[0]:.4f}, {straight_true_end[1]:.4f}]')\n",
    "print()\n",
    "for n in step_counts:\n",
    "    c_end = curved_eulers[n][-1]\n",
    "    s_end = straight_eulers[n][-1]\n",
    "    c_err = torch.norm(c_end - curved_true_end).item()\n",
    "    s_err = torch.norm(s_end - straight_true_end).item()\n",
    "    print(f'  N={n} steps:')\n",
    "    print(f'    Curved path error:   {c_err:.6f}')\n",
    "    print(f'    Straight path error: {s_err:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot Euler's method on both trajectories at N=5 and N=1\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\nfor row, n_steps in enumerate(step_counts):\n    # --- Curved trajectory ---\n    ax = axes[row, 0]\n    # True trajectory\n    ax.plot(curved_true[:, 0], curved_true[:, 1], color='white', linewidth=1.5,\n            alpha=0.4, label='True trajectory', linestyle='--')\n    # Euler approximation\n    euler_pts = torch.stack(curved_eulers[n_steps])\n    ax.plot(euler_pts[:, 0], euler_pts[:, 1], 'o-', color='#f59e0b', linewidth=2,\n            markersize=8, label=f'Euler ({n_steps} steps)')\n    # Mark start and endpoints\n    ax.plot(x_start[0], x_start[1], 's', color='#60a5fa', markersize=10, zorder=5, label='Start')\n    ax.plot(curved_true_end[0], curved_true_end[1], '*', color='#34d399', markersize=15, zorder=5, label='True end')\n    # Error line\n    euler_end = curved_eulers[n_steps][-1]\n    err = torch.norm(euler_end - curved_true_end).item()\n    ax.plot([euler_end[0], curved_true_end[0]], [euler_end[1], curved_true_end[1]],\n            color='#ef4444', linewidth=2, linestyle='-', label=f'Error: {err:.4f}')\n    ax.set_title(f'Curved Path (Spiral)\\u2014{n_steps} Euler Step{\"s\" if n_steps > 1 else \"\"}', fontsize=12)\n    ax.legend(fontsize=8)\n    ax.set_aspect('equal')\n    ax.grid(alpha=0.2)\n\n    # --- Straight trajectory ---\n    ax = axes[row, 1]\n    # True trajectory\n    ax.plot(straight_true[:, 0], straight_true[:, 1], color='white', linewidth=1.5,\n            alpha=0.4, label='True trajectory', linestyle='--')\n    # Euler approximation\n    euler_pts = torch.stack(straight_eulers[n_steps])\n    ax.plot(euler_pts[:, 0], euler_pts[:, 1], 'o-', color='#34d399', linewidth=2,\n            markersize=8, label=f'Euler ({n_steps} steps)')\n    # Mark start and endpoints\n    ax.plot(x_start[0], x_start[1], 's', color='#60a5fa', markersize=10, zorder=5, label='Start')\n    ax.plot(straight_true_end[0], straight_true_end[1], '*', color='#34d399', markersize=15, zorder=5, label='True end')\n    # Error line\n    euler_end = straight_eulers[n_steps][-1]\n    err = torch.norm(euler_end - straight_true_end).item()\n    ax.plot([euler_end[0], straight_true_end[0]], [euler_end[1], straight_true_end[1]],\n            color='#ef4444', linewidth=2, linestyle='-', label=f'Error: {err:.6f}')\n    ax.set_title(f'Straight Path (Line)\\u2014{n_steps} Euler Step{\"s\" if n_steps > 1 else \"\"}', fontsize=12)\n    ax.legend(fontsize=8)\n    ax.set_aspect('equal')\n    ax.grid(alpha=0.2)\n\nplt.suptitle(\n    'Euler\\'s Method: Curved Path (left) vs Straight Path (right)\\n'\n    'Top: 5 steps | Bottom: 1 step',\n    fontsize=13, y=1.02\n)\nplt.tight_layout()\nplt.show()\n\nprint('Key observations:')\nprint()\nprint('- CURVED path, 5 steps: Euler drifts off the true spiral. Each step')\nprint('  overshoots because the trajectory curves after the step. Visible error.')\nprint()\nprint('- CURVED path, 1 step: Euler goes way off course. A single linear')\nprint('  extrapolation completely misses the spiral. Large error.')\nprint()\nprint('- STRAIGHT path, 5 steps: Euler lands EXACTLY on the true endpoint.')\nprint('  Error is essentially zero (floating point precision only).')\nprint()\nprint('- STRAIGHT path, 1 step: STILL exact! One Euler step from start to')\nprint('  end is perfect because the true trajectory IS a straight line.')\nprint('  Euler extrapolates linearly, and the trajectory is linear. Match.')\nprint()\nprint('THIS is why flow matching needs fewer steps. The trajectory is straight,')\nprint('so Euler\\'s method (or any ODE solver) gets it right with minimal steps.')\nprint('On curved diffusion trajectories, more steps are always needed to')\nprint('compensate for the curvature error.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### What Just Happened\n\nYou applied Euler's method to curved and straight trajectories and verified the lesson's key claim:\n\n- **On a straight path, Euler is exact.** Even with a single step, Euler's linear extrapolation lands exactly on the true endpoint. The error is zero (up to floating point). This is because Euler's method assumes the trajectory continues in a straight line from the current point—and it does.\n\n- **On a curved path, Euler accumulates error.** Each step overshoots because the trajectory bends after the step. More curvature means more error. Reducing from 5 steps to 1 step makes the error dramatically worse.\n\n- **This IS the flow matching advantage.** Diffusion ODE trajectories curve (as you saw in Exercise 1). Flow matching trajectories are straight (by construction). ODE solvers like Euler's method are exact on straight paths, meaning flow matching models can generate in far fewer steps.\n\nThe negative experiment makes this concrete: on the curved path, 1 step is catastrophically wrong. On the straight path, 1 step is perfect. The trajectory geometry determines the solver's accuracy, not the solver itself.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 3: Train a Flow Matching Model on 2D Data `[Supported]`\n\nFrom the lesson: the flow matching training loop is structurally identical to DDPM's, but simpler:\n1. Sample data $x_0$ and noise $\\epsilon \\sim \\mathcal{N}(0, I)$\n2. Sample a random time $t \\sim \\text{Uniform}(0, 1)$\n3. Compute $x_t = (1-t)\\, x_0 + t\\, \\epsilon$\n4. Target velocity: $v = \\epsilon - x_0$ (constant, does not depend on $t$!)\n5. Loss = $\\text{MSE}(v_\\theta(x_t, t),\\; \\epsilon - x_0)$\n\nYour task: fill in the TODO markers to implement the flow matching training loop and the Euler ODE sampling.\n\n**Before running, predict:**\n- With 5 Euler steps, will the generated samples be recognizable as two-moons?\n- With 50 Euler steps, will the quality be noticeably better than 5 steps? (Think about what Exercise 2 showed you.)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Exercise 3: Train a flow matching model on two-moons\n# ============================================================\n# NOTE: Fill in ALL four TODOs before running this cell.\n# Running with None values will raise an error.\n\ntorch.manual_seed(42)\n\n# Create the model\nfm_model = ToyModel(hidden_dim=128)\noptimizer = torch.optim.Adam(fm_model.parameters(), lr=3e-4)\n\nn_epochs = 500\nbatch_size = 512\nlosses = []\n\nprint('Training flow matching model on two-moons...')\nprint('(This should take ~30 seconds on CPU)')\nprint()\n\nfor epoch in range(n_epochs):\n    # Step 1: Sample clean data\n    x_0 = sample_two_moons(batch_size)\n    \n    # Step 2: Sample random noise\n    epsilon = torch.randn_like(x_0)\n    \n    # TODO: Sample random time t ~ Uniform(0, 1), shape (batch_size, 1)\n    # Hint: Use torch.rand\n    t = None  # <-- Replace this line\n    \n    # TODO: Compute the interpolated point x_t = (1 - t) * x_0 + t * epsilon\n    # This is the flow matching interpolation from the lesson.\n    x_t = None  # <-- Replace this line\n    \n    # TODO: Compute the target velocity v = epsilon - x_0\n    # This is constant for each (x_0, epsilon) pair---it does not depend on t!\n    target_v = None  # <-- Replace this line\n    \n    # Guard: make sure TODOs are filled in before proceeding\n    if t is None or x_t is None or target_v is None:\n        raise NotImplementedError(\n            \"Fill in the three TODOs above (t, x_t, target_v) before running this cell.\"\n        )\n    \n    # Forward pass: network predicts velocity from (x_t, t)\n    pred_v = fm_model(x_t, t)\n    \n    # TODO: Compute the loss = MSE between predicted and target velocity\n    loss = None  # <-- Replace this line\n    \n    if loss is None:\n        raise NotImplementedError(\n            \"Fill in the loss TODO above before running this cell.\"\n        )\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    losses.append(loss.item())\n    if (epoch + 1) % 100 == 0:\n        print(f'  Epoch {epoch+1}/{n_epochs}, loss: {loss.item():.4f}')\n\n# Plot training loss\nfig, ax = plt.subplots(1, 1, figsize=(8, 3))\nax.plot(losses, color='#34d399', linewidth=1, alpha=0.7)\nax.set_xlabel('Epoch')\nax.set_ylabel('MSE Loss')\nax.set_title('Flow Matching Training Loss')\nplt.tight_layout()\nplt.show()\n\nprint(f'\\nFinal loss: {losses[-1]:.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Generate samples using Euler ODE solving\n# ============================================================\n#\n# Generation goes from noise (t=1) to data (t=0).\n# At each step, the model predicts the velocity v_theta(x_t, t),\n# and we step in the REVERSE direction (from t=1 toward t=0).\n#\n# Euler step: x_{t-dt} = x_t + (-dt) * v_theta(x_t, t)\n#   (negative dt because we go backward in time)\n# Equivalently: x_{t-dt} = x_t - dt * v_theta(x_t, t)\n\nfm_model.eval()\n\n@torch.no_grad()\ndef sample_flow_matching(model, n_samples, n_steps):\n    \"\"\"Generate samples by solving the flow matching ODE with Euler's method.\n    \n    Start from noise (t=1), step toward data (t=0).\n    \"\"\"\n    # Start from pure noise\n    x = torch.randn(n_samples, 2)\n    dt = 1.0 / n_steps\n    \n    for i in range(n_steps):\n        # TODO: Compute current time t (starts at 1.0, decreases by dt each step)\n        # Hint: t = 1.0 - i * dt\n        t = None  # <-- Replace this line\n        t_tensor = torch.full((n_samples, 1), t)\n        \n        # TODO: Get the model's velocity prediction\n        v_pred = None  # <-- Replace this line\n        \n        # TODO: Euler step in reverse time: x = x - dt * v_pred\n        x = None  # <-- Replace this line\n    \n    return x\n\n\n# Generate samples at different step counts\nstep_counts_to_test = [1, 5, 10, 50]\n\n# Also get real data for comparison\ntorch.manual_seed(0)\nreal_data = sample_two_moons(500)\n\nfig, axes = plt.subplots(1, len(step_counts_to_test) + 1, figsize=(18, 4))\n\n# Plot real data\nax = axes[0]\nax.scatter(real_data[:, 0], real_data[:, 1], s=3, alpha=0.5, color='#60a5fa')\nax.set_title('Real Data\\n(two-moons)', fontsize=11)\nax.set_xlim(-4, 4); ax.set_ylim(-4, 4)\nax.set_aspect('equal')\nax.grid(alpha=0.15)\n\n# Generate and plot for each step count\nfor idx, n_steps in enumerate(step_counts_to_test):\n    torch.manual_seed(0)\n    samples = sample_flow_matching(fm_model, 500, n_steps)\n    \n    ax = axes[idx + 1]\n    if samples is not None:\n        ax.scatter(samples[:, 0], samples[:, 1], s=3, alpha=0.5, color='#34d399')\n    ax.set_title(f'{n_steps} Euler Step{\"s\" if n_steps > 1 else \"\"}', fontsize=11)\n    ax.set_xlim(-4, 4); ax.set_ylim(-4, 4)\n    ax.set_aspect('equal')\n    ax.grid(alpha=0.15)\n\nplt.suptitle(\n    'Flow Matching: Sample Quality vs Number of Euler Steps',\n    fontsize=13, y=1.02\n)\nplt.tight_layout()\nplt.show()\n\nprint('Observations:')\nprint('- Even with 5 Euler steps, the two-moons shape is recognizable.')\nprint('  This is the flow matching advantage: nearly-straight trajectories')\nprint('  mean Euler\\'s method works well with very few steps.')\nprint()\nprint('- 1 step may be noisy (the aggregate learned field is not perfectly')\nprint('  straight\\u2014remember rectified flow from the lesson).')\nprint()\nprint('- 10 and 50 steps produce very similar quality. Once the trajectories')\nprint('  are nearly straight, adding more steps gives diminishing returns.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\nThe key insight is that the flow matching training loop is almost trivially simple compared to DDPM. No noise schedule, no alpha_bar, no cumulative products. Just a linear interpolation and a subtraction for the target.\n\n**Training TODOs:**\n```python\n# Sample random time\nt = torch.rand(batch_size, 1)\n\n# Interpolated point\nx_t = (1 - t) * x_0 + t * epsilon\n\n# Target velocity (constant for each pair!)\ntarget_v = epsilon - x_0\n\n# MSE loss\nloss = nn.functional.mse_loss(pred_v, target_v)\n```\n\n**Sampling TODOs:**\n```python\n# Current time\nt = 1.0 - i * dt\n\n# Model prediction\nv_pred = model(x, t_tensor)\n\n# Euler step (reverse time)\nx = x - dt * v_pred\n```\n\n**Why reverse time for sampling:** During training, $t=0$ is clean data and $t=1$ is pure noise. To generate, we start from noise ($t=1$) and walk backward to data ($t=0$). The velocity field $v_\\theta$ was trained in the forward direction (data to noise), so to go backward we subtract: $x_{t-dt} = x_t - dt \\cdot v_\\theta(x_t, t)$.\n\n**Common mistakes:**\n- Forgetting that `t` needs shape `(batch_size, 1)` for broadcasting with `x_0` which has shape `(batch_size, 2)`\n- Adding `dt * v_pred` instead of subtracting (wrong direction—generating noise instead of data)\n- Using integer timesteps instead of continuous $t \\in [0, 1]$ (flow matching uses continuous time, not DDPM's discrete timesteps)\n\n</details>\n\n### What Just Happened\n\nYou trained a flow matching model end-to-end and generated samples via Euler ODE solving:\n\n- **The training loop is simple.** No noise schedule, no alpha_bar lookup, no variance-preserving formulation. Just a linear interpolation for $x_t$, a subtraction for the target velocity, and MSE loss. Compare this to the DDPM training loop you implemented in the previous series.\n\n- **Few steps work.** With just 5 Euler steps, the generated samples already resemble the two-moons distribution. This confirms what Exercise 2 showed: on nearly-straight trajectories, Euler's method converges quickly.\n\n- **Diminishing returns from more steps.** Going from 10 to 50 steps shows minimal improvement. Once the trajectory is approximately straight, additional steps provide negligible accuracy gain.\n\n- **1 step is imperfect.** The aggregate learned velocity field is not perfectly straight (remember: individual conditional paths are straight, but the average introduces some curvature). This is why rectified flow exists—to straighten the aggregate field.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 4: Compare Flow Matching to DDPM on the Same Data `[Independent]`\n\nYou have trained a flow matching model on the two-moons distribution. Now train a DDPM model on the same data and compare them head-to-head.\n\n**Your task:**\n\n1. **Train a DDPM model** on two-moons using the noise prediction objective:\n   - Use the cosine schedule from `make_cosine_schedule(T=100)`\n   - Forward process: $x_t = \\sqrt{\\bar\\alpha_t}\\, x_0 + \\sqrt{1-\\bar\\alpha_t}\\, \\epsilon$\n   - Target: $\\epsilon$ (the noise itself)\n   - Loss: $\\text{MSE}(\\epsilon_\\theta(x_t, t),\\; \\epsilon)$\n   - Use the same `ToyModel` architecture and same hyperparameters (500 epochs, batch size 512, lr 3e-4)\n\n2. **Implement DDPM sampling** (DDIM-style deterministic, for fair comparison):\n   - Predict $\\hat{x}_0 = (x_t - \\sqrt{1-\\bar\\alpha_t}\\, \\epsilon_\\theta) / \\sqrt{\\bar\\alpha_t}$\n   - Step: $x_{t-1} = \\sqrt{\\bar\\alpha_{t-1}}\\, \\hat{x}_0 + \\sqrt{1-\\bar\\alpha_{t-1}}\\, \\epsilon_\\theta$\n   - To use N steps, create a sub-sequence of N evenly spaced timesteps from the full schedule\n\n3. **Generate samples at varying step counts** (1, 5, 10, 20, 50) for both models.\n\n4. **Plot the comparison:** for each step count, show flow matching samples vs DDPM samples side by side.\n\n**Expected result:** Flow matching should produce recognizable samples at fewer steps than DDPM.\n\n**Bonus:** Try one round of rectified flow—generate (noise, data) pairs using the trained flow matching model, then retrain on those pairs. Compare the rectified model's 1-step and 5-step samples to the original.\n\n**Before running, predict:**\n- At 5 steps, which model will produce better samples?\n- At 50 steps, will the quality difference between the two models be large or small?\n- At 1 step, will either model produce anything recognizable?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "#\n",
    "# Suggested structure:\n",
    "#\n",
    "# 1. Train DDPM model:\n",
    "#    - Create a new ToyModel for DDPM\n",
    "#    - Use make_cosine_schedule(T=100) for the noise schedule\n",
    "#    - Training loop: sample x_0, sample t (integer in [0, T-1]), sample epsilon,\n",
    "#      compute x_t using DDPM forward process, predict epsilon, MSE loss\n",
    "#    - IMPORTANT: the model takes continuous t, so normalize: t_input = t_int / T\n",
    "#\n",
    "# 2. Implement DDPM DDIM-style sampling:\n",
    "#    def sample_ddpm(model, alpha_bars, n_samples, n_steps):\n",
    "#        Create a subsequence of n_steps evenly spaced timesteps from [T-1, ..., 0]\n",
    "#        For each step:\n",
    "#            - Get alpha_bar at current and previous timestep\n",
    "#            - Predict epsilon\n",
    "#            - Compute predicted x_0\n",
    "#            - Compute x_{t-1} via DDIM formula\n",
    "#        Return final samples\n",
    "#\n",
    "# 3. Generate samples from both models at step counts [1, 5, 10, 20, 50]\n",
    "#    Use the SAME random seed for starting noise so the comparison is fair.\n",
    "#\n",
    "# 4. Plot a grid: rows = step counts, columns = [Real Data, Flow Matching, DDPM]\n",
    "#    Or: one row per step count with flow matching and DDPM side by side.\n",
    "#\n",
    "# Remember:\n",
    "# - fm_model is already trained from Exercise 3\n",
    "# - sample_flow_matching() is already defined from Exercise 3\n",
    "# - Use torch.no_grad() for sampling\n",
    "# - Clamp predicted x_0 to a reasonable range (e.g., -6 to 6) for stability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\nThe core insight is that both models use the same architecture and see the same data. The only differences are the interpolation scheme, the training target, and the time parameterization. This controlled comparison isolates the effect of trajectory geometry on sample quality at low step counts.\n\n```python\n# ============================================================\n# Step 1: Train a DDPM model on two-moons\n# ============================================================\n\ntorch.manual_seed(42)\n\nT = 100\nalpha_bars = make_cosine_schedule(T)\n\nddpm_model = ToyModel(hidden_dim=128)\noptimizer_ddpm = torch.optim.Adam(ddpm_model.parameters(), lr=3e-4)\n\nddpm_losses = []\nprint('Training DDPM model on two-moons...')\n\nfor epoch in range(500):\n    x_0 = sample_two_moons(512)\n    \n    # Sample random integer timesteps\n    t_int = torch.randint(0, T, (512,))\n    epsilon = torch.randn_like(x_0)\n    \n    # DDPM forward process\n    ab_t = alpha_bars[t_int].unsqueeze(-1)  # (batch, 1)\n    x_t = torch.sqrt(ab_t) * x_0 + torch.sqrt(1 - ab_t) * epsilon\n    \n    # Normalize timestep to [0, 1] for the model\n    t_input = t_int.float() / T\n    \n    # Predict noise\n    eps_pred = ddpm_model(x_t, t_input.unsqueeze(-1))\n    loss = nn.functional.mse_loss(eps_pred, epsilon)\n    \n    optimizer_ddpm.zero_grad()\n    loss.backward()\n    optimizer_ddpm.step()\n    \n    ddpm_losses.append(loss.item())\n    if (epoch + 1) % 100 == 0:\n        print(f'  Epoch {epoch+1}/500, loss: {loss.item():.4f}')\n\nprint(f'Final DDPM loss: {ddpm_losses[-1]:.4f}')\n\n\n# ============================================================\n# Step 2: DDPM sampling (DDIM-style deterministic)\n# ============================================================\n\nddpm_model.eval()\n\n@torch.no_grad()\ndef sample_ddpm(model, alpha_bars, n_samples, n_steps):\n    \"\"\"DDIM-style deterministic sampling from a DDPM model.\"\"\"\n    T = len(alpha_bars)\n    \n    # Create sub-sequence of timesteps\n    # Evenly spaced from T-1 down to 0\n    timesteps = torch.linspace(T - 1, 0, n_steps + 1).long()\n    \n    # Start from pure noise\n    x = torch.randn(n_samples, 2)\n    \n    for i in range(len(timesteps) - 1):\n        t_curr = timesteps[i]\n        t_prev = timesteps[i + 1]\n        \n        ab_curr = alpha_bars[t_curr]\n        ab_prev = alpha_bars[t_prev] if t_prev >= 0 else torch.tensor(1.0)\n        \n        # Predict noise\n        t_input = (t_curr.float() / T).unsqueeze(0).expand(n_samples, 1)\n        eps_pred = model(x, t_input)\n        \n        # DDIM: predict x_0, then jump to t_prev\n        pred_x0 = (x - torch.sqrt(1 - ab_curr) * eps_pred) / torch.sqrt(ab_curr)\n        pred_x0 = pred_x0.clamp(-6, 6)  # Stability\n        \n        # Jump to t_prev\n        x = torch.sqrt(ab_prev) * pred_x0 + torch.sqrt(1 - ab_prev) * eps_pred\n    \n    return pred_x0\n\n\n# ============================================================\n# Step 3 & 4: Generate and compare\n# ============================================================\n\nstep_counts = [1, 5, 10, 20, 50]\nn_samples = 500\n\n# Real data for reference\ntorch.manual_seed(99)\nreal = sample_two_moons(n_samples)\n\nfig, axes = plt.subplots(len(step_counts), 3, figsize=(12, 4 * len(step_counts)))\n\nfor row, n_steps in enumerate(step_counts):\n    # Same starting noise for both\n    torch.manual_seed(42)\n    fm_samples = sample_flow_matching(fm_model, n_samples, n_steps)\n    \n    torch.manual_seed(42)\n    ddpm_samples = sample_ddpm(ddpm_model, alpha_bars, n_samples, n_steps)\n    \n    # Real data\n    axes[row, 0].scatter(real[:, 0], real[:, 1], s=3, alpha=0.5, c='#60a5fa')\n    axes[row, 0].set_title(f'Real Data', fontsize=10)\n    axes[row, 0].set_xlim(-4, 4); axes[row, 0].set_ylim(-4, 4)\n    axes[row, 0].set_aspect('equal')\n    axes[row, 0].set_ylabel(f'{n_steps} steps', fontsize=12, fontweight='bold')\n    \n    # Flow matching\n    axes[row, 1].scatter(fm_samples[:, 0], fm_samples[:, 1], s=3, alpha=0.5, c='#34d399')\n    axes[row, 1].set_title(f'Flow Matching ({n_steps} steps)', fontsize=10)\n    axes[row, 1].set_xlim(-4, 4); axes[row, 1].set_ylim(-4, 4)\n    axes[row, 1].set_aspect('equal')\n    \n    # DDPM\n    axes[row, 2].scatter(ddpm_samples[:, 0], ddpm_samples[:, 1], s=3, alpha=0.5, c='#f59e0b')\n    axes[row, 2].set_title(f'DDPM ({n_steps} steps)', fontsize=10)\n    axes[row, 2].set_xlim(-4, 4); axes[row, 2].set_ylim(-4, 4)\n    axes[row, 2].set_aspect('equal')\n\nplt.suptitle(\n    'Flow Matching vs DDPM: Sample Quality at Varying Step Counts\\n'\n    'Same architecture, same data, same number of training epochs',\n    fontsize=13, y=1.01\n)\nplt.tight_layout()\nplt.show()\n\nprint('Expected observations:')\nprint('- At 50 steps: both models produce good two-moons samples. Similar quality.')\nprint('- At 10-20 steps: flow matching still looks good. DDPM may show some degradation.')\nprint('- At 5 steps: flow matching is recognizable. DDPM is more distorted.')\nprint('- At 1 step: neither is great, but flow matching is closer to the target.')\nprint()\nprint('The advantage is not about ultimate quality (both converge with enough steps).')\nprint('The advantage is about efficiency: flow matching reaches good quality FASTER.')\n```\n\n**Key decisions:**\n- Using DDIM-style deterministic sampling for DDPM (not stochastic DDPM) makes the comparison fair: both methods use the same ODE-solving approach, the only difference is the trajectory shape.\n- Normalizing DDPM's integer timesteps to [0, 1] for the model input keeps the architecture identical. The model does not know whether it is doing DDPM or flow matching—only the training target differs.\n- Using the same random seed for starting noise ensures both models start from the exact same noise vectors, so differences in the output are entirely due to the model, not the initialization.\n\n**Common mistakes:**\n- Using stochastic DDPM sampling (adding noise at each step) which introduces randomness that makes the comparison unfair.\n- Forgetting to normalize DDPM timesteps to [0, 1] for the model input, causing the model to receive out-of-range time values.\n- Not clamping the predicted $x_0$ in DDPM, which can lead to numerical instability at early timesteps where $\\bar\\alpha_t$ is very small.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **Flow matching interpolation is a straight line; DDPM interpolation curves.** The linear coefficients $(1-t)$ and $t$ produce a straight path with constant velocity. The DDPM coefficients $\\sqrt{\\bar\\alpha_t}$ and $\\sqrt{1-\\bar\\alpha_t}$ produce a curved path with time-varying velocity. This is the fundamental geometric difference.\n\n2. **On a straight path, Euler's method is exact in one step.** Curved paths cause Euler to overshoot at every step, requiring many small steps to stay accurate. Straight paths have no curvature to overshoot—one step lands you exactly at the endpoint. This is why flow matching models need fewer sampling steps.\n\n3. **The flow matching training loop is simpler.** No noise schedule, no $\\bar\\alpha_t$, no cumulative products. Just $x_t = (1-t)\\, x_0 + t\\, \\epsilon$ for the interpolation and $v = \\epsilon - x_0$ for the target. The entire training loop is a weighted average and a subtraction.\n\n4. **Flow matching reaches good sample quality at fewer steps than DDPM.** With the same architecture, data, and training budget, flow matching produces recognizable samples at 5–10 Euler steps where DDPM may need 20–50 steps. The advantage is efficiency, not ultimate quality—both converge with enough steps.\n\n5. **Same architecture, different training target.** The `ToyModel` MLP was identical for both flow matching and DDPM. The only changes were the interpolation scheme, the training target (velocity vs noise), and the time parameterization (continuous uniform vs discrete schedule). This is why velocity prediction does not require a new architecture—it is a training objective change, not an architectural one."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}