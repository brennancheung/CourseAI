{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP — Contrastive Learning and Shared Embedding Spaces\n",
    "\n",
    "**Module 6.3, Lesson 3** | CourseAI\n",
    "\n",
    "In the lesson, you learned how CLIP trains two separate encoders to put matching text and images near each other in a shared embedding space. Now you will work with CLIP directly — computing similarity matrices, exploring a pretrained model, performing zero-shot classification, and probing the model's limitations.\n",
    "\n",
    "**What you will do:**\n",
    "- Compute a cosine similarity matrix by hand from pre-computed embeddings and connect it to cross-entropy loss\n",
    "- Load a real pretrained CLIP model and visualize its similarity matrix as a heatmap\n",
    "- Perform zero-shot classification on CIFAR-10 images using text prompts — no training required\n",
    "- Probe CLIP's systematic failure modes: spatial relationships, counting, and adversarial text\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "Everything connects to concepts you already know. The cosine similarity matrix is the same operation from *Queries and Keys*. The cross-entropy loss is the same loss from *Transfer Learning*. The zero-shot classification uses the same embedding comparison pattern you saw in the lesson. No new theory — just practice.\n",
    "\n",
    "**Estimated time:** 30–45 minutes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\nRun this cell to install dependencies and import everything.\n\n**Note:** We use the `open_clip` package (OpenCLIP) rather than OpenAI's original `clip` package. OpenCLIP is actively maintained and available on PyPI. The model architecture is the same ViT-B/32, but the OpenCLIP version was trained on LAION-2B (~2 billion text-image pairs) rather than OpenAI's private WIT dataset (~400 million pairs). More data, same idea."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q open_clip_torch\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "print('\\nSetup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity_matrix(sim_matrix, row_labels, col_labels, title='Similarity Matrix'):\n",
    "    \"\"\"Plot a cosine similarity matrix as a heatmap.\n",
    "\n",
    "    Args:\n",
    "        sim_matrix: 2D numpy array or torch tensor of similarity values\n",
    "        row_labels: labels for each row (text descriptions)\n",
    "        col_labels: labels for each column (image descriptions)\n",
    "        title: plot title\n",
    "    \"\"\"\n",
    "    if isinstance(sim_matrix, torch.Tensor):\n",
    "        sim_matrix = sim_matrix.numpy()\n",
    "\n",
    "    n_rows, n_cols = sim_matrix.shape\n",
    "    fig, ax = plt.subplots(figsize=(max(6, n_cols * 1.4), max(5, n_rows * 1.2)))\n",
    "\n",
    "    im = ax.imshow(sim_matrix, cmap='viridis', vmin=0, vmax=1)\n",
    "\n",
    "    # Annotate each cell with its value\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            val = sim_matrix[i, j]\n",
    "            color = 'black' if val > 0.5 else 'white'\n",
    "            ax.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                    fontsize=10, fontweight='bold', color=color)\n",
    "\n",
    "    ax.set_xticks(range(n_cols))\n",
    "    ax.set_xticklabels(col_labels, rotation=30, ha='right', fontsize=9)\n",
    "    ax.set_yticks(range(n_rows))\n",
    "    ax.set_yticklabels(row_labels, fontsize=9)\n",
    "    ax.set_xlabel('Images', fontsize=11)\n",
    "    ax.set_ylabel('Text Prompts', fontsize=11)\n",
    "    ax.set_title(title, fontsize=13)\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print('Helpers loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Compute a Similarity Matrix by Hand [Guided]\n",
    "\n",
    "Before touching a real CLIP model, you will compute the similarity matrix yourself with simple 2D vectors. This makes the math concrete and shows that CLIP's training objective is just cross-entropy on a cosine similarity matrix.\n",
    "\n",
    "We have 4 text-image pairs. Each embedding is a 2D vector (for simplicity — real CLIP uses 512D, but the math is identical).\n",
    "\n",
    "| Pair | Text | Image |\n",
    "|------|------|-------|\n",
    "| 0 | \"a tabby cat\" | cat photo |\n",
    "| 1 | \"a red sports car\" | car photo |\n",
    "| 2 | \"a mountain landscape\" | mountain photo |\n",
    "| 3 | \"a bowl of ramen\" | ramen photo |\n",
    "\n",
    "The image encoder and text encoder have each produced a 2D embedding vector for their respective inputs. After training, matching pairs point in similar directions.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Which entries in the 4×4 similarity matrix should be highest? (The diagonal — matching pairs.)\n",
    "- If row 0 is the text \"a tabby cat,\" which column should have the highest value? (Column 0 — the cat image.)\n",
    "- What are the cross-entropy labels for all 4 rows? (Answer: [0, 1, 2, 3] — each row's correct answer is the diagonal entry.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-computed 2D embeddings (imagine these came from trained encoders)\n",
    "# These are designed so that matching pairs point in similar directions.\n",
    "\n",
    "# Image embeddings: each is a 2D vector\n",
    "image_embeddings = torch.tensor([\n",
    "    [0.9, 0.3],    # image 0: cat photo\n",
    "    [-0.7, 0.6],   # image 1: car photo\n",
    "    [0.1, -0.95],  # image 2: mountain photo\n",
    "    [-0.8, -0.5],  # image 3: ramen photo\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Text embeddings: each is a 2D vector\n",
    "text_embeddings = torch.tensor([\n",
    "    [0.85, 0.4],   # text 0: \"a tabby cat\"\n",
    "    [-0.6, 0.7],   # text 1: \"a red sports car\"\n",
    "    [0.2, -0.9],   # text 2: \"a mountain landscape\"\n",
    "    [-0.75, -0.6], # text 3: \"a bowl of ramen\"\n",
    "], dtype=torch.float32)\n",
    "\n",
    "pair_labels = ['cat', 'car', 'mountain', 'ramen']\n",
    "\n",
    "print('Image embeddings (4 images, 2 dimensions each):')\n",
    "for i, label in enumerate(pair_labels):\n",
    "    print(f'  Image {i} ({label:>8s}): [{image_embeddings[i, 0]:+.2f}, {image_embeddings[i, 1]:+.2f}]')\n",
    "\n",
    "print()\n",
    "print('Text embeddings (4 texts, 2 dimensions each):')\n",
    "for i, label in enumerate(pair_labels):\n",
    "    print(f'  Text  {i} ({label:>8s}): [{text_embeddings[i, 0]:+.2f}, {text_embeddings[i, 1]:+.2f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Normalize all embeddings to unit vectors\n",
    "# Cosine similarity = dot product of unit vectors\n",
    "# So we normalize first, then the matrix multiply gives us cosine similarities.\n",
    "\n",
    "image_normed = F.normalize(image_embeddings, dim=1)  # [4, 2]\n",
    "text_normed = F.normalize(text_embeddings, dim=1)    # [4, 2]\n",
    "\n",
    "print('Normalized image embeddings (unit vectors):')\n",
    "for i, label in enumerate(pair_labels):\n",
    "    v = image_normed[i]\n",
    "    norm = v.norm().item()\n",
    "    print(f'  Image {i} ({label:>8s}): [{v[0]:+.4f}, {v[1]:+.4f}]  (norm = {norm:.4f})')\n",
    "\n",
    "print()\n",
    "print('Normalized text embeddings (unit vectors):')\n",
    "for i, label in enumerate(pair_labels):\n",
    "    v = text_normed[i]\n",
    "    norm = v.norm().item()\n",
    "    print(f'  Text  {i} ({label:>8s}): [{v[0]:+.4f}, {v[1]:+.4f}]  (norm = {norm:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute the 4x4 similarity matrix\n",
    "# S[i, j] = cosine_similarity(text_i, image_j)\n",
    "# Since the vectors are normalized: S = text_normed @ image_normed.T\n",
    "\n",
    "similarity_matrix = text_normed @ image_normed.T  # [4, 4]\n",
    "\n",
    "print('Similarity matrix (text rows × image columns):')\n",
    "print()\n",
    "header = '                    ' + ''.join(f'{label:>12s}' for label in pair_labels)\n",
    "print(header)\n",
    "print('                    ' + ''.join(f'     img {i}  ' for i in range(4)))\n",
    "print('-' * len(header))\n",
    "for i, label in enumerate(pair_labels):\n",
    "    row_str = ''.join(f'{similarity_matrix[i, j]:>12.4f}' for j in range(4))\n",
    "    marker = '  <-- diagonal' if True else ''\n",
    "    print(f'  text {i} ({label:>8s})' + row_str)\n",
    "\n",
    "print()\n",
    "print('Diagonal entries (matching pairs):')\n",
    "for i, label in enumerate(pair_labels):\n",
    "    print(f'  S[{i},{i}] = {similarity_matrix[i, i]:.4f}  ({label} text <-> {label} image)')\n",
    "\n",
    "print()\n",
    "print('The diagonal entries should be the highest in each row and column.')\n",
    "print('This is the pattern CLIP is trained to produce: bright diagonal, dark off-diagonal.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the similarity matrix as a heatmap\n",
    "text_labels = ['\"a tabby cat\"', '\"a red sports car\"', '\"a mountain landscape\"', '\"a bowl of ramen\"']\n",
    "img_labels = ['cat img', 'car img', 'mountain img', 'ramen img']\n",
    "\n",
    "plot_similarity_matrix(\n",
    "    similarity_matrix,\n",
    "    row_labels=text_labels,\n",
    "    col_labels=img_labels,\n",
    "    title='4×4 Cosine Similarity Matrix (hand-computed)'\n",
    ")\n",
    "\n",
    "print('The bright diagonal shows that matching pairs have high cosine similarity.')\n",
    "print('The darker off-diagonal entries are the non-matching pairs.')\n",
    "print('This is exactly the pattern from the lesson\\'s similarity matrix example.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Connect to cross-entropy loss\n",
    "# Each row of the similarity matrix IS a classification problem.\n",
    "# Row i asks: \"which image matches text i?\"\n",
    "# The correct answer is always column i (the diagonal entry).\n",
    "#\n",
    "# Labels = [0, 1, 2, 3] -- each text i matches image i.\n",
    "\n",
    "labels = torch.arange(4)  # [0, 1, 2, 3]\n",
    "\n",
    "# Walk through row 0 (\"a tabby cat\")\n",
    "print('Walking through row 0: \"a tabby cat\"')\n",
    "print(f'  Similarities: {similarity_matrix[0].tolist()}')\n",
    "print(f'  Correct answer: column {labels[0].item()} (cat image)')\n",
    "print(f'  Highest similarity is at column {similarity_matrix[0].argmax().item()}')\n",
    "print()\n",
    "\n",
    "# In CLIP, a learnable temperature scales the logits before softmax.\n",
    "# We will use temperature = 1.0 for this exercise (no scaling).\n",
    "temperature = 1.0\n",
    "logits = similarity_matrix / temperature\n",
    "\n",
    "# Apply softmax to row 0 to get a probability distribution\n",
    "probs_row0 = F.softmax(logits[0], dim=0)\n",
    "print('Softmax probabilities for row 0:')\n",
    "for j, label in enumerate(pair_labels):\n",
    "    marker = ' <-- correct' if j == 0 else ''\n",
    "    print(f'  P(image={label:>8s}) = {probs_row0[j]:.4f}{marker}')\n",
    "\n",
    "print()\n",
    "\n",
    "# Cross-entropy loss for row 0: -log(P(correct class))\n",
    "ce_row0 = -torch.log(probs_row0[0])\n",
    "print(f'Cross-entropy loss for row 0: -log({probs_row0[0]:.4f}) = {ce_row0:.4f}')\n",
    "print()\n",
    "print('This is the same cross-entropy you used for MNIST and language modeling.')\n",
    "print('The only difference: the \"classes\" are images in the batch,')\n",
    "print('and the \"logits\" are cosine similarities.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compute the full CLIP loss (symmetric cross-entropy)\n",
    "# L = (L_image + L_text) / 2\n",
    "# L_image: each row is a classification (which image matches this text?)\n",
    "# L_text:  each column is a classification (which text matches this image?)\n",
    "\n",
    "# Using PyTorch's cross_entropy (which handles softmax internally)\n",
    "loss_image = F.cross_entropy(logits, labels)          # rows: which image?\n",
    "loss_text = F.cross_entropy(logits.T, labels)         # columns: which text?\n",
    "loss_clip = (loss_image + loss_text) / 2\n",
    "\n",
    "print('CLIP loss breakdown:')\n",
    "print(f'  L_image (text->image matching): {loss_image:.4f}')\n",
    "print(f'  L_text  (image->text matching): {loss_text:.4f}')\n",
    "print(f'  L_CLIP  (average):              {loss_clip:.4f}')\n",
    "print()\n",
    "print('The loss is symmetric: it asks BOTH \"which image matches this text?\"')\n",
    "print('AND \"which text matches this image?\" This ensures neither direction dominates.')\n",
    "print()\n",
    "print('If you lower this loss during training, you are pushing matching pairs together')\n",
    "print('and non-matching pairs apart in the shared embedding space.')\n",
    "print()\n",
    "print('Key insight: each row IS a classification problem. The labels are always')\n",
    "print('[0, 1, 2, ..., N-1] -- the diagonal. No human ever labeled anything as')\n",
    "print('\"not matching.\" The batch structure provides the negatives for free.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You computed a CLIP-style similarity matrix by hand:\n",
    "\n",
    "1. **Normalized** the embeddings to unit vectors (so dot product = cosine similarity).\n",
    "2. **Computed** the 4×4 similarity matrix via matrix multiplication.\n",
    "3. **Visualized** it as a heatmap — bright diagonal (matching pairs), dark off-diagonal (non-matches).\n",
    "4. **Connected** each row to a cross-entropy classification problem — \"which image matches this text?\"\n",
    "5. **Computed** the full symmetric CLIP loss — cross-entropy in both directions.\n",
    "\n",
    "The numbers you computed here are exactly what CLIP produces at scale. The only difference: real CLIP uses 512 dimensions and batches of 32,768. Same math, vastly more dimensions and data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 2: Explore a Pretrained CLIP Model [Guided]\n\nNow you will load a real pretrained CLIP model and see the same similarity matrix pattern — but produced by encoders trained on billions of text-image pairs.\n\nWe will load the ViT-B/32 model (Vision Transformer as the image encoder, 32×32 patch size) trained on LAION-2B via OpenCLIP, and:\n1. Download 5 images from the web\n2. Encode them with CLIP's image encoder\n3. Encode 5 text prompts with CLIP's text encoder\n4. Compute and visualize the 5×5 similarity matrix\n\n**Before running, predict:**\n- Will the diagonal entries be close to 1.0 or just \"higher than off-diagonal\"?\n- Among off-diagonal entries, will some be noticeably higher than others? (Think about which images and texts might be semantically similar even if they are not exact matches.)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained CLIP model (ViT-B/32)\n",
    "# This downloads the model weights (~350 MB) on first run.\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    'ViT-B-32', pretrained='laion2b_s34b_b79k'\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f'Model loaded: ViT-B-32')\n",
    "print(f'Image encoder: Vision Transformer (patches = 32×32)')\n",
    "print(f'Text encoder: Transformer')\n",
    "print(f'Embedding dimension: 512')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download 5 images from the web\n",
    "# These are public domain or CC-licensed images from Unsplash and similar.\n",
    "\n",
    "image_urls = {\n",
    "    'cat': 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg',\n",
    "    'sports car': 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/2019_Ferrari_488_Pista_3.9.jpg/1200px-2019_Ferrari_488_Pista_3.9.jpg',\n",
    "    'mountain': 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Everest_North_Face_toward_Base_Camp_Tibet_Luca_Galuzzi_2006.jpg/1200px-Everest_North_Face_toward_Base_Camp_Tibet_Luca_Galuzzi_2006.jpg',\n",
    "    'ramen': 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Shoyu_ramen%2C_at_Kasukabe_Station_%282014.05.05%29_1.jpg/1200px-Shoyu_ramen%2C_at_Kasukabe_Station_%282014.05.05%29_1.jpg',\n",
    "    'dog': 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/1200px-YellowLabradorLooking_new.jpg',\n",
    "}\n",
    "\n",
    "images = {}\n",
    "for name, url in image_urls.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        images[name] = img\n",
    "        print(f'  Downloaded: {name} ({img.size[0]}x{img.size[1]})')\n",
    "    except Exception as e:\n",
    "        print(f'  Failed to download {name}: {e}')\n",
    "\n",
    "print(f'\\nLoaded {len(images)} images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images\n",
    "fig, axes = plt.subplots(1, len(images), figsize=(15, 3))\n",
    "for ax, (name, img) in zip(axes, images.items()):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(name, fontsize=11)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Input Images', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode images and text with CLIP\n",
    "\n",
    "# Text prompts — one for each image, plus the order matches\n",
    "text_prompts = [\n",
    "    'a photo of a cat',\n",
    "    'a photo of a sports car',\n",
    "    'a photo of a mountain',\n",
    "    'a photo of a bowl of ramen',\n",
    "    'a photo of a dog',\n",
    "]\n",
    "\n",
    "# Preprocess images for CLIP (resize, crop, normalize)\n",
    "image_tensors = torch.stack([preprocess(img) for img in images.values()]).to(device)\n",
    "\n",
    "# Tokenize text\n",
    "text_tokens = tokenizer(text_prompts).to(device)\n",
    "\n",
    "# Encode\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_tensors)   # [5, 512]\n",
    "    text_features = model.encode_text(text_tokens)       # [5, 512]\n",
    "\n",
    "print(f'Image features shape: {image_features.shape}')   # [5, 512]\n",
    "print(f'Text features shape:  {text_features.shape}')    # [5, 512]\n",
    "print()\n",
    "print('Two separate encoders produced two sets of 512-dimensional vectors.')\n",
    "print('The image encoder and text encoder share NO weights.')\n",
    "print('The only thing that connected them during training was the contrastive loss.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 5x5 similarity matrix — same steps as Exercise 1\n",
    "# 1. Normalize to unit vectors\n",
    "# 2. Matrix multiply\n",
    "\n",
    "image_features_norm = F.normalize(image_features, dim=1)\n",
    "text_features_norm = F.normalize(text_features, dim=1)\n",
    "\n",
    "clip_sim_matrix = (text_features_norm @ image_features_norm.T).cpu()\n",
    "\n",
    "# Plot the heatmap\n",
    "img_names = list(images.keys())\n",
    "plot_similarity_matrix(\n",
    "    clip_sim_matrix,\n",
    "    row_labels=text_prompts,\n",
    "    col_labels=img_names,\n",
    "    title='5×5 CLIP Similarity Matrix (real pretrained model)'\n",
    ")\n",
    "\n",
    "print('Observations:')\n",
    "print('  1. The diagonal entries are the highest in their row — matching pairs.')\n",
    "print('  2. The off-diagonal entries are lower — non-matching pairs.')\n",
    "print('  3. This is the same pattern you computed by hand in Exercise 1!')\n",
    "print()\n",
    "\n",
    "# Check for interesting off-diagonal entries\n",
    "print('Interesting off-diagonal entries (semantically related non-matches):')\n",
    "# Look at dog-cat similarity\n",
    "dog_idx = img_names.index('dog')\n",
    "cat_text_idx = text_prompts.index('a photo of a cat')\n",
    "dog_text_idx = text_prompts.index('a photo of a dog')\n",
    "cat_idx = img_names.index('cat')\n",
    "\n",
    "print(f'  \"a photo of a cat\" <-> dog image:  {clip_sim_matrix[cat_text_idx, dog_idx]:.4f}')\n",
    "print(f'  \"a photo of a dog\" <-> cat image:  {clip_sim_matrix[dog_text_idx, cat_idx]:.4f}')\n",
    "print(f'  \"a photo of a cat\" <-> car image:  {clip_sim_matrix[cat_text_idx, 1]:.4f}')\n",
    "print()\n",
    "print('Cat and dog images are more similar to each other\\'s text than to car or ramen.')\n",
    "print('The shared embedding space captures semantic similarity — animals cluster together.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### What Just Happened\n\nYou loaded a real CLIP model trained on billions of text-image pairs and saw the same pattern you computed by hand:\n\n1. **Two separate encoders** produced 512-dimensional vectors for images and text.\n2. **The similarity matrix** has a bright diagonal — matching text-image pairs have high cosine similarity.\n3. **Off-diagonal entries** reveal semantic structure — a cat and a dog are more similar to each other than either is to a car or a mountain.\n\nThe numbers from Exercise 1 are exactly what CLIP produces at scale. Same math, 512 dimensions instead of 2, trained on ~2 billion pairs instead of hand-designed.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Zero-Shot Classification [Supported]\n",
    "\n",
    "The shared embedding space enables a surprising capability: **classify images without any training on the target classes**. In *Transfer Learning*, you replaced the classification head and retrained. CLIP skips both steps — the text encoder IS the classification head.\n",
    "\n",
    "The recipe:\n",
    "1. Create a text prompt for each class: `\"a photo of a [class name]\"`\n",
    "2. Encode all prompts with the text encoder (once)\n",
    "3. Encode each test image with the image encoder\n",
    "4. Find the text prompt with highest cosine similarity — that is the prediction\n",
    "\n",
    "**Your task:** Use CLIP to classify images from CIFAR-10 (10 classes, 32×32 images). The structure is provided with `# TODO` markers. Each TODO is 1–3 lines.\n",
    "\n",
    "**Hints:**\n",
    "- `F.normalize(features, dim=1)` normalizes each row to unit length.\n",
    "- After normalizing, `image_features @ text_features.T` gives cosine similarities.\n",
    "- `.argmax(dim=1)` finds the index of the highest value per row.\n",
    "- Compare `predictions == true_labels` to compute accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 test set\n",
    "cifar10_test = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True\n",
    ")\n",
    "\n",
    "cifar10_classes = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "\n",
    "print(f'CIFAR-10 test set: {len(cifar10_test)} images')\n",
    "print(f'Classes: {cifar10_classes}')\n",
    "print(f'Image size: 32×32 (CLIP will resize these to 224×224)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create and encode text prompts for each class\n",
    "# The standard CLIP template: \"a photo of a [class name]\"\n",
    "\n",
    "# TODO: Create a list of text prompts, one per class.\n",
    "# Use the template: \"a photo of a {class_name}\"\n",
    "# Hint: text_prompts_cifar = [f\"a photo of a {c}\" for c in cifar10_classes]\n",
    "\n",
    "\n",
    "# Encode all text prompts (do this ONCE — reuse for every image)\n",
    "text_tokens_cifar = tokenizer(text_prompts_cifar).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features_cifar = model.encode_text(text_tokens_cifar)  # [10, 512]\n",
    "\n",
    "# TODO: Normalize text features to unit vectors\n",
    "# Hint: text_features_cifar = F.normalize(text_features_cifar, dim=1)\n",
    "\n",
    "\n",
    "print(f'Text features shape: {text_features_cifar.shape}')  # [10, 512]\n",
    "print('Text prompts:')\n",
    "for i, prompt in enumerate(text_prompts_cifar):\n",
    "    print(f'  Class {i}: \"{prompt}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Classify a batch of test images\n",
    "# We will classify the first 500 images for speed.\n",
    "\n",
    "n_test = 500\n",
    "correct = 0\n",
    "total = 0\n",
    "per_class_correct = {c: 0 for c in cifar10_classes}\n",
    "per_class_total = {c: 0 for c in cifar10_classes}\n",
    "\n",
    "# Process in batches of 50\n",
    "batch_size = 50\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for start_idx in range(0, n_test, batch_size):\n",
    "    end_idx = min(start_idx + batch_size, n_test)\n",
    "\n",
    "    # Load and preprocess images\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    for i in range(start_idx, end_idx):\n",
    "        img, label = cifar10_test[i]\n",
    "        batch_images.append(preprocess(img))\n",
    "        batch_labels.append(label)\n",
    "\n",
    "    image_batch = torch.stack(batch_images).to(device)\n",
    "    label_batch = torch.tensor(batch_labels)\n",
    "\n",
    "    # Encode images\n",
    "    with torch.no_grad():\n",
    "        image_feats = model.encode_image(image_batch)  # [batch, 512]\n",
    "\n",
    "    # TODO: Normalize image features to unit vectors\n",
    "    # Hint: image_feats = F.normalize(image_feats, dim=1)\n",
    "\n",
    "\n",
    "    # TODO: Compute similarity between each image and all 10 text prompts\n",
    "    # Result should be shape [batch, 10]\n",
    "    # Hint: similarities = image_feats @ text_features_cifar.T\n",
    "\n",
    "\n",
    "    # TODO: Find the predicted class for each image (index of highest similarity)\n",
    "    # Hint: predictions = similarities.argmax(dim=1).cpu()\n",
    "\n",
    "\n",
    "    # Track accuracy\n",
    "    all_predictions.extend(predictions.tolist())\n",
    "    all_labels.extend(batch_labels)\n",
    "\n",
    "    for pred, true_label in zip(predictions.tolist(), batch_labels):\n",
    "        class_name = cifar10_classes[true_label]\n",
    "        per_class_total[class_name] += 1\n",
    "        if pred == true_label:\n",
    "            correct += 1\n",
    "            per_class_correct[class_name] += 1\n",
    "        total += 1\n",
    "\n",
    "overall_accuracy = correct / total * 100\n",
    "\n",
    "print(f'Zero-shot CLIP accuracy on CIFAR-10: {correct}/{total} = {overall_accuracy:.1f}%')\n",
    "print(f'Random chance: 10.0%')\n",
    "print()\n",
    "print('Per-class accuracy:')\n",
    "for class_name in cifar10_classes:\n",
    "    n_correct = per_class_correct[class_name]\n",
    "    n_total = per_class_total[class_name]\n",
    "    acc = n_correct / n_total * 100 if n_total > 0 else 0\n",
    "    bar = '#' * int(acc / 5)\n",
    "    print(f'  {class_name:>12s}: {acc:5.1f}%  {bar}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The three TODOs implement the zero-shot classification pipeline. The key insight: this is the exact same cosine similarity computation from Exercises 1 and 2, just applied as a classifier.\n",
    "\n",
    "**Create text prompts:**\n",
    "```python\n",
    "text_prompts_cifar = [f\"a photo of a {c}\" for c in cifar10_classes]\n",
    "```\n",
    "\n",
    "**Normalize text features:**\n",
    "```python\n",
    "text_features_cifar = F.normalize(text_features_cifar, dim=1)\n",
    "```\n",
    "\n",
    "**Normalize image features:**\n",
    "```python\n",
    "image_feats = F.normalize(image_feats, dim=1)\n",
    "```\n",
    "\n",
    "**Compute similarities:**\n",
    "```python\n",
    "similarities = image_feats @ text_features_cifar.T\n",
    "```\n",
    "\n",
    "**Find predictions:**\n",
    "```python\n",
    "predictions = similarities.argmax(dim=1).cpu()\n",
    "```\n",
    "\n",
    "The pattern is always the same: normalize, matrix multiply, argmax. CLIP never saw CIFAR-10 during training. The shared embedding space generalizes — the text encoder IS the classification head. No retraining, no fine-tuning, no new parameters.\n",
    "\n",
    "Common mistake: forgetting to normalize features before computing similarity. Without normalization, the dot product is NOT cosine similarity and the magnitudes of the vectors will dominate the comparison.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions — show images with their predicted and true labels\n",
    "fig, axes = plt.subplots(2, 10, figsize=(16, 4))\n",
    "\n",
    "# Show first 10 correct and first 10 incorrect predictions\n",
    "correct_indices = [i for i in range(n_test) if all_predictions[i] == all_labels[i]]\n",
    "wrong_indices = [i for i in range(n_test) if all_predictions[i] != all_labels[i]]\n",
    "\n",
    "# Top row: correct predictions\n",
    "for col, idx in enumerate(correct_indices[:10]):\n",
    "    img, _ = cifar10_test[idx]\n",
    "    axes[0, col].imshow(img)\n",
    "    axes[0, col].set_title(f'{cifar10_classes[all_predictions[idx]]}', fontsize=8, color='#86efac')\n",
    "    axes[0, col].axis('off')\n",
    "\n",
    "# Bottom row: wrong predictions\n",
    "for col, idx in enumerate(wrong_indices[:10]):\n",
    "    img, _ = cifar10_test[idx]\n",
    "    true_name = cifar10_classes[all_labels[idx]]\n",
    "    pred_name = cifar10_classes[all_predictions[idx]]\n",
    "    axes[1, col].imshow(img)\n",
    "    axes[1, col].set_title(f'{pred_name}\\n(true: {true_name})', fontsize=7, color='#f87171')\n",
    "    axes[1, col].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Correct', fontsize=10, color='#86efac')\n",
    "axes[1, 0].set_ylabel('Wrong', fontsize=10, color='#f87171')\n",
    "plt.suptitle('CLIP Zero-Shot Predictions on CIFAR-10', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Top row: correct predictions (green).')\n",
    "print('Bottom row: incorrect predictions (red) with true labels.')\n",
    "print()\n",
    "print('CLIP never saw CIFAR-10 during training. It never saw these specific 10 classes.')\n",
    "print('The shared embedding space generalizes beyond the training data.')\n",
    "print(f'\\nAccuracy ({overall_accuracy:.1f}%) is far above random chance (10%) but below')\n",
    "print('a trained classifier (~95%). CLIP trades peak accuracy for generalization to')\n",
    "print('ANY text description — not just a fixed set of 10 classes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You performed zero-shot classification — classifying images into categories CLIP was never explicitly trained on:\n",
    "\n",
    "1. **Created text prompts** for each class using the template \"a photo of a [class].\"\n",
    "2. **Encoded** both images and text into the shared 512-dimensional space.\n",
    "3. **Found the best match** using cosine similarity — no training, no fine-tuning.\n",
    "\n",
    "The accuracy is well above random chance, demonstrating that the shared embedding space generalizes beyond training data. In *Transfer Learning*, you had to retrain the classification head. CLIP does not even need that — the text encoder IS the classification head.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Probing CLIP's Limitations [Independent]\n",
    "\n",
    "CLIP is impressive, but it learns statistical co-occurrence, not deep understanding. The lesson identified several systematic failure modes:\n",
    "\n",
    "- **Spatial relationships:** \"a cat on a table\" vs \"a table on a cat\"\n",
    "- **Counting:** \"three dogs\" vs \"five dogs\"\n",
    "- **Compositional reasoning:** unusual combinations of familiar concepts\n",
    "\n",
    "**Your task:** Design experiments to test at least 3 of CLIP's known limitations. For each test:\n",
    "1. Create two or more text prompts that differ in a way CLIP should distinguish\n",
    "2. Find or describe an image that matches one prompt but not the other\n",
    "3. Compute similarities and check whether CLIP can tell them apart\n",
    "\n",
    "Document where CLIP succeeds and where it fails. The goal is not to catch CLIP making mistakes (that is easy) — it is to understand **why** those mistakes happen and what they reveal about how the model represents information.\n",
    "\n",
    "**No skeleton code is provided.** Use the encoding and similarity computation patterns from the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#\n",
    "# Design experiments to probe CLIP's limitations.\n",
    "#\n",
    "# Pattern to reuse from previous exercises:\n",
    "#   text_tokens = tokenizer(list_of_prompts).to(device)\n",
    "#   with torch.no_grad():\n",
    "#       text_feats = model.encode_text(text_tokens)\n",
    "#   text_feats = F.normalize(text_feats, dim=1)\n",
    "#\n",
    "# For text-only comparisons (no images needed):\n",
    "#   Compare two prompts that differ in spatial order, count, etc.\n",
    "#   If CLIP embeddings are nearly identical for different meanings,\n",
    "#   that reveals a limitation.\n",
    "#\n",
    "# For image-text comparisons:\n",
    "#   Use images from the web or CIFAR-10 to test specific claims.\n",
    "#\n",
    "# Suggested experiments:\n",
    "#   1. Spatial: \"a cat on a table\" vs \"a table on a cat\"\n",
    "#   2. Counting: \"one dog\" vs \"three dogs\" vs \"five dogs\"\n",
    "#   3. Negation: \"a photo of a dog\" vs \"a photo with no dog\"\n",
    "#   4. Attribute binding: \"a red car and a blue house\" vs \"a blue car and a red house\"\n",
    "#   5. Abstract: \"happiness\" vs \"sadness\" vs a photo of people smiling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The experiments reveal CLIP's systematic limitations. The key insight: CLIP's embedding space encodes **what things are** but not **how they relate spatially, how many there are, or how attributes bind to objects**.\n",
    "\n",
    "```python\n",
    "def compare_prompts(prompt_list, title=\"Prompt Comparison\"):\n",
    "    \"\"\"Compute and display pairwise similarities between text prompts.\"\"\"\n",
    "    tokens = tokenizer(prompt_list).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model.encode_text(tokens)\n",
    "    features = F.normalize(features, dim=1)\n",
    "    sim = (features @ features.T).cpu()\n",
    "\n",
    "    print(f'\\n{title}')\n",
    "    print('=' * 60)\n",
    "    for i, p in enumerate(prompt_list):\n",
    "        print(f'  [{i}] \"{p}\"')\n",
    "    print()\n",
    "    for i in range(len(prompt_list)):\n",
    "        for j in range(i + 1, len(prompt_list)):\n",
    "            print(f'  [{i}] vs [{j}]: {sim[i, j]:.4f}')\n",
    "    return sim\n",
    "\n",
    "# Test 1: Spatial relationships\n",
    "compare_prompts([\n",
    "    \"a cat sitting on a table\",\n",
    "    \"a table sitting on a cat\",\n",
    "], title=\"Spatial: Does CLIP understand 'on'?\")\n",
    "# Expected: very high similarity (>0.9) — CLIP cannot distinguish word order\n",
    "\n",
    "# Test 2: Counting\n",
    "compare_prompts([\n",
    "    \"one dog\",\n",
    "    \"three dogs\",\n",
    "    \"five dogs\",\n",
    "    \"ten dogs\",\n",
    "], title=\"Counting: Does CLIP distinguish quantities?\")\n",
    "# Expected: all very similar — CLIP encodes 'dog' strongly but 'number of dogs' weakly\n",
    "\n",
    "# Test 3: Negation\n",
    "compare_prompts([\n",
    "    \"a photo of a dog\",\n",
    "    \"a photo with no dog\",\n",
    "    \"a photo of a cat\",\n",
    "], title=\"Negation: Does CLIP understand 'no'?\")\n",
    "# Expected: \"a photo of a dog\" and \"a photo with no dog\" are more similar\n",
    "# than \"a photo of a dog\" and \"a photo of a cat\" — CLIP ignores negation\n",
    "\n",
    "# Test 4: Attribute binding\n",
    "compare_prompts([\n",
    "    \"a red car and a blue house\",\n",
    "    \"a blue car and a red house\",\n",
    "], title=\"Attribute Binding: Does CLIP bind colors to objects?\")\n",
    "# Expected: very high similarity — same words, different bindings\n",
    "\n",
    "# Test 5: What CLIP IS good at — basic categories\n",
    "compare_prompts([\n",
    "    \"a photo of a cat\",\n",
    "    \"a photo of a dog\",\n",
    "    \"a photo of a car\",\n",
    "    \"a photo of a mountain\",\n",
    "], title=\"Baseline: Basic category distinction (should work well)\")\n",
    "# Expected: moderate similarities between all pairs, clearly distinguishable\n",
    "```\n",
    "\n",
    "**Why these failures happen:** CLIP's training objective is contrastive matching — \"does this text go with this image?\" at the level of whole captions and whole images. It learns *what things are* (bag-of-concepts) but not the *relationships between things* (spatial structure, quantity, attribute binding). The embedding is a compressed summary of the entire input, not a structured representation of its parts.\n",
    "\n",
    "**What CLIP is good at:** Distinguishing categories (cat vs car), recognizing styles (painting vs photograph), matching descriptions to images at a semantic level. These are exactly the capabilities that matter for conditioning a diffusion model — the U-Net needs to know WHAT to generate, and CLIP provides that.\n",
    "\n",
    "**What CLIP is bad at:** Spatial layout, counting, negation, attribute binding — the compositional aspects of language. These limitations carry through to text-to-image models that rely on CLIP embeddings.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The similarity matrix IS the core of CLIP.** Normalize embeddings, matrix multiply, get cosine similarities. Matching pairs on the diagonal, non-matches off-diagonal. Each row is a classification problem with cross-entropy loss.\n",
    "\n",
    "2. **Two encoders, one shared space — the loss function creates the alignment.** The image encoder and text encoder share no weights. Only the contrastive loss — symmetric cross-entropy on the similarity matrix — forces their embedding spaces into alignment.\n",
    "\n",
    "3. **The shared space enables zero-shot transfer.** Classify any image by comparing its embedding to text descriptions of candidate classes. No training on the target classes required. The text encoder IS the classification head.\n",
    "\n",
    "4. **Useful does not mean perfect.** CLIP encodes what things are and what words mean, well enough to match them. It does not reliably encode spatial relationships, counts, negation, or attribute binding. Understanding the limitations is as important as understanding the capabilities.\n",
    "\n",
    "5. **Same building blocks, different question.** CNN/ViT image encoder, transformer text encoder, cosine similarity, cross-entropy loss — every piece is familiar from earlier in the course. The question is new: \"do this text and image match?\" The answer gives us text embeddings that encode visual meaning — exactly what the U-Net needs to generate images from text descriptions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}