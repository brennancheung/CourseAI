{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate with Stable Diffusion\n",
    "\n",
    "**Module 6.4, Lesson 3** | CourseAI\n",
    "\n",
    "For 16 lessons, you built every concept from scratch. You learned to compress images (VAE), to destroy and reconstruct (diffusion), to condition on time and text (U-Net, CLIP, cross-attention, CFG), to work in latent space, to assemble the pipeline, and to choose a sampler. This is the payoff.\n",
    "\n",
    "**What you will do:**\n",
    "- Load Stable Diffusion with the diffusers library and generate your first image with full understanding of what happens inside\n",
    "- Sweep `guidance_scale` and `num_inference_steps` to see the tradeoffs you predicted from the CFG and sampler lessons\n",
    "- Compare three schedulers on the same prompt/seed and measure the speed-quality tradeoff\n",
    "- Design your own controlled experiment to test a hypothesis about parameter interactions\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "This is a CONSOLIDATE notebook. There are zero new concepts. Every parameter maps to something you already know. The goal is not challengeâ€”it is satisfaction: \"I know what every parameter does.\"\n",
    "\n",
    "**Estimated time:** 30â€“45 minutes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies and import everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float16 if device.type == 'cuda' else torch.float32\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB')\n",
    "\n",
    "print('\\nSetup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Helpers\n",
    "\n",
    "Load the Stable Diffusion pipeline once and define helper functions. All exercises share the same model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import (\n",
    "    StableDiffusionPipeline,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    DDIMScheduler,\n",
    "    EulerDiscreteScheduler,\n",
    ")\n",
    "\n",
    "model_id = 'stable-diffusion-v1-5/stable-diffusion-v1-5'\n",
    "\n",
    "# Load the pipeline once.\n",
    "print('Loading Stable Diffusion pipeline...')\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=dtype,\n",
    "    safety_checker=None,\n",
    "    requires_safety_checker=False,\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "print(f'Pipeline loaded on {device}.')\n",
    "\n",
    "# Save the scheduler config so we can create fresh schedulers from it.\n",
    "scheduler_config = pipe.scheduler.config\n",
    "\n",
    "\n",
    "def show_images(images, titles, figsize=None):\n",
    "    \"\"\"Display a list of PIL images side by side.\"\"\"\n",
    "    n = len(images)\n",
    "    if figsize is None:\n",
    "        figsize = (5 * n, 5)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(np.array(img))\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_image_grid(images, titles, nrows, ncols, figsize=None, suptitle=None):\n",
    "    \"\"\"Display images in a grid with the given number of rows and columns.\"\"\"\n",
    "    if figsize is None:\n",
    "        figsize = (4 * ncols, 4 * nrows)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    axes_flat = axes.flat if nrows > 1 or ncols > 1 else [axes]\n",
    "    for ax, img, title in zip(axes_flat, images, titles):\n",
    "        ax.imshow(np.array(img))\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    # Hide any extra axes\n",
    "    for ax in list(axes_flat)[len(images):]:\n",
    "        ax.axis('off')\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle, fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print('Helpers defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Your First Generation [Guided]\n",
    "\n",
    "This is the moment. You will load the pipeline, call `pipe()`, and get an image. Every parameter in this call maps to something you built:\n",
    "\n",
    "| Parameter | What You Know It As | Source |\n",
    "|-----------|-------------------|--------|\n",
    "| `prompt` | CLIP tokenization â†’ [1, 77, 768] embeddings â†’ cross-attention K/V | CLIP, Text Conditioning & Guidance |\n",
    "| `guidance_scale` | The *w* in Îµ_cfg = Îµ_uncond + w Â· (Îµ_cond âˆ’ Îµ_uncond) | Text Conditioning & Guidance |\n",
    "| `num_inference_steps` | Sampler step count along the ODE trajectory | Samplers and Efficiency |\n",
    "| `generator` | Seed â†’ z_T [4, 64, 64], the initial random latent | The SD Pipeline |\n",
    "| `height` / `width` | Pixel dimensions â†’ latent dimensions via 8Ã— VAE downsampling | From Pixels to Latents |\n",
    "\n",
    "**Before running, predict:**\n",
    "- The pipeline will use DPM-Solver++ as the default scheduler. With 25 steps, how many U-Net forward passes will it make? (Remember: CFG means two passes per step.)\n",
    "- What is the latent tensor shape for a 512Ã—512 image? (Think: 512/8 = ?)\n",
    "- With `guidance_scale=7.5`, how strongly is the text conditioning amplified relative to the unconditional prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the scheduler to DPM-Solver++ (the recommended default from Samplers and Efficiency)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(scheduler_config)\n",
    "\n",
    "# Generate your first image.\n",
    "# Every parameter here maps to a concept you built:\n",
    "#   prompt         -> CLIP encoding -> cross-attention (Lessons 12-13)\n",
    "#   guidance_scale -> CFG w parameter (Lesson 13)\n",
    "#   num_inference_steps -> sampler step count (Lesson 16)\n",
    "#   generator      -> seed -> z_T (Lesson 15)\n",
    "#   height/width   -> latent dimensions via VAE 8x downsampling (Lesson 14)\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "result = pipe(\n",
    "    prompt=\"a cat sitting on a beach at sunset\",\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=25,\n",
    "    generator=generator,\n",
    "    height=512,\n",
    "    width=512,\n",
    ")\n",
    "\n",
    "image = result.images[0]\n",
    "show_images([image], ['\"a cat sitting on a beach at sunset\"'], figsize=(6, 6))\n",
    "\n",
    "# Verify your predictions:\n",
    "print(f'Scheduler: {pipe.scheduler.__class__.__name__}')\n",
    "print(f'Steps: 25 -> U-Net forward passes: {25 * 2} (2 per step for CFG)')\n",
    "print(f'Image size: {image.size[0]}x{image.size[1]} pixels')\n",
    "print(f'Latent size: {image.size[0]//8}x{image.size[1]//8}x4 = [{4}, {image.size[1]//8}, {image.size[0]//8}]')\n",
    "print(f'guidance_scale=7.5: the text direction is amplified 7.5x relative to unconditional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now change the prompt. Same seed, same everything else.\n",
    "# The seed determines z_T (the structural \"skeleton\"). The prompt determines\n",
    "# what concept the cross-attention steers toward.\n",
    "#\n",
    "# Before running, predict: will the composition (layout, shapes) be similar\n",
    "# to the cat image, or completely different?\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "result_2 = pipe(\n",
    "    prompt=\"a dog playing in a field of flowers\",\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=25,\n",
    "    generator=generator,\n",
    "    height=512,\n",
    "    width=512,\n",
    ")\n",
    "\n",
    "show_images(\n",
    "    [image, result_2.images[0]],\n",
    "    ['Same seed: \"a cat on a beach at sunset\"', 'Same seed: \"a dog in a field of flowers\"'],\n",
    "    figsize=(12, 6),\n",
    ")\n",
    "\n",
    "print('Same seed = same z_T = same structural starting point.')\n",
    "print('Different prompt = different cross-attention steering = different content.')\n",
    "print('The seed controls the \"skeleton.\" The prompt controls the \"concept.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You generated your first image with **full comprehension** of what happened inside:\n",
    "\n",
    "1. **Prompt** â†’ tokenized to at most 77 tokens, padded with SOT/EOT â†’ CLIP text encoder produced [1, 77, 768] contextual embeddings\n",
    "2. **Seed (42)** â†’ `torch.Generator` sampled z_T with shape [4, 64, 64] (the initial random latent in the VAE's compressed space)\n",
    "3. **25 steps with DPM-Solver++** â†’ at each step, the U-Net ran twice (once conditioned on the prompt, once unconditional for CFG), the scheduler combined the predictions using Îµ_cfg = Îµ_uncond + 7.5 Â· (Îµ_cond âˆ’ Îµ_uncond), and updated z_t\n",
    "4. **VAE decode** â†’ the final z_0 [4, 64, 64] was decoded to [3, 512, 512] pixel image\n",
    "\n",
    "When you changed the prompt but kept the same seed, the z_T starting point was identical. The structural decisions (composition, layout) are driven by early denoising steps where the noise pattern from z_T dominates. The prompt changes *what* appears in that structure via cross-attention, but the overall composition shares similarities because z_T is the same.\n",
    "\n",
    "This is not a tutorial. You know what the machine does inside.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Parameter Sweeps [Guided]\n",
    "\n",
    "Now you will see the parameters you studied come to life. Two systematic sweeps, both using the predict-then-verify pattern:\n",
    "\n",
    "**Part A â€” Guidance Scale Sweep**\n",
    "\n",
    "You know `guidance_scale` is the *w* in the CFG formula: Îµ_cfg = Îµ_uncond + w Â· (Îµ_cond âˆ’ Îµ_uncond). It is the \"contrast slider\" from Text Conditioning & Guidance.\n",
    "\n",
    "**Before running, predict:**\n",
    "- At w=1, there is no amplification. The conditional prediction is used directly. Will the image follow the prompt closely?\n",
    "- At w=7.5 (the typical default), how balanced will the prompt fidelity vs image naturalness be?\n",
    "- At w=25, the conditional-minus-unconditional direction is amplified 25Ã—. What happens when you extrapolate that far? (Think: oversaturated colors? Unnatural contrast? Distortion?)\n",
    "\n",
    "**Part B â€” Step Count Sweep**\n",
    "\n",
    "You know from Samplers and Efficiency that DPM-Solver++ plateaus around 20â€“30 steps.\n",
    "\n",
    "**Before running, predict:**\n",
    "- At 5 steps, how much of the ODE trajectory can DPM-Solver accurately traverse?\n",
    "- At 20 steps, will quality be acceptable?\n",
    "- Between 50 and 100 steps, will you see any visible improvement? (Remember: the quality curve plateaus.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A: Guidance Scale Sweep\n",
    "# Fix everything except guidance_scale. One variable at a time.\n",
    "\n",
    "prompt = \"a cat sitting on a beach at sunset\"\n",
    "seed = 42\n",
    "guidance_values = [1, 3, 7.5, 15, 25]\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(scheduler_config)\n",
    "\n",
    "guidance_images = []\n",
    "for w in guidance_values:\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    result = pipe(\n",
    "        prompt,\n",
    "        guidance_scale=w,\n",
    "        num_inference_steps=25,\n",
    "        generator=generator,\n",
    "        height=512,\n",
    "        width=512,\n",
    "    )\n",
    "    guidance_images.append(result.images[0])\n",
    "    print(f'  w={w} done')\n",
    "\n",
    "titles = [f'w = {w}' for w in guidance_values]\n",
    "show_images(guidance_images, titles, figsize=(25, 5))\n",
    "\n",
    "print('\\n=== Guidance Scale as the \"Contrast Slider\" ===')\n",
    "print('w=1:   No amplification. Conditional prediction used directly. Soft, may not follow prompt well.')\n",
    "print('w=3:   Mild amplification. Partially follows prompt. Dreamlike.')\n",
    "print('w=7.5: Balanced. Prompt-faithful with natural quality. The typical default.')\n",
    "print('w=15:  Strong amplification. Colors pushed. Oversaturated.')\n",
    "print('w=25:  Extreme extrapolation. Distorted, unnatural. The CFG formula overshoots.')\n",
    "print()\n",
    "print('guidance_scale is NOT a quality dial. It is a tradeoff between')\n",
    "print('prompt fidelity and image naturalness. Higher = \"follow the text harder,\"')\n",
    "print('not \"make the image better.\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B: Step Count Sweep\n",
    "# Fix everything except num_inference_steps. Same prompt, seed, guidance.\n",
    "\n",
    "step_counts = [5, 10, 20, 50, 100]\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(scheduler_config)\n",
    "\n",
    "step_images = []\n",
    "step_timings = []\n",
    "\n",
    "for n in step_counts:\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    start = time.time()\n",
    "    result = pipe(\n",
    "        prompt,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=n,\n",
    "        generator=generator,\n",
    "        height=512,\n",
    "        width=512,\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    step_images.append(result.images[0])\n",
    "    step_timings.append(elapsed)\n",
    "    print(f'  {n:>3d} steps: {elapsed:.1f}s')\n",
    "\n",
    "titles = [f'{n} steps ({t:.1f}s)' for n, t in zip(step_counts, step_timings)]\n",
    "show_images(step_images, titles, figsize=(25, 5))\n",
    "\n",
    "# Time comparison\n",
    "print('\\n=== Step Count vs Generation Time ===')\n",
    "for n, t in zip(step_counts, step_timings):\n",
    "    bar = 'â–ˆ' * int(t / max(step_timings) * 30)\n",
    "    print(f'  {n:>3d} steps: {t:>5.1f}s  {bar}')\n",
    "\n",
    "print(f'\\nSpeed ratio (100 steps / 20 steps): {step_timings[step_counts.index(100)] / step_timings[step_counts.index(20)]:.1f}x')\n",
    "print(f'Quality difference at 20 vs 100 steps: look at the images above.')\n",
    "print(f'The quality plateau is real. More steps past 20-30 with DPM-Solver++ is wasted compute.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "**Guidance scale sweep:** You verified the \"contrast slider\" analogy from Text Conditioning & Guidance. At low *w*, the unconditional prediction has too much influenceâ€”the image is soft and may not match the prompt. At high *w*, the conditional-minus-unconditional direction is amplified so aggressively that the image overshoots into oversaturation and distortion. The sweet spot (w=7â€“8) balances prompt fidelity with image naturalness. This is the CFG formula you implemented, made visible.\n",
    "\n",
    "**Step count sweep:** You verified the sweet spot from Samplers and Efficiency. DPM-Solver++ achieves good quality at ~20 steps because its higher-order method accurately follows the ODE trajectory with large steps. At 5 steps, the trajectory is traversed too coarsely (missing fine detail). At 50â€“100 steps, quality is indistinguishable from 20â€”but generation time scales linearly because each step requires 2 U-Net forward passes for CFG.\n",
    "\n",
    "The systematic workflow: fix everything except one parameter, vary it, observe the effect. This is controlled experimentation applied to generative AI.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Sampler Comparison [Supported]\n",
    "\n",
    "The lesson's central insight: **the model defines where to go, the sampler defines how to get there.** In the previous lesson's notebook, you compared DDPM, DDIM, and DPM-Solver. Now you will compare three practical schedulers at their recommended step counts using the high-level `pipe()` API.\n",
    "\n",
    "Your task: generate the **same image** (same prompt, same seed) with three different schedulers:\n",
    "1. **DPM-Solver++** at 25 steps (the current standard)\n",
    "2. **DDIM** at 50 steps (deterministic, for reproducibility)\n",
    "3. **Euler** at 30 steps (simple, good for debugging)\n",
    "\n",
    "Display all three images with their generation times. Then answer:\n",
    "- Are the three images identical? Why or why not? (Think: same model, same z_T, but different ODE solvers follow different numerical paths.)\n",
    "- Which is fastest? Does that match the step count?\n",
    "\n",
    "**Hints:**\n",
    "- Use `DPMSolverMultistepScheduler.from_config(scheduler_config)` to create DPM-Solver++\n",
    "- Use `DDIMScheduler.from_config(scheduler_config)` to create DDIM\n",
    "- Use `EulerDiscreteScheduler.from_config(scheduler_config)` to create Euler\n",
    "- Remember to create a fresh generator (same seed) for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a cozy cabin in a snowy forest at night, warm light from windows\"\n",
    "seed = 77\n",
    "guidance_scale = 7.5\n",
    "\n",
    "# Define the three scheduler configurations: (scheduler_class, num_steps, label)\n",
    "scheduler_configs = [\n",
    "    (DPMSolverMultistepScheduler, 25, 'DPM-Solver++ (25 steps)'),\n",
    "    (DDIMScheduler, 50, 'DDIM (50 steps)'),\n",
    "    (EulerDiscreteScheduler, 30, 'Euler (30 steps)'),\n",
    "]\n",
    "\n",
    "sampler_images = []\n",
    "sampler_timings = []\n",
    "\n",
    "for scheduler_cls, num_steps, label in scheduler_configs:\n",
    "    print(f'--- {label} ---')\n",
    "\n",
    "    # TODO: Set the pipeline's scheduler to a fresh instance of scheduler_cls.\n",
    "    # Use scheduler_cls.from_config(scheduler_config).\n",
    "    # (Look at Exercise 1 or Exercise 2 for the pattern.)\n",
    "\n",
    "    # TODO: Create a generator with the same seed for fair comparison.\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # TODO: Call pipe() with the prompt, num_steps, guidance_scale, generator,\n",
    "    # height=512, width=512. Store the result.\n",
    "    result = None\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    if result is not None:\n",
    "        sampler_images.append(result.images[0])\n",
    "        sampler_timings.append(elapsed)\n",
    "        print(f'  Scheduler: {pipe.scheduler.__class__.__name__}')\n",
    "        print(f'  Time: {elapsed:.1f}s')\n",
    "        print(f'  U-Net passes: {num_steps * 2}')\n",
    "    else:\n",
    "        print('  TODO: fill in the code above')\n",
    "        break\n",
    "\n",
    "# Display results\n",
    "if len(sampler_images) == 3:\n",
    "    titles = [\n",
    "        f'{label}\\n{t:.1f}s'\n",
    "        for (_, _, label), t in zip(scheduler_configs, sampler_timings)\n",
    "    ]\n",
    "    show_images(sampler_images, titles, figsize=(18, 6))\n",
    "\n",
    "    print('\\n=== Sampler Comparison ===')\n",
    "    for (_, steps, label), t in zip(scheduler_configs, sampler_timings):\n",
    "        print(f'  {label}: {t:.1f}s')\n",
    "    print()\n",
    "    print('Same model. Same weights. Same seed. Same prompt.')\n",
    "    print('Different samplers follow different numerical paths through the same ODE.')\n",
    "    print('The images are NOT identical, but the quality is comparable.')\n",
    "else:\n",
    "    print('\\nFill in the TODOs above to complete this exercise.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe key insight: all three schedulers use the same trained U-Net weights. The model's prediction (noise) is the same. The schedulers differ in *how they use that prediction to step from z_t to z_{t-1}*. Different numerical methods follow different paths through the same trajectory, producing similar but not identical images.\n\n```python\n    # Set the scheduler\n    pipe.scheduler = scheduler_cls.from_config(scheduler_config)\n\n    # Same seed for fair comparison\n    generator = torch.Generator(device=device).manual_seed(seed)\n\n    start = time.time()\n\n    result = pipe(\n        prompt,\n        num_inference_steps=num_steps,\n        guidance_scale=guidance_scale,\n        generator=generator,\n        height=512,\n        width=512,\n    )\n```\n\n**What to observe:**\n- The three images depict the same scene but with subtle differences in composition, color, and detail.\n- DPM-Solver++ at 25 steps is likely fastest (fewest steps, higher-order solver).\n- DDIM at 50 steps takes roughly twice as long (twice the steps, first-order solver).\n- Euler at 30 steps is in between.\n- The quality is comparable across all three. The \"same vehicle, different route\" analogy holds.\n\n**Common mistakes:**\n1. **Forgetting to re-seed the generator.** Without creating a fresh `torch.Generator(device=device).manual_seed(seed)` for each scheduler, z_T differs between runs, making the comparison meaningless. The generator's state is consumed during sampling and cannot be reused.\n2. **Generator device mismatch.** If the pipeline is on CUDA but you create `torch.Generator(\"cpu\").manual_seed(seed)`, you will get an error or silently incorrect results. Always match the generator's device to the pipeline: `torch.Generator(device=device).manual_seed(seed)` where `device` matches `pipe.device`. This is a frequent diffusers stumbling block.\n\n</details>\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Design Your Own Experiment [Independent]\n",
    "\n",
    "You have explored `guidance_scale`, `num_inference_steps`, and `scheduler`. Now it is your turn to design a controlled experiment.\n",
    "\n",
    "**Your task:**\n",
    "1. **Pick a parameter** to investigate. Suggestions:\n",
    "   - `negative_prompt` â€” How does adding \"blurry, low quality, deformed\" change the generation? (Remember: negative prompts replace the empty-string unconditional in CFG. They steer, not erase.)\n",
    "   - **Seed variation** â€” How much does the seed change the image while keeping the same prompt? (Seeds control the structural \"skeleton\" via z_T.)\n",
    "   - **Prompt wording** â€” Does \"a cat sitting on a beach\" vs \"cat beach sitting\" produce different results? (CLIP's transformer produces context-dependent embeddings.)\n",
    "   - **Parameter interaction** â€” Does guidance_scale interact with num_inference_steps? (Try low guidance at few steps vs high guidance at few steps.)\n",
    "\n",
    "2. **Form a hypothesis** â€” predict what will happen, based on the concepts you know.\n",
    "\n",
    "3. **Design a controlled comparison** â€” fix everything except your target variable. Generate a set of images.\n",
    "\n",
    "4. **Display the results** in a grid.\n",
    "\n",
    "5. **Write a one-sentence conclusion** â€” did your hypothesis hold? If not, what did you learn?\n",
    "\n",
    "This is your experiment. There is no single correct answer. The goal is systematic exploration with understanding, not random parameter tweaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR EXPERIMENT\n",
    "#\n",
    "# 1. Choose your parameter and state your hypothesis\n",
    "# 2. Set up the controlled comparison (fix everything else)\n",
    "# 3. Generate images\n",
    "# 4. Display them\n",
    "# 5. Print your conclusion\n",
    "#\n",
    "# Use the patterns from Exercises 1-3:\n",
    "#   - pipe.scheduler = SchedulerClass.from_config(scheduler_config)\n",
    "#   - generator = torch.Generator(device=device).manual_seed(seed)\n",
    "#   - result = pipe(prompt, guidance_scale=..., num_inference_steps=...,\n",
    "#                   generator=generator, height=512, width=512)\n",
    "#   - For negative prompts: result = pipe(prompt, negative_prompt=\"...\", ...)\n",
    "#   - show_images() or show_image_grid() to display results\n",
    "\n",
    "print('Hypothesis: ')\n",
    "print()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print()\n",
    "print('Conclusion: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Example Experiment: Negative Prompt Steering</summary>\n",
    "\n",
    "**Hypothesis:** Adding a negative prompt \"blurry, low quality, deformed, watermark\" will produce a noticeably sharper, cleaner image than generating without a negative promptâ€”not by removing blur from the image, but by steering the entire generation trajectory away from blurry outputs from step 1 onward.\n",
    "\n",
    "**Why this hypothesis:** The negative prompt replaces the empty-string unconditional embedding in the CFG formula. Instead of Îµ_cfg = Îµ_uncond + w Â· (Îµ_cond âˆ’ Îµ_uncond), it becomes Îµ_cfg = Îµ_neg + w Â· (Îµ_cond âˆ’ Îµ_neg). The generation is steered AWAY from the negative prompt's semantic direction at every step. It is a compass pointing away from undesirable directions, not an eraser.\n",
    "\n",
    "```python\n",
    "prompt = \"a detailed portrait of an elderly woman, oil painting\"\n",
    "seed = 42\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(scheduler_config)\n",
    "\n",
    "# Without negative prompt\n",
    "gen = torch.Generator(device=device).manual_seed(seed)\n",
    "img_without = pipe(\n",
    "    prompt, guidance_scale=7.5, num_inference_steps=25,\n",
    "    generator=gen, height=512, width=512,\n",
    ").images[0]\n",
    "\n",
    "# With negative prompt\n",
    "gen = torch.Generator(device=device).manual_seed(seed)\n",
    "img_with = pipe(\n",
    "    prompt,\n",
    "    negative_prompt=\"blurry, low quality, deformed, watermark\",\n",
    "    guidance_scale=7.5, num_inference_steps=25,\n",
    "    generator=gen, height=512, width=512,\n",
    ").images[0]\n",
    "\n",
    "show_images(\n",
    "    [img_without, img_with],\n",
    "    ['No negative prompt', 'negative_prompt=\"blurry, low quality, deformed, watermark\"'],\n",
    "    figsize=(12, 6),\n",
    ")\n",
    "\n",
    "print('Conclusion: The negative prompt produces a cleaner, more detailed image.')\n",
    "print('The two images are NOT the same image with blur removed.')\n",
    "print('They are fundamentally different generations because the CFG direction')\n",
    "print('changed at every step. The negative prompt steered the trajectory,')\n",
    "print('not erased artifacts from the result.')\n",
    "```\n",
    "\n",
    "**What to notice:**\n",
    "- The two images are not the same composition with blur removed. They are different generations because the CFG direction was different at every denoising step.\n",
    "- This confirms that negative prompts are a *steering* mechanism (directional change in the CFG formula), not a *post-processing* mechanism (no erasing).\n",
    "- The negative prompt is most effective at preventing common failure modes (blur, deformation) rather than removing specific content.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Example Experiment: Prompt Wording (Sentences vs Keywords)</summary>\n",
    "\n",
    "**Hypothesis:** \"a cat sitting on a beach at sunset\" will produce a higher-quality, more coherent image than \"cat beach sunset sitting a on\" because CLIP's transformer produces context-dependent embeddingsâ€”word order and syntax matter.\n",
    "\n",
    "```python\n",
    "prompts = [\n",
    "    \"a cat sitting on a beach at sunset\",\n",
    "    \"cat beach sunset sitting a on\",\n",
    "    \"a beach sitting on a cat at sunset\",\n",
    "]\n",
    "seed = 42\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(scheduler_config)\n",
    "\n",
    "wording_images = []\n",
    "for p in prompts:\n",
    "    gen = torch.Generator(device=device).manual_seed(seed)\n",
    "    result = pipe(\n",
    "        p, guidance_scale=7.5, num_inference_steps=25,\n",
    "        generator=gen, height=512, width=512,\n",
    "    )\n",
    "    wording_images.append(result.images[0])\n",
    "\n",
    "show_images(\n",
    "    wording_images,\n",
    "    [f'\"{p}\"' for p in prompts],\n",
    "    figsize=(18, 6),\n",
    ")\n",
    "\n",
    "print('Conclusion: Prompts are sentences, not keyword bags.')\n",
    "print('CLIP\\'s self-attention makes each token\\'s embedding context-dependent.')\n",
    "print('Scrambled syntax produces different embeddings, different cross-attention,')\n",
    "print('and therefore different (usually worse) images.')\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Every parameter maps to a concept you built from scratch.** `prompt` â†’ CLIP encoding â†’ cross-attention. `guidance_scale` â†’ CFG *w* parameter. `num_inference_steps` â†’ sampler step count along the ODE trajectory. `scheduler` â†’ sampler choice. `negative_prompt` â†’ CFG unconditional substitution. `generator` â†’ seed â†’ z_T. `height`/`width` â†’ latent dimensions via VAE 8Ã— downsampling.\n",
    "\n",
    "2. **`guidance_scale` is a tradeoff, not a quality dial.** Low values produce soft, prompt-unfaithful images. High values produce oversaturated, distorted images. The sweet spot (~7â€“8) balances prompt fidelity with image naturalness. This is the CFG formula made visible.\n",
    "\n",
    "3. **DPM-Solver++ plateaus at 20â€“30 steps.** More steps past the sweet spot waste compute with negligible quality improvement. The default of 50 in many tutorials is a conservative holdover from DDIM. Use 20â€“25 steps as your default.\n",
    "\n",
    "4. **Systematic experimentation beats random exploration.** Fix everything except one parameter, vary it, observe the effect. Start with defaults (DPM-Solver++ at 25 steps, guidance_scale=7.5, 512Ã—512) and refine from there.\n",
    "\n",
    "5. **The API is a dashboard, not a black box.** You did not follow a tutorial. You understood the system. Every parameter change had a predictable effect because you built the underlying concepts across 16 lessons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}