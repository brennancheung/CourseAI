{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Preference Optimization\n",
    "\n",
    "In this notebook, you'll implement DPO from scratch and train a small model with it.\n",
    "\n",
    "**What you'll do:**\n",
    "- Compute the DPO loss by hand for preference pairs, verifying the formula with real numbers\n",
    "- Implement the DPO loss function in PyTorch (~10 lines) and inspect its gradients\n",
    "- Train GPT-2 small on preference pairs using your DPO loss function\n",
    "- Extract the implicit reward model from the trained policy and verify it generalizes\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones — they reveal gaps in your mental model.\n",
    "\n",
    "**Important:** Exercises are cumulative. Exercise 2 verifies against Exercise 1's numbers. Exercise 3 uses Exercise 2's loss function. Exercise 4 uses Exercise 3's trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup — self-contained for Google Colab\n",
    "# transformers and torch are pre-installed in Colab\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared helper: compute log-probability of a response given a prompt\n",
    "#\n",
    "# This is the fundamental quantity in DPO. The loss operates on\n",
    "# log P(response | prompt) under both the policy and reference models.\n",
    "# A full response's log-probability is the sum of per-token log-probs.\n",
    "\n",
    "def get_response_log_prob(model, tokenizer, prompt: str, response: str) -> torch.Tensor:\n",
    "    \"\"\"Compute log P(response | prompt) under a model.\n",
    "\n",
    "    Returns a scalar tensor (sum of per-token log-probs).\n",
    "    Keeps gradients if the model has requires_grad=True.\n",
    "    \"\"\"\n",
    "    prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    full_ids = tokenizer.encode(prompt + response, return_tensors=\"pt\").to(model.device)\n",
    "    response_start = prompt_ids.shape[1]\n",
    "\n",
    "    # If the response is empty or only the prompt, return 0\n",
    "    if full_ids.shape[1] <= response_start:\n",
    "        return torch.tensor(0.0, device=model.device)\n",
    "\n",
    "    outputs = model(full_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # For each response position, get the log-prob of the actual next token\n",
    "    # logits[0, t] predicts token t+1, so we shift by 1\n",
    "    log_probs_all = F.log_softmax(logits[0], dim=-1)\n",
    "    response_token_ids = full_ids[0, response_start:]\n",
    "    token_log_probs = log_probs_all[response_start - 1 : -1]\n",
    "    token_log_probs = token_log_probs.gather(1, response_token_ids.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    return token_log_probs.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Verify the DPO Loss by Hand (Guided)\n",
    "\n",
    "The DPO loss for a single preference pair is:\n",
    "\n",
    "$$\\mathcal{L}_\\text{DPO} = -\\log \\sigma\\!\\left(\\beta \\left(\\log \\frac{\\pi(y_w|x)}{\\pi_\\text{ref}(y_w|x)} - \\log \\frac{\\pi(y_l|x)}{\\pi_\\text{ref}(y_l|x)}\\right)\\right)$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function and $\\beta$ controls how much the policy can deviate from the reference.\n",
    "\n",
    "In this exercise, you have pre-computed log-probabilities for 5 preference pairs. You will:\n",
    "1. Compute the DPO loss for each pair step by step\n",
    "2. Compute the total loss (average across pairs)\n",
    "3. Vary $\\beta$ and observe how it controls conservatism\n",
    "\n",
    "**Before running, predict:**\n",
    "- For a pair where the policy already strongly prefers the preferred response (large positive log-ratio difference), will the loss be large or small?\n",
    "- For a pair where the policy disagrees with the human (negative log-ratio difference), will the loss be large or small?\n",
    "- As $\\beta$ increases, does the loss become more or less sensitive to the log-ratio difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pre-computed log-probabilities for 5 preference pairs ---\n",
    "#\n",
    "# These are log P(response | prompt) under the policy and reference models.\n",
    "# Negative numbers (log-probs of full sequences are always negative).\n",
    "# The policy has started to learn preferences but hasn't converged.\n",
    "\n",
    "preference_pairs = [\n",
    "    {\n",
    "        \"prompt\": \"Explain quantum computing to a 10-year-old.\",\n",
    "        \"preferred\": \"Age-appropriate analogy response\",\n",
    "        \"dispreferred\": \"Jargon-heavy technical response\",\n",
    "        \"policy_logp_w\": -45.2,   # log pi(y_w|x)\n",
    "        \"policy_logp_l\": -42.8,   # log pi(y_l|x)\n",
    "        \"ref_logp_w\": -48.1,      # log pi_ref(y_w|x)\n",
    "        \"ref_logp_l\": -43.0,      # log pi_ref(y_l|x)\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Is it safe to eat raw cookie dough?\",\n",
    "        \"preferred\": \"Nuanced safety answer with caveats\",\n",
    "        \"dispreferred\": \"Overconfident 'yes it's fine' answer\",\n",
    "        \"policy_logp_w\": -38.5,\n",
    "        \"policy_logp_l\": -35.2,\n",
    "        \"ref_logp_w\": -39.0,\n",
    "        \"ref_logp_l\": -36.1,\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a haiku about machine learning.\",\n",
    "        \"preferred\": \"Creative, follows haiku structure\",\n",
    "        \"dispreferred\": \"Generic, doesn't follow structure\",\n",
    "        \"policy_logp_w\": -28.3,\n",
    "        \"policy_logp_l\": -31.7,\n",
    "        \"ref_logp_w\": -30.1,\n",
    "        \"ref_logp_l\": -30.5,\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What causes rain?\",\n",
    "        \"preferred\": \"Accurate scientific explanation\",\n",
    "        \"dispreferred\": \"Vague, slightly wrong explanation\",\n",
    "        \"policy_logp_w\": -52.0,\n",
    "        \"policy_logp_l\": -55.1,\n",
    "        \"ref_logp_w\": -53.2,\n",
    "        \"ref_logp_l\": -54.0,\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Summarize the plot of Romeo and Juliet.\",\n",
    "        \"preferred\": \"Concise, accurate summary\",\n",
    "        \"dispreferred\": \"Rambling, inaccurate summary\",\n",
    "        \"policy_logp_w\": -61.3,\n",
    "        \"policy_logp_l\": -60.8,\n",
    "        \"ref_logp_w\": -62.0,\n",
    "        \"ref_logp_l\": -61.5,\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Pre-computed log-probabilities for 5 preference pairs:\")\n",
    "print(f\"{'Pair':<6} {'Prompt':<45} {'policy_w':>10} {'policy_l':>10} {'ref_w':>10} {'ref_l':>10}\")\n",
    "print(\"-\" * 95)\n",
    "for i, pair in enumerate(preference_pairs):\n",
    "    prompt_short = pair['prompt'][:42] + '...' if len(pair['prompt']) > 42 else pair['prompt']\n",
    "    print(f\"{i+1:<6} {prompt_short:<45} {pair['policy_logp_w']:>10.1f} {pair['policy_logp_l']:>10.1f} {pair['ref_logp_w']:>10.1f} {pair['ref_logp_l']:>10.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step-by-step DPO loss computation ---\n",
    "#\n",
    "# For each pair, we compute:\n",
    "# 1. Log-ratio for preferred:    log(pi/pi_ref) for y_w\n",
    "# 2. Log-ratio for dispreferred: log(pi/pi_ref) for y_l\n",
    "# 3. Difference: log_ratio_w - log_ratio_l\n",
    "# 4. Scale by beta: beta * difference\n",
    "# 5. Apply -log(sigmoid(...)): the DPO loss for this pair\n",
    "\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"The logistic sigmoid function: maps any real number to (0, 1).\"\"\"\n",
    "    return 1.0 / (1.0 + math.exp(-x))\n",
    "\n",
    "\n",
    "beta = 0.1  # A typical value — controls conservatism\n",
    "\n",
    "print(f\"Computing DPO loss step-by-step with beta = {beta}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i, pair in enumerate(preference_pairs):\n",
    "    # Step 1: Log-ratios (how much has the policy shifted from the reference?)\n",
    "    log_ratio_w = pair['policy_logp_w'] - pair['ref_logp_w']\n",
    "    log_ratio_l = pair['policy_logp_l'] - pair['ref_logp_l']\n",
    "\n",
    "    # Step 2: Difference in log-ratios\n",
    "    diff = log_ratio_w - log_ratio_l\n",
    "\n",
    "    # Step 3: Scale by beta\n",
    "    scaled = beta * diff\n",
    "\n",
    "    # Step 4: Apply -log(sigmoid(...))\n",
    "    sig = sigmoid(scaled)\n",
    "    loss = -math.log(sig)\n",
    "    losses.append(loss)\n",
    "\n",
    "    print(f\"\\nPair {i+1}: {pair['prompt']}\")\n",
    "    print(f\"  Preferred:    {pair['preferred']}\")\n",
    "    print(f\"  Dispreferred: {pair['dispreferred']}\")\n",
    "    print(f\"  Log-ratio (preferred):    {pair['policy_logp_w']:.1f} - ({pair['ref_logp_w']:.1f}) = {log_ratio_w:+.2f}\")\n",
    "    print(f\"  Log-ratio (dispreferred): {pair['policy_logp_l']:.1f} - ({pair['ref_logp_l']:.1f}) = {log_ratio_l:+.2f}\")\n",
    "    print(f\"  Difference:  {log_ratio_w:.2f} - {log_ratio_l:.2f} = {diff:+.2f}\")\n",
    "    print(f\"  Scaled:      {beta} * {diff:.2f} = {scaled:+.4f}\")\n",
    "    print(f\"  sigma({scaled:.4f}) = {sig:.4f}\")\n",
    "    print(f\"  Loss = -log({sig:.4f}) = {loss:.4f}\")\n",
    "\n",
    "    # Interpretation\n",
    "    if diff > 1.0:\n",
    "        print(f\"  --> Model AGREES with preference (positive diff). Low-ish loss.\")\n",
    "    elif diff > 0:\n",
    "        print(f\"  --> Model SLIGHTLY agrees. Moderate loss, still learning.\")\n",
    "    else:\n",
    "        print(f\"  --> Model DISAGREES with preference (negative diff). High loss, strong gradient.\")\n",
    "\n",
    "avg_loss = sum(losses) / len(losses)\n",
    "print(f\"\\n{'=' * 90}\")\n",
    "print(f\"Average DPO loss across all {len(losses)} pairs: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vary beta and observe its effect ---\n",
    "#\n",
    "# beta controls how much the policy can deviate from the reference.\n",
    "# Higher beta = more conservative (larger loss for the same log-ratio difference).\n",
    "# Lower beta = more aggressive (allows larger deviations before penalizing).\n",
    "\n",
    "beta_values = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
    "\n",
    "print(f\"DPO loss at different beta values:\")\n",
    "print(f\"{'Beta':<8}\" + \"\".join(f\"{'Pair ' + str(i+1):>10}\" for i in range(5)) + f\"{'Average':>10}\")\n",
    "print(\"-\" * 68)\n",
    "\n",
    "avg_losses_by_beta = []\n",
    "\n",
    "for beta_val in beta_values:\n",
    "    pair_losses = []\n",
    "    for pair in preference_pairs:\n",
    "        log_ratio_w = pair['policy_logp_w'] - pair['ref_logp_w']\n",
    "        log_ratio_l = pair['policy_logp_l'] - pair['ref_logp_l']\n",
    "        diff = log_ratio_w - log_ratio_l\n",
    "        scaled = beta_val * diff\n",
    "        loss = -math.log(sigmoid(scaled))\n",
    "        pair_losses.append(loss)\n",
    "\n",
    "    avg = sum(pair_losses) / len(pair_losses)\n",
    "    avg_losses_by_beta.append(avg)\n",
    "    vals = \"\".join(f\"{l:>10.4f}\" for l in pair_losses)\n",
    "    print(f\"{beta_val:<8.2f}{vals}{avg:>10.4f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Average loss vs beta\n",
    "ax = axes[0]\n",
    "ax.plot(beta_values, avg_losses_by_beta, 'o-', color='#a78bfa', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Beta', fontsize=11)\n",
    "ax.set_ylabel('Average DPO Loss', fontsize=11)\n",
    "ax.set_title('Average Loss vs Beta', fontsize=12)\n",
    "ax.grid(alpha=0.2)\n",
    "\n",
    "# Right: Per-pair loss at different betas\n",
    "ax2 = axes[1]\n",
    "colors = ['#6366f1', '#f59e0b', '#22c55e', '#06b6d4', '#ef4444']\n",
    "for pair_idx in range(5):\n",
    "    pair_losses_across_beta = []\n",
    "    for beta_val in beta_values:\n",
    "        pair = preference_pairs[pair_idx]\n",
    "        log_ratio_w = pair['policy_logp_w'] - pair['ref_logp_w']\n",
    "        log_ratio_l = pair['policy_logp_l'] - pair['ref_logp_l']\n",
    "        diff = log_ratio_w - log_ratio_l\n",
    "        scaled = beta_val * diff\n",
    "        loss = -math.log(sigmoid(scaled))\n",
    "        pair_losses_across_beta.append(loss)\n",
    "    ax2.plot(beta_values, pair_losses_across_beta, 'o-', color=colors[pair_idx],\n",
    "             label=f'Pair {pair_idx+1}', linewidth=2, markersize=6)\n",
    "\n",
    "ax2.set_xlabel('Beta', fontsize=11)\n",
    "ax2.set_ylabel('DPO Loss', fontsize=11)\n",
    "ax2.set_title('Per-Pair Loss vs Beta', fontsize=12)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"- Higher beta amplifies the loss. The same log-ratio difference produces a LARGER loss.\")\n",
    "print(\"  This means higher beta = more conservative: the model is penalized more for deviating.\")\n",
    "print(\"- Lower beta shrinks the loss. The model can deviate more before the loss becomes large.\")\n",
    "print(\"  This means lower beta = more aggressive optimization.\")\n",
    "print(\"- Pairs where the model disagrees (Pair 2, 5) always have higher loss at every beta.\")\n",
    "print(\"  DPO naturally focuses learning on the cases where the model is most wrong.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just computed:** The DPO loss for each preference pair, step by step. The loss is small when the model already agrees with the preference (the log-ratio for the preferred response is higher than for the dispreferred) and large when the model disagrees.\n",
    "\n",
    "The beta parameter controls conservatism. Higher beta amplifies the loss — the model is penalized more heavily for any deviation from the reference, making it more conservative. Lower beta allows more aggressive optimization. This falls directly out of the formula: beta scales the log-ratio difference before passing it through the sigmoid.\n",
    "\n",
    "Notice that the loss operates on **log-ratios** (policy minus reference), not absolute log-probabilities. This is the implicit KL penalty — DPO measures how much the policy has *changed*, not the absolute quality of the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Implement the DPO Loss Function (Supported)\n",
    "\n",
    "Now implement the DPO loss as a PyTorch function. The implementation is short (~5-10 lines of core logic) but each line maps to a step in the derivation you just computed by hand.\n",
    "\n",
    "After implementing, you'll:\n",
    "1. Verify your implementation against Exercise 1's hand calculations\n",
    "2. Use autograd to compute the gradient and confirm it pushes in the right direction\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "The steps are exactly what you did by hand:\n",
    "1. Compute log-ratios: `policy_logp - ref_logp` for preferred and dispreferred\n",
    "2. Compute the difference: `log_ratio_w - log_ratio_l`\n",
    "3. Scale by beta: `beta * difference`\n",
    "4. Apply negative log-sigmoid: `-F.logsigmoid(scaled)` (use `F.logsigmoid` — it is numerically stable)\n",
    "5. Average over the batch: `.mean()`\n",
    "\n",
    "`F.logsigmoid(x)` computes `log(sigmoid(x))` in a numerically stable way.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpo_loss(\n",
    "    policy_logps_w: torch.Tensor,   # log P(y_w|x) under policy, shape [batch]\n",
    "    policy_logps_l: torch.Tensor,   # log P(y_l|x) under policy, shape [batch]\n",
    "    ref_logps_w: torch.Tensor,      # log P(y_w|x) under reference, shape [batch]\n",
    "    ref_logps_l: torch.Tensor,      # log P(y_l|x) under reference, shape [batch]\n",
    "    beta: float = 0.1,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute the DPO loss.\n",
    "\n",
    "    Returns a scalar tensor: the mean loss across the batch.\n",
    "\n",
    "    The DPO loss is:\n",
    "      -log sigma(beta * (log(pi/pi_ref)(y_w) - log(pi/pi_ref)(y_l)))\n",
    "    averaged over the batch.\n",
    "    \"\"\"\n",
    "    # TODO: Compute log-ratios for preferred and dispreferred responses\n",
    "    # Each is: policy_logp - ref_logp\n",
    "    # YOUR CODE HERE (2 lines)\n",
    "    log_ratio_w = None  # REPLACE THIS\n",
    "    log_ratio_l = None  # REPLACE THIS\n",
    "\n",
    "    # TODO: Compute the scaled difference and apply -log(sigmoid(...))\n",
    "    # Use F.logsigmoid() for numerical stability, then negate and take the mean.\n",
    "    # YOUR CODE HERE (2 lines)\n",
    "    logits = None  # REPLACE THIS\n",
    "    loss = None    # REPLACE THIS\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verify against Exercise 1's hand calculations ---\n",
    "\n",
    "# Convert the hand-computed data to tensors\n",
    "policy_w = torch.tensor([p['policy_logp_w'] for p in preference_pairs])\n",
    "policy_l = torch.tensor([p['policy_logp_l'] for p in preference_pairs])\n",
    "ref_w = torch.tensor([p['ref_logp_w'] for p in preference_pairs])\n",
    "ref_l = torch.tensor([p['ref_logp_l'] for p in preference_pairs])\n",
    "\n",
    "# Compute with your implementation\n",
    "computed_loss = dpo_loss(policy_w, policy_l, ref_w, ref_l, beta=0.1)\n",
    "\n",
    "# Compare to the hand-computed average from Exercise 1\n",
    "hand_computed_avg = avg_loss  # From the earlier cell\n",
    "\n",
    "print(f\"Hand-computed average loss (Exercise 1): {hand_computed_avg:.4f}\")\n",
    "print(f\"PyTorch implementation loss:             {computed_loss.item():.4f}\")\n",
    "print(f\"Difference:                              {abs(computed_loss.item() - hand_computed_avg):.6f}\")\n",
    "\n",
    "if abs(computed_loss.item() - hand_computed_avg) < 0.001:\n",
    "    print(\"\\nMatch! Your implementation agrees with the hand calculation.\")\n",
    "else:\n",
    "    print(\"\\nMismatch. Check your implementation — it should produce the same result.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inspect the gradient direction via autograd ---\n",
    "#\n",
    "# The DPO gradient should push the policy to:\n",
    "#   - INCREASE log-prob of preferred responses (positive gradient on policy_logps_w)\n",
    "#   - DECREASE log-prob of dispreferred responses (negative gradient on policy_logps_l)\n",
    "#\n",
    "# The magnitude should be proportional to how much the model disagrees:\n",
    "# large gradient when the model is wrong, small when it already agrees.\n",
    "\n",
    "# Create tensors that require gradients (simulating differentiable policy log-probs)\n",
    "policy_w_grad = torch.tensor([p['policy_logp_w'] for p in preference_pairs], requires_grad=True)\n",
    "policy_l_grad = torch.tensor([p['policy_logp_l'] for p in preference_pairs], requires_grad=True)\n",
    "ref_w_fixed = torch.tensor([p['ref_logp_w'] for p in preference_pairs])  # Reference is frozen\n",
    "ref_l_fixed = torch.tensor([p['ref_logp_l'] for p in preference_pairs])  # Reference is frozen\n",
    "\n",
    "# Forward pass\n",
    "loss = dpo_loss(policy_w_grad, policy_l_grad, ref_w_fixed, ref_l_fixed, beta=0.1)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(\"Gradients of the DPO loss with respect to policy log-probs:\")\n",
    "print(f\"{'Pair':<6} {'grad(policy_w)':>15} {'grad(policy_l)':>15} {'Direction':>25}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i in range(5):\n",
    "    gw = policy_w_grad.grad[i].item()\n",
    "    gl = policy_l_grad.grad[i].item()\n",
    "    # Negative gradient on policy_w means: decreasing policy_w INCREASES loss,\n",
    "    # so the optimizer will INCREASE policy_w (gradient descent = subtract gradient).\n",
    "    direction = \"\"\n",
    "    if gw < 0 and gl > 0:\n",
    "        direction = \"Increase w, decrease l\"\n",
    "    elif gw < 0 and gl < 0:\n",
    "        direction = \"Increase w, increase l\"\n",
    "    elif gw > 0 and gl > 0:\n",
    "        direction = \"Decrease w, decrease l\"\n",
    "    else:\n",
    "        direction = \"Decrease w, increase l\"\n",
    "    print(f\"{i+1:<6} {gw:>15.6f} {gl:>15.6f} {direction:>25}\")\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"- grad(policy_w) is NEGATIVE for all pairs: the optimizer will INCREASE preferred log-probs.\")\n",
    "print(\"- grad(policy_l) is POSITIVE for all pairs: the optimizer will DECREASE dispreferred log-probs.\")\n",
    "print(\"- Gradient MAGNITUDE is larger for pairs where the model disagrees (pairs 2, 5).\")\n",
    "print(\"  DPO focuses learning on the hard cases, just as we saw in Exercise 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The DPO loss implementation maps directly to the derivation steps:\n",
    "\n",
    "```python\n",
    "def dpo_loss(\n",
    "    policy_logps_w: torch.Tensor,\n",
    "    policy_logps_l: torch.Tensor,\n",
    "    ref_logps_w: torch.Tensor,\n",
    "    ref_logps_l: torch.Tensor,\n",
    "    beta: float = 0.1,\n",
    ") -> torch.Tensor:\n",
    "    # Log-ratios: how much has the policy shifted from the reference?\n",
    "    log_ratio_w = policy_logps_w - ref_logps_w\n",
    "    log_ratio_l = policy_logps_l - ref_logps_l\n",
    "\n",
    "    # DPO loss: -log sigma(beta * (log_ratio_w - log_ratio_l))\n",
    "    logits = beta * (log_ratio_w - log_ratio_l)\n",
    "    loss = -F.logsigmoid(logits).mean()\n",
    "\n",
    "    return loss\n",
    "```\n",
    "\n",
    "**Why each line:**\n",
    "- `log_ratio_w` and `log_ratio_l`: These are the log-ratios from the derivation. They measure how much the policy has shifted from the reference for each response. This is the implicit KL penalty — large shifts produce large ratios.\n",
    "- `logits`: The difference in log-ratios, scaled by beta. When this is large and positive, the model agrees with the preference. When negative, it disagrees.\n",
    "- `-F.logsigmoid(logits).mean()`: The negative log-sigmoid produces high loss when logits are negative (model disagrees) and low loss when logits are positive (model agrees). `.mean()` averages over the batch.\n",
    "\n",
    "**Why `F.logsigmoid` instead of `torch.log(torch.sigmoid(...))`?** `F.logsigmoid` is numerically stable for large negative inputs where `sigmoid(x)` would underflow to 0 and `log(0)` would produce `-inf`.\n",
    "\n",
    "**Common mistake:** Forgetting to negate the log-sigmoid. `F.logsigmoid(x)` returns `log(sigmoid(x))`, which is always negative. The DPO loss is `-log(sigmoid(x))`, which is always positive. Without the negation, you would be *rewarding* disagreement with preferences.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just verified:** Your DPO loss implementation produces the exact same result as the hand calculation from Exercise 1. The gradient inspection confirms the loss pushes in the right direction: increase preferred log-probs, decrease dispreferred log-probs, with stronger gradients for pairs where the model disagrees.\n",
    "\n",
    "The entire loss function is ~5 lines of core logic. Each line maps to a step in the derivation from the lesson: compute log-ratios (the implicit KL), take their difference (comparing preferred vs dispreferred), scale by beta (control conservatism), apply negative log-sigmoid (convert to a loss). The complexity is in the derivation that justifies these lines, not in the code itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Train a Small Model with DPO (Supported)\n",
    "\n",
    "Now use your DPO loss function to train GPT-2 small on preference pairs. The training loop follows the familiar pattern — forward, loss, backward, step — with the DPO loss replacing cross-entropy.\n",
    "\n",
    "You will:\n",
    "1. Load GPT-2 as the policy model (trainable) and a frozen copy as the reference\n",
    "2. Create preference pairs with clear quality differences\n",
    "3. Run a DPO training loop using your loss function from Exercise 2\n",
    "4. Compare the model's outputs before and after training\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "The training loop structure is:\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for pair in preference_pairs:\n",
    "        # Compute log-probs under policy (with gradient)\n",
    "        # Compute log-probs under reference (no gradient, use torch.no_grad())\n",
    "        # Compute DPO loss using your function\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        # optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "The reference model is frozen — wrap its forward pass in `torch.no_grad()`. The policy model needs gradients — do NOT use `torch.no_grad()` for it.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load GPT-2 as policy and reference ---\n",
    "\n",
    "print(\"Loading GPT-2 as policy model...\")\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Creating frozen reference model (copy of initial policy)...\")\n",
    "ref_model = deepcopy(policy_model)\n",
    "ref_model.eval()\n",
    "# Freeze the reference model — no gradients, no updates\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Policy model parameters: {sum(p.numel() for p in policy_model.parameters()):,}\")\n",
    "print(f\"Reference model parameters: {sum(p.numel() for p in ref_model.parameters()):,} (frozen)\")\n",
    "print(\"Models loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preference pairs for training ---\n",
    "#\n",
    "# Each pair: a prompt, a preferred response (y_w), and a dispreferred response (y_l).\n",
    "# We use clear quality differences so the effect is visible even with a small model\n",
    "# and few training steps.\n",
    "\n",
    "training_pairs = [\n",
    "    {\n",
    "        \"prompt\": \"What is the capital of France? \",\n",
    "        \"preferred\": \"The capital of France is Paris, a city known for its rich history, art, and culture.\",\n",
    "        \"dispreferred\": \"France is a country in Europe. It has many cities. Some cities are big.\",\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain why the sky is blue. \",\n",
    "        \"preferred\": \"The sky appears blue because molecules in the atmosphere scatter shorter wavelengths of sunlight more than longer ones, a phenomenon called Rayleigh scattering.\",\n",
    "        \"dispreferred\": \"The sky is blue because it just is. That's the color of the sky. It's always been blue.\",\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How does a bicycle stay upright? \",\n",
    "        \"preferred\": \"A bicycle stays upright through a combination of the rider's balance adjustments, gyroscopic effects of the spinning wheels, and the geometry of the front fork which provides a self-correcting steering effect.\",\n",
    "        \"dispreferred\": \"Bicycles stay up because of balance. You have to balance on them. If you don't balance, you fall.\",\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is machine learning? \",\n",
    "        \"preferred\": \"Machine learning is a branch of artificial intelligence where systems learn patterns from data rather than being explicitly programmed with rules. The model improves its performance on a task as it sees more examples.\",\n",
    "        \"dispreferred\": \"Machine learning is when computers learn things. They use data and algorithms. It's very complex and difficult to understand.\",\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Why do leaves change color in autumn? \",\n",
    "        \"preferred\": \"Leaves change color in autumn because shorter days trigger trees to stop producing chlorophyll. As the green chlorophyll breaks down, other pigments like carotenoids (yellow, orange) and anthocyanins (red, purple) become visible.\",\n",
    "        \"dispreferred\": \"Leaves change color because of the seasons. When it gets cold, the leaves turn different colors and then they fall off the trees.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Training data: {len(training_pairs)} preference pairs\")\n",
    "for i, pair in enumerate(training_pairs):\n",
    "    print(f\"\\nPair {i+1}: {pair['prompt'].strip()}\")\n",
    "    print(f\"  Preferred:    {pair['preferred'][:80]}...\")\n",
    "    print(f\"  Dispreferred: {pair['dispreferred'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Measure log-probs BEFORE training (baseline) ---\n",
    "#\n",
    "# Before training, the policy and reference are identical.\n",
    "# All log-ratios should be 0, and the loss should be -log(sigma(0)) = log(2) ~ 0.693.\n",
    "\n",
    "print(\"Log-probabilities BEFORE training (policy = reference):\")\n",
    "print(f\"{'Pair':<6} {'policy_w':>12} {'policy_l':>12} {'ref_w':>12} {'ref_l':>12} {'log_ratio_diff':>16}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "policy_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, pair in enumerate(training_pairs):\n",
    "        pw = get_response_log_prob(policy_model, tokenizer, pair['prompt'], pair['preferred']).item()\n",
    "        pl = get_response_log_prob(policy_model, tokenizer, pair['prompt'], pair['dispreferred']).item()\n",
    "        rw = get_response_log_prob(ref_model, tokenizer, pair['prompt'], pair['preferred']).item()\n",
    "        rl = get_response_log_prob(ref_model, tokenizer, pair['prompt'], pair['dispreferred']).item()\n",
    "        diff = (pw - rw) - (pl - rl)\n",
    "        print(f\"{i+1:<6} {pw:>12.2f} {pl:>12.2f} {rw:>12.2f} {rl:>12.2f} {diff:>16.4f}\")\n",
    "\n",
    "print(f\"\\nLog-ratio differences are ~0 because policy = reference (no training yet).\")\n",
    "print(f\"Expected loss: -log(sigma(0)) = log(2) = {math.log(2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DPO Training Loop ---\n",
    "#\n",
    "# This is the core of the exercise. The training loop looks like supervised\n",
    "# learning — the complexity is in the loss function, not the loop.\n",
    "\n",
    "beta = 0.1\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 3\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_model.parameters(), lr=learning_rate)\n",
    "loss_history = []\n",
    "\n",
    "policy_model.train()\n",
    "\n",
    "print(f\"Training with beta={beta}, lr={learning_rate}, epochs={num_epochs}\")\n",
    "print(f\"{'Epoch':<8} {'Step':<8} {'Loss':>10} {'Prompt':>40}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "\n",
    "    for step, pair in enumerate(training_pairs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # TODO: Compute log-probs under the POLICY model (with gradient)\n",
    "        # Use get_response_log_prob() for both preferred and dispreferred.\n",
    "        # Do NOT wrap in torch.no_grad() — we need gradients!\n",
    "        # YOUR CODE HERE (2 lines)\n",
    "        policy_logp_w = None  # REPLACE THIS\n",
    "        policy_logp_l = None  # REPLACE THIS\n",
    "\n",
    "        # TODO: Compute log-probs under the REFERENCE model (no gradient)\n",
    "        # Use torch.no_grad() — the reference model is frozen.\n",
    "        # YOUR CODE HERE (3 lines: with block + 2 calls)\n",
    "        ref_logp_w = None  # REPLACE THIS\n",
    "        ref_logp_l = None  # REPLACE THIS\n",
    "\n",
    "        # TODO: Compute the DPO loss using your function from Exercise 2\n",
    "        # Note: the log-probs are scalars, so unsqueeze to make them [1]-shaped tensors\n",
    "        # YOUR CODE HERE (1 line)\n",
    "        loss = None  # REPLACE THIS\n",
    "\n",
    "        # Backward pass and optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = loss.item()\n",
    "        epoch_losses.append(loss_val)\n",
    "        loss_history.append(loss_val)\n",
    "\n",
    "        prompt_short = pair['prompt'].strip()[:37] + '...' if len(pair['prompt'].strip()) > 37 else pair['prompt'].strip()\n",
    "        print(f\"{epoch+1:<8} {step+1:<8} {loss_val:>10.4f} {prompt_short:>40}\")\n",
    "\n",
    "    avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    print(f\"  Epoch {epoch+1} average loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining complete. Final average loss: {sum(loss_history[-len(training_pairs):]) / len(training_pairs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize training loss ---\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(loss_history, 'o-', color='#a78bfa', linewidth=1.5, markersize=4, alpha=0.8)\n",
    "\n",
    "# Add epoch boundaries\n",
    "for epoch_end in range(len(training_pairs), len(loss_history), len(training_pairs)):\n",
    "    ax.axvline(x=epoch_end - 0.5, color='white', linestyle='--', alpha=0.2)\n",
    "\n",
    "# Add the initial loss reference line: log(2)\n",
    "ax.axhline(y=math.log(2), color='#ef4444', linestyle='--', alpha=0.5, label=f'Initial loss = log(2) = {math.log(2):.3f}')\n",
    "\n",
    "ax.set_xlabel('Training Step', fontsize=11)\n",
    "ax.set_ylabel('DPO Loss', fontsize=11)\n",
    "ax.set_title('DPO Training Loss', fontsize=12)\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The loss starts at log(2) ~ 0.693 (policy = reference, random guess).\")\n",
    "print(\"It decreases as the policy learns to prefer the preferred responses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare log-probs AFTER training ---\n",
    "#\n",
    "# The log-ratio differences should now be POSITIVE for all pairs:\n",
    "# the policy has shifted to assign relatively more probability to\n",
    "# preferred responses compared to the reference.\n",
    "\n",
    "print(\"Log-probabilities AFTER training:\")\n",
    "print(f\"{'Pair':<6} {'policy_w':>12} {'policy_l':>12} {'ref_w':>12} {'ref_l':>12} {'log_ratio_diff':>16}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "policy_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, pair in enumerate(training_pairs):\n",
    "        pw = get_response_log_prob(policy_model, tokenizer, pair['prompt'], pair['preferred']).item()\n",
    "        pl = get_response_log_prob(policy_model, tokenizer, pair['prompt'], pair['dispreferred']).item()\n",
    "        rw = get_response_log_prob(ref_model, tokenizer, pair['prompt'], pair['preferred']).item()\n",
    "        rl = get_response_log_prob(ref_model, tokenizer, pair['prompt'], pair['dispreferred']).item()\n",
    "        diff = (pw - rw) - (pl - rl)\n",
    "        print(f\"{i+1:<6} {pw:>12.2f} {pl:>12.2f} {rw:>12.2f} {rl:>12.2f} {diff:>16.4f}\")\n",
    "\n",
    "print(f\"\\nLog-ratio differences are now POSITIVE: the policy prefers the preferred responses.\")\n",
    "print(f\"Compare to before training, where all differences were ~0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The training loop fills look like this:\n",
    "\n",
    "```python\n",
    "# Policy log-probs (with gradient)\n",
    "policy_logp_w = get_response_log_prob(policy_model, tokenizer, pair['prompt'], pair['preferred'])\n",
    "policy_logp_l = get_response_log_prob(policy_model, tokenizer, pair['prompt'], pair['dispreferred'])\n",
    "\n",
    "# Reference log-probs (no gradient — frozen model)\n",
    "with torch.no_grad():\n",
    "    ref_logp_w = get_response_log_prob(ref_model, tokenizer, pair['prompt'], pair['preferred'])\n",
    "    ref_logp_l = get_response_log_prob(ref_model, tokenizer, pair['prompt'], pair['dispreferred'])\n",
    "\n",
    "# DPO loss (unsqueeze scalars to [1]-shaped tensors for the batch dimension)\n",
    "loss = dpo_loss(policy_logp_w.unsqueeze(0), policy_logp_l.unsqueeze(0),\n",
    "                ref_logp_w.unsqueeze(0), ref_logp_l.unsqueeze(0), beta=beta)\n",
    "```\n",
    "\n",
    "**Why the reference model uses `torch.no_grad()`:** The reference model is frozen — it represents what the model knew before alignment. We never update it. Wrapping its forward pass in `torch.no_grad()` saves memory (no gradient computation graph) and ensures no gradients accidentally flow through it.\n",
    "\n",
    "**Why `unsqueeze(0)`:** The `dpo_loss` function expects batch tensors (shape `[batch_size]`), but `get_response_log_prob` returns a scalar. `unsqueeze(0)` adds a batch dimension: scalar `x` becomes tensor `[x]` with shape `[1]`.\n",
    "\n",
    "**Key observation about the loop:** It looks like standard supervised training — forward, loss, backward, step. The only difference is the loss function. This is the \"DPO partially restores the familiar training loop shape\" insight from the RLHF lesson. The complexity lives in the loss, not the loop.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just observed:** DPO training looks like supervised learning. The training loop is forward, loss, backward, step — the same pattern you have used many times. The only difference is the loss function: instead of cross-entropy on a single target, DPO's loss operates on pairs of responses through the reference model.\n",
    "\n",
    "The loss starts at `log(2)` because the policy and reference are initially identical (all log-ratios are 0, sigmoid(0) = 0.5, -log(0.5) = log(2)). As training proceeds, the loss decreases as the policy learns to assign relatively more probability to preferred responses.\n",
    "\n",
    "The reference model never changes. It is a frozen snapshot of the initial policy. The DPO loss measures how much the policy has shifted *relative to this anchor* — this is the implicit KL penalty that prevents probability mass collapse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 4: Explore the Implicit Reward (Independent)\n\nThe derivation in the lesson showed that any policy paired with a reference implicitly defines a reward function:\n\n$$r(y, x) = \\beta \\log \\frac{\\pi(y \\mid x)}{\\pi_\\text{ref}(y \\mid x)} + \\beta \\log Z(x)$$\n\nThe $\\beta \\log Z(x)$ term is the same for all responses to the same prompt, so for *comparing* responses to the same prompt, you can drop it. The implicit reward (up to a per-prompt constant) is just:\n\n$$r(y, x) \\propto \\beta \\log \\frac{\\pi(y \\mid x)}{\\pi_\\text{ref}(y \\mid x)}$$\n\nUsing the trained model from Exercise 3, you will:\n1. Compute the implicit reward for the preferred and dispreferred responses in the training data\n2. Verify that preferred responses have higher implicit reward\n3. Compute implicit reward for NEW responses that were NOT in the training data\n4. Check whether the implicit reward generalizes beyond the training set\n\nThis exercise is independent — you write all the code yourself.\n\n<details>\n<summary>Hint</summary>\n\nThe implicit reward for a response is:\n```python\nreward = beta * (policy_log_prob - ref_log_prob)\n```\n\nUse `get_response_log_prob()` with both the policy model and the reference model, then compute the difference. Wrap everything in `torch.no_grad()` — you are evaluating, not training.\n\nFor new responses, write responses of varying quality to the same prompts used in training. One should be clearly good (informative, accurate), one mediocre, and one poor. The implicit reward should rank them sensibly.\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 1: Compute implicit reward for training data ---\n",
    "#\n",
    "# For each training pair, compute the implicit reward for both the\n",
    "# preferred and dispreferred response. Verify preferred > dispreferred.\n",
    "#\n",
    "# TODO: Write the code to compute and display implicit rewards for all\n",
    "# training pairs. Use get_response_log_prob() with policy_model and ref_model.\n",
    "# The implicit reward is: beta * (policy_log_prob - ref_log_prob)\n",
    "#\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 2: Implicit reward for NEW responses (not in training data) ---\n",
    "#\n",
    "# Test whether the implicit reward generalizes. Write new responses of\n",
    "# varying quality to prompts from the training data (or new prompts).\n",
    "# Compute their implicit rewards and check the ranking.\n",
    "#\n",
    "# TODO: Create at least 3 new responses per prompt (good, mediocre, bad).\n",
    "# Compute implicit rewards and display them ranked.\n",
    "#\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 3: Visualize implicit rewards ---\n",
    "#\n",
    "# TODO: Create a bar chart showing implicit rewards for all responses\n",
    "# (training preferred, training dispreferred, new responses).\n",
    "# Color-code by quality: preferred=green, dispreferred=red, new=blue.\n",
    "#\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Part 1: Implicit reward for training data**\n",
    "\n",
    "The key insight is that the implicit reward is just `beta * log_ratio`. Preferred responses should have higher implicit reward.\n",
    "\n",
    "```python\n",
    "beta = 0.1\n",
    "\n",
    "print(\"Implicit rewards for training data:\")\n",
    "print(f\"{'Pair':<6} {'Reward (preferred)':>20} {'Reward (dispreferred)':>22} {'Preferred > Dispreferred?':>28}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "policy_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, pair in enumerate(training_pairs):\n",
    "        # Implicit reward = beta * (log pi(y|x) - log pi_ref(y|x))\n",
    "        pw = get_response_log_prob(policy_model, tokenizer, pair['prompt'], pair['preferred']).item()\n",
    "        rw = get_response_log_prob(ref_model, tokenizer, pair['prompt'], pair['preferred']).item()\n",
    "        reward_w = beta * (pw - rw)\n",
    "\n",
    "        pl = get_response_log_prob(policy_model, tokenizer, pair['prompt'], pair['dispreferred']).item()\n",
    "        rl = get_response_log_prob(ref_model, tokenizer, pair['prompt'], pair['dispreferred']).item()\n",
    "        reward_l = beta * (pl - rl)\n",
    "\n",
    "        correct = reward_w > reward_l\n",
    "        print(f\"{i+1:<6} {reward_w:>20.4f} {reward_l:>22.4f} {str(correct):>28}\")\n",
    "```\n",
    "\n",
    "**Part 2: New responses**\n",
    "\n",
    "```python\n",
    "new_test_cases = [\n",
    "    {\n",
    "        \"prompt\": \"What is the capital of France? \",\n",
    "        \"responses\": [\n",
    "            (\"Paris is the capital and largest city of France, situated along the Seine River.\", \"good\"),\n",
    "            (\"The capital is Paris, I think.\", \"mediocre\"),\n",
    "            (\"France has a capital city. Many people live there.\", \"bad\"),\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain why the sky is blue. \",\n",
    "        \"responses\": [\n",
    "            (\"Sunlight contains all colors. Earth's atmosphere scatters blue light most because its shorter wavelength interacts more with air molecules.\", \"good\"),\n",
    "            (\"Something about light scattering in the atmosphere makes it blue.\", \"mediocre\"),\n",
    "            (\"Nobody really knows for sure why the sky looks blue.\", \"bad\"),\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\nImplicit rewards for NEW responses (not in training data):\")\n",
    "with torch.no_grad():\n",
    "    for case in new_test_cases:\n",
    "        print(f\"\\nPrompt: {case['prompt'].strip()}\")\n",
    "        for response, quality in case['responses']:\n",
    "            p_logp = get_response_log_prob(policy_model, tokenizer, case['prompt'], response).item()\n",
    "            r_logp = get_response_log_prob(ref_model, tokenizer, case['prompt'], response).item()\n",
    "            reward = beta * (p_logp - r_logp)\n",
    "            print(f\"  [{quality:>8}] reward={reward:+.4f}  '{response[:60]}...'\")\n",
    "```\n",
    "\n",
    "**Part 3: Visualization**\n",
    "\n",
    "```python\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "labels = []\n",
    "rewards = []\n",
    "bar_colors = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Training data\n",
    "    for i, pair in enumerate(training_pairs[:3]):  # First 3 for readability\n",
    "        pw = get_response_log_prob(policy_model, tokenizer, pair['prompt'], pair['preferred']).item()\n",
    "        rw = get_response_log_prob(ref_model, tokenizer, pair['prompt'], pair['preferred']).item()\n",
    "        reward_w = beta * (pw - rw)\n",
    "        labels.append(f\"P{i+1} preferred\")\n",
    "        rewards.append(reward_w)\n",
    "        bar_colors.append('#22c55e')\n",
    "\n",
    "        pl = get_response_log_prob(policy_model, tokenizer, pair['prompt'], pair['dispreferred']).item()\n",
    "        rl = get_response_log_prob(ref_model, tokenizer, pair['prompt'], pair['dispreferred']).item()\n",
    "        reward_l = beta * (pl - rl)\n",
    "        labels.append(f\"P{i+1} dispreferred\")\n",
    "        rewards.append(reward_l)\n",
    "        bar_colors.append('#ef4444')\n",
    "\n",
    "    # New responses\n",
    "    for case in new_test_cases[:1]:  # First prompt for readability\n",
    "        for response, quality in case['responses']:\n",
    "            p_logp = get_response_log_prob(policy_model, tokenizer, case['prompt'], response).item()\n",
    "            r_logp = get_response_log_prob(ref_model, tokenizer, case['prompt'], response).item()\n",
    "            reward = beta * (p_logp - r_logp)\n",
    "            labels.append(f\"New ({quality})\")\n",
    "            rewards.append(reward)\n",
    "            bar_colors.append('#6366f1')\n",
    "\n",
    "ax.barh(range(len(rewards)), rewards, color=bar_colors, alpha=0.8)\n",
    "ax.set_yticks(range(len(rewards)))\n",
    "ax.set_yticklabels(labels, fontsize=9)\n",
    "ax.set_xlabel('Implicit Reward', fontsize=11)\n",
    "ax.set_title('Implicit Reward: Training Data vs New Responses', fontsize=12)\n",
    "ax.axvline(x=0, color='white', linestyle='--', alpha=0.3)\n",
    "ax.grid(alpha=0.2, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Why this works:** The implicit reward extracts a reward function from the policy without ever training a separate reward model. After DPO training, the policy has learned to assign relatively higher probability to preferred responses (compared to the reference). The log-ratio captures this shift. Crucially, the implicit reward also applies to new responses the model was not trained on — the reward generalizes because the policy's distribution has shifted globally, not just on the specific training examples.\n",
    "\n",
    "**What if the ranking is imperfect for new responses?** With only 5 training pairs and a small model, the implicit reward's generalization is limited. With more training data and a larger model, the ranking quality improves substantially. The key insight is structural: the implicit reward *exists* and *generalizes to some degree* — it is not just memorization of the training pairs.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just explored:** After DPO training, the policy implicitly defines a reward function through its log-ratio with the reference. Preferred responses have higher implicit reward than dispreferred ones — this confirms the training worked. More importantly, the implicit reward applies to new responses that were never in the training data.\n",
    "\n",
    "This is the deepest insight of DPO: the reward model was always inside the policy. DPO does not eliminate the reward model — it *absorbs* it into the policy. The log-ratio between policy and reference IS the reward function. Any policy paired with a reference defines a reward, and you can extract it at any time by computing how much the policy has shifted from its starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The DPO loss is simple to compute but each step maps to the derivation.** Log-ratios (implicit KL), difference (comparing preferred vs dispreferred), scale by beta (control conservatism), negative log-sigmoid (convert to loss). Five lines of code, each justified by the math.\n",
    "\n",
    "2. **Beta controls conservatism.** Higher beta produces larger losses for the same log-ratio difference, keeping the policy closer to the reference. Lower beta allows more aggressive deviation. The choice of beta is the practical knob for the KL-reward tradeoff.\n",
    "\n",
    "3. **DPO's gradient focuses on the hard cases.** The gradient is strongest when the model disagrees with human preferences and near zero when it already agrees. Learning effort concentrates where it matters most — you confirmed this with autograd.\n",
    "\n",
    "4. **DPO training looks like supervised learning.** The training loop is forward, loss, backward, step — the familiar pattern. The complexity is in the loss function, not the loop. The reference model is a frozen snapshot that never updates.\n",
    "\n",
    "5. **The implicit reward generalizes beyond the training data.** After training, the log-ratio between policy and reference defines a reward function that applies to ANY response, not just the ones in the training set. The reward model was always inside the policy — DPO makes this explicit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}