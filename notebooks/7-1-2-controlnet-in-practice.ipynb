{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ControlNet in Practice\n",
    "\n",
    "**Module 7.1, Lesson 2** | CourseAI\n",
    "\n",
    "You know the architectureâ€”trainable encoder copy, zero convolutions, additive features at skip connections. This notebook is where the architecture becomes a tool. Real preprocessors, real images, real control.\n",
    "\n",
    "**What you will do:**\n",
    "- Extract Canny edges from a photograph with different threshold settings and observe how preprocessing quality affects generation\n",
    "- Use three different preprocessors (Canny, depth, OpenPose) and verify the pipeline API is identical for all of them\n",
    "- Sweep the conditioning scale from 0.3 to 2.0 and discover the control-creativity tradeoff firsthand\n",
    "- Stack two ControlNets (Canny + depth) and see how complementary spatial constraints compose\n",
    "- Choose your own source image, preprocessor(s), and settings to create a controlled composition\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "Every concept in this notebook comes from the lesson. Preprocessor types, conditioning scale as a volume knob, multi-ControlNet stacking by additive composition. No new theoryâ€”just hands-on practice with real models.\n",
    "\n",
    "**Estimated time:** 40â€“60 minutes (model downloads may take several minutes on first run).\n",
    "\n",
    "**VRAM requirements:** This notebook is designed for a T4 GPU (16 GB). It carefully manages GPU memory by never loading two full pipelines simultaneously. Follow the cleanup cells between exercises.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies, import everything, and configure the environment.\n",
    "\n",
    "**Important:** Set the runtime to GPU before running. In Colab: Runtime â†’ Change runtime type â†’ T4 GPU.\n",
    "\n",
    "The first run will download model weights (~5 GB for SD v1.5 + ~1.5 GB per ControlNet checkpoint). Subsequent runs use cached weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate safetensors controlnet_aux opencv-python-headless\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from PIL import Image\n",
    "from diffusers import (\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    StableDiffusionPipeline,\n",
    "    ControlNetModel,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('WARNING: No GPU detected. This notebook requires a GPU for image generation.')\n",
    "    print('In Colab: Runtime â†’ Change runtime type â†’ T4 GPU')\n",
    "\n",
    "print('\\nSetup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Helpers\n",
    "\n",
    "Utility functions used across multiple exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def download_sample_image(url, filename=\"sample.jpg\"):\n    \"\"\"Download an image from a URL and return it as a PIL Image.\"\"\"\n    import urllib.request\n    urllib.request.urlretrieve(url, filename)\n    return Image.open(filename).convert(\"RGB\")\n\n\ndef show_images(images, titles, figsize=None, suptitle=None):\n    \"\"\"Display a row of images with titles.\"\"\"\n    n = len(images)\n    if figsize is None:\n        figsize = (5 * n, 5)\n    fig, axes = plt.subplots(1, n, figsize=figsize)\n    if n == 1:\n        axes = [axes]\n    for ax, img, title in zip(axes, images, titles):\n        ax.imshow(img)\n        ax.set_title(title, fontsize=10)\n        ax.axis('off')\n    if suptitle:\n        plt.suptitle(suptitle, fontsize=13, y=1.02)\n    plt.tight_layout()\n    plt.show()\n\n\ndef cleanup_pipeline(pipe):\n    \"\"\"Delete a pipeline and free GPU memory.\"\"\"\n    del pipe\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        allocated = torch.cuda.memory_allocated() / 1e9\n        print(f\"GPU memory after cleanup: {allocated:.2f} GB allocated\")\n\n\ndef make_generator(seed):\n    \"\"\"Create a torch Generator with the given seed for reproducible results.\"\"\"\n    return torch.Generator(device=device).manual_seed(seed)\n\n\n# Download a sample image that works well for all exercises.\n# This is a Creative Commons photo of a person standing in front of architecture,\n# giving us edges (building), depth (foreground/background), and pose (person).\nSOURCE_URL = \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\nsource_image = download_sample_image(SOURCE_URL, \"source.jpg\")\nsource_image = source_image.resize((512, 512))\n\nshow_images([source_image], [\"Source Image (512x512)\"])\nprint(\"This source image will be used throughout the notebook.\")\nprint(\"It has clear edges, depth layering, and a visible personâ€”ideal for all three preprocessors.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 1: Canny Edge Preprocessing and Generation `[Guided]`\n\nFrom the lesson: preprocessing quality is the **most impactful practical decision** you will make with ControlNet. The spatial map is what ControlNet followsâ€”garbage in, garbage out.\n\nCanny edge detection has two thresholds (low, high) that control edge sensitivity:\n- Edges below the low threshold are discarded\n- Edges above the high threshold are kept\n- Edges in between are kept only if connected to strong edges\n\nWe will extract Canny edges at three different threshold pairs, then generate an image with ControlNet for each. The goal: see how preprocessing quality directly affects output quality.\n\n**Before running, predict:**\n- Which threshold settingâ€”(50, 100), (100, 200), or (200, 300)â€”will produce the best-controlled generation?\n- What goes wrong with too-low thresholds (too many edges)?\n- What goes wrong with too-high thresholds (too few edges)?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Extract Canny edges at three threshold settings\n\nsource_np = np.array(source_image)\n\n# Three threshold pairs: too many edges, good edges, too few edges\nthreshold_pairs = [\n    (50, 100),   # Low thresholdsâ€”picks up lots of texture/noise edges\n    (100, 200),  # Moderate thresholdsâ€”clean structural edges\n    (200, 300),  # High thresholdsâ€”only the strongest edges survive\n]\n\ncanny_maps = {}\nfor low, high in threshold_pairs:\n    edges = cv2.Canny(source_np, low, high)\n    # Convert to 3-channel RGB PIL Image (ControlNet expects RGB)\n    edges_rgb = np.stack([edges] * 3, axis=-1)\n    canny_maps[(low, high)] = Image.fromarray(edges_rgb)\n\n# Display the three edge maps side by side\nshow_images(\n    [source_image] + list(canny_maps.values()),\n    [\"Source\"] + [f\"Canny ({lo}, {hi})\" for lo, hi in threshold_pairs],\n    suptitle=\"Same photo, three Canny threshold settings\",\n)\n\nprint(\"Low thresholds (50, 100): many edges, including texture and noise.\")\nprint(\"Moderate thresholds (100, 200): clean structural edges, good detail.\")\nprint(\"High thresholds (200, 300): only strongest edges, missing structure.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate with ControlNet using each edge map\n",
    "#\n",
    "# VRAM note: We load one pipeline, generate all three images, then clean up.\n",
    "# All models use float16 to fit within T4's 16 GB VRAM.\n",
    "\n",
    "controlnet_canny = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet_canny,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    ").to(device)\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "prompt = \"a beautiful watercolor painting of a woman, masterpiece, high quality\"\n",
    "seed = 42\n",
    "num_steps = 20\n",
    "\n",
    "generated_images = {}\n",
    "for (low, high), edge_map in canny_maps.items():\n",
    "    generator = make_generator(seed)\n",
    "    result = pipe(\n",
    "        prompt,\n",
    "        image=edge_map,\n",
    "        num_inference_steps=num_steps,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    generated_images[(low, high)] = result\n",
    "\n",
    "print(\"Generation complete for all three threshold settings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Compare the results\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 10))\n\n# Top row: edge maps\naxes[0][0].imshow(source_image)\naxes[0][0].set_title(\"Source Image\", fontsize=11)\naxes[0][0].axis('off')\n\nfor i, ((low, high), edge_map) in enumerate(canny_maps.items()):\n    axes[0][i + 1].imshow(edge_map)\n    axes[0][i + 1].set_title(f\"Canny ({low}, {high})\", fontsize=11)\n    axes[0][i + 1].axis('off')\n\n# Bottom row: generated images\naxes[1][0].axis('off')  # empty cell\naxes[1][0].set_title(\"(source, no generation)\", fontsize=9, color='gray')\n\nlabels = [\"Too many edges\", \"Good edges âœ“\", \"Too few edges\"]\nfor i, ((low, high), gen_img) in enumerate(generated_images.items()):\n    axes[1][i + 1].imshow(gen_img)\n    axes[1][i + 1].set_title(f\"Generated ({low}, {high})\\n{labels[i]}\", fontsize=11)\n    axes[1][i + 1].axis('off')\n\nplt.suptitle(\"Preprocessing quality directly affects ControlNet output quality\", fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"Observations:\")\nprint(\"- (50, 100): Too many edges from texture. Model over-constrains to noise.\")\nprint(\"- (100, 200): Clean structural edges. Model follows composition precisely, natural output.\")\nprint(\"- (200, 300): Too few edges. Model has insufficient guidance, loose structure.\")\nprint(\"\")\nprint(\"Key insight: preprocessing is the most impactful practical decision.\")\nprint(\"ControlNet faithfully follows whatever spatial map you give it.\")\nprint(\"Garbage in, garbage out.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save the best edge map for later exercises.\n# Exercise 3 (conditioning scale sweep) will reuse this.\nbest_canny_map = canny_maps[(100, 200)]\n\n# VRAM cleanup: keep the pipeline loaded. Exercise 2 will use the same Canny ControlNet\n# before switching to other preprocessors.\nprint(\"Best Canny edge map saved for Exercise 3.\")\nprint(f\"Pipeline still loaded. GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### What Just Happened\n\nYou extracted Canny edges from the same photograph at three different threshold settings and generated an image with each. The results demonstrate the lesson's central practical point:\n\n- **Too many edges (50, 100):** Texture details, noise, and grain all become edges. ControlNet tries to follow every spurious contour, producing over-constrained, artifacted output.\n- **Good edges (100, 200):** Clean structural edges capture object boundaries and composition. ControlNet follows the composition precisely while the text prompt fills in natural-looking details.\n- **Too few edges (200, 300):** Only the strongest gradients survive. The model lacks sufficient spatial guidanceâ€”the result is loosely controlled, with the composition only roughly matching.\n\n**The threshold tuning was the most impactful decision.** The same ControlNet checkpoint, the same prompt, the same seedâ€”only the preprocessing changed, and the output quality varied dramatically.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Three Preprocessors, One Pipeline `[Guided]`\n",
    "\n",
    "From the lesson: the ControlNet pipeline is **identical for all spatial map types**. Only two things change: (1) which preprocessor extracts the map, and (2) which ControlNet checkpoint you load. The architecture is genuinely map-agnostic.\n",
    "\n",
    "We will extract three types of spatial maps from the same source imageâ€”Canny edges, MiDaS depth, and OpenPose skeletonâ€”and generate with each. Pay attention to what **stays the same** in the API code across all three.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Will the generated images look similar to each other, or qualitatively different?\n",
    "- What kind of structural control does each map type provide? (edges = ?, depth = ?, pose = ?)\n",
    "- How much of the pipeline code will change when switching from one map type to another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: We already have the Canny edge map from Exercise 1.\n# Now extract depth and pose maps.\n\n# --- Depth map via DPT (MiDaS-based) ---\nfrom transformers import pipeline as hf_pipeline\n\ndepth_estimator = hf_pipeline(\"depth-estimation\", model=\"Intel/dpt-hybrid-midas\")\ndepth_result = depth_estimator(source_image)\ndepth_map = depth_result[\"depth\"]  # PIL Image, grayscale\ndepth_map = depth_map.resize((512, 512))\n\n# Clean up the depth model (we only need the map)\ndel depth_estimator\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(\"Depth map extracted.\")\n\n# --- Pose map via OpenPose ---\nfrom controlnet_aux import OpenposeDetector\n\nopenpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\npose_map = openpose(source_image)\npose_map = pose_map.resize((512, 512))\n\n# Clean up the pose model\ndel openpose\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(\"Pose map extracted.\")\n\n# Display all three spatial maps\nshow_images(\n    [source_image, best_canny_map, depth_map, pose_map],\n    [\"Source\", \"Canny Edges\", \"Depth Map (MiDaS)\", \"OpenPose Skeleton\"],\n    suptitle=\"Same source image, three preprocessors, three types of spatial information\",\n)\n\nprint(\"Each preprocessor captures different spatial information:\")\nprint(\"  Canny: 2D contours and silhouettes\")\nprint(\"  Depth: 3D structure, perspective, foreground/background layering\")\nprint(\"  Pose:  Body joint positions and limb angles\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate with each preprocessor and its corresponding ControlNet checkpoint.\n",
    "#\n",
    "# VRAM management: We load ONE ControlNet pipeline at a time.\n",
    "# After generating with Canny, we delete the pipeline, then load depth, etc.\n",
    "# This keeps us well within T4's 16 GB VRAM budget.\n",
    "\n",
    "prompt = \"a beautiful watercolor painting of a woman, masterpiece, high quality\"\n",
    "seed = 42\n",
    "num_steps = 20\n",
    "\n",
    "# --- Generate with Canny (pipeline already loaded from Exercise 1) ---\n",
    "generator = make_generator(seed)\n",
    "img_canny = pipe(\n",
    "    prompt,\n",
    "    image=best_canny_map,\n",
    "    num_inference_steps=num_steps,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "print(\"Canny generation complete.\")\n",
    "\n",
    "# Clean up Canny pipeline before loading depth\n",
    "cleanup_pipeline(pipe)\n",
    "del controlnet_canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate with Depth ---\n",
    "controlnet_depth = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-depth\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet_depth,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    ").to(device)\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "generator = make_generator(seed)\n",
    "img_depth = pipe(\n",
    "    prompt,\n",
    "    image=depth_map,\n",
    "    num_inference_steps=num_steps,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "print(\"Depth generation complete.\")\n",
    "\n",
    "# Save depth map and controlnet for Exercise 4 (stacking)\n",
    "# But clean up the pipeline to free VRAM for OpenPose\n",
    "cleanup_pipeline(pipe)\n",
    "del controlnet_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate with OpenPose ---\n",
    "controlnet_pose = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-openpose\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet_pose,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    ").to(device)\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "generator = make_generator(seed)\n",
    "img_pose = pipe(\n",
    "    prompt,\n",
    "    image=pose_map,\n",
    "    num_inference_steps=num_steps,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "print(\"OpenPose generation complete.\")\n",
    "\n",
    "# Clean up the pose pipeline\n",
    "cleanup_pipeline(pipe)\n",
    "del controlnet_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Compare all three side by side\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 10))\n\n# Top row: spatial maps\naxes[0][0].imshow(source_image)\naxes[0][0].set_title(\"Source Image\", fontsize=11)\naxes[0][0].axis('off')\n\nmaps_and_titles = [\n    (best_canny_map, \"Canny Edges\"),\n    (depth_map, \"Depth Map\"),\n    (pose_map, \"OpenPose Skeleton\"),\n]\nfor i, (m, t) in enumerate(maps_and_titles):\n    axes[0][i + 1].imshow(m)\n    axes[0][i + 1].set_title(t, fontsize=11)\n    axes[0][i + 1].axis('off')\n\n# Bottom row: generated images\naxes[1][0].axis('off')\naxes[1][0].set_title(\"(source)\", fontsize=9, color='gray')\n\ngen_and_titles = [\n    (img_canny, \"Canny â†’ Contours\"),\n    (img_depth, \"Depth â†’ 3D Structure\"),\n    (img_pose, \"Pose â†’ Body Position\"),\n]\nfor i, (img, t) in enumerate(gen_and_titles):\n    axes[1][i + 1].imshow(img)\n    axes[1][i + 1].set_title(t, fontsize=11)\n    axes[1][i + 1].axis('off')\n\nplt.suptitle(\"Same prompt, same seedâ€”different preprocessor, qualitatively different control\", fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"Notice what stayed the SAME in the code across all three:\")\nprint(\"  - StableDiffusionControlNetPipeline.from_pretrained(...)\")\nprint(\"  - pipe(prompt, image=spatial_map, num_inference_steps=..., generator=...)\")\nprint(\"\")\nprint(\"Only TWO things changed:\")\nprint(\"  1. Which preprocessor extracted the map (cv2.Canny vs depth_estimator vs openpose)\")\nprint(\"  2. Which ControlNet checkpoint was loaded (sd-controlnet-canny vs -depth vs -openpose)\")\nprint(\"\")\nprint(\"The pipeline does not know or care what kind of spatial map it receives.\")\nprint(\"The architecture is genuinely map-agnostic.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### What Just Happened\n\nYou used three different preprocessors on the same source image and generated with each using the corresponding ControlNet checkpoint:\n\n- **Canny edges** controlled 2D contours and silhouettesâ€”the model preserved object boundaries and composition.\n- **MiDaS depth** controlled 3D structure and layeringâ€”the model preserved foreground/background arrangement and perspective.\n- **OpenPose skeleton** controlled body poseâ€”the model generated a figure matching the skeleton's joint positions.\n\nThe three outputs look qualitatively different because each spatial map captures a **different kind** of structural information. Yet the pipeline code was identical. The modularity from lesson 1â€”\"four translators, one pipeline\"â€”is real. Swap the preprocessor and the checkpoint; the socket does not change.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Conditioning Scale Sweep `[Supported]`\n",
    "\n",
    "From the lesson: conditioning scale is a **volume knob for spatial control**. Low scale means the spatial map is a faint suggestion and the model generates freely from text. High scale means the model rigidly follows the spatial map, losing natural variation. The sweet spot is typically 0.7â€“1.0.\n",
    "\n",
    "This exercise uses the best Canny edge map from Exercise 1. Your task: write the generation loop that sweeps across conditioning scale values and displays the results as a comparison grid.\n",
    "\n",
    "After the sweep, you will also verify that **text conditioning remains active** even at scale=1.0 by generating with two different prompts on the same edge map.\n",
    "\n",
    "**Before running, predict:**\n",
    "- At scale=0.3, will the output follow the edge map at all?\n",
    "- At scale=2.0, what will the output look like? Sharper? More detailed? Or something else?\n",
    "- At scale=1.0 with two different prompts, will the images look identical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Canny ControlNet pipeline for this exercise\n",
    "\n",
    "controlnet_canny = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet_canny,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    ").to(device)\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "print(\"Canny ControlNet pipeline loaded.\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TASK: Sweep the conditioning scale from 0.3 to 2.0.\n",
    "#\n",
    "# The scale values and display code are provided.\n",
    "# You write the generation loop that fills in `sweep_results`.\n",
    "\n",
    "scales = [0.3, 0.5, 0.7, 1.0, 1.5, 2.0]\n",
    "prompt = \"a beautiful watercolor painting of a woman, masterpiece, high quality\"\n",
    "seed = 42\n",
    "num_steps = 20\n",
    "\n",
    "sweep_results = {}  # Maps scale -> generated PIL Image\n",
    "\n",
    "for scale in scales:\n",
    "    # TODO: Generate an image using the pipeline with this conditioning scale.\n",
    "    #\n",
    "    # Use: pipe(prompt, image=best_canny_map, num_inference_steps=num_steps,\n",
    "    #          generator=make_generator(seed),\n",
    "    #          controlnet_conditioning_scale=???)\n",
    "    #\n",
    "    # Store the result in sweep_results[scale]\n",
    "    # (the pipeline returns .images[0] for the first image)\n",
    "    pass\n",
    "\n",
    "print(f\"Generated {len(sweep_results)} images across conditioning scales.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display the conditioning scale sweep as a comparison grid\n\nif not sweep_results:\n    print(\"sweep_results is emptyâ€”go back to the previous cell and fill in the TODO.\")\n    print(\"You need to generate an image for each scale value and store it in sweep_results[scale].\")\n    print(\"Check the solution below the TODO cell if you get stuck.\")\nelse:\n    fig, axes = plt.subplots(1, len(scales) + 1, figsize=(4 * (len(scales) + 1), 4))\n\n    # First column: the edge map\n    axes[0].imshow(best_canny_map)\n    axes[0].set_title(\"Edge Map\\n(input)\", fontsize=10)\n    axes[0].axis('off')\n\n    # One column per scale value\n    scale_labels = {\n        0.3: \"0.3\\n(faint suggestion)\",\n        0.5: \"0.5\\n(visible influence)\",\n        0.7: \"0.7\\n(good balance)\",\n        1.0: \"1.0\\n(strong control)\",\n        1.5: \"1.5\\n(over-constraining)\",\n        2.0: \"2.0\\n(heavily rigid)\",\n    }\n\n    for i, scale in enumerate(scales):\n        axes[i + 1].imshow(sweep_results[scale])\n        axes[i + 1].set_title(f\"Scale {scale_labels[scale]}\", fontsize=10)\n        axes[i + 1].axis('off')\n\n    plt.suptitle(\"Conditioning Scale Sweep: same edges, same prompt, varying spatial strength\", fontsize=13, y=1.02)\n    plt.tight_layout()\n    plt.show()\n\n    print(\"The volume knob for spatial control:\")\n    print(\"  Low (0.3-0.5): spatial map is a suggestion, model generates freely\")\n    print(\"  Sweet spot (0.7-1.0): clear structural adherence, natural textures\")\n    print(\"  High (1.5-2.0): over-constrained, rigid textures, artifacts\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Verify that text conditioning remains active at scale=1.0.\n#\n# Same edge map, same scale, same seedâ€”TWO different prompts.\n# If spatial conditioning disabled text conditioning, the images would be identical.\n\nprompt_a = \"a beautiful watercolor painting of a woman, masterpiece\"\nprompt_b = \"a cyberpunk android with neon tattoos, digital art\"\n\ngenerator = make_generator(seed)\nimg_prompt_a = pipe(\n    prompt_a,\n    image=best_canny_map,\n    num_inference_steps=num_steps,\n    generator=generator,\n    controlnet_conditioning_scale=1.0,\n).images[0]\n\ngenerator = make_generator(seed)\nimg_prompt_b = pipe(\n    prompt_b,\n    image=best_canny_map,\n    num_inference_steps=num_steps,\n    generator=generator,\n    controlnet_conditioning_scale=1.0,\n).images[0]\n\nshow_images(\n    [best_canny_map, img_prompt_a, img_prompt_b],\n    [\"Edge Map\", f'Prompt A:\\n\"{prompt_a[:40]}...\"', f'Prompt B:\\n\"{prompt_b[:40]}...\"'],\n    suptitle=\"Same edges, scale=1.0, different prompts â†’ same structure, different content\",\n)\n\nprint(\"The structure is identical (both follow the same edges).\")\nprint(\"The content is completely different (watercolor vs cyberpunk).\")\nprint(\"\")\nprint(\"Conditioning scale 1.0 does NOT disable text conditioning.\")\nprint(\"Remember WHEN/WHAT/WHERE: the scale controls how loud the WHERE signal is,\")\nprint(\"not whether the WHAT signal is on.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the pipeline before Exercise 4\n",
    "cleanup_pipeline(pipe)\n",
    "del controlnet_canny\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Pipeline cleaned up. Ready for Exercise 4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe key insight is that `controlnet_conditioning_scale` is a single parameter in the pipeline callâ€”your manual volume knob for spatial control. The sweep loop is just calling the pipeline with a different scale value each time.\n\n```python\nfor scale in scales:\n    generator = make_generator(seed)\n    result = pipe(\n        prompt,\n        image=best_canny_map,\n        num_inference_steps=num_steps,\n        generator=generator,\n        controlnet_conditioning_scale=scale,\n    ).images[0]\n    sweep_results[scale] = result\n```\n\n**Why this works:** The conditioning scale multiplies the ControlNet's output features before they are added to the frozen encoder's skip connections. At scale=0, the ControlNet's contribution is zeroed out (muted). At scale=1.0, it is at full trained strength. Above 1.0, you are amplifying the signal beyond what training optimized for, which is why over-constraining occurs.\n\n**Common mistake:** Forgetting to recreate the generator for each scale value. Without resetting the seed, each generation starts from different random noise, making the comparison unfairâ€”you would not be isolating the effect of the scale parameter.\n\n</details>\n\n### What Just Happened\n\nYou swept the conditioning scale from 0.3 to 2.0 and discovered the control-creativity tradeoff firsthand:\n\n- **Low scales (0.3â€“0.5):** The spatial map is a faint suggestion. The model generates mostly from the text prompt, with rough composition loosely following the edges.\n- **Sweet spot (0.7â€“1.0):** Clear structural adherence with natural textures. The model follows the composition precisely while maintaining the visual quality you expect from SD.\n- **High scales (1.5â€“2.0):** Over-constrained. Textures flatten, details stiffen, the image looks mechanical. The model is trying too hard to match every edge pixel.\n\nYou also verified that **text conditioning remains active** at scale=1.0. Two different prompts with the same edge map produced the same structure but completely different content. The WHEN/WHAT/WHERE channels coexistâ€”conditioning scale turns up the WHERE volume, it does not mute the WHAT channel.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 4: Multi-ControlNet Stacking `[Supported]`\n\nFrom the lesson: each ControlNet independently contributes additive features to the skip connections. They compose by summation: `e_i + z_i_canny + z_i_depth`. Stacking is not doubling control strengthâ€”it is providing two **complementary** types of structural constraint.\n\nYour task: load both the Canny and depth ControlNet checkpoints, extract both maps from the same source image (you already have these from earlier exercises), and generate three comparisons:\n1. Canny edges only\n2. Depth only\n3. Both stacked together\n\nThe model loading is done for you. You write the pipeline construction and generation calls.\n\n**Before running, predict:**\n- Will the stacked result look like \"Canny result + depth result\" blended together?\n- Will the stacked result be more precisely controlled than either alone, or more artifacted?\n- What conditioning scales should you use for the stacked version?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate with Canny only\n",
    "#\n",
    "# Load a single-ControlNet pipeline for Canny, generate, clean up.\n",
    "\n",
    "prompt = \"a beautiful watercolor painting of a woman, masterpiece, high quality\"\n",
    "seed = 42\n",
    "num_steps = 20\n",
    "\n",
    "controlnet_canny = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipe_canny = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet_canny,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    ").to(device)\n",
    "pipe_canny.scheduler = UniPCMultistepScheduler.from_config(pipe_canny.scheduler.config)\n",
    "\n",
    "generator = make_generator(seed)\n",
    "img_canny_only = pipe_canny(\n",
    "    prompt,\n",
    "    image=best_canny_map,\n",
    "    num_inference_steps=num_steps,\n",
    "    generator=generator,\n",
    "    controlnet_conditioning_scale=0.8,\n",
    ").images[0]\n",
    "print(\"Canny-only generation complete.\")\n",
    "\n",
    "# Clean up before loading depth pipeline\n",
    "cleanup_pipeline(pipe_canny)\n",
    "del controlnet_canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate with Depth only\n",
    "\n",
    "controlnet_depth = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-depth\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipe_depth = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet_depth,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    ").to(device)\n",
    "pipe_depth.scheduler = UniPCMultistepScheduler.from_config(pipe_depth.scheduler.config)\n",
    "\n",
    "generator = make_generator(seed)\n",
    "img_depth_only = pipe_depth(\n",
    "    prompt,\n",
    "    image=depth_map,\n",
    "    num_inference_steps=num_steps,\n",
    "    generator=generator,\n",
    "    controlnet_conditioning_scale=0.8,\n",
    ").images[0]\n",
    "print(\"Depth-only generation complete.\")\n",
    "\n",
    "# Clean up before loading the stacked pipeline\n",
    "cleanup_pipeline(pipe_depth)\n",
    "del controlnet_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Generate with BOTH Canny + Depth stacked.\n#\n# The ControlNet models are loaded for you.\n# YOUR TASK: Build the stacked pipeline and generate.\n#\n# VRAM note: Two ControlNet models + SD base model will use ~8-9 GB.\n# This fits on a T4 since we cleaned up the previous pipelines.\n\ncontrolnet_canny = ControlNetModel.from_pretrained(\n    \"lllyasviel/sd-controlnet-canny\",\n    torch_dtype=torch.float16,\n)\ncontrolnet_depth = ControlNetModel.from_pretrained(\n    \"lllyasviel/sd-controlnet-depth\",\n    torch_dtype=torch.float16,\n)\n\n# TODO: Build a StableDiffusionControlNetPipeline with BOTH ControlNets.\n#\n# The key difference from single ControlNet: pass a LIST of ControlNets.\n#   controlnet=[controlnet_canny, controlnet_depth]\n#\n# Use the same base model, torch_dtype, and safety_checker settings as before.\n# Don't forget to set the scheduler and move to device.\n#\n# pipe_stacked = StableDiffusionControlNetPipeline.from_pretrained(\n#     ...,\n#     controlnet=???,\n#     ...,\n# ).to(device)\n# pipe_stacked.scheduler = ...\n\n\n\n# TODO: Generate with the stacked pipeline.\n#\n# The pipeline takes LISTS for image and controlnet_conditioning_scale:\n#   image=[best_canny_map, depth_map]\n#   controlnet_conditioning_scale=[0.7, 0.5]\n#\n# Use moderate scales (0.5-0.8) for stacking. The combined effect is stronger\n# than either alone, so you need lower per-ControlNet scales.\n#\n# generator = make_generator(seed)\n# img_stacked = pipe_stacked(\n#     ...,\n#     image=???,\n#     controlnet_conditioning_scale=???,\n#     ...,\n# ).images[0]\n\n\n\nprint(\"Stacked generation complete.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Display the comparison (edges only vs depth only vs both stacked)\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Top row: spatial maps\naxes[0][0].imshow(best_canny_map)\naxes[0][0].set_title(\"Canny Edges\", fontsize=11)\naxes[0][0].axis('off')\n\naxes[0][1].imshow(depth_map)\naxes[0][1].set_title(\"Depth Map\", fontsize=11)\naxes[0][1].axis('off')\n\naxes[0][2].imshow(source_image)\naxes[0][2].set_title(\"Source Image\", fontsize=11)\naxes[0][2].axis('off')\n\n# Bottom row: generated images\naxes[1][0].imshow(img_canny_only)\naxes[1][0].set_title(\"Canny Only\\n(contour control)\", fontsize=11)\naxes[1][0].axis('off')\n\naxes[1][1].imshow(img_depth_only)\naxes[1][1].set_title(\"Depth Only\\n(layering control)\", fontsize=11)\naxes[1][1].axis('off')\n\naxes[1][2].imshow(img_stacked)\naxes[1][2].set_title(\"Canny + Depth Stacked\\n(contours AND layering)\", fontsize=11)\naxes[1][2].axis('off')\n\nplt.suptitle(\"Multi-ControlNet: complementary constraints compose by additive features\", fontsize=13, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"Observations:\")\nprint(\"  Canny only: precise contour control, but depth/perspective is model's choice.\")\nprint(\"  Depth only: correct spatial layering, but specific contours are loose.\")\nprint(\"  Stacked: BOTH contours AND layering are controlledâ€”more precise than either alone.\")\nprint(\"\")\nprint(\"The stacked result is not '2x more controlled.'\")\nprint(\"It is controlled in TWO DIFFERENT WAYS simultaneously.\")\nprint(\"Edges enforce contours. Depth enforces layering. Complementary, not redundant.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the stacked pipeline\n",
    "cleanup_pipeline(pipe_stacked)\n",
    "del controlnet_canny, controlnet_depth\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Pipeline cleaned up. Ready for Exercise 5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe core insight is that stacking uses Python **lists**â€”a list of ControlNet models, a list of conditioning images, and a list of per-ControlNet conditioning scales. The pipeline handles the rest (running each ControlNet independently and summing their additive features at the skip connections).\n\n**Building the stacked pipeline:**\n```python\npipe_stacked = StableDiffusionControlNetPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    controlnet=[controlnet_canny, controlnet_depth],\n    torch_dtype=torch.float16,\n    safety_checker=None,\n).to(device)\npipe_stacked.scheduler = UniPCMultistepScheduler.from_config(pipe_stacked.scheduler.config)\n```\n\n**Generating with the stacked pipeline:**\n```python\ngenerator = make_generator(seed)\nimg_stacked = pipe_stacked(\n    prompt,\n    image=[best_canny_map, depth_map],\n    num_inference_steps=num_steps,\n    generator=generator,\n    controlnet_conditioning_scale=[0.7, 0.5],\n).images[0]\n```\n\n**Why moderate scales (0.7 and 0.5)?** Each ControlNet independently contributes additive features. Their combined effect at the skip connections is `e_i + z_i_canny + z_i_depth`. If both are at scale 1.0, the combined spatial constraint can be too strong, producing artifacts. Starting with moderate scales (0.5â€“0.8) gives each map influence while leaving room for the model to generate natural details.\n\n**Why Canny at 0.7 and depth at 0.5?** Canny edges provide fine-grained contour control that is visually preciseâ€”you want this signal to be reasonably strong. Depth provides broad spatial layeringâ€”a softer influence is usually sufficient. You could tune these differently depending on your creative intent.\n\n**Common mistake:** Passing `image=best_canny_map` (a single image) instead of `image=[best_canny_map, depth_map]` (a list). When you have multiple ControlNets, the pipeline expects a list of conditioning imagesâ€”one per ControlNet.\n\n</details>\n\n### What Just Happened\n\nYou stacked two ControlNetsâ€”Canny edges and depthâ€”and compared the result to each individual ControlNet:\n\n- **Canny only:** Precise contour control. Object boundaries and silhouettes match the edges. But depth and perspective are the model's choice.\n- **Depth only:** Correct spatial layering and perspective. But specific contours are loosely controlled.\n- **Stacked:** Both contours AND layering are controlled. The combination is more precisely controlled than either alone because the two maps provide **complementary** structural information.\n\nThe stacked result is not \"2x more controlled.\" It is controlled in two different ways simultaneouslyâ€”edges enforce contours while depth enforces layering. This is the additive composition from the architecture: `e_i + z_i_canny + z_i_depth`.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Your Composition `[Independent]`\n",
    "\n",
    "You now have all the practical skills:\n",
    "- **Preprocessing:** Extract Canny edges, depth maps, or pose skeletons from any image\n",
    "- **Conditioning scale:** Tune the volume knob for spatial control (sweet spot: 0.7â€“1.0)\n",
    "- **Stacking:** Combine multiple spatial constraints with per-ControlNet scales\n",
    "\n",
    "**Your task:** Create a controlled composition from scratch.\n",
    "\n",
    "1. Choose a source image (options provided below, or use your own)\n",
    "2. Decide which preprocessor(s) to use based on your creative intent\n",
    "3. Tune the preprocessing (Canny thresholds if using edges)\n",
    "4. Choose your conditioning scale(s)\n",
    "5. Write a text prompt that complements the spatial control\n",
    "6. Generate and iterate\n",
    "\n",
    "No scaffolding. You decide the full workflow.\n",
    "\n",
    "**Available source images:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Source image options (pick one or upload your own)\n\nimage_options = {\n    \"vermeer\": \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\",\n    \"architecture\": \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_input.png\",\n    \"person\": \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png\",\n}\n\n# Show what is available\noption_images = []\noption_titles = []\nfor name, url in image_options.items():\n    try:\n        img = download_sample_image(url, f\"{name}.jpg\").resize((512, 512))\n        option_images.append(img)\n        option_titles.append(name)\n    except Exception as e:\n        print(f\"Could not download {name}: {e}\")\n\nshow_images(option_images, option_titles, suptitle=\"Choose a source image (or upload your own)\")\n\nprint(\"Pick one of these, or upload your own image to Colab.\")\nprint(\"Think about what kind of spatial control you want:\")\nprint(\"  - Canny edges for precise contours\")\nprint(\"  - Depth for spatial layering\")\nprint(\"  - Pose for body positioning\")\nprint(\"  - Stacking for combined control\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your composition code goes here.\n",
    "#\n",
    "# Workflow:\n",
    "# 1. Choose your source image\n",
    "# 2. Preprocess it (Canny, depth, pose, or multiple)\n",
    "# 3. Load the appropriate ControlNet checkpoint(s)\n",
    "# 4. Build the pipeline\n",
    "# 5. Generate with your chosen prompt and conditioning scale(s)\n",
    "# 6. Display the result\n",
    "#\n",
    "# Remember:\n",
    "# - Use torch.float16 for all models\n",
    "# - Clean up preprocessor models after extracting maps\n",
    "# - Start with conditioning_scale around 0.7-1.0\n",
    "# - For stacking, use moderate scales (0.5-0.8 per ControlNet)\n",
    "#\n",
    "# Have fun!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution (example)</summary>\n\nThere is no single correct answerâ€”this exercise is about making practical decisions. Here is one example using the architecture image with stacked Canny + depth:\n\n**Why this approach:** The architecture image has clear geometric edges (good for Canny) and visible depth layering (good for depth ControlNet). Stacking both gives precise contour control AND correct perspective.\n\n```python\n# Choose source image\nmy_image = download_sample_image(image_options[\"architecture\"], \"my_source.jpg\")\nmy_image = my_image.resize((512, 512))\n\n# Preprocess: Canny edges\nmy_edges = cv2.Canny(np.array(my_image), 100, 200)\nmy_canny = Image.fromarray(np.stack([my_edges] * 3, axis=-1))\n\n# Preprocess: depth map\nfrom transformers import pipeline as hf_pipeline\ndepth_est = hf_pipeline(\"depth-estimation\", model=\"Intel/dpt-hybrid-midas\")\nmy_depth = depth_est(my_image)[\"depth\"].resize((512, 512))\ndel depth_est; gc.collect(); torch.cuda.empty_cache()\n\n# Load stacked pipeline\ncn_canny = ControlNetModel.from_pretrained(\n    \"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16\n)\ncn_depth = ControlNetModel.from_pretrained(\n    \"lllyasviel/sd-controlnet-depth\", torch_dtype=torch.float16\n)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    controlnet=[cn_canny, cn_depth],\n    torch_dtype=torch.float16,\n    safety_checker=None,\n).to(device)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n\n# Generate\ngenerator = make_generator(42)\nresult = pipe(\n    \"a fantasy castle in a magical forest, digital art, detailed\",\n    image=[my_canny, my_depth],\n    num_inference_steps=20,\n    generator=generator,\n    controlnet_conditioning_scale=[0.7, 0.5],\n).images[0]\n\n# Display\nshow_images(\n    [my_image, my_canny, my_depth, result],\n    [\"Source\", \"Canny Edges\", \"Depth Map\", \"Generated\"],\n    suptitle=\"Stacked Canny + Depth: contours AND perspective controlled\",\n)\n\n# Clean up\ncleanup_pipeline(pipe)\ndel cn_canny, cn_depth\n```\n\n**Key decisions:**\n- Canny thresholds (100, 200) because the building has clear geometric edges at moderate contrast\n- Stacking because architecture benefits from both contour precision (Canny) and perspective accuracy (depth)\n- Canny at 0.7 (strong contours matter for architecture), depth at 0.5 (softer spatial layering)\n- The prompt describes the creative transformation while the spatial maps lock down the composition\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **Preprocessing quality is the most impactful practical decision.** ControlNet faithfully follows whatever spatial map you give it. Bad Canny thresholds produce noisy edges; noisy edges produce artifacted output. Garbage in, garbage out. Tune the preprocessing before touching anything else.\n\n2. **The pipeline API is identical for all spatial map types.** Only the preprocessor and the ControlNet checkpoint change. `StableDiffusionControlNetPipeline` does not know or care what kind of map it receives. The architecture is genuinely map-agnostic.\n\n3. **Conditioning scale is your volume knob for spatial control.** Low scale = spatial map is a suggestion. High scale = model rigidly follows the map. Sweet spot is typically 0.7â€“1.0. Same tradeoff as CFG guidance scaleâ€”precision vs creativity. Two knobs on the same mixing board.\n\n4. **Multiple ControlNets stack by summing their additive features.** Use complementary maps from the same source image, moderate scales (0.5â€“0.8 each), and add complexity gradually. Stacking is not \"2x control\"â€”it is two different types of control simultaneously.\n\n5. **The practical workflow is: choose image â†’ choose preprocessor(s) â†’ tune preprocessing â†’ tune conditioning scale â†’ iterate.** This is a creative process. The tools are the same every time; the decisions are what change."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}