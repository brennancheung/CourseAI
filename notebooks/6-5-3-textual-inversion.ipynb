{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Textual Inversion\n\n**Module 6.5, Lesson 3** | CourseAI\n\nYou know that CLIP's embedding table maps token IDs to 768-dimensional vectors. You know those vectors are learned parameters. You know CLIP's embedding space is continuous and meaningful. This notebook takes those three premises and makes textual inversion tangible---you will inspect the embedding table, verify gradient flow through frozen layers, train a real embedding, and compare it to LoRA.\n\n**What you will do:**\n- Inspect the CLIP embedding table, compute cosine similarities between tokens, and add a new pseudo-token to verify the table grows by one row\n- Run one textual inversion training step by hand, verifying that gradients flow through the frozen pipeline to the embedding table, and that only the pseudo-token's row is actually *updated* (via the restore-original-rows pattern)\n- Train a full textual inversion embedding on a concept dataset (~200--500 steps) and generate images with and without the learned pseudo-token\n- Compare textual inversion vs LoRA on the same concept images: file size, training time, output quality\n\n**For each exercise, PREDICT the output before running the cell.**\n\n**Estimated time:** 45--60 minutes.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies and import everything. This notebook requires a GPU (T4 or better) for training and inference."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "!pip install -q diffusers transformers accelerate peft datasets scikit-learn\n\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nimport gc\nimport time\nimport os\n\n# Reproducible results\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Nice plots\nplt.style.use('dark_background')\nplt.rcParams['figure.figsize'] = [10, 4]\n\n# Device setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndtype = torch.float16 if device.type == 'cuda' else torch.float32\nprint(f'Using device: {device}')\nif device.type == 'cuda':\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB')\n\nprint('\\nSetup complete.')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Helpers\n",
    "\n",
    "Display helpers, model ID, and VRAM management. Each exercise loads only the components it needs and cleans up afterward to stay within free-tier Colab VRAM (~16 GB on a T4).\n",
    "\n",
    "> **VRAM tip:** If you encounter an out-of-memory error, go to Runtime -> Restart runtime and rerun from Setup. Exercises 3 and 4 use the most VRAM because they run training loops."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_id = 'stable-diffusion-v1-5/stable-diffusion-v1-5'\n",
    "\n",
    "\n",
    "def show_images(images, titles, figsize=None):\n",
    "    \"\"\"Display a list of PIL images side by side.\"\"\"\n",
    "    n = len(images)\n",
    "    if figsize is None:\n",
    "        figsize = (5 * n, 5)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(np.array(img))\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_image_grid(images, titles, nrows, ncols, figsize=None, suptitle=None):\n",
    "    \"\"\"Display images in a grid with the given number of rows and columns.\"\"\"\n",
    "    if figsize is None:\n",
    "        figsize = (4 * ncols, 4 * nrows)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    axes_flat = axes.flat if nrows > 1 or ncols > 1 else [axes]\n",
    "    for ax, img, title in zip(axes_flat, images, titles):\n",
    "        ax.imshow(np.array(img))\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    for ax in list(axes_flat)[len(images):]:\n",
    "        ax.axis('off')\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle, fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Free GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print('Helpers defined.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Inspect the CLIP Embedding Space [Guided]\n",
    "\n",
    "The lesson explained that CLIP's vocabulary has 49,408 entries and that each entry maps a token ID to a 768-dimensional vector. You learned that these vectors are learned parameters (trainable rows in a matrix) and that CLIP's embedding space is continuous and meaningful---nearby vectors represent similar concepts.\n",
    "\n",
    "This exercise makes the embedding table tangible. You will load CLIP's text model, inspect the embedding matrix dimensions, look up specific token embeddings, compute cosine similarities between related and unrelated tokens, and visualize a cluster of embeddings with PCA. Then you will add a new pseudo-token and verify the table grows by exactly one row.\n",
    "\n",
    "**Before running, predict:**\n",
    "- What shape will the embedding table have? (Think: how many tokens in CLIP's vocabulary, and what is the embedding dimension?)\n",
    "- Will \"cat\" be closer to \"kitten\" or to \"airplane\" in cosine similarity? (Think: the \"geometry encodes meaning\" mental model from Embeddings & Position.)\n",
    "- After adding a new token `<my-concept>`, what will the new embedding table shape be? (Think: one new row, same column dimension.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "# Load CLIP's tokenizer and text encoder.\n",
    "# We load in float32 for this exercise because we need precise\n",
    "# cosine similarity computations (not training/inference).\n",
    "print('Loading CLIP tokenizer and text encoder...')\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder='tokenizer')\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    model_id, subfolder='text_encoder', torch_dtype=torch.float32\n",
    ").to(device)\n",
    "print('Loaded.\\n')\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: Inspect the embedding table dimensions.\n",
    "# ============================================================\n",
    "# The embedding table is the FIRST layer of the CLIP text encoder.\n",
    "# It maps token IDs to dense vectors---the same nn.Embedding you\n",
    "# built in Embeddings & Position.\n",
    "\n",
    "embedding_layer = text_encoder.get_input_embeddings()\n",
    "embedding_table = embedding_layer.weight.data\n",
    "\n",
    "print(f'Embedding table shape: {list(embedding_table.shape)}')\n",
    "print(f'  Vocabulary size:     {embedding_table.shape[0]} tokens')\n",
    "print(f'  Embedding dimension: {embedding_table.shape[1]}')\n",
    "print(f'  Total parameters:    {embedding_table.numel():,}')\n",
    "print(f'\\nThis is the lookup table from Embeddings & Position.')\n",
    "print(f'Token ID i maps to row i of this matrix.')\n",
    "print(f'Each row is a {embedding_table.shape[1]}-dimensional vector.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# STEP 2: Look up embeddings for specific tokens.\n",
    "# ============================================================\n",
    "# Recall from Embeddings & Position: embedding.weight[i] gives you\n",
    "# the same vector as embedding(tensor([i])). The embedding table\n",
    "# is just a matrix, and lookup is just row selection.\n",
    "\n",
    "words = ['cat', 'kitten', 'dog', 'puppy', 'airplane', 'mountain', 'sunset']\n",
    "word_ids = {}\n",
    "word_embeds = {}\n",
    "\n",
    "for word in words:\n",
    "    # Tokenize the word. CLIP's tokenizer may split into subword tokens,\n",
    "    # but common words like these are single tokens.\n",
    "    tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "    token_id = tokens[0]\n",
    "    word_ids[word] = token_id\n",
    "    word_embeds[word] = embedding_table[token_id]\n",
    "    print(f'  \"{word}\" -> token ID {token_id}, embedding shape: {list(word_embeds[word].shape)}')\n",
    "\n",
    "print(f'\\nEach word maps to a single row in the [{embedding_table.shape[0]}, {embedding_table.shape[1]}] table.')\n",
    "print(f'These are the INITIAL embeddings---before the CLIP transformer contextualizes them.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# STEP 3: Compute cosine similarities between token pairs.\n",
    "# ============================================================\n",
    "# The lesson reinforced the \"geometry encodes meaning\" mental model:\n",
    "# nearby vectors represent similar concepts. Let's verify this\n",
    "# with concrete numbers.\n",
    "\n",
    "pairs = [\n",
    "    ('cat', 'kitten'),     # very related (same animal, age variant)\n",
    "    ('cat', 'dog'),        # related (both common pets)\n",
    "    ('dog', 'puppy'),      # very related (same animal, age variant)\n",
    "    ('cat', 'airplane'),   # unrelated\n",
    "    ('mountain', 'sunset'),# somewhat related (both nature/landscape)\n",
    "    ('airplane', 'sunset'),# weakly related at best\n",
    "]\n",
    "\n",
    "print('Cosine similarities between token embeddings:')\n",
    "print('-' * 50)\n",
    "similarities = []\n",
    "for w1, w2 in pairs:\n",
    "    e1 = word_embeds[w1].unsqueeze(0)\n",
    "    e2 = word_embeds[w2].unsqueeze(0)\n",
    "    cos_sim = F.cosine_similarity(e1, e2).item()\n",
    "    similarities.append((w1, w2, cos_sim))\n",
    "    print(f'  cos(\"{w1}\", \"{w2}\") = {cos_sim:.4f}')\n",
    "\n",
    "print(f'\\n\"Geometry encodes meaning.\" Related words have higher similarity.')\n",
    "print(f'Cat-kitten and dog-puppy are the most similar pairs.')\n",
    "print(f'Cat-airplane is the least similar.')\n",
    "print(f'\\nThis is the space where textual inversion will place a new point.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# STEP 4: Visualize embeddings with PCA.\n",
    "# ============================================================\n",
    "# Project the 768-dim embeddings to 2D and see if semantic\n",
    "# clusters are visible---even with just a few tokens.\n",
    "# This connects to the EmbeddingSpaceExplorer from Embeddings & Position.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Gather a broader set of tokens for visualization.\n",
    "viz_words = [\n",
    "    'cat', 'kitten', 'tabby', 'fur',           # cat-related\n",
    "    'dog', 'puppy', 'retriever',                # dog-related\n",
    "    'airplane', 'helicopter', 'airport',        # aviation\n",
    "    'mountain', 'sunset', 'ocean', 'forest',    # nature\n",
    "    'painting', 'sketch', 'watercolor',         # art\n",
    "]\n",
    "\n",
    "viz_embeds = []\n",
    "viz_labels = []\n",
    "for word in viz_words:\n",
    "    tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "    embed = embedding_table[tokens[0]].cpu().numpy()\n",
    "    viz_embeds.append(embed)\n",
    "    viz_labels.append(word)\n",
    "\n",
    "viz_matrix = np.stack(viz_embeds)\n",
    "\n",
    "# PCA to 2D.\n",
    "pca = PCA(n_components=2)\n",
    "coords_2d = pca.fit_transform(viz_matrix)\n",
    "\n",
    "# Color by category.\n",
    "category_colors = {\n",
    "    'cat': '#22c55e', 'kitten': '#22c55e', 'tabby': '#22c55e', 'fur': '#22c55e',\n",
    "    'dog': '#3b82f6', 'puppy': '#3b82f6', 'retriever': '#3b82f6',\n",
    "    'airplane': '#ef4444', 'helicopter': '#ef4444', 'airport': '#ef4444',\n",
    "    'mountain': '#f59e0b', 'sunset': '#f59e0b', 'ocean': '#f59e0b', 'forest': '#f59e0b',\n",
    "    'painting': '#a855f7', 'sketch': '#a855f7', 'watercolor': '#a855f7',\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "for i, word in enumerate(viz_labels):\n",
    "    color = category_colors.get(word, '#ffffff')\n",
    "    ax.scatter(coords_2d[i, 0], coords_2d[i, 1], c=color, s=80, zorder=5)\n",
    "    ax.annotate(word, (coords_2d[i, 0], coords_2d[i, 1]),\n",
    "                textcoords='offset points', xytext=(8, 5),\n",
    "                fontsize=10, color=color)\n",
    "\n",
    "ax.set_title('CLIP Token Embeddings Projected to 2D (PCA)', fontsize=12)\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=10)\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=10)\n",
    "ax.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Even in a 2D projection of 768-dim space, semantic clusters are visible.')\n",
    "print('Cat-related tokens cluster together. Aviation tokens cluster together.')\n",
    "print('Textual inversion will find a NEW point in this space---not at any')\n",
    "print('existing token\\'s position, but in the continuous space between them.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# STEP 5: Add a new pseudo-token and verify the table grows.\n",
    "# ============================================================\n",
    "# This is the mechanical heart of textual inversion: extend the\n",
    "# vocabulary by one token, resize the embedding table by one row.\n",
    "\n",
    "original_vocab_size = len(tokenizer)\n",
    "original_table_shape = list(text_encoder.get_input_embeddings().weight.shape)\n",
    "\n",
    "print(f'BEFORE adding token:')\n",
    "print(f'  Vocabulary size:     {original_vocab_size}')\n",
    "print(f'  Embedding table:     {original_table_shape}')\n",
    "\n",
    "# Add the pseudo-token to the tokenizer.\n",
    "num_added = tokenizer.add_tokens(['<my-concept>'])\n",
    "new_token_id = tokenizer.convert_tokens_to_ids('<my-concept>')\n",
    "\n",
    "# Resize the text encoder's embedding table to accommodate the new token.\n",
    "# This adds one row of randomly initialized values.\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "new_vocab_size = len(tokenizer)\n",
    "new_table_shape = list(text_encoder.get_input_embeddings().weight.shape)\n",
    "\n",
    "print(f'\\nAFTER adding token:')\n",
    "print(f'  Vocabulary size:     {new_vocab_size}')\n",
    "print(f'  Embedding table:     {new_table_shape}')\n",
    "print(f'  New token \"<my-concept>\" -> ID {new_token_id}')\n",
    "print(f'  Tokens added:        {num_added}')\n",
    "\n",
    "# Inspect the new row.\n",
    "new_embed = text_encoder.get_input_embeddings().weight.data[new_token_id]\n",
    "print(f'\\nNew embedding row shape: {list(new_embed.shape)}')\n",
    "print(f'New embedding (first 5 values): {new_embed[:5].tolist()}')\n",
    "print(f'These are random values. The embedding has no meaning yet.')\n",
    "print(f'\\nTextual inversion will optimize this one row---768 floats---so that')\n",
    "print(f'prompts containing <my-concept> generate images of a specific concept.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# STEP 6: Initialize from a related word (optional strategy).\n",
    "# ============================================================\n",
    "# Instead of starting from a random point, start from \"cat\".\n",
    "# This places the optimization in the right neighborhood.\n",
    "\n",
    "cat_id = tokenizer.convert_tokens_to_ids('cat')\n",
    "cat_embed = text_encoder.get_input_embeddings().weight.data[cat_id].clone()\n",
    "\n",
    "# Overwrite the random embedding with the \"cat\" embedding.\n",
    "text_encoder.get_input_embeddings().weight.data[new_token_id] = cat_embed\n",
    "\n",
    "# Verify the initialization.\n",
    "initialized_embed = text_encoder.get_input_embeddings().weight.data[new_token_id]\n",
    "cos_sim = F.cosine_similarity(\n",
    "    initialized_embed.unsqueeze(0),\n",
    "    cat_embed.unsqueeze(0),\n",
    ").item()\n",
    "\n",
    "print(f'Initialized <my-concept> from \"cat\" embedding.')\n",
    "print(f'Cosine similarity between <my-concept> and \"cat\": {cos_sim:.4f}')\n",
    "print(f'(Should be 1.0---they are identical before training.)')\n",
    "print(f'\\nInitializing from a related word is the \"geometry encodes meaning\"')\n",
    "print(f'mental model applied to optimization: start in the right neighborhood,')\n",
    "print(f'and the optimizer has less distance to travel.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Clean up before Exercise 2.\n",
    "del text_encoder, tokenizer, embedding_table\n",
    "cleanup()\n",
    "print('CLIP text encoder freed from VRAM.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You inspected CLIP's embedding table as a concrete data structure and verified three things from the lesson:\n",
    "\n",
    "1. **The embedding table is a matrix.** Shape [49408, 768]---49,408 tokens, each mapped to a 768-dim vector. This is the same `nn.Embedding` you built in Embeddings & Position, just larger.\n",
    "\n",
    "2. **Geometry encodes meaning.** Cosine similarities confirmed that related tokens (cat/kitten, dog/puppy) are closer in embedding space than unrelated tokens (cat/airplane). The PCA visualization showed semantic clusters even in 2D projection.\n",
    "\n",
    "3. **Adding a new token is mechanically simple.** `tokenizer.add_tokens()` extends the vocabulary, `resize_token_embeddings()` adds one row to the embedding matrix. The new row starts with random values (or can be initialized from a related word). That one row---768 floats---is the entire optimization target of textual inversion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 2: One Textual Inversion Training Step by Hand [Guided]\n\nThe lesson showed the textual inversion training loop as a side-by-side comparison with the LoRA training loop. The steps are identical---VAE encode, sample timestep, noise, U-Net prediction, MSE loss, backprop. The only difference: what receives the gradient update. In LoRA, it was the adapter matrices. Here, it is a single embedding row.\n\nThis exercise runs one training step from scratch. You will create a pseudo-token embedding, construct a prompt containing it, run the full forward pass (tokenizer -> embedding lookup -> CLIP text encoder -> U-Net noise prediction -> MSE loss), then backprop and inspect the gradients.\n\n**Important subtlety:** The embedding table is a single tensor. When we enable `requires_grad` on it, *all* rows that participate in the forward pass receive gradients through CLIP's self-attention (every token interacts with every other token). The trick in textual inversion is not that only one row *gets* gradients---it is that only one row *gets updated*. We achieve this by saving the original embeddings and restoring them after each optimizer step, so only the pseudo-token's row actually changes.\n\n**Before running, predict:**\n- Will the embedding rows for tokens like \"a\", \"photo\", \"of\" also receive non-zero gradients? (Think: CLIP's transformer applies self-attention across all 77 tokens.)\n- Will the CLIP text encoder's transformer weights have gradients? (Think: they are frozen with `requires_grad=False`, but gradients flow THROUGH them to reach the embedding.)\n- What pattern ensures only the pseudo-token's row is actually updated? (Think: save the original rows, restore them after each optimizer step.)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "# Load all pipeline components.\n",
    "# We use float32 for this exercise because we need to inspect gradients\n",
    "# precisely. In Exercises 3 and 4, we will switch to float16 for speed.\n",
    "print('Loading pipeline components in float32...')\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder='tokenizer')\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    model_id, subfolder='text_encoder', torch_dtype=torch.float32\n",
    ").to(device)\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    model_id, subfolder='vae', torch_dtype=torch.float32\n",
    ").to(device)\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    model_id, subfolder='unet', torch_dtype=torch.float32\n",
    ").to(device)\n",
    "\n",
    "print('Components loaded.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# STEP 1: Create the pseudo-token and set up freeze/unfreeze.\n",
    "# ============================================================\n",
    "# This is the 5-step setup from the lesson pseudocode:\n",
    "#   1. Add token to tokenizer\n",
    "#   2. Resize embedding table\n",
    "#   3. Initialize from a related word\n",
    "#   4. Freeze EVERYTHING\n",
    "#   5. Unfreeze ONLY the new embedding row\n",
    "\n",
    "# 1. Add the pseudo-token.\n",
    "placeholder_token = '<my-concept>'\n",
    "tokenizer.add_tokens([placeholder_token])\n",
    "placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n",
    "\n",
    "# 2. Resize the embedding table.\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 3. Initialize from \"cat\" (a related word).\n",
    "token_embeds = text_encoder.get_input_embeddings().weight\n",
    "cat_id = tokenizer.convert_tokens_to_ids('cat')\n",
    "with torch.no_grad():\n",
    "    token_embeds[placeholder_token_id] = token_embeds[cat_id].clone()\n",
    "\n",
    "# 4. Freeze EVERYTHING.\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "# 5. Unfreeze ONLY the new embedding row.\n",
    "# The embedding weight is the whole table; we need to enable grad for it\n",
    "# and then mask the update in the optimizer. For verification purposes,\n",
    "# we enable requires_grad on the entire embedding weight (so gradients\n",
    "# are computed for the new row), but we will check that gradients are\n",
    "# effectively zero for all other rows.\n",
    "token_embeds.requires_grad_(True)\n",
    "\n",
    "print(f'Pseudo-token: \"{placeholder_token}\" -> ID {placeholder_token_id}')\n",
    "print(f'Embedding table shape: {list(token_embeds.shape)}')\n",
    "print(f'\\nFrozen:')\n",
    "print(f'  VAE:          {sum(1 for p in vae.parameters() if not p.requires_grad)} params frozen')\n",
    "print(f'  U-Net:        {sum(1 for p in unet.parameters() if not p.requires_grad)} params frozen')\n",
    "print(f'  Text encoder: all weights frozen except embedding table')\n",
    "print(f'\\nTrainable: embedding table ({token_embeds.numel():,} floats total,')\n",
    "print(f'  but only row {placeholder_token_id} will be optimized = 768 floats)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# STEP 2: Create a synthetic training image.\n",
    "# ============================================================\n",
    "# For this single-step exercise we do not need a real dataset.\n",
    "# A random image is sufficient---we just want to verify gradient flow.\n",
    "\n",
    "# Create a random \"training image\" tensor in [-1, 1].\n",
    "torch.manual_seed(42)\n",
    "fake_image = torch.randn(1, 3, 512, 512, device=device, dtype=torch.float32)\n",
    "\n",
    "# VAE encode the image to latent space (frozen).\n",
    "with torch.no_grad():\n",
    "    latent_dist = vae.encode(fake_image)\n",
    "    z_0 = latent_dist.latent_dist.mode() * vae.config.scaling_factor\n",
    "\n",
    "print(f'Fake training image shape:  {list(fake_image.shape)}')\n",
    "print(f'VAE-encoded latent z_0:     {list(z_0.shape)}')\n",
    "print(f'\\nThe VAE encoder converts the image to latent space. Frozen---no gradients.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# STEP 3: Construct and tokenize the prompt.\n",
    "# ============================================================\n",
    "# The prompt contains the pseudo-token. The tokenizer converts it\n",
    "# to an integer ID, which the embedding table maps to a vector.\n",
    "\n",
    "prompt = f'a photo of {placeholder_token}'\n",
    "\n",
    "text_input = tokenizer(\n",
    "    prompt,\n",
    "    padding='max_length',\n",
    "    max_length=tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "input_ids = text_input.input_ids.to(device)\n",
    "\n",
    "print(f'Prompt: \"{prompt}\"')\n",
    "print(f'Token IDs: {input_ids[0, :10].tolist()}... (padded to {input_ids.shape[1]})')\n",
    "print(f'\\nToken ID {placeholder_token_id} is the pseudo-token <my-concept>.')\n",
    "print(f'The tokenizer treats it like any other token---it is just a new entry')\n",
    "print(f'in the vocabulary.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# STEP 4: Forward pass through CLIP text encoder.\n",
    "# ============================================================\n",
    "# This is the two-stage pipeline from the lesson:\n",
    "#   Stage 1: Token IDs -> embedding lookup (our trainable row is here)\n",
    "#   Stage 2: Embeddings -> CLIP transformer -> contextual embeddings\n",
    "#\n",
    "# We need to enable gradients for this pass even though the CLIP\n",
    "# transformer is frozen, because gradients must flow THROUGH the\n",
    "# frozen layers to reach the embedding table.\n",
    "\n",
    "# Forward pass with gradients enabled (no torch.no_grad!).\n",
    "encoder_output = text_encoder(input_ids)\n",
    "text_embeddings = encoder_output[0]  # [1, 77, 768]\n",
    "\n",
    "print(f'Text embeddings shape: {list(text_embeddings.shape)}')\n",
    "print(f'  Batch size:          {text_embeddings.shape[0]}')\n",
    "print(f'  Sequence length:     {text_embeddings.shape[1]} (padded to 77)')\n",
    "print(f'  Embedding dimension: {text_embeddings.shape[2]}')\n",
    "print(f'\\nThese are CONTEXTUAL embeddings---the CLIP transformer has applied')\n",
    "print(f'self-attention across all tokens. The pseudo-token\\'s embedding has')\n",
    "print(f'been contextualized with \"a\", \"photo\", \"of\", and the padding tokens.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# STEP 5: The DDPM training step (sample t, noise, predict, loss).\n",
    "# ============================================================\n",
    "# This is the same training loop from Learning to Denoise and LoRA\n",
    "# Fine-Tuning. The only difference: what receives the gradient.\n",
    "\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(model_id, subfolder='scheduler')\n",
    "\n",
    "# Sample a random timestep.\n",
    "torch.manual_seed(42)\n",
    "timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (1,), device=device)\n",
    "\n",
    "# Sample random noise.\n",
    "noise = torch.randn_like(z_0)\n",
    "\n",
    "# Add noise to the latent using the forward process formula:\n",
    "# z_t = sqrt(alpha_bar_t) * z_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
    "noisy_latents = noise_scheduler.add_noise(z_0, noise, timesteps)\n",
    "\n",
    "# U-Net predicts the noise (frozen weights, but gradients flow through).\n",
    "noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "# MSE loss between predicted noise and actual noise.\n",
    "loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "print(f'Timestep t:         {timesteps.item()}')\n",
    "print(f'Noisy latent shape: {list(noisy_latents.shape)}')\n",
    "print(f'Noise pred shape:   {list(noise_pred.shape)}')\n",
    "print(f'MSE loss:           {loss.item():.6f}')\n",
    "print(f'\\nThis is the same DDPM training step from Learning to Denoise.')\n",
    "print(f'Same formula, same loss, same pipeline. The only difference is next.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================\n# STEP 6: Backprop and inspect gradient flow.\n# ============================================================\n# Gradients flow backward through:\n#   loss -> U-Net (frozen) -> cross-attention (frozen) ->\n#   CLIP transformer (frozen) -> embedding lookup\n#\n# IMPORTANT: Because the embedding table is ONE tensor with\n# requires_grad=True, ALL rows that participated in the forward\n# pass receive gradients (via CLIP's self-attention, every token\n# interacts with every other). The textual inversion trick is\n# not that only one row GETS gradients---it is that only one\n# row GETS UPDATED (via the restore-original-rows pattern).\n\nloss.backward()\n\n# Check the embedding table's gradient.\nembed_grad = token_embeds.grad\n\nprint(f'Embedding table gradient shape: {list(embed_grad.shape)}')\nprint(f'  (Same shape as the embedding table itself)')\n\n# Check which rows have non-zero gradients.\nrow_grad_norms = embed_grad.norm(dim=1)  # L2 norm per row\nnonzero_rows = (row_grad_norms > 0).nonzero(as_tuple=True)[0]\n\nprint(f'\\nRows with non-zero gradients: {len(nonzero_rows)} rows')\n\n# Show gradients for all tokens in the prompt.\nprompt_token_ids = input_ids[0].tolist()\nunique_prompt_ids = sorted(set(prompt_token_ids))\n\nprint(f'\\nGradient norms for tokens used in the prompt:')\nfor tid in unique_prompt_ids:\n    token_str = tokenizer.decode([tid])\n    norm = row_grad_norms[tid].item()\n    marker = '*** PSEUDO-TOKEN ***' if tid == placeholder_token_id else ''\n    print(f'  Token {tid:>5d} \"{token_str:>12s}\": gradient norm = {norm:.6f}  {marker}')\n\nprint(f'\\nNotice: MULTIPLE rows have non-zero gradients, not just the pseudo-token.')\nprint(f'This is because CLIP\\'s self-attention means every token interacts with')\nprint(f'every other token. Gradients flow back to ALL participating embeddings.')\nprint(f'\\nSo how does textual inversion update ONLY the pseudo-token? See next cell.')\n\n# ============================================================\n# STEP 6b: Demonstrate the restore-original-rows pattern.\n# ============================================================\n# This is the actual mechanism used in textual inversion training:\n# after each optimizer step, restore all rows except the pseudo-token's.\n\nprint(f'\\n{\"=\" * 55}')\nprint(f'The restore-original-rows pattern:')\nprint(f'{\"=\" * 55}')\n\n# Save the current state of the pseudo-token row before update.\noriginal_pseudo_embed = token_embeds.data[placeholder_token_id].clone()\n\n# Save original embeddings for all OTHER rows.\noriginal_other_embeds = token_embeds.data[:placeholder_token_id].clone()\n\n# Simulate an optimizer step (this would update ALL rows with gradients).\nsimple_optimizer = torch.optim.AdamW([token_embeds], lr=5e-4)\nsimple_optimizer.step()\n\n# Check: the pseudo-token row has changed.\npseudo_changed = not torch.equal(token_embeds.data[placeholder_token_id], original_pseudo_embed)\nprint(f'\\nAfter optimizer.step():')\nprint(f'  Pseudo-token row changed: {pseudo_changed}')\n\n# Now restore the original rows (this is the key trick).\nwith torch.no_grad():\n    token_embeds.data[:placeholder_token_id] = original_other_embeds\n\n# Verify: a prompt-participating token (e.g., \"a\") is back to its original value.\na_token_id = tokenizer.convert_tokens_to_ids('a</w>')\na_restored = torch.equal(token_embeds.data[:placeholder_token_id], original_other_embeds)\nprint(f'  Other rows restored to original: {a_restored}')\nprint(f'\\nThis is how textual inversion ensures only one row is updated:')\nprint(f'  1. Enable requires_grad on the full embedding table')\nprint(f'  2. Run the optimizer step (updates all rows with gradients)')\nprint(f'  3. Restore all rows except the pseudo-token to their originals')\nprint(f'\\nThe result: only the pseudo-token\\'s embedding actually changes.')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# STEP 7: Verify frozen model weights have zero weight updates.\n",
    "# ============================================================\n",
    "# The CLIP transformer and U-Net are frozen---their weights should\n",
    "# have NO gradients (requires_grad=False).\n",
    "\n",
    "print('Checking frozen model weights for gradients...')\n",
    "print()\n",
    "\n",
    "# Check U-Net.\n",
    "unet_grads = sum(1 for p in unet.parameters() if p.grad is not None and p.grad.abs().sum() > 0)\n",
    "print(f'U-Net parameters with non-zero gradients: {unet_grads}')\n",
    "\n",
    "# Check VAE.\n",
    "vae_grads = sum(1 for p in vae.parameters() if p.grad is not None and p.grad.abs().sum() > 0)\n",
    "print(f'VAE parameters with non-zero gradients:   {vae_grads}')\n",
    "\n",
    "# Check CLIP text encoder (excluding the embedding table, which we unfroze).\n",
    "clip_grads = 0\n",
    "for name, p in text_encoder.named_parameters():\n",
    "    if 'token_embedding' in name:\n",
    "        continue  # Skip the embedding table---we unfroze it\n",
    "    if p.grad is not None and p.grad.abs().sum() > 0:\n",
    "        clip_grads += 1\n",
    "print(f'CLIP encoder weights with non-zero gradients (excluding embedding): {clip_grads}')\n",
    "\n",
    "print(f'\\nAll zeros. The gradients flowed THROUGH the frozen CLIP encoder and')\n",
    "print(f'U-Net to reach the embedding table, but no frozen weights were updated.')\n",
    "print(f'This is exactly the mechanism from the lesson: the frozen layers are a')\n",
    "print(f'pathway, not a destination. Gradients pass through to reach the one')\n",
    "print(f'trainable parameter at the very start of the pipeline.')\n",
    "\n",
    "print(f'\\n=== Summary ===')\n",
    "print(f'Trainable parameters:  {token_embeds.shape[1]} (one embedding row)')\n",
    "print(f'Frozen parameters:     ~{sum(p.numel() for p in unet.parameters()) / 1e6:.0f}M (U-Net) + ')\n",
    "print(f'                       ~{sum(p.numel() for p in text_encoder.parameters()) / 1e6:.0f}M (CLIP) + ')\n",
    "print(f'                       ~{sum(p.numel() for p in vae.parameters()) / 1e6:.0f}M (VAE)')\n",
    "print(f'\\nOne embedding row of 768 floats. That is the entire \"model\" you train.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Clean up before Exercise 3.\n",
    "del text_encoder, tokenizer, vae, unet, z_0, noise_pred, loss, embed_grad\n",
    "cleanup()\n",
    "print('All components freed from VRAM.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### What Just Happened\n\nYou ran one complete textual inversion training step by hand and verified the core mechanism:\n\n1. **The training step is identical to DDPM/LoRA training.** VAE encode, sample timestep, noise, U-Net prediction, MSE loss. The same loop you have seen three times before.\n\n2. **Multiple embedding rows receive gradients, not just the pseudo-token.** Because CLIP's transformer applies self-attention across all 77 tokens, gradients flow back to every embedding row that participated in the forward pass. This is a consequence of the embedding table being a single tensor with `requires_grad=True`.\n\n3. **Only the pseudo-token's row is actually *updated*.** The restore-original-rows pattern ensures this: after each optimizer step, all rows except the pseudo-token's are restored to their original values. The mechanism is simple but essential---it is how 768 parameters are isolated within a 49,409 x 768 table.\n\n4. **Frozen model weights have zero gradient updates.** The U-Net, VAE, and CLIP transformer weights are frozen (`requires_grad=False`), so they accumulate no gradients at all. Gradients flow *through* these frozen layers to reach the embedding table, but the frozen layers themselves are not updated. The CLIP encoder is a frozen pathway, not a wall.\n\n5. **768 trainable parameters (effectively).** One embedding row, protected by the restore pattern. Compare to LoRA's ~200K--400K parameters, or full U-Net fine-tuning's ~860M. Three orders of magnitude smaller than LoRA.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 3: Train Textual Inversion on a Concept Dataset [Supported]\n\nYou verified the mechanism in Exercise 2 with a single step and a fake image. Now you will train a real textual inversion embedding on a small concept dataset. We use `lambdalabs/naruto-blip-captions` (the same dataset used in the LoRA lesson) as our training data---it provides a consistent visual style that we can attempt to capture.\n\n**Deliberate choice:** The lesson explained that textual inversion works best for specific visual objects (like \"my cat\") and struggles with complex styles. We are *intentionally* training on a style dataset here to give you firsthand experience of that limitation. Notice how the results compare to what LoRA achieved on the same dataset---this sets up Exercise 4's direct comparison.\n\nWe train for 200--500 steps (shortened from the paper's 3,000--5,000 for Colab time constraints). You will generate images at training checkpoints to observe convergence, then compare generation with and without the learned pseudo-token.\n\n**Your tasks:**\n- Complete the training loop (fill in the TODO sections)\n- Generate images at checkpoints to observe convergence\n- Generate with and without the pseudo-token after training"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, DPMSolverMultistepScheduler, StableDiffusionPipeline\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load all components in float32 (needed for stable training gradients).\n",
    "print('Loading pipeline components...')\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder='tokenizer')\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    model_id, subfolder='text_encoder', torch_dtype=torch.float32\n",
    ").to(device)\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    model_id, subfolder='vae', torch_dtype=torch.float32\n",
    ").to(device)\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    model_id, subfolder='unet', torch_dtype=torch.float32\n",
    ").to(device)\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(model_id, subfolder='scheduler')\n",
    "\n",
    "print('Loading dataset...')\n",
    "dataset = load_dataset('lambdalabs/naruto-blip-captions', split='train')\n",
    "print(f'Dataset size: {len(dataset)} images')\n",
    "print('Components loaded.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Set up the pseudo-token (same 5 steps as Exercise 2).\n",
    "# ============================================================\n",
    "\n",
    "placeholder_token = '<naruto-style>'\n",
    "initializer_token = 'anime'  # Initialize from a semantically relevant word\n",
    "\n",
    "# 1. Add the pseudo-token to the tokenizer.\n",
    "tokenizer.add_tokens([placeholder_token])\n",
    "placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n",
    "\n",
    "# 2. Resize the embedding table.\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 3. Initialize from the semantically relevant word.\n",
    "token_embeds = text_encoder.get_input_embeddings().weight\n",
    "init_id = tokenizer.convert_tokens_to_ids(initializer_token)\n",
    "with torch.no_grad():\n",
    "    token_embeds[placeholder_token_id] = token_embeds[init_id].clone()\n",
    "\n",
    "# 4. Freeze EVERYTHING.\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "# 5. Unfreeze the embedding table so gradients can reach the new row.\n",
    "text_encoder.get_input_embeddings().weight.requires_grad_(True)\n",
    "\n",
    "print(f'Pseudo-token: \"{placeholder_token}\" -> ID {placeholder_token_id}')\n",
    "print(f'Initialized from: \"{initializer_token}\" (ID {init_id})')\n",
    "print(f'Embedding dimension: {token_embeds.shape[1]}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Prepare the dataset and training utilities.\n",
    "# ============================================================\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.CenterCrop(512),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),  # Scale to [-1, 1]\n",
    "])\n",
    "\n",
    "# The prompt template replaces the original caption with our pseudo-token.\n",
    "# This is the standard textual inversion approach: associate the pseudo-token\n",
    "# with the visual content of the training images.\n",
    "prompt_template = f'a painting in the style of {placeholder_token}'\n",
    "\n",
    "\n",
    "def get_training_batch(dataset, batch_idx, batch_size=1):\n",
    "    \"\"\"Get a batch of (image_tensor, tokenized_prompt) from the dataset.\"\"\"\n",
    "    idx = batch_idx % len(dataset)\n",
    "    example = dataset[idx]\n",
    "    image = example['image'].convert('RGB')\n",
    "    image_tensor = image_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    text_input = tokenizer(\n",
    "        prompt_template,\n",
    "        padding='max_length',\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids = text_input.input_ids.to(device)\n",
    "\n",
    "    return image_tensor, input_ids\n",
    "\n",
    "\n",
    "print(f'Prompt template: \"{prompt_template}\"')\n",
    "print(f'Image resolution: 512x512')\n",
    "print('Training utilities ready.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Training loop.\n",
    "# ============================================================\n",
    "# This is the same DDPM training loop from Learning to Denoise\n",
    "# and LoRA Fine-Tuning. The only change: the optimizer targets\n",
    "# a single embedding row instead of LoRA adapter matrices.\n",
    "\n",
    "num_training_steps = 300\n",
    "learning_rate = 5e-3  # Higher LR than LoRA because we optimize fewer params\n",
    "checkpoint_steps = [50, 100, 200, 300]  # Steps at which to save embeddings\n",
    "\n",
    "# The optimizer targets only the embedding table.\n",
    "# In a production implementation, you would create a custom optimizer\n",
    "# that masks updates to all rows except the new one. Here, we use a\n",
    "# simpler approach: after each step, we restore the original embeddings\n",
    "# for all rows except the pseudo-token's row.\n",
    "\n",
    "# Save the original embeddings so we can restore them.\n",
    "original_embeds = token_embeds.data[:placeholder_token_id].clone()\n",
    "\n",
    "# TODO 1: Create the optimizer.\n",
    "# The optimizer should target text_encoder.get_input_embeddings().weight\n",
    "# with the learning rate defined above. Use torch.optim.AdamW.\n",
    "#\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     [text_encoder.get_input_embeddings().weight],\n",
    "#     lr=learning_rate,\n",
    "# )\n",
    "optimizer = None  # Replace this line\n",
    "\n",
    "# Storage for loss history and checkpoint embeddings.\n",
    "loss_history = []\n",
    "checkpoint_embeds = {}\n",
    "\n",
    "print(f'Training for {num_training_steps} steps...')\n",
    "print(f'Learning rate: {learning_rate}')\n",
    "print(f'Checkpoints at steps: {checkpoint_steps}')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for step in range(num_training_steps):\n",
    "    # Get a training batch.\n",
    "    image_tensor, input_ids = get_training_batch(dataset, step)\n",
    "\n",
    "    # TODO 2: Implement one DDPM training step.\n",
    "    # This is the same 5-step pattern from Exercise 2 and the LoRA lesson:\n",
    "    #   a) VAE encode the image (with torch.no_grad)\n",
    "    #   b) Sample a random timestep\n",
    "    #   c) Sample noise and add it to the latent (noise_scheduler.add_noise)\n",
    "    #   d) Forward pass through text_encoder and unet\n",
    "    #   e) Compute MSE loss between predicted noise and actual noise\n",
    "    #\n",
    "    # Hints:\n",
    "    #   - VAE encode: vae.encode(image_tensor).latent_dist.mode() * vae.config.scaling_factor\n",
    "    #   - Random timestep: torch.randint(0, noise_scheduler.config.num_train_timesteps, (1,), device=device)\n",
    "    #   - Add noise: noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "    #   - Text encoder: text_encoder(input_ids)[0]  (no torch.no_grad---gradients must flow!)\n",
    "    #   - U-Net: unet(noisy_latents, timesteps, encoder_hidden_states=text_emb).sample\n",
    "    #   - Loss: F.mse_loss(noise_pred, noise)\n",
    "\n",
    "    loss = None  # Replace with actual loss computation\n",
    "\n",
    "    if loss is None:\n",
    "        print('Fill in TODO 2 to run the training loop.')\n",
    "        break\n",
    "\n",
    "    # Backprop and optimizer step.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Restore original embeddings for all tokens EXCEPT the pseudo-token.\n",
    "    # This ensures that only the new token's embedding is modified.\n",
    "    with torch.no_grad():\n",
    "        token_embeds.data[:placeholder_token_id] = original_embeds\n",
    "\n",
    "    loss_val = loss.item()\n",
    "    loss_history.append(loss_val)\n",
    "\n",
    "    # Save checkpoint embedding.\n",
    "    if (step + 1) in checkpoint_steps:\n",
    "        checkpoint_embeds[step + 1] = token_embeds[placeholder_token_id].clone().detach()\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'  Step {step + 1}/{num_training_steps}: loss = {loss_val:.4f} ({elapsed:.1f}s elapsed)')\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f'\\nTraining complete in {elapsed:.1f}s.')\n",
    "print(f'Final loss: {loss_history[-1]:.4f}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Why this approach works:** The training loop is identical to the DDPM loop from Learning to Denoise and LoRA Fine-Tuning. The only difference is the optimization target. We compute gradients through the entire frozen pipeline (CLIP encoder + U-Net) to update a single embedding row.\n",
    "\n",
    "**TODO 1---Create the optimizer:**\n",
    "```python\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [text_encoder.get_input_embeddings().weight],\n",
    "    lr=learning_rate,\n",
    ")\n",
    "```\n",
    "The optimizer targets the full embedding weight tensor, but we manually restore all rows except the pseudo-token's row after each step. This is a simple approach for a short training run.\n",
    "\n",
    "**TODO 2---One DDPM training step:**\n",
    "```python\n",
    "# a) VAE encode\n",
    "with torch.no_grad():\n",
    "    latents = vae.encode(image_tensor).latent_dist.mode() * vae.config.scaling_factor\n",
    "\n",
    "# b) Sample random timestep\n",
    "timesteps = torch.randint(\n",
    "    0, noise_scheduler.config.num_train_timesteps, (1,), device=device\n",
    ")\n",
    "\n",
    "# c) Sample noise and add to latent\n",
    "noise = torch.randn_like(latents)\n",
    "noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "# d) Forward pass (text encoder needs gradients, so no torch.no_grad)\n",
    "text_emb = text_encoder(input_ids)[0]\n",
    "noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=text_emb).sample\n",
    "\n",
    "# e) MSE loss\n",
    "loss = F.mse_loss(noise_pred, noise)\n",
    "```\n",
    "\n",
    "**Common mistakes:**\n",
    "- Wrapping the text encoder forward pass in `torch.no_grad()`. Gradients must flow through the CLIP encoder to reach the embedding table.\n",
    "- Forgetting the VAE scaling factor (`* vae.config.scaling_factor`). The latents are scaled to have unit variance.\n",
    "- Using `latent_dist.sample()` instead of `latent_dist.mode()`. Either works for training, but `mode()` is deterministic.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Visualize training loss.\n",
    "# ============================================================\n",
    "\n",
    "if len(loss_history) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.plot(loss_history, color='cyan', alpha=0.3, linewidth=0.5, label='Per-step loss')\n",
    "\n",
    "    # Smoothed loss (rolling average over 20 steps).\n",
    "    window = 20\n",
    "    if len(loss_history) >= window:\n",
    "        smoothed = np.convolve(loss_history, np.ones(window)/window, mode='valid')\n",
    "        ax.plot(range(window - 1, len(loss_history)), smoothed,\n",
    "                color='cyan', linewidth=2, label=f'Smoothed ({window}-step avg)')\n",
    "\n",
    "    for step in checkpoint_steps:\n",
    "        if step <= len(loss_history):\n",
    "            ax.axvline(x=step, color='#a855f7', linestyle='--', alpha=0.5)\n",
    "            ax.annotate(f'Step {step}', (step, ax.get_ylim()[1]),\n",
    "                       textcoords='offset points', xytext=(5, -15),\n",
    "                       fontsize=8, color='#a855f7')\n",
    "\n",
    "    ax.set_xlabel('Training Step', fontsize=11)\n",
    "    ax.set_ylabel('MSE Loss', fontsize=11)\n",
    "    ax.set_title('Textual Inversion Training Loss', fontsize=12)\n",
    "    ax.legend(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No training data to plot. Complete TODO 2 first.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Measure how far the embedding has moved from initialization.\n",
    "# ============================================================\n",
    "\n",
    "if len(checkpoint_embeds) > 0:\n",
    "    init_embed = token_embeds[init_id].detach()\n",
    "\n",
    "    print('Cosine similarity between checkpoints and initialization word:')\n",
    "    print('-' * 55)\n",
    "    for step, embed in sorted(checkpoint_embeds.items()):\n",
    "        cos_sim = F.cosine_similarity(\n",
    "            embed.unsqueeze(0), init_embed.unsqueeze(0)\n",
    "        ).item()\n",
    "        l2_dist = (embed - init_embed).norm().item()\n",
    "        print(f'  Step {step:>4d}: cos_sim = {cos_sim:.4f}, L2 distance = {l2_dist:.4f}')\n",
    "\n",
    "    print(f'\\nThe embedding starts identical to \"{initializer_token}\" and drifts away')\n",
    "    print(f'during training. It finds a NEW point in CLIP\\'s continuous space---')\n",
    "    print(f'not at any existing token\\'s position, but nearby.')\n",
    "else:\n",
    "    print('No checkpoints saved. Complete the training loop first.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Generate images WITH and WITHOUT the pseudo-token.\n",
    "# ============================================================\n",
    "# This tests the \"zero weight change\" claim from the lesson:\n",
    "# prompts without the pseudo-token should produce the same output\n",
    "# as before training.\n",
    "\n",
    "# Build a pipeline from the trained components.\n",
    "# We switch to float16 for faster inference.\n",
    "vae_f16 = vae.to(dtype=torch.float16)\n",
    "unet_f16 = unet.to(dtype=torch.float16)\n",
    "text_encoder_f16 = text_encoder.to(dtype=torch.float16)\n",
    "\n",
    "pipeline = StableDiffusionPipeline(\n",
    "    vae=vae_f16,\n",
    "    text_encoder=text_encoder_f16,\n",
    "    tokenizer=tokenizer,\n",
    "    unet=unet_f16,\n",
    "    scheduler=DPMSolverMultistepScheduler.from_pretrained(model_id, subfolder='scheduler'),\n",
    "    safety_checker=None,\n",
    "    feature_extractor=None,\n",
    "    requires_safety_checker=False,\n",
    ")\n",
    "\n",
    "# Generate with the pseudo-token.\n",
    "prompt_with = f'a painting of a warrior in the style of {placeholder_token}'\n",
    "prompt_without = 'a painting of a warrior in anime style'\n",
    "prompt_generic = 'a beautiful sunset over the ocean'  # No pseudo-token at all\n",
    "\n",
    "seed = 42\n",
    "num_steps = 25\n",
    "\n",
    "results = []\n",
    "prompts_list = [prompt_with, prompt_without, prompt_generic]\n",
    "titles = [\n",
    "    f'With {placeholder_token}',\n",
    "    'Without pseudo-token\\n(\"anime style\")',\n",
    "    'Unrelated prompt\\n(\"sunset\")',\n",
    "]\n",
    "\n",
    "for p in prompts_list:\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    result = pipeline(\n",
    "        prompt=p,\n",
    "        num_inference_steps=num_steps,\n",
    "        guidance_scale=7.5,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    results.append(result)\n",
    "\n",
    "show_images(results, titles, figsize=(18, 6))\n",
    "\n",
    "print(f'\\nPrompts used:')\n",
    "for p, t in zip(prompts_list, titles):\n",
    "    print(f'  {t.split(chr(10))[0]:>30s}: \"{p}\"')\n",
    "print(f'\\nThe pseudo-token prompt should reflect the training concept (naruto style).')\n",
    "print(f'The unrelated prompt (\"sunset\") verifies the model is UNCHANGED---it')\n",
    "print(f'never routes through the new embedding, so the output is identical')\n",
    "print(f'to what an unmodified model would produce.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the learned embedding for later comparison in Exercise 4.\n",
    "learned_embed_ti = token_embeds[placeholder_token_id].clone().detach().cpu()\n",
    "ti_file_size = learned_embed_ti.numel() * 4  # 768 float32 values = 3072 bytes\n",
    "print(f'Textual inversion embedding saved.')\n",
    "print(f'  Shape: {list(learned_embed_ti.shape)}')\n",
    "print(f'  Parameters: {learned_embed_ti.numel()}')\n",
    "print(f'  File size: {ti_file_size:,} bytes ({ti_file_size / 1024:.1f} KB)')\n",
    "\n",
    "# Record training time for comparison.\n",
    "ti_training_time = elapsed\n",
    "ti_training_steps = num_training_steps\n",
    "\n",
    "# Clean up.\n",
    "del pipeline, vae, unet, text_encoder, vae_f16, unet_f16, text_encoder_f16\n",
    "cleanup()\n",
    "print('Pipeline freed from VRAM.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### What Just Happened\n\nYou trained a real textual inversion embedding and observed several key phenomena:\n\n1. **The training loop is the same DDPM loop.** VAE encode, sample timestep, noise, predict noise, MSE loss, backprop. Identical to Learning to Denoise and LoRA Fine-Tuning. The only difference is the optimization target.\n\n2. **The embedding drifts from its initialization.** Starting from \"anime,\" the embedding gradually moves to a new point that encodes the visual characteristics of the training images. It is no longer at \"anime\"'s position---it is a genuinely new point in CLIP's continuous space.\n\n3. **Style training previews the expressiveness limitation.** A single 768-dim vector can capture some visual flavor (color palette, general aesthetic) but struggles with the full complexity of an artistic style (spatial composition, lighting patterns, rendering techniques). If your results look vague or inconsistent, that is *expected*---this is the \"one word ceiling\" from the lesson. Exercise 4 will show how LoRA handles the same dataset.\n\n4. **Zero weight change verification.** Prompts without the pseudo-token produce the same output as an unmodified model. The model is unchanged. Only the embedding vocabulary has grown by one entry.\n\n5. **Tiny output file.** The learned embedding is 768 float32 values = ~3 KB. Compare this to a LoRA adapter file (2--50 MB). Three orders of magnitude smaller.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Compare Textual Inversion vs LoRA [Independent]\n",
    "\n",
    "The lesson claimed that textual inversion is the most lightweight customization technique but less expressive than LoRA for complex styles. You experienced textual inversion firsthand in Exercise 3. Now, train a LoRA adapter on the same dataset and compare.\n",
    "\n",
    "**Your task:**\n",
    "1. Train a LoRA adapter on the same `lambdalabs/naruto-blip-captions` dataset (200--500 steps, same as the LoRA lesson's notebook)\n",
    "2. Generate images from both textual inversion and LoRA using the same prompts and seeds\n",
    "3. Compare: file size, training time, parameter count, and output quality\n",
    "4. Write a brief comparison: which captures the concept better? Why?\n",
    "\n",
    "**Key APIs:**\n",
    "- LoRA setup: `from peft import LoraConfig, get_peft_model`\n",
    "- LoRA targets: `['to_k', 'to_v', 'to_q', 'to_out.0']` (cross-attention projections)\n",
    "- LoRA training loop: same DDPM loop, but optimizer targets LoRA params instead of embedding\n",
    "- Generate with LoRA: load adapters into a StableDiffusionPipeline\n",
    "\n",
    "**VRAM constraint:** Train LoRA in float16 with gradient accumulation if needed. Clean up between training and generation.\n",
    "\n",
    "**What to compare:**\n",
    "- Parameter count (768 vs LoRA's count)\n",
    "- File size (~3 KB vs LoRA's file size)\n",
    "- Training time for the same number of steps\n",
    "- Output quality on simple prompts (\"a warrior in [style]\")\n",
    "- Output quality on complex prompts (\"a detailed landscape with dramatic lighting in [style]\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# YOUR COMPARISON EXPERIMENT\n",
    "#\n",
    "# Step 1: Train a LoRA adapter on the same dataset.\n",
    "#\n",
    "# Load components:\n",
    "#   tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder='tokenizer')\n",
    "#   text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder='text_encoder', torch_dtype=torch.float32).to(device)\n",
    "#   vae = AutoencoderKL.from_pretrained(model_id, subfolder='vae', torch_dtype=torch.float32).to(device)\n",
    "#   unet = UNet2DConditionModel.from_pretrained(model_id, subfolder='unet', torch_dtype=torch.float32).to(device)\n",
    "#   noise_scheduler = DDPMScheduler.from_pretrained(model_id, subfolder='scheduler')\n",
    "#\n",
    "# Apply LoRA to U-Net:\n",
    "#   from peft import LoraConfig, get_peft_model\n",
    "#   lora_config = LoraConfig(\n",
    "#       r=4,\n",
    "#       lora_alpha=4,\n",
    "#       target_modules=['to_k', 'to_v', 'to_q', 'to_out.0'],\n",
    "#       lora_dropout=0.0,\n",
    "#   )\n",
    "#   unet = get_peft_model(unet, lora_config)\n",
    "#   unet.print_trainable_parameters()  # See the parameter count\n",
    "#\n",
    "# Freeze everything except LoRA params:\n",
    "#   vae.requires_grad_(False)\n",
    "#   text_encoder.requires_grad_(False)\n",
    "#   # LoRA params are already unfrozen by get_peft_model\n",
    "#\n",
    "# Training loop (same as Exercise 3, but optimizer targets LoRA params):\n",
    "#   optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-4)\n",
    "#   for step in range(num_training_steps):\n",
    "#       image_tensor, _ = get_training_batch(dataset, step)\n",
    "#       caption = 'a painting in naruto anime style'  # Use regular text, no pseudo-token\n",
    "#       ... same DDPM steps as Exercise 3 ...\n",
    "#       ... but text_encoder forward pass CAN use torch.no_grad() since we\n",
    "#           are not training the embedding ...\n",
    "#\n",
    "# Step 2: Generate from both and compare.\n",
    "#   Load the textual inversion embedding from Exercise 3 (learned_embed_ti).\n",
    "#   Generate with matched prompts and seeds.\n",
    "#\n",
    "# Step 3: Compare file sizes, training times, parameter counts, and output quality.\n",
    "\n",
    "print('Build your comparison experiment here.')\n",
    "print('Train LoRA, generate from both, and compare.')\n",
    "print()\n",
    "print(f'Textual inversion stats from Exercise 3:')\n",
    "print(f'  Parameters:    768')\n",
    "print(f'  File size:     {ti_file_size:,} bytes ({ti_file_size / 1024:.1f} KB)')\n",
    "print(f'  Training time: {ti_training_time:.1f}s ({ti_training_steps} steps)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Why this comparison matters:** The lesson claimed that textual inversion captures \"what a concept looks like\" (object identity, simple visual attributes) while LoRA captures \"how images in a style should look\" (spatial composition, color palettes, rendering techniques). This experiment lets you verify that claim with firsthand experience rather than taking the lesson's word for it.\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import (AutoencoderKL, UNet2DConditionModel, DDPMScheduler,\n",
    "                       DPMSolverMultistepScheduler, StableDiffusionPipeline)\n",
    "\n",
    "# ============================================================\n",
    "# PART 1: Train LoRA on the same dataset.\n",
    "# ============================================================\n",
    "\n",
    "print('Loading components for LoRA training...')\n",
    "tokenizer_lora = CLIPTokenizer.from_pretrained(model_id, subfolder='tokenizer')\n",
    "text_encoder_lora = CLIPTextModel.from_pretrained(\n",
    "    model_id, subfolder='text_encoder', torch_dtype=torch.float32\n",
    ").to(device)\n",
    "vae_lora = AutoencoderKL.from_pretrained(\n",
    "    model_id, subfolder='vae', torch_dtype=torch.float32\n",
    ").to(device)\n",
    "unet_lora = UNet2DConditionModel.from_pretrained(\n",
    "    model_id, subfolder='unet', torch_dtype=torch.float32\n",
    ").to(device)\n",
    "noise_scheduler_lora = DDPMScheduler.from_pretrained(model_id, subfolder='scheduler')\n",
    "\n",
    "# Apply LoRA to U-Net cross-attention projections.\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=4,\n",
    "    target_modules=['to_k', 'to_v', 'to_q', 'to_out.0'],\n",
    "    lora_dropout=0.0,\n",
    ")\n",
    "unet_lora = get_peft_model(unet_lora, lora_config)\n",
    "unet_lora.print_trainable_parameters()\n",
    "\n",
    "# Freeze non-LoRA components.\n",
    "vae_lora.requires_grad_(False)\n",
    "text_encoder_lora.requires_grad_(False)\n",
    "\n",
    "# Training loop.\n",
    "lora_training_steps = 300\n",
    "optimizer_lora = torch.optim.AdamW(\n",
    "    [p for p in unet_lora.parameters() if p.requires_grad],\n",
    "    lr=1e-4,\n",
    ")\n",
    "\n",
    "lora_prompt = 'a painting in naruto anime style'\n",
    "lora_loss_history = []\n",
    "\n",
    "lora_text_input = tokenizer_lora(\n",
    "    lora_prompt,\n",
    "    padding='max_length',\n",
    "    max_length=tokenizer_lora.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "lora_input_ids = lora_text_input.input_ids.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    lora_text_emb = text_encoder_lora(lora_input_ids)[0]\n",
    "\n",
    "print(f'\\nTraining LoRA for {lora_training_steps} steps...')\n",
    "lora_start = time.time()\n",
    "\n",
    "for step in range(lora_training_steps):\n",
    "    idx = step % len(dataset)\n",
    "    example = dataset[idx]\n",
    "    image = example['image'].convert('RGB')\n",
    "    image_tensor = image_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        latents = vae_lora.encode(image_tensor).latent_dist.mode() * vae_lora.config.scaling_factor\n",
    "\n",
    "    timesteps = torch.randint(0, noise_scheduler_lora.config.num_train_timesteps, (1,), device=device)\n",
    "    noise = torch.randn_like(latents)\n",
    "    noisy_latents = noise_scheduler_lora.add_noise(latents, noise, timesteps)\n",
    "\n",
    "    noise_pred = unet_lora(noisy_latents, timesteps, encoder_hidden_states=lora_text_emb).sample\n",
    "    loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "    optimizer_lora.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_lora.step()\n",
    "\n",
    "    lora_loss_history.append(loss.item())\n",
    "    if (step + 1) % 100 == 0:\n",
    "        print(f'  Step {step + 1}/{lora_training_steps}: loss = {loss.item():.4f}')\n",
    "\n",
    "lora_training_time = time.time() - lora_start\n",
    "print(f'LoRA training complete in {lora_training_time:.1f}s.')\n",
    "\n",
    "# Count LoRA parameters and estimate file size.\n",
    "lora_param_count = sum(p.numel() for p in unet_lora.parameters() if p.requires_grad)\n",
    "lora_file_size = lora_param_count * 4  # float32\n",
    "print(f'LoRA parameters:  {lora_param_count:,}')\n",
    "print(f'LoRA file size:   {lora_file_size:,} bytes ({lora_file_size / 1024:.1f} KB)')\n",
    "\n",
    "# ============================================================\n",
    "# PART 2: Generate from both and compare.\n",
    "# ============================================================\n",
    "\n",
    "# Build a pipeline with the LoRA adapter.\n",
    "unet_lora_f16 = unet_lora.to(dtype=torch.float16)\n",
    "text_encoder_lora_f16 = text_encoder_lora.to(dtype=torch.float16)\n",
    "vae_lora_f16 = vae_lora.to(dtype=torch.float16)\n",
    "\n",
    "lora_pipeline = StableDiffusionPipeline(\n",
    "    vae=vae_lora_f16,\n",
    "    text_encoder=text_encoder_lora_f16,\n",
    "    tokenizer=tokenizer_lora,\n",
    "    unet=unet_lora_f16,\n",
    "    scheduler=DPMSolverMultistepScheduler.from_pretrained(model_id, subfolder='scheduler'),\n",
    "    safety_checker=None,\n",
    "    feature_extractor=None,\n",
    "    requires_safety_checker=False,\n",
    ")\n",
    "\n",
    "# Simple and complex prompts for comparison.\n",
    "simple_prompt = 'a painting of a warrior'\n",
    "complex_prompt = 'a detailed landscape with dramatic lighting and rain'\n",
    "\n",
    "seed = 42\n",
    "lora_results = []\n",
    "for p in [simple_prompt, complex_prompt]:\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    result = lora_pipeline(\n",
    "        prompt=f'{p} in naruto anime style',\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=7.5,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    lora_results.append(result)\n",
    "\n",
    "del lora_pipeline, unet_lora, text_encoder_lora, vae_lora\n",
    "del unet_lora_f16, text_encoder_lora_f16, vae_lora_f16\n",
    "cleanup()\n",
    "\n",
    "# Now generate from textual inversion for comparison.\n",
    "# Reload the components with the textual inversion embedding.\n",
    "tokenizer_ti = CLIPTokenizer.from_pretrained(model_id, subfolder='tokenizer')\n",
    "text_encoder_ti = CLIPTextModel.from_pretrained(\n",
    "    model_id, subfolder='text_encoder', torch_dtype=torch.float16\n",
    ").to(device)\n",
    "vae_ti = AutoencoderKL.from_pretrained(model_id, subfolder='vae', torch_dtype=torch.float16).to(device)\n",
    "unet_ti = UNet2DConditionModel.from_pretrained(model_id, subfolder='unet', torch_dtype=torch.float16).to(device)\n",
    "\n",
    "# Re-add the pseudo-token and load the trained embedding.\n",
    "tokenizer_ti.add_tokens([placeholder_token])\n",
    "text_encoder_ti.resize_token_embeddings(len(tokenizer_ti))\n",
    "ti_token_id = tokenizer_ti.convert_tokens_to_ids(placeholder_token)\n",
    "with torch.no_grad():\n",
    "    text_encoder_ti.get_input_embeddings().weight[ti_token_id] = learned_embed_ti.to(\n",
    "        device=device, dtype=torch.float16\n",
    "    )\n",
    "\n",
    "ti_pipeline = StableDiffusionPipeline(\n",
    "    vae=vae_ti,\n",
    "    text_encoder=text_encoder_ti,\n",
    "    tokenizer=tokenizer_ti,\n",
    "    unet=unet_ti,\n",
    "    scheduler=DPMSolverMultistepScheduler.from_pretrained(model_id, subfolder='scheduler'),\n",
    "    safety_checker=None,\n",
    "    feature_extractor=None,\n",
    "    requires_safety_checker=False,\n",
    ")\n",
    "\n",
    "ti_results = []\n",
    "for p in [simple_prompt, complex_prompt]:\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    result = ti_pipeline(\n",
    "        prompt=f'{p} in the style of {placeholder_token}',\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=7.5,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    ti_results.append(result)\n",
    "\n",
    "del ti_pipeline, unet_ti, text_encoder_ti, vae_ti\n",
    "cleanup()\n",
    "\n",
    "# ============================================================\n",
    "# PART 3: Display comparison.\n",
    "# ============================================================\n",
    "\n",
    "show_image_grid(\n",
    "    [ti_results[0], lora_results[0], ti_results[1], lora_results[1]],\n",
    "    ['TI: Simple Prompt', 'LoRA: Simple Prompt',\n",
    "     'TI: Complex Prompt', 'LoRA: Complex Prompt'],\n",
    "    nrows=2, ncols=2, figsize=(12, 12),\n",
    "    suptitle='Textual Inversion vs LoRA on Same Dataset',\n",
    ")\n",
    "\n",
    "print(f'\\n{\"Metric\":<25s} {\"Textual Inversion\":<25s} {\"LoRA (r=4)\":<25s}')\n",
    "print(f'{\"-\" * 75}')\n",
    "print(f'{\"Parameters\":<25s} {\"768\":<25s} {\"{:,}\".format(lora_param_count):<25s}')\n",
    "print(f'{\"File size\":<25s} {\"{:.1f} KB\".format(ti_file_size / 1024):<25s} {\"{:.1f} KB\".format(lora_file_size / 1024):<25s}')\n",
    "print(f'{\"Training time\":<25s} {\"{:.1f}s\".format(ti_training_time):<25s} {\"{:.1f}s\".format(lora_training_time):<25s}')\n",
    "print(f'{\"Training steps\":<25s} {\"{}\".format(ti_training_steps):<25s} {\"{}\".format(lora_training_steps):<25s}')\n",
    "print(f'{\"What changes\":<25s} {\"Embedding table (+1 row)\":<25s} {\"U-Net cross-attn weights\":<25s}')\n",
    "print(f'{\"Best for\":<25s} {\"Object identity, textures\":<25s} {\"Complex styles, spatial\":<25s}')\n",
    "```\n",
    "\n",
    "**What to look for in the comparison:**\n",
    "- On **simple prompts** (single subject), textual inversion and LoRA may produce similarly stylized results. Both can capture the general \"naruto anime\" visual flavor.\n",
    "- On **complex prompts** (spatial composition, lighting), LoRA typically produces more consistent style transfer because it modifies cross-attention projections across the entire U-Net---each spatial resolution can encode different aspects of the style.\n",
    "- Textual inversion's 768 parameters encode \"what does this style look like as a concept descriptor.\" LoRA's thousands of parameters encode \"how should each attention layer process features to produce this style.\"\n",
    "- The file size difference is dramatic: ~3 KB vs hundreds of KB or MB. This matters for sharing and storage.\n",
    "\n",
    "**Common mistakes:**\n",
    "- Using different seeds for LoRA and textual inversion generation. Same seed + same prompt structure ensures a fair comparison.\n",
    "- Forgetting to clean up between LoRA training and textual inversion generation. VRAM will overflow.\n",
    "- Training LoRA with the text encoder in `torch.no_grad()` is correct here (unlike textual inversion, where gradients must reach the embedding).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **CLIP's embedding table is a concrete data structure---49,408 rows of 768 floats.** Each row maps a token ID to a vector. Textual inversion adds one row and optimizes it. That is the entire technique: find the right point in a continuous, meaningful space.\n\n2. **The training loop is the same DDPM loop you have seen four times.** VAE encode, sample timestep, noise, U-Net prediction, MSE loss, backprop. The only difference is what receives the gradient: one 768-float embedding vector instead of LoRA adapter matrices or full model weights.\n\n3. **Frozen does not mean invisible to gradients.** Gradients flow through the entire frozen CLIP encoder and U-Net to reach the embedding table. The frozen layers are a pathway, not a wall. Multiple embedding rows receive gradients (via CLIP's self-attention), but the restore-original-rows pattern ensures only the pseudo-token's row is actually updated.\n\n4. **Textual inversion is the most lightweight customization (768 parameters, ~3 KB file) but least expressive for complex styles.** LoRA's thousands of cross-attention parameters capture spatial composition, color palettes, and rendering techniques that a single embedding vector cannot encode. Choose the tool that matches the scope of the customization.\n\n5. **Three knobs on three different parts of the pipeline.** LoRA changes the weights. Img2img/inpainting changes the inference process. Textual inversion changes the embeddings. You now understand all three."
  }
 ]
}