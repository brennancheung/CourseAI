{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Transformers (DiT)\n",
    "\n",
    "**Module 7.4, Lesson 2** | CourseAI\n",
    "\n",
    "DiT replaces the U-Net with a standard vision transformer operating on latent patches. Two deep knowledge threads converge: transformers from Series 4 and latent diffusion from Series 6. Every component of the DiT block is something you already know, except the conditioning mechanism (adaLN-Zero).\n",
    "\n",
    "**What you will do:**\n",
    "- Implement patchify and unpatchify from scratch, verify tensor shapes at every step, and confirm the round-trip recovers the original latent\n",
    "- Build one adaLN-Zero conditioning step, verify the identity property at alpha=0, and watch the block's contribution grow as alpha increases\n",
    "- Load a pretrained DiT model and a U-Net, compare their architectures side-by-side: parameter counts, layer types, and the adaLN-Zero components\n",
    "- Generate class-conditional ImageNet images with DiT-XL/2 at different classifier-free guidance scales\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "Every concept in this notebook comes from the lesson. Patchify as \"tokenize the image,\" adaLN-Zero as adaptive norm with a zero-initialized gate, the two-knob scaling recipe. No new theory—just hands-on verification of what you just read.\n",
    "\n",
    "**Estimated time:** 45–60 minutes. Exercises 1–2 are pure PyTorch (no GPU needed). Exercises 3–4 use pretrained models and benefit from a GPU runtime (~3 GB VRAM for DiT-XL/2 in float16)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies and configure the environment.\n",
    "\n",
    "**Important:** For Exercises 3–4, switch to a GPU runtime in Colab (Runtime > Change runtime type > T4 GPU). Exercises 1–2 work on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate safetensors timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import gc\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [14, 5]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(f'Device: {device}')\n",
    "print(f'Dtype: {dtype}')\n",
    "if device.type == 'cpu':\n",
    "    print('Note: No GPU detected. Exercises 1-2 work fine on CPU.')\n",
    "    print('For Exercises 3-4, switch to GPU: Runtime > Change runtime type > T4 GPU')\n",
    "print()\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Helpers\n",
    "\n",
    "Utility functions used across multiple exercises. Run this cell now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count total parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def count_parameters_by_type(model):\n",
    "    \"\"\"Count parameters grouped by module type (e.g., Linear, LayerNorm).\"\"\"\n",
    "    counts = {}\n",
    "    for name, module in model.named_modules():\n",
    "        module_type = module.__class__.__name__\n",
    "        if module_type == type(model).__name__:\n",
    "            continue  # skip the top-level model itself\n",
    "        n_params = sum(p.numel() for p in module.parameters(recurse=False))\n",
    "        if n_params > 0:\n",
    "            counts[module_type] = counts.get(module_type, 0) + n_params\n",
    "    return dict(sorted(counts.items(), key=lambda x: -x[1]))\n",
    "\n",
    "\n",
    "def show_image_row(images, titles, suptitle=None, figsize=None):\n",
    "    \"\"\"Display a row of PIL images with titles.\"\"\"\n",
    "    n = len(images)\n",
    "    fig_w = figsize[0] if figsize else max(5 * n, 12)\n",
    "    fig_h = figsize[1] if figsize else 5\n",
    "    fig, axes = plt.subplots(1, n, figsize=(fig_w, fig_h))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle, fontsize=13, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def free_memory(*objects):\n",
    "    \"\"\"Delete objects and free GPU memory.\"\"\"\n",
    "    for obj in objects:\n",
    "        del obj\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print('Memory freed.')\n",
    "\n",
    "\n",
    "print('Helpers defined: count_parameters, count_parameters_by_type, show_image_row, free_memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Patchify and Unpatchify `[Guided]`\n",
    "\n",
    "The lesson taught that DiT \"tokenizes the image\": take the noisy latent `[C, H, W]`, split it into non-overlapping patches of size `p x p`, flatten each patch, and project to `d_model` dimensions. This is the image equivalent of text tokenization.\n",
    "\n",
    "The reverse operation—unpatchify—takes the transformer output sequence back to a spatial tensor. Together, patchify and unpatchify are the bridge between the latent diffusion world (spatial tensors) and the transformer world (token sequences).\n",
    "\n",
    "In this exercise, you will implement both operations from scratch and verify:\n",
    "1. The tensor shapes match the lesson's trace at every step\n",
    "2. The round-trip patchify → unpatchify recovers the original tensor (before projection)\n",
    "\n",
    "**Before running, predict:**\n",
    "- A latent of shape `[4, 32, 32]` with patch size `p=2`: how many patches? What is each patch's raw dimension before projection?\n",
    "- A latent of shape `[4, 64, 64]` with patch size `p=4`: how many patches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1: Implement patchify and unpatchify from scratch\n",
    "# ============================================================\n",
    "\n",
    "# --- Step 1: Define dimensions ---\n",
    "C = 4        # latent channels (VAE output)\n",
    "H = 32       # latent height (e.g., 256x256 image with 8x VAE downsampling)\n",
    "W = 32       # latent width\n",
    "p = 2        # patch size\n",
    "d_model = 1152  # DiT-XL hidden dimension\n",
    "\n",
    "# Create a random \"noisy latent\" tensor\n",
    "z = torch.randn(1, C, H, W)  # batch=1 for clarity\n",
    "print(f'Input noisy latent: {list(z.shape)}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Patchify ---\n",
    "# Split the spatial dimensions into patches of size p x p.\n",
    "# Each patch is a [C, p, p] volume, flattened to C*p*p dimensions.\n",
    "\n",
    "num_patches_h = H // p   # patches along height\n",
    "num_patches_w = W // p   # patches along width\n",
    "num_patches = num_patches_h * num_patches_w  # total patch tokens\n",
    "patch_dim = C * p * p    # raw dimension per patch before projection\n",
    "\n",
    "print(f'Patches per row:    {num_patches_h}')\n",
    "print(f'Patches per column: {num_patches_w}')\n",
    "print(f'Total patches (L):  {num_patches}')\n",
    "print(f'Raw patch dim:      C * p * p = {C} * {p} * {p} = {patch_dim}')\n",
    "print()\n",
    "\n",
    "# Reshape: [1, C, H, W] -> [1, C, num_patches_h, p, num_patches_w, p]\n",
    "patches = z.reshape(1, C, num_patches_h, p, num_patches_w, p)\n",
    "\n",
    "# Permute to group patch spatial dims together:\n",
    "# [1, C, num_patches_h, p, num_patches_w, p]\n",
    "# -> [1, num_patches_h, num_patches_w, C, p, p]\n",
    "patches = patches.permute(0, 2, 4, 1, 3, 5)\n",
    "\n",
    "# Flatten the patch grid into a sequence, and flatten each patch:\n",
    "# [1, num_patches_h, num_patches_w, C, p, p] -> [1, L, C*p*p]\n",
    "patches = patches.reshape(1, num_patches, patch_dim)\n",
    "\n",
    "print(f'After patchify: {list(patches.shape)}')\n",
    "print(f'  = [batch, {num_patches} tokens, {patch_dim} dims]')\n",
    "print()\n",
    "print('This matches the lesson\\'s trace:')\n",
    "print(f'  [4, 32, 32] -> [{num_patches} tokens, {patch_dim} dims]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Linear projection to d_model ---\n",
    "# This is the equivalent of the token embedding lookup in text transformers.\n",
    "# A learned nn.Linear maps from patch_dim to d_model.\n",
    "\n",
    "proj = nn.Linear(patch_dim, d_model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = proj(patches)  # [1, L, d_model]\n",
    "\n",
    "print(f'After linear projection: {list(tokens.shape)}')\n",
    "print(f'  = [batch, {num_patches} tokens, {d_model} dims]')\n",
    "print()\n",
    "print('The transformer now has a sequence of 256 tokens, each with 1152 dimensions.')\n",
    "print('It does not know or care that these tokens came from image patches.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Add positional embeddings ---\n",
    "# Same concept as positional encoding from Series 4.\n",
    "# DiT uses learned positional embeddings: one vector per patch position.\n",
    "\n",
    "pos_embed = nn.Parameter(torch.randn(1, num_patches, d_model) * 0.02)\n",
    "\n",
    "tokens_with_pos = tokens + pos_embed\n",
    "\n",
    "print(f'Positional embeddings shape: {list(pos_embed.shape)}')\n",
    "print(f'Tokens + position:           {list(tokens_with_pos.shape)}')\n",
    "print()\n",
    "print(f'Final input to transformer:  [{num_patches}, {d_model}]')\n",
    "print('(Same as the lesson\\'s trace: [256, 1152])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Unpatchify (reverse the operation) ---\n",
    "# After N transformer blocks, the model has [L, d_model] tokens.\n",
    "# Project back to patch dimensions, then reshape to spatial grid.\n",
    "\n",
    "# Simulate transformer output (just use the patches before projection\n",
    "# to verify the round-trip without the projection lossy step)\n",
    "transformer_output = patches  # [1, L, patch_dim]\n",
    "\n",
    "# In the real DiT: nn.Linear(d_model, patch_dim) projects back to patch dims.\n",
    "# We skip projection here to verify exact round-trip.\n",
    "\n",
    "# Reshape: [1, L, C*p*p] -> [1, num_patches_h, num_patches_w, C, p, p]\n",
    "spatial = transformer_output.reshape(1, num_patches_h, num_patches_w, C, p, p)\n",
    "\n",
    "# Permute back: [1, num_patches_h, num_patches_w, C, p, p]\n",
    "# -> [1, C, num_patches_h, p, num_patches_w, p]\n",
    "spatial = spatial.permute(0, 3, 1, 4, 2, 5)\n",
    "\n",
    "# Reshape to original spatial dimensions: -> [1, C, H, W]\n",
    "reconstructed = spatial.reshape(1, C, H, W)\n",
    "\n",
    "print(f'Unpatchified output: {list(reconstructed.shape)}')\n",
    "print()\n",
    "\n",
    "# Verify round-trip: patchify -> unpatchify should recover the original\n",
    "match = torch.allclose(z, reconstructed, atol=1e-6)\n",
    "max_diff = (z - reconstructed).abs().max().item()\n",
    "print(f'Round-trip exact match: {match}')\n",
    "print(f'Max absolute difference: {max_diff}')\n",
    "print()\n",
    "print('Patchify -> unpatchify is lossless (ignoring the learned projections).')\n",
    "print('The spatial information is preserved through the reshape operations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Try different latent sizes and patch sizes ---\n",
    "# The lesson showed: L = (H/p) * (W/p)\n",
    "# Smaller patch size = more tokens = finer spatial detail = quadratic attention cost.\n",
    "\n",
    "print('Sequence length L = (H/p) * (W/p):')\n",
    "print()\n",
    "print(f'{\"Latent\":<16} {\"Patch\":<8} {\"Tokens (L)\":<12} {\"Raw patch dim\":<16} {\"Attention cost\"}')\n",
    "print('-' * 70)\n",
    "\n",
    "configs = [\n",
    "    (4, 32, 32, 2),   # DiT-XL/2 on 256x256\n",
    "    (4, 32, 32, 4),   # DiT-XL/4 on 256x256\n",
    "    (4, 32, 32, 8),   # DiT-XL/8 on 256x256\n",
    "    (4, 64, 64, 2),   # DiT on 512x512\n",
    "    (4, 64, 64, 4),   # DiT on 512x512\n",
    "]\n",
    "\n",
    "for c, h, w, ps in configs:\n",
    "    L = (h // ps) * (w // ps)\n",
    "    raw_dim = c * ps * ps\n",
    "    attn_cost = L * L  # O(L^2)\n",
    "    print(f'[{c},{h},{w}]      p={ps:<4} L={L:<8} {c}*{ps}*{ps}={raw_dim:<10} O({attn_cost:,})')\n",
    "\n",
    "print()\n",
    "print('Key insight: halving patch size quadruples tokens AND increases')\n",
    "print('attention cost by 16x (because attention is O(L^2)).')\n",
    "print('This is the same tradeoff from SDXL, now controlled by patch size.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You implemented the patchify and unpatchify operations from scratch and verified the full shape trace from the lesson:\n",
    "\n",
    "```\n",
    "Noisy latent:      [4, 32, 32]\n",
    "After patchify:    [256, 16]     (256 tokens, each 4*2*2=16 dims)\n",
    "After projection:  [256, 1152]   (projected to d_model)\n",
    "After pos embed:   [256, 1152]   (same shape, position info added)\n",
    "... N transformer blocks ...\n",
    "After unpatchify:  [4, 32, 32]   (back to spatial tensor)\n",
    "```\n",
    "\n",
    "Key observations:\n",
    "\n",
    "- **Patchify is reshaping, not convolution.** No learned parameters in the reshape itself. The learning happens in the linear projection (`nn.Linear(16, 1152)`).\n",
    "\n",
    "- **The round-trip is exact.** Patchify followed by unpatchify (without projection) perfectly recovers the original tensor. No spatial information is lost—it is just rearranged.\n",
    "\n",
    "- **Patch size is the resolution knob.** `p=2` on a `[4, 32, 32]` latent gives 256 tokens. `p=4` gives 64 tokens (4x fewer). `p=8` gives 16 tokens (16x fewer). Halving the patch size quadruples the tokens and increases attention cost by 16x. Same quadratic tradeoff from SDXL.\n",
    "\n",
    "- **\"Tokenize the image\" is literal.** The patchify + linear projection is structurally identical to tokenization + embedding lookup in text transformers. The transformer processes both kinds of sequences the same way.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: adaLN-Zero Forward Pass `[Guided]`\n",
    "\n",
    "The lesson taught that DiT conditions each transformer block via adaLN-Zero: the conditioning vector `c` (timestep + class embedding) is projected through an MLP to produce six parameters per block:\n",
    "\n",
    "```\n",
    "c -> MLP -> (γ₁, β₁, α₁, γ₂, β₂, α₂)\n",
    "\n",
    "MHA sub-layer:  x' = x + α₁ * MHA(γ₁ * LayerNorm(x) + β₁)\n",
    "FFN sub-layer:  output = x' + α₂ * FFN(γ₂ * LayerNorm(x') + β₂)\n",
    "```\n",
    "\n",
    "The critical design choice: all alpha values are **initialized to zero**. This means every DiT block starts as an identity function—input in, same input out. The model gradually learns what each block should contribute.\n",
    "\n",
    "In this exercise, you will:\n",
    "1. Build the adaLN-Zero MLP that produces the six parameters\n",
    "2. Apply adaptive layer norm (scale + shift) to a LayerNorm output\n",
    "3. Apply the gated residual connection\n",
    "4. Verify the identity property at alpha=0\n",
    "5. Watch the block's contribution grow as alpha increases\n",
    "\n",
    "**Before running, predict:**\n",
    "- When alpha=0, what will the output of the block be? (Think about the residual: `x + alpha * f(x)`.)\n",
    "- When alpha=1, will the output be exactly `x + f(x)` (the standard residual connection)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 2: Build adaLN-Zero from scratch\n",
    "# ============================================================\n",
    "\n",
    "# --- Step 1: Define dimensions ---\n",
    "d_model = 384    # Use DiT-S dimension for speed\n",
    "seq_len = 16     # Small sequence for clarity (16 patch tokens)\n",
    "cond_dim = 384   # Conditioning vector dimension (same as d_model in DiT)\n",
    "\n",
    "print(f'd_model:  {d_model}')\n",
    "print(f'seq_len:  {seq_len} (patch tokens)')\n",
    "print(f'cond_dim: {cond_dim}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Build the adaLN-Zero MLP ---\n",
    "# This MLP takes the conditioning vector c and produces 6 * d_model parameters:\n",
    "# (gamma_1, beta_1, alpha_1, gamma_2, beta_2, alpha_2)\n",
    "# Each is a vector of d_model dimensions.\n",
    "\n",
    "adaln_mlp = nn.Sequential(\n",
    "    nn.SiLU(),\n",
    "    nn.Linear(cond_dim, 6 * d_model),\n",
    ")\n",
    "\n",
    "# Critical: initialize the final linear layer to output zeros.\n",
    "# This ensures all six parameter vectors start at zero.\n",
    "nn.init.zeros_(adaln_mlp[1].weight)\n",
    "nn.init.zeros_(adaln_mlp[1].bias)\n",
    "\n",
    "print(f'adaLN MLP output dimension: {6 * d_model} = 6 * {d_model}')\n",
    "print(f'  = gamma_1[{d_model}] + beta_1[{d_model}] + alpha_1[{d_model}]')\n",
    "print(f'  + gamma_2[{d_model}] + beta_2[{d_model}] + alpha_2[{d_model}]')\n",
    "print()\n",
    "print(f'MLP parameters: {count_parameters(adaln_mlp):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Produce the six conditioning parameters ---\n",
    "# Feed a random conditioning vector through the MLP.\n",
    "\n",
    "# Simulate conditioning: c = timestep_embedding + class_embedding\n",
    "c = torch.randn(1, cond_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    params = adaln_mlp(c)  # [1, 6 * d_model]\n",
    "\n",
    "# Split into the six parameter vectors\n",
    "gamma_1, beta_1, alpha_1, gamma_2, beta_2, alpha_2 = params.chunk(6, dim=-1)\n",
    "\n",
    "print('Six adaLN-Zero parameters (all initialized to ~zero):')\n",
    "print(f'  gamma_1: mean={gamma_1.mean().item():.6f}, max_abs={gamma_1.abs().max().item():.6f}')\n",
    "print(f'  beta_1:  mean={beta_1.mean().item():.6f}, max_abs={beta_1.abs().max().item():.6f}')\n",
    "print(f'  alpha_1: mean={alpha_1.mean().item():.6f}, max_abs={alpha_1.abs().max().item():.6f}')\n",
    "print(f'  gamma_2: mean={gamma_2.mean().item():.6f}, max_abs={gamma_2.abs().max().item():.6f}')\n",
    "print(f'  beta_2:  mean={beta_2.mean().item():.6f}, max_abs={beta_2.abs().max().item():.6f}')\n",
    "print(f'  alpha_2: mean={alpha_2.mean().item():.6f}, max_abs={alpha_2.abs().max().item():.6f}')\n",
    "print()\n",
    "print('All values are near zero because we initialized the MLP output layer to zeros.')\n",
    "print('This is the \"Zero\" in adaLN-Zero.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Apply adaptive layer norm + gated residual ---\n",
    "# Trace one MHA sub-layer step by step.\n",
    "\n",
    "# Create a random input (simulating patch token sequence)\n",
    "x = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# Standard LayerNorm (no learnable parameters needed;\n",
    "# adaLN replaces the learned gamma/beta with conditioning-dependent ones)\n",
    "layer_norm = nn.LayerNorm(d_model, elementwise_affine=False)\n",
    "\n",
    "# Step 4a: Adaptive layer norm\n",
    "# Standard LN: normalize, then gamma * x + beta\n",
    "# adaLN: normalize, then gamma(c) * x + beta(c)\n",
    "# gamma_1 and beta_1 are [1, d_model] -> broadcast over [1, seq_len, d_model]\n",
    "normed = layer_norm(x)\n",
    "adaln_output = gamma_1.unsqueeze(1) * normed + beta_1.unsqueeze(1)\n",
    "\n",
    "print('Step 4a: Adaptive Layer Norm')\n",
    "print(f'  x shape:          {list(x.shape)}')\n",
    "print(f'  LayerNorm(x):     {list(normed.shape)}')\n",
    "print(f'  gamma_1 * LN + beta_1: {list(adaln_output.shape)}')\n",
    "print()\n",
    "\n",
    "# Step 4b: Simulate MHA output (just use random for demonstration)\n",
    "# In a real DiT block, this would be MultiHeadAttention(adaln_output)\n",
    "mha_output = torch.randn_like(x) * 0.1  # small random \"attention output\"\n",
    "\n",
    "# Step 4c: Gated residual connection\n",
    "# x' = x + alpha_1 * MHA_output\n",
    "x_prime = x + alpha_1.unsqueeze(1) * mha_output\n",
    "\n",
    "print('Step 4c: Gated Residual')\n",
    "print(f'  x\\' = x + alpha_1 * MHA(...)  shape: {list(x_prime.shape)}')\n",
    "print(f'  alpha_1 max_abs: {alpha_1.abs().max().item():.6f}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Verify the identity property at alpha=0 ---\n",
    "# With the zero-initialized MLP, alpha is ~0.\n",
    "# So: x' = x + 0 * MHA(...) = x\n",
    "\n",
    "# Check: is x' approximately equal to x?\n",
    "diff = (x_prime - x).abs().max().item()\n",
    "print(f'Max |x\\' - x|: {diff:.8f}')\n",
    "print(f'Identity property holds: {diff < 1e-5}')\n",
    "print()\n",
    "print('At initialization (alpha=0), the gated residual connection')\n",
    "print('means the MHA output contributes NOTHING. The input passes')\n",
    "print('through unchanged. The block is an identity function.')\n",
    "print()\n",
    "print('This is the same principle as:')\n",
    "print('  - ControlNet zero convolution: start contributing nothing')\n",
    "print('  - LoRA B matrix at zero: bypass starts at zero')\n",
    "print('All three ensure new components start undamaged.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Watch the block's contribution grow ---\n",
    "# Manually vary alpha from 0 to 1 and measure how much the block\n",
    "# output deviates from the input.\n",
    "\n",
    "x_test = torch.randn(1, seq_len, d_model)\n",
    "mha_test = torch.randn_like(x_test) * 0.1  # simulated MHA output\n",
    "\n",
    "alphas = [0.0, 0.01, 0.05, 0.1, 0.25, 0.5, 1.0]\n",
    "deviations = []\n",
    "\n",
    "print(f'{\"alpha\":<10} {\"mean |output - input|\":<25} {\"max |output - input|\"}')\n",
    "print('-' * 60)\n",
    "\n",
    "for a in alphas:\n",
    "    output = x_test + a * mha_test\n",
    "    mean_dev = (output - x_test).abs().mean().item()\n",
    "    max_dev = (output - x_test).abs().max().item()\n",
    "    deviations.append(mean_dev)\n",
    "    print(f'{a:<10.2f} {mean_dev:<25.6f} {max_dev:.6f}')\n",
    "\n",
    "print()\n",
    "print('The block\\'s contribution grows linearly with alpha.')\n",
    "print('At alpha=0: identity. At alpha=1: full standard residual connection.')\n",
    "print('During training, the model learns the optimal alpha for each block.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 7: Visualize alpha's effect ---\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "ax.plot(alphas, deviations, 'o-', color='#22d3ee', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('alpha (gate value)', fontsize=12)\n",
    "ax.set_ylabel('Mean |output - input|', fontsize=12)\n",
    "ax.set_title('adaLN-Zero: Block Contribution vs Gate Value', fontsize=13)\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.annotate('Identity\\n(alpha=0)', xy=(0, 0), xytext=(0.15, deviations[-1]*0.15),\n",
    "            fontsize=10, color='#fbbf24',\n",
    "            arrowprops=dict(arrowstyle='->', color='#fbbf24', lw=1.5))\n",
    "ax.annotate('Full residual\\n(alpha=1)', xy=(1, deviations[-1]),\n",
    "            xytext=(0.6, deviations[-1]*0.85),\n",
    "            fontsize=10, color='#34d399',\n",
    "            arrowprops=dict(arrowstyle='->', color='#34d399', lw=1.5))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('At initialization, all DiT blocks sit at the left (alpha=0, identity).')\n",
    "print('Through training, each block learns its own alpha value,')\n",
    "print('gradually moving rightward as it learns what to contribute.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You built the adaLN-Zero conditioning mechanism from scratch and verified its key properties:\n",
    "\n",
    "- **Six parameters per block.** The conditioning MLP produces `(γ₁, β₁, α₁, γ₂, β₂, α₂)` from the conditioning vector. Three per sub-layer: scale (γ), shift (β), gate (α).\n",
    "\n",
    "- **Zero initialization means identity.** With the output layer initialized to zeros, all six parameters start at zero. The alpha gates are zero, so `x' = x + 0 * f(x) = x`. The entire block is an identity function at initialization.\n",
    "\n",
    "- **The gate controls contribution magnitude.** As alpha increases from 0 to 1, the block's output deviates more from its input. At alpha=1, you get the standard residual connection `x + f(x)`. The model learns the optimal alpha for each block during training.\n",
    "\n",
    "- **This extends adaptive group norm.** The U-Net's AdaGN has gamma and beta (2 parameters). adaLN-Zero adds alpha (3 parameters per sub-layer). The alpha + zero initialization is the key difference—it is the same safety principle as ControlNet's zero convolution and LoRA's zero-initialized B matrix.\n",
    "\n",
    "- **Compare to AdaGN:** adaptive group norm starts with gamma=1, beta=0 (standard normalization behavior). adaLN-Zero starts with alpha=0 (identity function). The U-Net block immediately contributes from the first training step. The DiT block starts contributing nothing and gradually learns to contribute.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Architecture Inspection `[Supported]`\n",
    "\n",
    "The lesson compared the U-Net and DiT architectures side by side:\n",
    "\n",
    "| | U-Net | DiT |\n",
    "|---|---|---|\n",
    "| Basic unit | Conv residual block + optional attention | Standard transformer block (MHA + FFN) |\n",
    "| Convolutions | Every layer | None (only patchify/unpatchify) |\n",
    "| Skip connections | Encoder-to-decoder | None (only within-block residual) |\n",
    "| Scaling recipe | Ad hoc | d_model and N |\n",
    "\n",
    "Now you will verify this by loading real models and comparing their architectures. You will:\n",
    "1. Load a pretrained DiT model\n",
    "2. Load a U-Net from a diffusion pipeline\n",
    "3. Compare parameter counts and layer types\n",
    "4. Find the adaLN-Zero components inside the DiT\n",
    "5. Observe how DiT parameter counts scale across model sizes\n",
    "\n",
    "Fill in the TODO markers to complete the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Exercise 3: Load and inspect DiT vs U-Net\n# ============================================================\n\n# --- Step 1: Load a pretrained DiT-XL/2 model ---\n# The DiT authors released pretrained models on HuggingFace.\n# We load via DiTPipeline from diffusers, then access pipe.transformer\n# as a proper model object—this lets us use the same inspection tools\n# (named_modules, count_parameters_by_type) on both DiT and U-Net.\n\nfrom diffusers import DiTPipeline\n\nprint('Loading DiT-XL/2 (256x256 ImageNet) via DiTPipeline...')\ndit_pipe = DiTPipeline.from_pretrained(\n    'facebook/DiT-XL-2-256',\n    torch_dtype=torch.float32,  # float32 for inspection (no GPU needed)\n)\ndit_model = dit_pipe.transformer  # DiTTransformer2DModel—a proper nn.Module\n\nprint(f'DiT model type: {type(dit_model).__name__}')\nprint(f'DiT-XL/2 loaded as a model object.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Step 2: Analyze DiT-XL/2 parameter structure ---\n# Now that we have a proper model object, we can use standard PyTorch inspection.\n\ntotal_params = count_parameters(dit_model)\nprint(f'DiT-XL/2 total parameters: {total_params:,} ({total_params / 1e6:.1f}M)')\nprint()\n\n# Group parameters by top-level module\ngroups = {}\nfor name, module in dit_model.named_children():\n    n_params = count_parameters(module)\n    if n_params > 0:\n        groups[name] = n_params\n\nprint('Parameter breakdown by top-level component:')\nfor group, count in sorted(groups.items(), key=lambda x: -x[1]):\n    pct = count / total_params * 100\n    print(f'  {group:<35} {count:>12,} ({pct:.1f}%)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Step 3: Inspect one DiT block's parameters ---\n# Look at block 0 to find all the components the lesson described:\n# MHA (Q/K/V/O projections), FFN, LayerNorm, and the adaLN-Zero MLP.\n\nblock_0 = dit_model.transformer_blocks[0]\nprint(f'DiT block 0 type: {type(block_0).__name__}')\nprint()\nprint('Sub-modules in DiT block 0:')\nfor name, module in block_0.named_modules():\n    if name == '':\n        continue\n    n_params = sum(p.numel() for p in module.parameters(recurse=False))\n    if n_params > 0:\n        print(f'  {name:<40} {type(module).__name__:<20} ({n_params:,} params)')\n\nblock_0_params = count_parameters(block_0)\nprint(f'\\n  Block 0 total: {block_0_params:,} parameters')\nprint()\nprint('Look for:')\nprint('  - attn1: the self-attention module (Q/K/V/O projections)')\nprint('  - ff: the FFN (feed-forward network)')\nprint('  - norm1/norm2: LayerNorm layers')\nprint('  - The adaLN modulation MLP that produces the 6 conditioning parameters')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Load SD v1.5 U-Net for comparison ---\n",
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "print('Loading SD v1.5 U-Net...')\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n",
    "    subfolder='unet',\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "unet_params = count_parameters(unet)\n",
    "print(f'SD v1.5 U-Net parameters: {unet_params:,} ({unet_params / 1e6:.1f}M)')\n",
    "print(f'DiT-XL/2 parameters:      {total_params:,} ({total_params / 1e6:.1f}M)')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Step 5: Compare layer types ---\n# The lesson's key claim: DiT has NO convolutions in the transformer blocks.\n# The U-Net has convolutions everywhere.\n\n# TODO: Use count_parameters_by_type() on BOTH models.\n# Print the U-Net breakdown first, then the DiT breakdown.\n# Compare: which module types dominate each architecture?\n# Hint: The function returns a dict of {module_type: param_count}.\nraise NotImplementedError(\n    \"TODO: Call count_parameters_by_type(unet) and count_parameters_by_type(dit_model),\\n\"\n    \"then print both results side by side.\"\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Step 6: Verify no convolutions in DiT transformer blocks ---\n# Scan the DiT model's named_modules for any Conv2d layers.\n\n# TODO: Iterate through dit_model.named_modules() and collect any module\n# that is an instance of nn.Conv2d. Print the results.\n# Expected: zero Conv2d layers in the transformer blocks themselves.\n# (There may be a conv-like projection in the patchify/unpatchify layer,\n#  but the transformer blocks should be convolution-free.)\nraise NotImplementedError(\n    \"TODO: Search for Conv2d modules in dit_model.named_modules(). See hint above.\"\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Step 7: DiT scaling across model sizes ---\n# The lesson showed DiT's systematic scaling recipe:\n#   DiT-S:  d_model=384,  N=12, ~33M\n#   DiT-B:  d_model=768,  N=12, ~130M\n#   DiT-L:  d_model=1024, N=24, ~458M\n#   DiT-XL: d_model=1152, N=28, ~675M\n#\n# We can estimate parameter counts from the architecture formula.\n# A transformer block has roughly:\n#   MHA: 4 * d_model^2 (Q, K, V, O projections)\n#   FFN: 2 * d_model * 4*d_model = 8 * d_model^2 (two linear layers)\n#   adaLN: ~6 * d_model * d_model (MLP producing 6*d_model outputs)\n#   Total per block: ~18 * d_model^2 (approximate)\n\n# TODO: For each DiT variant (S, B, L, XL), compute the estimated\n# total parameter count using: N_blocks * 18 * d_model^2.\n# Print a table comparing the estimate to the published values.\n# Also print the actual count for DiT-XL from the model we loaded.\n\ndit_variants = [\n    ('DiT-S',  384,  12,  33),\n    ('DiT-B',  768,  12, 130),\n    ('DiT-L',  1024, 24, 458),\n    ('DiT-XL', 1152, 28, 675),\n]\n\nraise NotImplementedError(\n    \"TODO: Estimate parameters for each variant and print a comparison table.\\n\"\n    f\"Hint: DiT-XL actual count from loaded model = {total_params:,}\"\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\nThe key insight is that the U-Net is dominated by `Conv2d` layers (convolutions at every resolution), while DiT has no convolutions in its transformer blocks—only `Linear` layers (for Q/K/V projections, FFN, and adaLN-Zero MLP). Because we loaded both as proper model objects, `count_parameters_by_type()` works symmetrically on both.\n\n**Step 5: Compare layer types (both models)**\n```python\nunet_by_type = count_parameters_by_type(unet)\ndit_by_type = count_parameters_by_type(dit_model)\n\nprint('SD v1.5 U-Net parameters by module type:')\nfor module_type, count in unet_by_type.items():\n    pct = count / unet_params * 100\n    print(f'  {module_type:<30} {count:>12,} ({pct:.1f}%)')\n\nprint()\nprint('DiT-XL/2 parameters by module type:')\nfor module_type, count in dit_by_type.items():\n    pct = count / total_params * 100\n    print(f'  {module_type:<30} {count:>12,} ({pct:.1f}%)')\n```\n\nYou should see Conv2d as the dominant module type in the U-Net (often 60%+ of parameters), with GroupNorm, Linear, and other types making up the rest. In the DiT, Linear should dominate with LayerNorm and no Conv2d in the transformer blocks.\n\n**Step 6: No convolutions in DiT transformer blocks**\n```python\nconv_modules = [\n    (name, module)\n    for name, module in dit_model.named_modules()\n    if isinstance(module, nn.Conv2d)\n]\nprint(f'Conv2d modules in DiT: {len(conv_modules)}')\nfor name, module in conv_modules:\n    n_params = sum(p.numel() for p in module.parameters())\n    print(f'  {name:<50} {n_params:,} params')\nif len(conv_modules) == 0:\n    print('  None found. DiT uses no convolutions at all.')\nelse:\n    # Check if any are inside transformer_blocks\n    block_convs = [n for n, _ in conv_modules if 'transformer_blocks' in n]\n    print(f'\\n  Conv2d inside transformer_blocks: {len(block_convs)}')\n    if len(block_convs) == 0:\n        print('  No convolutions in the transformer blocks themselves.')\n        print('  Any Conv2d found is in the patchify/unpatchify projection.')\n```\n\n**Step 7: DiT scaling estimates**\n```python\nprint(f'{\"Model\":<10} {\"d_model\":<10} {\"N\":<6} {\"Est (M)\":<12} {\"Published (M)\":<15} {\"Ratio\"}')\nprint('-' * 65)\nfor name, d, n, published in dit_variants:\n    est = n * 18 * d * d / 1e6  # rough estimate\n    ratio = est / published\n    print(f'{name:<10} {d:<10} {n:<6} {est:<12.1f} {published:<15} {ratio:.2f}x')\nprint()\nprint(f'DiT-XL actual (from loaded model): {total_params:,} ({total_params / 1e6:.1f}M)')\nprint()\nprint('The estimates are rough (they ignore embeddings, final layer, biases),')\nprint('but they show the scaling pattern: doubling d_model quadruples parameters')\nprint('per block (because of d_model^2 terms). Doubling N doubles total parameters.')\nprint('Two knobs, predictable scaling.')\n```\n\nThe estimates will not match exactly because they ignore the patch embedder, positional embeddings, final layer norm, and output projection. But the ratios between model sizes should be approximately correct, demonstrating the systematic scaling recipe.\n\n**Common mistakes:**\n- Expecting the parameter estimates to be exact. The `18 * d_model^2` formula is a rough approximation. The actual count includes biases, embedding layers, and the final output head.\n- Confusing the patchify projection (which may use Conv2d with kernel_size=patch_size and stride=patch_size) with convolutions inside the transformer blocks. The patchify Conv2d is equivalent to the flattened-patch linear projection—it is just the input embedding, not a spatial operation in the denoising backbone.\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Cleanup before Exercise 4 ---\nfree_memory(unet, dit_model, dit_pipe)\ngc.collect()\nprint('Ready for Exercise 4.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### What Just Happened\n\nYou loaded a real DiT model and a real U-Net as proper model objects and compared them with the same inspection tools. Key observations:\n\n- **DiT has ~675M parameters, U-Net has ~860M.** Different architectures, similar scale. DiT achieves better results with fewer parameters.\n\n- **The U-Net is dominated by Conv2d layers.** `count_parameters_by_type()` shows convolutions at every resolution level as the primary spatial processing operation. GroupNorm normalizes within each conv block.\n\n- **DiT is dominated by Linear layers.** The same `count_parameters_by_type()` function on the DiT model shows Linear layers for Q/K/V projections, FFN, and adaLN-Zero MLP. No Conv2d inside the transformer blocks.\n\n- **The adaLN modulation layer is clearly visible** in each DiT block. It is the MLP that takes the conditioning vector and produces the six modulation parameters (γ₁, β₁, α₁, γ₂, β₂, α₂).\n\n- **DiT scaling is systematic.** From DiT-S to DiT-XL, parameter count grows predictably with `d_model^2 * N`. Two knobs, not twenty.\n\n- **Symmetric comparison matters.** Loading both models as `nn.Module` objects means the same inspection functions (`count_parameters_by_type`, `named_modules`) work on both. This makes the architectural differences concrete and verifiable.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Generate with DiT `[Independent]`\n",
    "\n",
    "From the lesson: DiT-XL/2 (patch size 2, 675M params) achieved FID 2.27 on ImageNet 256×256 class-conditional generation—state-of-the-art at the time. The scaling argument is not just theory—you can see it in generated images.\n",
    "\n",
    "DiT is class-conditional on ImageNet. It uses class labels (integers 0–1000), not text prompts. Classifier-free guidance works the same way: amplify the difference between the class-conditioned and unconditional predictions.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "1. **Load DiT-XL/2** using the HuggingFace `DiTPipeline` from `diffusers`\n",
    "2. **Generate class-conditional images** for at least two different ImageNet classes (e.g., golden retriever=207, macaw=88, volcano=980, castle=483)\n",
    "3. **Vary the classifier-free guidance scale** (e.g., 1.0, 1.5, 4.0, 7.5) for the same class and seed\n",
    "4. **Compare the results**: observe how guidance scale affects quality, detail, and diversity\n",
    "5. **Bonus**: vary the number of sampling steps and observe the quality-speed tradeoff\n",
    "\n",
    "### Hints\n",
    "\n",
    "- The pipeline class is `DiTPipeline` from `diffusers`\n",
    "- The model ID is `\"facebook/DiT-XL-2-256\"`\n",
    "- Generation uses `pipe(class_labels=[207], num_inference_steps=50, guidance_scale=4.0)`\n",
    "- `class_labels` takes a list of ImageNet class indices\n",
    "- Use `generator=torch.Generator(device='cpu').manual_seed(42)` for reproducibility\n",
    "- DiT-XL/2 needs ~3 GB VRAM in float16. Use `torch_dtype=torch.float16` when loading\n",
    "- Some useful ImageNet classes: 207 (golden retriever), 88 (macaw), 980 (volcano), 483 (castle), 388 (panda), 279 (arctic fox), 33 (loggerhead turtle), 250 (husky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 4: Generate class-conditional images with DiT-XL/2\n",
    "# ============================================================\n",
    "#\n",
    "# Load the DiT pipeline and generate images.\n",
    "# Compare different class labels and guidance scales.\n",
    "#\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Your guidance scale comparison ---\n",
    "#\n",
    "# Pick one class, keep the seed fixed, vary guidance_scale.\n",
    "# Use show_image_row() to display the results.\n",
    "#\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (Bonus) Your step count comparison ---\n",
    "#\n",
    "# Pick one class and guidance scale, vary num_inference_steps.\n",
    "# Try steps: 10, 25, 50, 100\n",
    "#\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The key insight is that DiT generation looks just like any other diffusion pipeline—the only difference is the denoising backbone. The pipeline handles patchify/unpatchify, adaLN-Zero conditioning, and sampling internally. You provide a class label instead of a text prompt.\n",
    "\n",
    "```python\n",
    "from diffusers import DiTPipeline\n",
    "\n",
    "# --- Load DiT-XL/2 ---\n",
    "print('Loading DiT-XL/2...')\n",
    "pipe = DiTPipeline.from_pretrained(\n",
    "    'facebook/DiT-XL-2-256',\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "print('DiT-XL/2 loaded.')\n",
    "\n",
    "# --- Generate multiple classes ---\n",
    "classes = {\n",
    "    207: 'Golden Retriever',\n",
    "    88: 'Macaw',\n",
    "    980: 'Volcano',\n",
    "    279: 'Arctic Fox',\n",
    "}\n",
    "\n",
    "class_images = []\n",
    "class_titles = []\n",
    "for class_id, class_name in classes.items():\n",
    "    generator = torch.Generator(device='cpu').manual_seed(42)\n",
    "    result = pipe(\n",
    "        class_labels=[class_id],\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=4.0,\n",
    "        generator=generator,\n",
    "    )\n",
    "    class_images.append(result.images[0])\n",
    "    class_titles.append(f'Class {class_id}\\n{class_name}')\n",
    "    print(f'  Generated class {class_id} ({class_name})')\n",
    "\n",
    "show_image_row(\n",
    "    class_images, class_titles,\n",
    "    suptitle='DiT-XL/2: Class-Conditional ImageNet Generation (guidance=4.0, 50 steps)',\n",
    "    figsize=(20, 5),\n",
    ")\n",
    "\n",
    "# --- Guidance scale comparison ---\n",
    "guidance_scales = [1.0, 1.5, 4.0, 7.5]\n",
    "cfg_images = []\n",
    "cfg_titles = []\n",
    "\n",
    "for gs in guidance_scales:\n",
    "    generator = torch.Generator(device='cpu').manual_seed(42)\n",
    "    result = pipe(\n",
    "        class_labels=[207],  # golden retriever\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=gs,\n",
    "        generator=generator,\n",
    "    )\n",
    "    cfg_images.append(result.images[0])\n",
    "    cfg_titles.append(f'cfg={gs}')\n",
    "    print(f'  Generated guidance_scale={gs}')\n",
    "\n",
    "show_image_row(\n",
    "    cfg_images, cfg_titles,\n",
    "    suptitle='DiT-XL/2: Golden Retriever (class 207) at Different Guidance Scales',\n",
    "    figsize=(20, 5),\n",
    ")\n",
    "\n",
    "print('Observations:')\n",
    "print('  cfg=1.0: No guidance. Diverse but potentially lower quality.')\n",
    "print('  cfg=1.5: Mild guidance. Good balance of diversity and quality.')\n",
    "print('  cfg=4.0: Standard DiT guidance. Sharp, detailed, class-typical.')\n",
    "print('  cfg=7.5: Strong guidance. Very class-typical but may oversaturate.')\n",
    "\n",
    "# --- (Bonus) Step count comparison ---\n",
    "step_counts = [10, 25, 50, 100]\n",
    "step_images = []\n",
    "step_titles = []\n",
    "\n",
    "for steps in step_counts:\n",
    "    generator = torch.Generator(device='cpu').manual_seed(42)\n",
    "    start = time.time()\n",
    "    result = pipe(\n",
    "        class_labels=[207],\n",
    "        num_inference_steps=steps,\n",
    "        guidance_scale=4.0,\n",
    "        generator=generator,\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    step_images.append(result.images[0])\n",
    "    step_titles.append(f'{steps} steps\\n{elapsed:.1f}s')\n",
    "    print(f'  {steps} steps: {elapsed:.1f}s')\n",
    "\n",
    "show_image_row(\n",
    "    step_images, step_titles,\n",
    "    suptitle='DiT-XL/2: Quality vs Steps (Golden Retriever, cfg=4.0)',\n",
    "    figsize=(20, 5),\n",
    ")\n",
    "```\n",
    "\n",
    "**Key observations:**\n",
    "- DiT generates high-quality class-conditional images. The golden retriever, macaw, and volcano should be clearly recognizable and well-composed.\n",
    "- Guidance scale has a significant effect. At cfg=1.0, images are diverse but may lack coherence. At cfg=4.0 (the paper's recommended value), images are sharp and class-typical. At cfg=7.5, oversaturation may appear—same phenomenon as with U-Net models.\n",
    "- More steps generally means better quality, but with diminishing returns. The jump from 10 to 25 steps is dramatic; from 50 to 100 is subtle.\n",
    "- The pipeline API looks nearly identical to Stable Diffusion—`class_labels` instead of `prompt` is the main difference. This confirms the lesson: DiT replaces only the denoising network. The sampling loop, scheduler, and output processing are the same.\n",
    "\n",
    "**Common mistakes:**\n",
    "- Forgetting `torch_dtype=torch.float16` when loading. DiT-XL/2 in float32 needs ~6 GB VRAM, which may exceed Colab's free T4.\n",
    "- Using text prompts instead of class labels. DiT is class-conditional, not text-conditional. Text conditioning comes in the next lesson (SD3/Flux).\n",
    "- Not setting a manual seed for reproducibility. Without a fixed seed, each generation is different, making guidance scale comparisons meaningless.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Patchify is reshaping + linear projection, not magic.** Split the latent `[C, H, W]` into `(H/p) * (W/p)` patches of size `[C, p, p]`, flatten to `C*p*p` dimensions, project to `d_model`. The round-trip is lossless. This is the image equivalent of text tokenization—the transformer processes both kinds of sequences identically.\n",
    "\n",
    "2. **adaLN-Zero = adaptive norm + zero-initialized gate.** Six parameters per block from one conditioning vector: scale (γ), shift (β), and gate (α) for each sub-layer. At initialization, alpha=0 makes every block an identity function. The model learns what each block should contribute. Same zero-initialization safety pattern as ControlNet and LoRA.\n",
    "\n",
    "3. **DiT has no convolutions in its transformer blocks.** Only Linear layers for Q/K/V projections, FFN, and adaLN-Zero MLP. The U-Net is dominated by Conv2d. DiT removes convolutions, encoder-decoder hierarchy, and skip connections—and gets better results at sufficient scale.\n",
    "\n",
    "4. **Two knobs, predictable scaling.** From DiT-S (33M) to DiT-XL (675M): increase d_model and N. Parameter count scales as `d_model^2 * N`. No ad hoc decisions about channels, resolutions, or where to add attention. The same recipe that scaled GPT-2 to GPT-3.\n",
    "\n",
    "5. **Same pipeline, different denoising network.** DiT-XL/2 generates through the same sampling loop as every diffusion model you have seen. Class labels instead of text prompts. Guidance scale works the same way. The VAE, scheduler, and output processing are unchanged. Only the middle box—the denoising network—is different."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}