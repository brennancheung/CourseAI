{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Denoise\n",
    "\n",
    "**Module 6.2, Lesson 3** | CourseAI\n",
    "\n",
    "In this notebook you will use the closed-form formula from The Forward Process to create noisy images, compute the DDPM training loss by hand, write the training algorithm in code, and reason about which noise levels are hardest for the model.\n",
    "\n",
    "**What you'll do:**\n",
    "- Create noisy versions of Fashion-MNIST images at different timesteps using the closed-form formula\n",
    "- Compute MSE loss between \"predicted\" noise and actual noise, both manually and with `nn.MSELoss`\n",
    "- Fill in the DDPM training loop skeleton â€” the data preparation and loss computation steps\n",
    "- Reason about the loss landscape: which timesteps are hardest for the model, and why\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "**Estimated time:** 20-30 minutes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to import everything and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Setup: Data and Noise Schedule\n",
    "\n",
    "We load Fashion-MNIST and define the noise schedule. These are the tools you will use in every exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST (same dataset from the web lesson examples)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize to [-1, 1] â€” standard for diffusion models\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Class names for labels\n",
    "class_names = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]\n",
    "\n",
    "print(f'Dataset size: {len(dataset)}')\n",
    "print(f'Image shape: {dataset[0][0].shape}')\n",
    "print(f'Pixel range: [{dataset[0][0].min():.1f}, {dataset[0][0].max():.1f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the noise schedule\n",
    "# We use a cosine schedule (Nichol & Dhariwal 2021), which preserves\n",
    "# more signal at early timesteps than a linear schedule.\n",
    "\n",
    "T = 1000  # Total timesteps\n",
    "\n",
    "def cosine_alpha_bar_schedule(T, s=0.008):\n",
    "    \"\"\"Compute alpha_bar for each timestep using the cosine schedule.\n",
    "\n",
    "    alpha_bar_t represents how much of the original signal remains at step t.\n",
    "    At t=0, alpha_bar ~ 1 (all signal). At t=T, alpha_bar ~ 0 (all noise).\n",
    "    \"\"\"\n",
    "    steps = torch.arange(T + 1, dtype=torch.float32)\n",
    "    f = torch.cos(((steps / T) + s) / (1 + s) * (math.pi / 2)) ** 2\n",
    "    alpha_bar = f / f[0]\n",
    "    return alpha_bar\n",
    "\n",
    "# alpha_bar[t] = cumulative signal fraction at timestep t\n",
    "# Index 0 = clean, index T = nearly pure noise\n",
    "alpha_bar = cosine_alpha_bar_schedule(T)\n",
    "\n",
    "print(f'alpha_bar shape: {alpha_bar.shape}')\n",
    "print(f'alpha_bar[0]   = {alpha_bar[0]:.4f}  (clean image â€” nearly all signal)')\n",
    "print(f'alpha_bar[500] = {alpha_bar[500]:.4f}  (midpoint â€” signal-to-noise dial halfway)')\n",
    "print(f'alpha_bar[999] = {alpha_bar[999]:.6f}  (nearly pure noise)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Create Noisy Images (Guided)\n",
    "\n",
    "This is the **data preparation** step of the DDPM training algorithm. Given a clean image $x_0$, a timestep $t$, and sampled noise $\\epsilon$, you create the noisy image using the closed-form formula:\n",
    "\n",
    "$$x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon$$\n",
    "\n",
    "You derived this formula in The Forward Process. Now you use it as a tool.\n",
    "\n",
    "**Before running, predict:**\n",
    "- At $t = 100$, most of the signal remains. What will the image look like? Will you clearly see the clothing item?\n",
    "- At $t = 500$, the midpoint. What do you expect?\n",
    "- At $t = 800$, almost all signal is gone. Can you still recognize anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a T-shirt image (index 0 in Fashion-MNIST is a T-shirt/top)\n",
    "x_0, label = dataset[0]\n",
    "print(f'Image: {class_names[label]}')\n",
    "print(f'Shape: {x_0.shape}')  # [1, 28, 28]\n",
    "\n",
    "# Sample noise (same shape as the image)\n",
    "torch.manual_seed(42)\n",
    "epsilon = torch.randn_like(x_0)\n",
    "\n",
    "# Create noisy images at different timesteps\n",
    "timesteps = [0, 100, 300, 500, 700, 900]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(timesteps), figsize=(15, 3))\n",
    "\n",
    "for i, t in enumerate(timesteps):\n",
    "    # The closed-form formula: x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
    "    signal_coeff = torch.sqrt(alpha_bar[t])\n",
    "    noise_coeff = torch.sqrt(1 - alpha_bar[t])\n",
    "    x_t = signal_coeff * x_0 + noise_coeff * epsilon\n",
    "\n",
    "    # Display\n",
    "    axes[i].imshow(x_t.squeeze(), cmap='gray', vmin=-2, vmax=2)\n",
    "    axes[i].set_title(f't={t}\\n\\u03b1\\u0305={alpha_bar[t]:.3f}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "    # Print the coefficients so you can see the signal/noise balance\n",
    "    print(f't={t:>3d} | signal coeff: {signal_coeff:.3f} | noise coeff: {noise_coeff:.3f} | \\u03b1\\u0305_t: {alpha_bar[t]:.4f}')\n",
    "\n",
    "plt.suptitle('The same T-shirt at different noise levels', y=1.05, fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You used the closed-form formula as a tool â€” the same formula you derived in The Forward Process. Each timestep $t$ dials the signal-to-noise ratio:\n",
    "\n",
    "- **t=0:** $\\bar{\\alpha}_0 \\approx 1$. Nearly all signal, no noise. The T-shirt is crystal clear.\n",
    "- **t=100:** $\\bar{\\alpha}_{100}$ is still high. A light dusting of static â€” the T-shirt is easily recognizable.\n",
    "- **t=500:** Around the midpoint. Signal and noise are mixing. You can see a vague shape.\n",
    "- **t=900:** $\\bar{\\alpha}_{900}$ is nearly zero. Almost pure noise. The T-shirt is barely visible (if at all).\n",
    "\n",
    "During training, the network sees noisy images across this entire range. At low $t$ it must detect subtle perturbations. At high $t$ it must hallucinate plausible structure from near-pure static. **Same algorithm, same loss, vastly different difficulty.**\n",
    "\n",
    "Notice: we jumped directly to each timestep. No iterating through intermediate steps. That is the power of the closed-form formula â€” it lets training \"teleport\" to any noise level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Compute the Loss by Hand (Guided)\n",
    "\n",
    "The DDPM training loss is:\n",
    "\n",
    "$$L = \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 = \\frac{1}{n} \\sum_{i=1}^{n} (\\epsilon_i - \\hat{\\epsilon}_i)^2$$\n",
    "\n",
    "That is MSE loss. The same formula from Series 1 (Loss Functions), the same `nn.MSELoss` from Series 2 (Training Loop). The only difference is what goes in: the actual noise $\\epsilon$ and the network's predicted noise $\\hat{\\epsilon}$.\n",
    "\n",
    "We do not have a trained denoising network yet (that is the capstone lesson). Instead, we will use **random noise as the prediction** â€” a stand-in for an untrained network that guesses randomly.\n",
    "\n",
    "**Before running, predict:**\n",
    "- If the \"predicted\" noise is random (no relation to the actual noise), will the MSE be close to 0, close to 1, or close to 2?\n",
    "- Hint: both $\\epsilon$ and $\\hat{\\epsilon}$ are drawn from $\\mathcal{N}(0, 1)$. The variance of their difference is the sum of their variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the T-shirt image from Exercise 1\n",
    "x_0, label = dataset[0]\n",
    "\n",
    "# Step 1: Pick a timestep\n",
    "t = 500\n",
    "\n",
    "# Step 2: Sample the actual noise (the \"answer key\")\n",
    "torch.manual_seed(123)\n",
    "epsilon = torch.randn_like(x_0)\n",
    "\n",
    "# Step 3: Create the noisy image using the closed-form formula\n",
    "x_t = torch.sqrt(alpha_bar[t]) * x_0 + torch.sqrt(1 - alpha_bar[t]) * epsilon\n",
    "\n",
    "# Step 4: Simulate a network prediction\n",
    "# A real network would take x_t and t as inputs and output its noise prediction.\n",
    "# We do not have a trained network, so we use random noise as a stand-in\n",
    "# (an untrained network's output is effectively random).\n",
    "torch.manual_seed(456)\n",
    "epsilon_hat = torch.randn_like(x_0)  # \"predicted\" noise (random guess)\n",
    "\n",
    "# Step 5: Compute MSE loss MANUALLY\n",
    "# The formula: L = (1/n) * sum((epsilon_i - epsilon_hat_i)^2)\n",
    "n = epsilon.numel()  # number of elements (1 * 28 * 28 = 784)\n",
    "squared_errors = (epsilon - epsilon_hat) ** 2\n",
    "manual_mse = squared_errors.sum() / n\n",
    "\n",
    "print(f'Number of elements: {n}')\n",
    "print(f'Manual MSE:         {manual_mse:.4f}')\n",
    "\n",
    "# Step 6: Verify with nn.MSELoss â€” the same PyTorch loss from Series 2\n",
    "criterion = nn.MSELoss()\n",
    "pytorch_mse = criterion(epsilon_hat, epsilon)\n",
    "\n",
    "print(f'nn.MSELoss:         {pytorch_mse.item():.4f}')\n",
    "print(f'Match: {torch.allclose(manual_mse, pytorch_mse)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is the MSE approximately 2?\n",
    "# Both epsilon and epsilon_hat are drawn from N(0, 1).\n",
    "# Their difference (epsilon - epsilon_hat) has:\n",
    "#   mean = 0 - 0 = 0\n",
    "#   variance = 1 + 1 = 2   (variances add for independent variables)\n",
    "# MSE = E[(epsilon - epsilon_hat)^2] = Var(epsilon - epsilon_hat) = 2\n",
    "#\n",
    "# So MSE ~ 2 is what you get from a random guess.\n",
    "# A trained model should push this MUCH lower.\n",
    "\n",
    "print('--- Why MSE ~ 2 for random predictions ---')\n",
    "print(f'Var(epsilon):                 {epsilon.var():.4f}   (should be ~1)')\n",
    "print(f'Var(epsilon_hat):             {epsilon_hat.var():.4f}   (should be ~1)')\n",
    "print(f'Var(epsilon - epsilon_hat):   {(epsilon - epsilon_hat).var():.4f}   (should be ~2)')\n",
    "print(f'MSE (= mean of squared diff): {manual_mse:.4f}   (should be ~2)')\n",
    "print()\n",
    "print('A trained network would predict epsilon_hat close to the actual epsilon,')\n",
    "print('driving MSE far below 2. That is the entire learning objective.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You computed the DDPM training loss by hand and verified it matches `nn.MSELoss`. The formula is identical to every other MSE computation you have done in this course:\n",
    "\n",
    "| Context | Prediction | Target |\n",
    "|---------|-----------|--------|\n",
    "| Linear regression (Series 1) | $\\hat{y}$ (predicted price) | $y$ (actual price) |\n",
    "| Autoencoder (Module 6.1) | $\\hat{x}$ (reconstructed image) | $x$ (input image) |\n",
    "| **DDPM (this lesson)** | $\\hat{\\epsilon}$ (predicted noise) | $\\epsilon$ (actual noise) |\n",
    "\n",
    "Same formula. Same `nn.MSELoss()`. Same gradients. Different question.\n",
    "\n",
    "The random-guess MSE of ~2 is your baseline. A trained model should do much better because it learns the relationship between the noisy image $x_t$ and the noise $\\epsilon$ that produced it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 3: Write the DDPM Training Loop (Supported)\n\nNow you will write the core of the DDPM training algorithm in code. We will not train a real model (there is no denoising network yet â€” that is the capstone lesson). Instead, you will write the **data preparation and loss computation** steps as a function.\n\n**Note:** This exercise builds on the code patterns from Exercises 1 and 2. If you skipped them, make sure to run the Shared Setup cells first and review how the closed-form formula is applied in Exercise 1 and how MSE is computed in Exercise 2.\n\nThe DDPM training algorithm has seven steps:\n1. **Sample** a training image $x_0$ from the dataset\n2. **Sample** a random timestep $t \\sim \\text{Uniform}(1, T)$\n3. **Sample** noise $\\epsilon \\sim \\mathcal{N}(0, I)$\n4. **Create** the noisy image: $x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon$\n5. **Predict**: $\\hat{\\epsilon} = \\text{network}(x_t, t)$\n6. **Compute loss**: $L = \\text{MSE}(\\hat{\\epsilon}, \\epsilon)$\n7. **Backpropagate** and update weights\n\nSteps 1-4 are the diffusion-specific data preparation. Steps 5-7 are the standard training loop you already know.\n\n**Task:** Fill in the three TODO markers below. Each is 1-2 lines of code.\n\n<details>\n<summary>ðŸ’¡ Solution</summary>\n\nThe key insight: the DDPM training loop is the standard training loop with diffusion-specific data preparation. The three TODOs are the three diffusion-specific sampling steps â€” everything else is the familiar forward-loss-backward-update pattern.\n\n```python\n# TODO 1: Sample a random timestep for each image in the batch\nt = torch.randint(1, T, (batch_size,))\n\n# TODO 2: Sample fresh noise (the \"answer key\")\nepsilon = torch.randn_like(x_0)\n\n# TODO 3: Create the noisy image using the closed-form formula\n# Gather the right alpha_bar for each image's timestep, reshape for broadcasting\nalpha_bar_t = alpha_bar[t].view(batch_size, 1, 1, 1)\nx_t = torch.sqrt(alpha_bar_t) * x_0 + torch.sqrt(1 - alpha_bar_t) * epsilon\n```\n\nNote the `.view(batch_size, 1, 1, 1)` â€” this reshapes alpha_bar from `[batch_size]` to `[batch_size, 1, 1, 1]` so it broadcasts correctly against the image tensor `[batch_size, 1, 28, 28]`. Each image in the batch gets its own alpha_bar value based on its randomly sampled timestep.\n\n</details>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpm_training_step(model, x_0, alpha_bar, T, criterion):\n",
    "    \"\"\"One step of the DDPM training algorithm.\n",
    "\n",
    "    Args:\n",
    "        model: the denoising network (takes x_t and t, returns predicted noise)\n",
    "        x_0: batch of clean images [batch_size, 1, 28, 28]\n",
    "        alpha_bar: precomputed alpha_bar schedule [T+1]\n",
    "        T: total number of timesteps\n",
    "        criterion: nn.MSELoss()\n",
    "\n",
    "    Returns:\n",
    "        loss: the MSE between predicted and actual noise\n",
    "    \"\"\"\n",
    "    batch_size = x_0.shape[0]\n",
    "\n",
    "    # Step 1: x_0 is already provided (the batch of clean images)\n",
    "\n",
    "    # Step 2: Sample a random timestep for each image in the batch\n",
    "    # Each image gets its own random t from Uniform(1, T)\n",
    "    # TODO: t = ???\n",
    "\n",
    "    # Step 3: Sample noise â€” the \"answer key\" for this training step\n",
    "    # Fresh Gaussian noise, same shape as x_0\n",
    "    # TODO: epsilon = ???\n",
    "\n",
    "    # Step 4: Create the noisy image using the closed-form formula\n",
    "    # x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
    "    # Hint: alpha_bar[t] gives a value per image. You need to reshape it\n",
    "    # to [batch_size, 1, 1, 1] so it broadcasts against [batch_size, 1, 28, 28]\n",
    "    # TODO: alpha_bar_t = ??? and x_t = ???\n",
    "\n",
    "    # Step 5: Predict the noise (forward pass)\n",
    "    epsilon_hat = model(x_t, t)\n",
    "\n",
    "    # Step 6: Compute MSE loss (same loss from Series 1)\n",
    "    loss = criterion(epsilon_hat, epsilon)\n",
    "\n",
    "    return loss\n",
    "\n",
    "print('ddpm_training_step defined.')\n",
    "print('(We cannot run it yet â€” we have no denoising network.)')\n",
    "print('But the structure is the complete DDPM training algorithm.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify your data preparation steps work with a dummy model.\n",
    "# This \"model\" just returns random noise â€” it is not a real denoising network.\n",
    "# The point is to confirm your steps 2-4 produce tensors of the right shape.\n",
    "\n",
    "class DummyModel(nn.Module):\n",
    "    \"\"\"A fake denoising network that returns random noise.\n",
    "    Real architecture comes in Module 6.3.\"\"\"\n",
    "    def forward(self, x_t, t):\n",
    "        return torch.randn_like(x_t)\n",
    "\n",
    "# Grab a small batch\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "batch_images, batch_labels = next(iter(loader))\n",
    "\n",
    "print(f'Batch shape: {batch_images.shape}')  # [8, 1, 28, 28]\n",
    "\n",
    "# Run one training step\n",
    "dummy_model = DummyModel()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = ddpm_training_step(dummy_model, batch_images, alpha_bar, T, criterion)\n",
    "print(f'Loss: {loss.item():.4f}')\n",
    "print(f'Expected: ~2.0 (random predictions vs random noise)')\n",
    "print()\n",
    "if 1.0 < loss.item() < 3.0:\n",
    "    print('Loss is in the expected range. Your training step is correct!')\n",
    "else:\n",
    "    print('Loss is outside the expected range. Check your TODOs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now see what a full training loop looks like.\n",
    "# This is the standard training pattern â€” the heartbeat hasn't changed.\n",
    "\n",
    "print('--- The Full DDPM Training Loop (pseudocode in real Python) ---')\n",
    "print()\n",
    "\n",
    "# This code WON'T run (we use a dummy model), but the structure is exactly\n",
    "# what a real implementation looks like. Compare to the training loops\n",
    "# you wrote in Series 2.\n",
    "\n",
    "dummy_model = DummyModel()\n",
    "# In a real implementation:\n",
    "#   model = UNet(...)  # the denoising architecture (Module 6.3)\n",
    "#   optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Simulate 3 \"training steps\" with our dummy model\n",
    "for step in range(3):\n",
    "    batch_images, _ = next(iter(loader))\n",
    "\n",
    "    # The DDPM training step â€” all the diffusion-specific work is here\n",
    "    loss = ddpm_training_step(dummy_model, batch_images, alpha_bar, T, criterion)\n",
    "\n",
    "    # Standard training loop continues:\n",
    "    # optimizer.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    print(f'Step {step + 1} | Loss: {loss.item():.4f}')\n",
    "\n",
    "print()\n",
    "print('The heartbeat of training (forward -> loss -> backward -> update)')\n",
    "print('is identical to every training loop since Series 2.')\n",
    "print('The only new part: how you prepare the data (steps 2-4).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You wrote the DDPM training algorithm in code. The three TODOs were the three diffusion-specific steps:\n",
    "\n",
    "1. Sample a random timestep for each image (`torch.randint`)\n",
    "2. Sample noise â€” the answer key (`torch.randn_like`)\n",
    "3. Create the noisy image using the closed-form formula\n",
    "\n",
    "Everything after that â€” the forward pass, MSE loss, backprop, optimizer step â€” is the standard training loop you have used since Series 2. The heartbeat has not changed.\n",
    "\n",
    "Key detail: each image in the batch gets its **own random timestep**. In one batch, the model might see $t=42$ for one image and $t=891$ for another. No sequential order. This is why the closed-form formula matters â€” it lets you teleport to any noise level without iterating through intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Predict the Loss Landscape (Independent)\n",
    "\n",
    "You now understand the training algorithm. Here is a question about how a **trained** model would behave:\n",
    "\n",
    "**Question:** Imagine a well-trained denoising model. If you compute the average MSE loss at each timestep separately, which timesteps will have the **highest** loss? Which will have the **lowest**?\n",
    "\n",
    "Think about what the noisy image $x_t$ looks like at different noise levels, and how hard it is for the network to predict the noise $\\epsilon$ from it.\n",
    "\n",
    "Consider three regimes:\n",
    "- **Low $t$ (e.g., $t = 50$):** The image is nearly clean, with a light dusting of noise.\n",
    "- **Middle $t$ (e.g., $t = 500$):** Roughly equal parts signal and noise.\n",
    "- **High $t$ (e.g., $t = 950$):** Nearly pure noise, almost no signal left.\n",
    "\n",
    "Write your prediction in the cell below, then run the simulation to check.\n",
    "\n",
    "**Note:** We use a dummy model to illustrate the *shape* of the loss landscape by measuring how much the noisy image \"looks like\" the noise versus the original signal. A real trained model would show a similar pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PREDICTION\n",
    "# Before running the next cell, write your prediction here:\n",
    "#\n",
    "# Which timesteps will have the HIGHEST loss (hardest for the model)?\n",
    "# Your answer: ...\n",
    "#\n",
    "# Which timesteps will have the LOWEST loss (easiest for the model)?\n",
    "# Your answer: ...\n",
    "#\n",
    "# Why?\n",
    "# Your reasoning: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's measure the difficulty at each noise level.\n",
    "#\n",
    "# We'll use a simple heuristic as a stand-in for a trained model:\n",
    "# the \"oracle\" predictor that uses the closed-form formula to estimate epsilon.\n",
    "#\n",
    "# Given x_t and x_0, you can recover epsilon exactly:\n",
    "#   epsilon = (x_t - sqrt(alpha_bar_t) * x_0) / sqrt(1 - alpha_bar_t)\n",
    "#\n",
    "# But the model doesn't have x_0! It must guess.\n",
    "# To simulate difficulty, we measure how much information about epsilon\n",
    "# is \"visible\" in x_t at each noise level. At low noise, the signal\n",
    "# dominates â€” small perturbations are hard to detect. At high noise,\n",
    "# x_t is almost pure epsilon, so the noise is \"obvious\" but the\n",
    "# signal needed to correct the prediction is buried.\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Use a batch of images\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "test_batch, _ = next(iter(test_loader))\n",
    "\n",
    "# For each timestep, compute the \"naive predictor\" loss:\n",
    "# Predict epsilon = 0 (the prior mean). This gives loss = ||epsilon||^2 / n = 1.0 everywhere.\n",
    "# A better predictor: use the noisy image itself as a clue.\n",
    "# At high t, x_t ~ epsilon, so predicting epsilon_hat = x_t / sqrt(1 - alpha_bar_t) is good.\n",
    "# At low t, x_t ~ x_0, so the noise is invisible and prediction is hard.\n",
    "\n",
    "timestep_range = torch.arange(1, T, 10)  # sample every 10th timestep\n",
    "losses_at_t = []\n",
    "\n",
    "for t_val in timestep_range:\n",
    "    t = t_val.item()\n",
    "    epsilon = torch.randn_like(test_batch)\n",
    "\n",
    "    ab_t = alpha_bar[t]\n",
    "    x_t = torch.sqrt(ab_t) * test_batch + torch.sqrt(1 - ab_t) * epsilon\n",
    "\n",
    "    # Simple estimator: assume the image content is the dataset mean (~0)\n",
    "    # Then x_t ~ sqrt(1 - alpha_bar_t) * epsilon, so:\n",
    "    # epsilon_hat = x_t / sqrt(1 - alpha_bar_t)\n",
    "    # This works well at high noise but fails at low noise where x_t ~ x_0\n",
    "    noise_scale = torch.sqrt(1 - ab_t)\n",
    "    if noise_scale > 1e-6:\n",
    "        epsilon_hat = x_t / noise_scale\n",
    "    else:\n",
    "        epsilon_hat = torch.zeros_like(epsilon)\n",
    "\n",
    "    mse = nn.functional.mse_loss(epsilon_hat, epsilon).item()\n",
    "    losses_at_t.append(mse)\n",
    "\n",
    "# Plot the loss landscape\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(timestep_range.numpy(), losses_at_t, color='#c084fc', linewidth=2)\n",
    "ax.set_xlabel('Timestep t', fontsize=12)\n",
    "ax.set_ylabel('MSE Loss', fontsize=12)\n",
    "ax.set_title('Denoising difficulty across timesteps', fontsize=13)\n",
    "ax.axhline(y=2.0, color='gray', linestyle='--', alpha=0.5, label='Random guess baseline (MSE=2)')\n",
    "\n",
    "# Annotate the regions\n",
    "ax.annotate('Low noise\\n(subtle perturbations)', xy=(50, losses_at_t[4]),\n",
    "            fontsize=9, ha='left', color='#86efac')\n",
    "ax.annotate('High noise\\n(almost pure noise)', xy=(850, losses_at_t[-15]),\n",
    "            fontsize=9, ha='right', color='#fca5a5')\n",
    "\n",
    "ax.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Loss at t=10:  {losses_at_t[0]:.2f}  (low noise â€” hard to detect subtle perturbations)')\n",
    "print(f'Loss at t=500: {losses_at_t[len(losses_at_t)//2]:.2f}  (medium noise â€” mixed signal and noise)')\n",
    "print(f'Loss at t=990: {losses_at_t[-1]:.2f}  (high noise â€” noise dominates, easier to estimate)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "**Middle timesteps tend to have the highest loss.** Here is the reasoning for each regime:\n",
    "\n",
    "**Low $t$ (easy in a different way):** The image is nearly clean. The noise is a tiny perturbation â€” like a light dusting of static on a clear photo. The network can see the image clearly, but the noise is so subtle that precisely predicting every noise value is difficult. However, the noise *magnitude* is small, so the squared errors are also small. Low MSE.\n",
    "\n",
    "**High $t$ (somewhat predictable):** The image is almost pure noise. There is very little signal left. But this means $x_t \\approx \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon$, so the noisy image *is* mostly the noise (scaled). The network can make a reasonable guess just from $x_t$ itself. Moderate MSE.\n",
    "\n",
    "**Middle $t$ (hardest):** This is where signal and noise are roughly balanced. The network cannot ignore either component. It must simultaneously understand the image content (to subtract the signal) AND estimate the noise pattern. This ambiguity creates the most room for error. Highest MSE.\n",
    "\n",
    "The intuition: at the extremes, one of the two components (signal or noise) dominates and the problem simplifies. In the middle, neither dominates â€” maximum ambiguity, maximum difficulty.\n",
    "\n",
    "Note: In practice, a real trained model's loss landscape is shaped by the specific architecture, dataset, and training dynamics. The general pattern (middle timesteps being hardest) is widely observed, though the exact curve varies.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The closed-form formula is a training tool.** You use $x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon$ to create noisy images at arbitrary timesteps â€” no iterating through intermediate steps.\n",
    "\n",
    "2. **The DDPM loss is MSE.** The same formula, the same `nn.MSELoss()`, the same gradients as every MSE computation since Series 1. The only difference: prediction = predicted noise, target = actual noise.\n",
    "\n",
    "3. **The training loop is the standard loop with diffusion-specific data preparation.** Steps 1-4 (sample image, sample timestep, sample noise, create noisy image) are new. Steps 5-7 (forward pass, loss, backward, update) are the same heartbeat from Series 2.\n",
    "\n",
    "4. **Each image in a batch gets a random timestep.** No sequential order. The closed-form formula lets training teleport to any noise level efficiently.\n",
    "\n",
    "5. **Difficulty varies with the timestep.** Low noise = subtle perturbations. High noise = hallucinating structure. Middle noise = maximum ambiguity. Same algorithm handles all of them.\n",
    "\n",
    "**Mental model:** Same building blocks, different question. The building blocks: MSE loss, backprop, gradient descent. The question: \"what noise was added to this image?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}