{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Saving and Loading Models\n\nIn this notebook, you'll practice saving and loading PyTorch models â€” the essential skill that lets you keep trained models, resume interrupted training, and implement early stopping.\n\n**What you'll do:**\n- Save and load a trained MNIST model, verifying predictions match\n- Add checkpointing to a training loop\n- Simulate a training crash and resume from a checkpoint\n- Implement the full early stopping pattern\n\n**For each exercise, PREDICT the output before running the cell.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport os\n\n# Reproducible results\ntorch.manual_seed(42)\n\n# Use GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n# For nice plots\nplt.style.use('dark_background')\nplt.rcParams['figure.figsize'] = [10, 4]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Train an MNIST Model\n",
    "\n",
    "First we need a trained model to work with. This is the same MNISTClassifier architecture from your PyTorch training lesson: Flatten â†’ Linear(784, 256) â†’ ReLU â†’ Linear(256, 128) â†’ ReLU â†’ Linear(128, 10). We'll train it for 3 epochs so it's not random but also trains quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=False\n",
    ")\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "print(f'Parameters: {sum(p.numel() for p in MNISTClassifier().parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, criterion):\n",
    "    \"\"\"Train for one epoch. Returns average loss.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    return running_loss / n_batches\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    \"\"\"Evaluate model on test set. Returns (loss, accuracy).\"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return running_loss / n_batches, 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 3 epochs\n",
    "model = MNISTClassifier().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(3):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, test_loader)\n",
    "    print(f'Epoch {epoch+1}/3 | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "print('\\nModel trained. Ready for exercises.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 1: Save and Load a Trained Model (Supported)\n\nThe simplest form of saving: capture a model's learned weights (`state_dict`) to a file, then load them into a new model instance.\n\n**Before running, predict:** After saving and loading the state dict into a brand-new model, will `torch.allclose()` return True or False? Will the file size be larger or smaller than 1 MB?\n\n**Task:**\n1. Save `model.state_dict()` to a file\n2. Create a brand-new `MNISTClassifier` instance (with random weights)\n3. Load the saved state dict into it\n4. Run both models on the same test batch\n5. Use `torch.allclose()` to verify the outputs match exactly\n\nFill in the blanks below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for saved models\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# Step 1: Save the trained model's state dict\n",
    "save_path = 'saved_models/mnist_classifier.pth'\n",
    "____  # FILL IN: save model.state_dict() to save_path using torch.save()\n",
    "\n",
    "# Print file size\n",
    "file_size = os.path.getsize(save_path)\n",
    "print(f'Saved model to {save_path}')\n",
    "print(f'File size: {file_size / 1024:.1f} KB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a new model with random weights\n",
    "loaded_model = MNISTClassifier().to(device)\n",
    "\n",
    "# Step 3: Load the saved state dict into the new model\n",
    "____  # FILL IN: load the state dict from save_path and load it into loaded_model\n",
    "# Hint: torch.load() returns the state dict, then use loaded_model.load_state_dict()\n",
    "\n",
    "print('State dict loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Get a test batch and run both models\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "model.eval()\n",
    "loaded_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    original_outputs = model(test_images)\n",
    "    loaded_outputs = loaded_model(test_images)\n",
    "\n",
    "# Step 5: Verify they match\n",
    "match = torch.allclose(original_outputs, loaded_outputs)\n",
    "\n",
    "if match:\n",
    "    print('Predictions match!')\n",
    "else:\n",
    "    print('Predictions DO NOT match. Something went wrong.')\n",
    "\n",
    "# Show a few predictions side by side\n",
    "print(f'\\nFirst 5 predictions (original): {original_outputs.argmax(dim=1)[:5].tolist()}')\n",
    "print(f'First 5 predictions (loaded):   {loaded_outputs.argmax(dim=1)[:5].tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\n**Why this works:** `state_dict()` captures every learned parameter as a plain dictionary. `torch.save()` serializes it, and `torch.load()` + `load_state_dict()` restores the exact same values â€” so outputs match exactly.\n\n```python\n# Step 1: Save\ntorch.save(model.state_dict(), save_path)\n\n# Step 3: Load\nstate_dict = torch.load(save_path, map_location=device, weights_only=True)\nloaded_model.load_state_dict(state_dict)\n```\n\n**Key points:**\n- `model.state_dict()` returns a dictionary mapping layer names to their parameter tensors.\n- `torch.save()` serializes any Python object (dicts, tensors, etc.) to a file.\n- `torch.load()` deserializes it back. `map_location=device` ensures tensors go to the right device.\n- `weights_only=True` is a security best practice â€” it prevents loading arbitrary Python objects.\n- `.pth` is the conventional file extension for PyTorch saved files.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 2: Add Checkpointing to a Training Loop (Supported)\n\nSaving just the model weights is fine for deployment, but during training you need **checkpoints** â€” snapshots that include everything needed to resume: model weights, optimizer state, epoch number, and loss.\n\n**Task:**\n1. Add checkpoint saving every 5 epochs (`if (epoch+1) % 5 == 0`)\n2. Track the best validation loss seen so far\n3. Save the best model whenever validation loss improves\n4. Each checkpoint should be a dict with keys: `'model_state_dict'`, `'optimizer_state_dict'`, `'epoch'`, `'loss'`\n\n**Hints:**\n- Initialize `best_loss = float('inf')` before the loop\n- Check `if val_loss < best_loss:` after each epoch\n- Use different filenames for periodic vs best checkpoints"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fresh model and optimizer\n",
    "model_2 = MNISTClassifier().to(device)\n",
    "optimizer_2 = optim.Adam(model_2.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 15\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_one_epoch(model_2, train_loader, optimizer_2, criterion)\n",
    "    val_loss, val_acc = evaluate(model_2, test_loader)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%', end='')\n",
    "\n",
    "    # TODO: Save checkpoint every 5 epochs\n",
    "    # The checkpoint dict should have keys:\n",
    "    #   'model_state_dict', 'optimizer_state_dict', 'epoch', 'loss'\n",
    "    # Save to: f'saved_models/checkpoint_epoch_{epoch+1}.pth'\n",
    "\n",
    "    # TODO: Save best model when val_loss improves\n",
    "    # Save to: 'saved_models/best_model.pth'\n",
    "    # Print ' <- best!' when saving\n",
    "\n",
    "    print()  # newline\n",
    "\n",
    "print(f'\\nBest validation loss: {best_loss:.4f}')\n",
    "\n",
    "# Show what files we created\n",
    "print('\\nSaved files:')\n",
    "for f in sorted(os.listdir('saved_models')):\n",
    "    size = os.path.getsize(f'saved_models/{f}')\n",
    "    print(f'  {f} ({size / 1024:.1f} KB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\n**The key insight:** A checkpoint is just a dictionary with everything needed to resume. The two separate saves (periodic + best) serve different purposes â€” periodic checkpoints let you go back to any point, the best checkpoint is what you deploy.\n\n```python\n    # Save checkpoint every 5 epochs\n    if (epoch + 1) % 5 == 0:\n        checkpoint = {\n            'model_state_dict': model_2.state_dict(),\n            'optimizer_state_dict': optimizer_2.state_dict(),\n            'epoch': epoch,\n            'loss': val_loss,\n        }\n        torch.save(checkpoint, f'saved_models/checkpoint_epoch_{epoch+1}.pth')\n        print(f' [checkpoint saved]', end='')\n\n    # Save best model when val_loss improves\n    if val_loss < best_loss:\n        best_loss = val_loss\n        checkpoint = {\n            'model_state_dict': model_2.state_dict(),\n            'optimizer_state_dict': optimizer_2.state_dict(),\n            'epoch': epoch,\n            'loss': val_loss,\n        }\n        torch.save(checkpoint, 'saved_models/best_model.pth')\n        print(' <- best!', end='')\n```\n\n**Why save the optimizer state?** The optimizer (Adam) maintains running averages of gradients (momentum). If you load just the model and create a fresh optimizer, those running averages reset, causing a spike in the loss curve. Saving the optimizer state preserves smooth training.\n\n**Why save the epoch number?** So you know where you left off. When you resume, you start from `checkpoint['epoch'] + 1`, not from 0.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 3: Simulate a Training Crash and Resume (Supported)\n\nThe real test of checkpointing: can you actually resume training and get a continuous loss curve? In this exercise you'll simulate a crash by training for 10 epochs, saving a checkpoint, then creating a completely fresh model and optimizer, loading the checkpoint, and continuing for 10 more epochs.\n\n**Before running, predict:** Will the loss curve be smooth across the crash point, or will there be a visible jump? What would happen if you loaded only the model weights but NOT the optimizer state?\n\n**Task:**\n1. Train a fresh model for 10 epochs, saving a checkpoint at the end\n2. Create a brand-new model and optimizer (simulating a fresh Python session)\n3. Load the checkpoint â€” restore model weights, optimizer state, and epoch number\n4. Print the epoch and loss from the checkpoint to confirm it loaded\n5. Continue training for 10 more epochs\n6. Plot the full 20-epoch loss curve and verify there's no discontinuity\n\n**Hint:** The key to a smooth loss curve is loading *both* `model_state_dict` and `optimizer_state_dict`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Phase 1: Train for 10 epochs =====\n",
    "print('=== Phase 1: Training for 10 epochs ===')\n",
    "\n",
    "model_3 = MNISTClassifier().to(device)\n",
    "optimizer_3 = optim.Adam(model_3.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "losses_phase1 = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss = train_one_epoch(model_3, train_loader, optimizer_3, criterion)\n",
    "    val_loss, val_acc = evaluate(model_3, test_loader)\n",
    "    losses_phase1.append(val_loss)\n",
    "    print(f'Epoch {epoch+1}/10 | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "# Save checkpoint at the end of phase 1\n",
    "checkpoint = {\n",
    "    'model_state_dict': model_3.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_3.state_dict(),\n",
    "    'epoch': 9,  # 0-indexed, so epoch 9 = 10th epoch\n",
    "    'loss': losses_phase1[-1],\n",
    "}\n",
    "torch.save(checkpoint, 'saved_models/crash_checkpoint.pth')\n",
    "print(f'\\nCheckpoint saved after epoch 10.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Phase 2: Simulate crash â€” start fresh and resume =====\n",
    "print('=== Phase 2: Simulating crash â€” creating fresh model and optimizer ===')\n",
    "\n",
    "# Create completely fresh model and optimizer (as if Python restarted)\n",
    "model_resumed = MNISTClassifier().to(device)\n",
    "optimizer_resumed = optim.Adam(model_resumed.parameters(), lr=1e-3)\n",
    "\n",
    "# TODO: Load the checkpoint from 'saved_models/crash_checkpoint.pth'\n",
    "# checkpoint = ...\n",
    "\n",
    "# TODO: Restore model weights and optimizer state\n",
    "# model_resumed.load_state_dict(...)\n",
    "# optimizer_resumed.load_state_dict(...)\n",
    "\n",
    "# TODO: Get the starting epoch and loss from the checkpoint\n",
    "# start_epoch = ...\n",
    "# print(f'Resumed from epoch {start_epoch + 1}, loss: {checkpoint[\"loss\"]:.4f}')\n",
    "\n",
    "# Continue training for 10 more epochs\n",
    "losses_phase2 = []\n",
    "\n",
    "for epoch in range(start_epoch + 1, start_epoch + 11):\n",
    "    train_loss = train_one_epoch(model_resumed, train_loader, optimizer_resumed, criterion)\n",
    "    val_loss, val_acc = evaluate(model_resumed, test_loader)\n",
    "    losses_phase2.append(val_loss)\n",
    "    print(f'Epoch {epoch+1}/20 | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "print('\\nTraining complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the full 20-epoch loss curve\n",
    "all_losses = losses_phase1 + losses_phase2\n",
    "epochs = range(1, 21)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(list(epochs)[:10], losses_phase1, 'o-', label='Phase 1 (before crash)', linewidth=2)\n",
    "plt.plot(list(epochs)[10:], losses_phase2, 's-', label='Phase 2 (after resume)', linewidth=2)\n",
    "plt.axvline(x=10.5, color='red', linestyle='--', alpha=0.7, label='Crash / Resume point')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Loss Curve Across Training Crash')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Check for discontinuity\n",
    "gap = abs(losses_phase2[0] - losses_phase1[-1])\n",
    "print(f'Loss before crash:  {losses_phase1[-1]:.4f}')\n",
    "print(f'Loss after resume:  {losses_phase2[0]:.4f}')\n",
    "print(f'Gap: {gap:.4f}')\n",
    "\n",
    "if gap < 0.05:\n",
    "    print('Loss curve is continuous â€” checkpoint resume worked!')\n",
    "else:\n",
    "    print('Significant gap detected â€” did you load the optimizer state?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\n**The key insight:** A smooth loss curve requires restoring both the model AND the optimizer. Without the optimizer state, Adam's momentum estimates reset and the loss jumps.\n\n```python\n# Load the checkpoint\ncheckpoint = torch.load('saved_models/crash_checkpoint.pth', map_location=device, weights_only=True)\n\n# Restore model weights and optimizer state\nmodel_resumed.load_state_dict(checkpoint['model_state_dict'])\noptimizer_resumed.load_state_dict(checkpoint['optimizer_state_dict'])\n\n# Get the starting epoch\nstart_epoch = checkpoint['epoch']\nprint(f'Resumed from epoch {start_epoch + 1}, loss: {checkpoint[\"loss\"]:.4f}')\n```\n\n**Why the optimizer state matters:** Try commenting out `optimizer_resumed.load_state_dict(...)` and watch the gap at epoch 10. Adam maintains per-parameter momentum and variance estimates. Without them, the optimizer \"forgets\" how it was navigating the loss landscape and has to re-learn â€” causing a visible discontinuity.\n\n**The resume pattern:**\n1. Create fresh model and optimizer (same architecture and hyperparameters)\n2. `torch.load()` the checkpoint dict\n3. `model.load_state_dict(checkpoint['model_state_dict'])`\n4. `optimizer.load_state_dict(checkpoint['optimizer_state_dict'])`\n5. `start_epoch = checkpoint['epoch']`\n6. Resume the loop from `start_epoch + 1`\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 4: Implement Early Stopping (Independent)\n\nTraining too long causes overfitting. Early stopping watches the validation loss: if it stops improving for a set number of epochs (the **patience**), training stops and the best model is restored.\n\n**Task:** Implement early stopping from scratch:\n1. Track the best validation loss and a patience counter (`patience=5`)\n2. When validation loss improves, save the best model and reset the counter\n3. When validation loss does not improve, increment the counter\n4. If the counter reaches `patience`, stop training early\n5. After training ends (naturally or early), restore the best model\n6. Print when patience runs out\n\nYou have all the building blocks from the previous exercises. This is about putting them together into a complete pattern."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fresh model\n",
    "model_4 = MNISTClassifier().to(device)\n",
    "optimizer_4 = optim.Adam(model_4.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Early stopping config\n",
    "max_epochs = 50\n",
    "patience = 5\n",
    "\n",
    "# TODO: Initialize tracking variables\n",
    "# best_val_loss = ...\n",
    "# patience_counter = ...\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_loss = train_one_epoch(model_4, train_loader, optimizer_4, criterion)\n",
    "    val_loss, val_acc = evaluate(model_4, test_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # TODO: Check if validation loss improved\n",
    "    #   - If improved: save the model, reset patience counter, update best_val_loss\n",
    "    #   - If not improved: increment patience counter\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{max_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | Patience: {patience_counter}/{patience}')\n",
    "\n",
    "    # TODO: Check if patience has run out â€” if so, print a message and break\n",
    "\n",
    "# TODO: Restore the best model\n",
    "# (load the saved best model state dict back into model_4)\n",
    "\n",
    "# Final evaluation\n",
    "final_loss, final_acc = evaluate(model_4, test_loader)\n",
    "print(f'\\nBest model restored â€” Val Loss: {final_loss:.4f} | Val Acc: {final_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "ax.plot(epochs_range, train_losses, 'o-', label='Train Loss', linewidth=2, markersize=4)\n",
    "ax.plot(epochs_range, val_losses, 's-', label='Val Loss', linewidth=2, markersize=4)\n",
    "ax.axvline(x=len(train_losses) - patience, color='green', linestyle='--', alpha=0.7, label='Best model epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Early Stopping: Training vs Validation Loss')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f'Training stopped at epoch {len(train_losses)} (max was {max_epochs})')\n",
    "print(f'Best model was from epoch {len(train_losses) - patience}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>ðŸ’¡ Solution</summary>\n\n**The key insight:** Early stopping combines three patterns you already know: tracking best loss, saving checkpoints, and restoring the best model. The patience counter is the only new piece â€” it counts how many epochs have passed without improvement.\n\n```python\n# Initialize tracking variables\nbest_val_loss = float('inf')\npatience_counter = 0\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(max_epochs):\n    train_loss = train_one_epoch(model_4, train_loader, optimizer, criterion)\n    val_loss, val_acc = evaluate(model_4, test_loader)\n\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n\n    # Check if validation loss improved\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        patience_counter = 0\n        torch.save(model_4.state_dict(), 'saved_models/early_stop_best.pth')\n    else:\n        patience_counter += 1\n\n    print(f'Epoch {epoch+1}/{max_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | Patience: {patience_counter}/{patience}')\n\n    # Check if patience has run out\n    if patience_counter >= patience:\n        print(f'\\nEarly stopping triggered! No improvement for {patience} epochs.')\n        break\n\n# Restore the best model\nbest_state = torch.load('saved_models/early_stop_best.pth', map_location=device, weights_only=True)\nmodel_4.load_state_dict(best_state)\n```\n\n**The early stopping pattern:**\n1. Set `best_val_loss = float('inf')` and `patience_counter = 0`\n2. Each epoch: if val loss improved -> save model, reset counter. If not -> increment counter.\n3. If counter reaches patience -> break.\n4. After the loop -> restore the saved best model.\n\n**Why restore the best model?** When training stops, the current model is the one from the *last* epoch â€” which had a higher val loss than the best. You want the model from the epoch with the lowest validation loss.\n\n**Patience tuning:** Too low (1-2) and you stop at normal fluctuations. Too high (20+) and you overfit before stopping. 3-10 is typical.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **`model.state_dict()`** is a dictionary of learned parameters. Save it with `torch.save()`, load it with `torch.load()` + `model.load_state_dict()`.\n2. **Checkpoints** bundle model weights, optimizer state, epoch, and loss into one dict â€” everything needed to resume training exactly where you left off.\n3. **Optimizer state matters.** Without it, resumed training shows a visible gap in the loss curve because Adam's momentum estimates reset.\n4. **Early stopping** prevents overfitting by tracking validation loss and stopping when it stops improving. Always restore the best model at the end.\n5. **The `.pth` convention** is standard for PyTorch saved files. Use `weights_only=True` when loading for security.\n\n**Clean up** (optional): delete the `saved_models/` directory when you're done experimenting."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: clean up saved files\n",
    "# import shutil\n",
    "# shutil.rmtree('saved_models', ignore_errors=True)\n",
    "# print('Cleaned up saved_models/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}