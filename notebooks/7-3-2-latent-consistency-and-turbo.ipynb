{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Consistency & Turbo\n",
    "\n",
    "**Module 7.3, Lesson 2** | CourseAI\n",
    "\n",
    "You know the theory from the lesson: Latent Consistency Models (LCM) apply the consistency distillation you already know to Stable Diffusion's latent space, and LCM-LoRA captures that distillation as a tiny plug-and-play adapter. This notebook makes that concrete.\n",
    "\n",
    "**What you will do:**\n",
    "- Load LCM-LoRA onto SD v1.5 and compare 50-step standard, 4-step standard (bad), and 4-step LCM-LoRA (good) generation with timing\n",
    "- Apply the same LCM-LoRA to a community fine-tune and verify that it generalizes — the \"one adapter, many models\" insight\n",
    "- Sweep step counts (1, 2, 4, 8) and guidance scales (1.0, 1.5, 2.0, 4.0) to find the practical sweet spots for LCM-LoRA\n",
    "- Compose LCM-LoRA with a style LoRA and compare to the style LoRA alone at 50 steps\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "Every concept in this notebook comes from the lesson. LCM as consistency distillation on latent diffusion, LCM-LoRA as a speed adapter, the reduced guidance scale, LoRA composability. No new theory — just hands-on practice with production-ready tools.\n",
    "\n",
    "**Estimated time:** 35–50 minutes. All exercises use pre-trained models (no training). Requires a GPU runtime for reasonable generation times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies and configure the environment.\n",
    "\n",
    "**Important:** Switch to a GPU runtime in Colab (Runtime → Change runtime type → T4 GPU). Generation works on CPU but is very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate safetensors peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import (\n",
    "    StableDiffusionPipeline,\n",
    "    LCMScheduler,\n",
    "    DPMSolverMultistepScheduler,\n",
    ")\n",
    "from IPython.display import display\n",
    "\n",
    "# Reproducible results\n",
    "SEED = 42\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [14, 5]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(f'Device: {device}')\n",
    "print(f'Dtype: {dtype}')\n",
    "if device.type == 'cpu':\n",
    "    print('WARNING: No GPU detected. Generation will be very slow.')\n",
    "    print('Switch to a GPU runtime: Runtime → Change runtime type → T4 GPU')\n",
    "print()\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Helpers\n",
    "\n",
    "Utility functions for timing generation, displaying image grids, and managing pipelines. Run this cell now — these are used across all four exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timed(pipe, prompt, num_inference_steps, guidance_scale, seed=SEED):\n",
    "    \"\"\"Generate an image and return (image, elapsed_seconds).\"\"\"\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    start = time.time()\n",
    "    result = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator,\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    return result.images[0], elapsed\n",
    "\n",
    "\n",
    "def show_image_row(images, titles, suptitle=None, figsize=None):\n",
    "    \"\"\"Display a row of PIL images with titles.\"\"\"\n",
    "    n = len(images)\n",
    "    fig_w = figsize[0] if figsize else max(4 * n, 12)\n",
    "    fig_h = figsize[1] if figsize else 5\n",
    "    fig, axes = plt.subplots(1, n, figsize=(fig_w, fig_h))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle, fontsize=13, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_image_grid(images, titles, nrows, ncols, suptitle=None):\n",
    "    \"\"\"Display a grid of PIL images with titles.\"\"\"\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4 * ncols, 4.5 * nrows))\n",
    "    for idx, (img, title) in enumerate(zip(images, titles)):\n",
    "        row = idx // ncols\n",
    "        col = idx % ncols\n",
    "        ax = axes[row][col] if nrows > 1 else axes[col]\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title, fontsize=9)\n",
    "        ax.axis('off')\n",
    "    # Hide unused axes\n",
    "    for idx in range(len(images), nrows * ncols):\n",
    "        row = idx // ncols\n",
    "        col = idx % ncols\n",
    "        ax = axes[row][col] if nrows > 1 else axes[col]\n",
    "        ax.axis('off')\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle, fontsize=13, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print('Helpers defined: generate_timed, show_image_row, show_image_grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: LCM-LoRA — 4-Step Generation `[Guided]`\n",
    "\n",
    "From the lesson: LCM-LoRA is a 4 MB LoRA adapter that turns any compatible SD model into a few-step model. It captures \"how to generate in 4 steps\" as a low-rank bypass — the same LoRA mechanism you know from style and subject adaptation, but capturing a *skill* instead of a *style*.\n",
    "\n",
    "We will generate three images from the same prompt and seed:\n",
    "1. **Standard SD v1.5 at 50 steps** — the baseline (good quality, slow)\n",
    "2. **Standard SD v1.5 at 4 steps** — same model, fewer steps (bad quality, fast)\n",
    "3. **SD v1.5 + LCM-LoRA at 4 steps** — same model with the speed adapter (good quality, fast)\n",
    "\n",
    "We time each generation so the speedup is concrete.\n",
    "\n",
    "**Before running, predict:**\n",
    "- What will the standard model at 4 steps look like? (Hint: the model was not trained for this — it expects 20–50 steps to refine details.)\n",
    "- The LCM-LoRA version also runs 4 steps. Will it be closer in quality to the 4-step standard or the 50-step standard?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1: Load SD v1.5 and compare generation modes\n",
    "# ============================================================\n",
    "\n",
    "PROMPT = \"a small cottage on a hillside at sunset, warm golden light, detailed\"\n",
    "\n",
    "# --- Step 1: Load SD v1.5 with standard scheduler ---\n",
    "print(\"Loading SD v1.5...\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    torch_dtype=dtype,\n",
    "    safety_checker=None,\n",
    ").to(device)\n",
    "\n",
    "# Save the original scheduler config so we can swap back and forth\n",
    "original_scheduler_config = pipe.scheduler.config\n",
    "print(\"SD v1.5 loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Generate at 50 steps (standard baseline) ---\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(original_scheduler_config)\n",
    "\n",
    "print(\"Generating: 50 steps, standard scheduler, guidance_scale=7.5...\")\n",
    "img_50step, time_50step = generate_timed(\n",
    "    pipe, PROMPT, num_inference_steps=50, guidance_scale=7.5\n",
    ")\n",
    "print(f\"  Done in {time_50step:.2f}s\")\n",
    "\n",
    "# --- Step 3: Generate at 4 steps (standard scheduler — bad quality) ---\n",
    "print(\"Generating: 4 steps, standard scheduler, guidance_scale=7.5...\")\n",
    "img_4step_bad, time_4step_bad = generate_timed(\n",
    "    pipe, PROMPT, num_inference_steps=4, guidance_scale=7.5\n",
    ")\n",
    "print(f\"  Done in {time_4step_bad:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Load LCM-LoRA and generate at 4 steps ---\n",
    "# Three lines change: load the LoRA, swap the scheduler, change steps + guidance\n",
    "\n",
    "print(\"Loading LCM-LoRA adapter...\")\n",
    "pipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n",
    "pipe.scheduler = LCMScheduler.from_config(original_scheduler_config)\n",
    "print(\"LCM-LoRA loaded. Scheduler swapped to LCMScheduler.\")\n",
    "\n",
    "# Note the reduced guidance scale: 1.5 instead of 7.5\n",
    "# The augmented PF-ODE already incorporates guidance into the trajectory,\n",
    "# so additional guidance is redundant and causes oversaturation.\n",
    "print(\"Generating: 4 steps, LCM scheduler, guidance_scale=1.5...\")\n",
    "img_4step_lcm, time_4step_lcm = generate_timed(\n",
    "    pipe, PROMPT, num_inference_steps=4, guidance_scale=1.5\n",
    ")\n",
    "print(f\"  Done in {time_4step_lcm:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Compare all three side by side ---\n",
    "show_image_row(\n",
    "    [img_50step, img_4step_bad, img_4step_lcm],\n",
    "    [\n",
    "        f\"50 Steps (standard)\\n{time_50step:.1f}s | guidance=7.5\",\n",
    "        f\"4 Steps (standard)\\n{time_4step_bad:.1f}s | guidance=7.5\",\n",
    "        f\"4 Steps + LCM-LoRA\\n{time_4step_lcm:.1f}s | guidance=1.5\",\n",
    "    ],\n",
    "    suptitle=f'SD v1.5: \"{PROMPT}\"',\n",
    ")\n",
    "\n",
    "print(\"Timing comparison:\")\n",
    "print(f\"  50-step standard:   {time_50step:.2f}s (baseline quality)\")\n",
    "print(f\"  4-step standard:    {time_4step_bad:.2f}s (model out of comfort zone)\")\n",
    "print(f\"  4-step + LCM-LoRA:  {time_4step_lcm:.2f}s (near-baseline quality)\")\n",
    "print(f\"  Speedup: {time_50step / time_4step_lcm:.1f}x\")\n",
    "print()\n",
    "print(\"The only difference between the middle and right images is a 4 MB LoRA file.\")\n",
    "print(\"The LoRA changed WHAT the model does at each step, so 4 steps produce\")\n",
    "print(\"good results. The standard model at 4 steps produces poor results because\")\n",
    "print(\"it was not trained for so few steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You saw the LCM-LoRA drop-in acceleration pattern in action:\n",
    "\n",
    "- **The standard model at 4 steps is poor.** It expects 20–50 steps to refine details. At 4 steps, the denoising process is incomplete — the image is blurry, incoherent, or has obvious artifacts. This is the \"model out of its comfort zone\" from the lesson.\n",
    "\n",
    "- **LCM-LoRA at 4 steps is near-baseline quality.** The LoRA modified the U-Net weights (via a low-rank bypass) so that each step covers more ground. The model was *trained* to produce good results in 4 steps via consistency distillation.\n",
    "\n",
    "- **Three lines changed.** Load the LoRA (`load_lora_weights`), swap the scheduler (`LCMScheduler`), reduce the guidance scale (1.5 instead of 7.5). Everything else — the pipeline, the prompt, the VAE, the text encoder — is identical. This is the modularity principle from **Stable Diffusion Architecture** in action.\n",
    "\n",
    "- **The reduced guidance scale matters.** LCM uses the augmented PF-ODE, which incorporates classifier-free guidance *into the trajectory*. Additional guidance on top of that is redundant and causes oversaturation. That is why we dropped from 7.5 to 1.5.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: LCM-LoRA Universality `[Guided]`\n",
    "\n",
    "From the lesson: the LCM-LoRA trained on the base SD v1.5 checkpoint can be applied to Dreamshaper, Realistic Vision, Anything V5, or any other SD v1.5-compatible fine-tune. One 4 MB file turns all of them into 4-step models.\n",
    "\n",
    "Why does this work? The LCM-LoRA captures a general behavior change: \"collapse the denoising trajectory into 4 steps.\" This is about the *dynamics of denoising*, not about specific content or style.\n",
    "\n",
    "We will:\n",
    "1. Load a community fine-tune compatible with SD v1.5\n",
    "2. Apply the SAME LCM-LoRA from Exercise 1\n",
    "3. Generate at 4 steps and compare to the base model's 4-step LCM-LoRA output\n",
    "\n",
    "**Before running, predict:**\n",
    "- Will the same LCM-LoRA work on a different fine-tune? Why or why not?\n",
    "- The LCM-LoRA was trained on the base SD v1.5 checkpoint. The fine-tune has different weights. Should the LoRA still be effective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 2: Apply LCM-LoRA to a community fine-tune\n",
    "# ============================================================\n",
    "\n",
    "# Clean up the previous pipeline to free GPU memory\n",
    "del pipe\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "PROMPT_2 = \"a medieval castle on a cliff overlooking the sea, dramatic lighting\"\n",
    "\n",
    "# --- Step 1: Load a community fine-tune ---\n",
    "# Dreamshaper is a popular SD v1.5-compatible fine-tune with a\n",
    "# distinctive artistic style.\n",
    "print(\"Loading Dreamshaper (SD v1.5-compatible fine-tune)...\")\n",
    "pipe_finetune = StableDiffusionPipeline.from_pretrained(\n",
    "    \"Lykon/dreamshaper-8\",\n",
    "    torch_dtype=dtype,\n",
    "    safety_checker=None,\n",
    ").to(device)\n",
    "\n",
    "original_ft_scheduler_config = pipe_finetune.scheduler.config\n",
    "print(\"Dreamshaper loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Generate at 50 steps (standard) for baseline ---\n",
    "pipe_finetune.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "    original_ft_scheduler_config\n",
    ")\n",
    "\n",
    "print(\"Generating: Dreamshaper at 50 steps (baseline)...\")\n",
    "img_ft_50, time_ft_50 = generate_timed(\n",
    "    pipe_finetune, PROMPT_2, num_inference_steps=50, guidance_scale=7.5\n",
    ")\n",
    "print(f\"  Done in {time_ft_50:.2f}s\")\n",
    "\n",
    "# --- Step 3: Load the SAME LCM-LoRA and generate at 4 steps ---\n",
    "print(\"Loading LCM-LoRA onto Dreamshaper...\")\n",
    "pipe_finetune.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n",
    "pipe_finetune.scheduler = LCMScheduler.from_config(original_ft_scheduler_config)\n",
    "print(\"LCM-LoRA loaded onto fine-tune.\")\n",
    "\n",
    "print(\"Generating: Dreamshaper + LCM-LoRA at 4 steps...\")\n",
    "img_ft_4_lcm, time_ft_4_lcm = generate_timed(\n",
    "    pipe_finetune, PROMPT_2, num_inference_steps=4, guidance_scale=1.5\n",
    ")\n",
    "print(f\"  Done in {time_ft_4_lcm:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Compare the fine-tune at 50 steps vs 4 steps with LCM-LoRA ---\n",
    "show_image_row(\n",
    "    [img_ft_50, img_ft_4_lcm],\n",
    "    [\n",
    "        f\"Dreamshaper — 50 Steps (standard)\\n{time_ft_50:.1f}s | guidance=7.5\",\n",
    "        f\"Dreamshaper + LCM-LoRA — 4 Steps\\n{time_ft_4_lcm:.1f}s | guidance=1.5\",\n",
    "    ],\n",
    "    suptitle=f'Universality Test: \"{PROMPT_2}\"',\n",
    ")\n",
    "\n",
    "print(\"The SAME LCM-LoRA file that worked on base SD v1.5 works on Dreamshaper.\")\n",
    "print()\n",
    "print(\"Why this works:\")\n",
    "print(\"- The LCM-LoRA captures 'how to generate in 4 steps' — the dynamics\")\n",
    "print(\"  of fast denoising, not a specific visual style.\")\n",
    "print(\"- Dreamshaper is SD v1.5-compatible: same U-Net architecture, same\")\n",
    "print(\"  layer dimensions, same attention heads. The LoRA matrices fit.\")\n",
    "print(\"- The fine-tune follows the same ODE trajectory structure — it just\")\n",
    "print(\"  arrives at a different endpoint (its distinctive style).\")\n",
    "print(\"- One 4 MB adapter → every SD v1.5-compatible model becomes a 4-step model.\")\n",
    "print()\n",
    "print(\"Note: the 4-step output may look slightly different in character from\")\n",
    "print(\"the 50-step output. The LCM-LoRA was distilled from the base model,\")\n",
    "print(\"not from Dreamshaper specifically. Minor quality variations are expected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You verified the \"one adapter, many models\" universality insight:\n",
    "\n",
    "- **The same LCM-LoRA works on a different fine-tune.** The adapter was trained on base SD v1.5, but it transfers to Dreamshaper because both share the same U-Net architecture. The LoRA captures the *skill* of fast generation, not model-specific content.\n",
    "\n",
    "- **This is the same swappability pattern from LoRA Fine-Tuning and ControlNet.** A small adapter file works across all compatible base models. LCM-LoRA extends this pattern to speed: one speed adapter for an entire model family.\n",
    "\n",
    "- **Universality has limits.** This LCM-LoRA is architecture-specific. It works on any SD v1.5-compatible model but does NOT work on SDXL (different U-Net dimensions, different attention heads). Separate LCM-LoRA checkpoints exist for SD v1.5 and SDXL.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Step Count and Guidance Scale Exploration `[Supported]`\n",
    "\n",
    "From the lesson: LCM typically uses a reduced guidance scale (1.0–2.0) compared to standard SD (7.0–7.5), and quality plateaus at around 4 steps. But where exactly are the sweet spots?\n",
    "\n",
    "Your task: generate a grid of images varying step count and guidance scale, then identify:\n",
    "- At which step count does quality plateau?\n",
    "- At which guidance scale does oversaturation appear?\n",
    "- What is the practical sweet spot for LCM-LoRA?\n",
    "\n",
    "Fill in the TODO markers to complete the sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Exercise 3: Sweep step counts and guidance scales\n# ============================================================\n\n# We reuse the pipeline from Exercise 2 (Dreamshaper + LCM-LoRA already loaded).\n# If you restarted the runtime, re-run Exercises 1–2 first.\n\nPROMPT_3 = \"a red fox sitting in a snowy forest, soft morning light, highly detailed\"\n\nstep_counts = [1, 2, 4, 8]\nguidance_scales = [1.0, 1.5, 2.0, 4.0]\n\n# TODO: Generate an image for each (step_count, guidance_scale) combination.\n# Store the results in `grid_images` (a list of PIL images) and\n# `grid_titles` (a list of strings describing each image).\n#\n# Use the `generate_timed` helper. You need:\n#   - pipe_finetune as the pipeline (already has LCM-LoRA and LCMScheduler)\n#   - PROMPT_3 as the prompt\n#   - The same seed (SEED) for all images so differences come from\n#     step count and guidance scale, not random noise\n#\n# Loop structure:\n#   for steps in step_counts:\n#       for gs in guidance_scales:\n#           image, elapsed = generate_timed(pipe, prompt, steps, gs)\n#           append image and a descriptive title\n\ngrid_images = []\ngrid_titles = []\n\nprint(f\"Generating {len(step_counts) * len(guidance_scales)} images...\")\nprint(f\"Step counts: {step_counts}\")\nprint(f\"Guidance scales: {guidance_scales}\")\nprint()\n\nfor steps in step_counts:\n    for gs in guidance_scales:\n        # TODO: Generate the image and append to grid_images and grid_titles\n        # Title format suggestion: f\"{steps} steps | cfg={gs}\\n{elapsed:.1f}s\"\n        raise NotImplementedError(\n            \"TODO: Generate the image with generate_timed() and append to \"\n            \"grid_images and grid_titles. See the loop structure hint above.\"\n        )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the grid: 4 rows (step counts) × 4 columns (guidance scales)\n",
    "show_image_grid(\n",
    "    grid_images,\n",
    "    grid_titles,\n",
    "    nrows=len(step_counts),\n",
    "    ncols=len(guidance_scales),\n",
    "    suptitle=(\n",
    "        f'LCM-LoRA Parameter Sweep: \"{PROMPT_3}\"\\n'\n",
    "        f'Rows: step count ({step_counts}) | Columns: guidance scale ({guidance_scales})'\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"What to look for:\")\n",
    "print()\n",
    "print(\"STEP COUNT (rows, top to bottom):\")\n",
    "print(\"- 1 step: recognizable subject and composition, but soft textures.\")\n",
    "print(\"  Adequate for previews and rapid iteration.\")\n",
    "print(\"- 2 steps: significant quality jump. Most textures resolved.\")\n",
    "print(\"- 4 steps: the practical sweet spot. Near-baseline quality.\")\n",
    "print(\"- 8 steps: diminishing returns. Hard to distinguish from 4 steps.\")\n",
    "print()\n",
    "print(\"GUIDANCE SCALE (columns, left to right):\")\n",
    "print(\"- 1.0: minimal guidance. May look slightly less prompt-faithful.\")\n",
    "print(\"- 1.5: the typical LCM sweet spot. Good balance.\")\n",
    "print(\"- 2.0: slightly more saturated. Still acceptable.\")\n",
    "print(\"- 4.0: likely oversaturated. Colors pushed too far.\")\n",
    "print(\"  The augmented PF-ODE already incorporates guidance, so adding more\")\n",
    "print(\"  is redundant. This is unlike standard SD where 7.5 is typical.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The key insight is that LCM-LoRA has a different sweet spot than standard SD: fewer steps (4 instead of 50) and lower guidance (1.0–2.0 instead of 7.0–7.5). The augmented PF-ODE bakes guidance into the trajectory, making additional guidance redundant.\n",
    "\n",
    "```python\n",
    "grid_images = []\n",
    "grid_titles = []\n",
    "\n",
    "print(f\"Generating {len(step_counts) * len(guidance_scales)} images...\")\n",
    "print(f\"Step counts: {step_counts}\")\n",
    "print(f\"Guidance scales: {guidance_scales}\")\n",
    "print()\n",
    "\n",
    "for steps in step_counts:\n",
    "    for gs in guidance_scales:\n",
    "        img, elapsed = generate_timed(\n",
    "            pipe_finetune, PROMPT_3,\n",
    "            num_inference_steps=steps,\n",
    "            guidance_scale=gs,\n",
    "        )\n",
    "        grid_images.append(img)\n",
    "        grid_titles.append(f\"{steps} steps | cfg={gs}\\n{elapsed:.1f}s\")\n",
    "        print(f\"  {steps} steps, cfg={gs}: {elapsed:.1f}s\")\n",
    "```\n",
    "\n",
    "**What you should observe:**\n",
    "- Quality improves significantly from 1 to 2 steps, moderately from 2 to 4, and barely from 4 to 8\n",
    "- Guidance scale 1.0–1.5 looks natural; 4.0 likely shows oversaturation (overly vivid colors, harsh contrasts)\n",
    "- The practical sweet spot is 4 steps at guidance 1.5\n",
    "\n",
    "**Common mistakes:**\n",
    "- Forgetting to pass the same `seed` to all calls (makes the comparison unfair — differences would come from random noise, not parameters)\n",
    "- Using standard guidance scale (7.5) — this causes severe oversaturation with LCM because the augmented PF-ODE already incorporates guidance\n",
    "\n",
    "</details>\n",
    "\n",
    "### What Just Happened\n",
    "\n",
    "You mapped the practical parameter space of LCM-LoRA:\n",
    "\n",
    "- **Quality plateaus at 4 steps.** The jump from 1 to 2 steps is dramatic (soft to mostly-resolved). From 2 to 4 is noticeable. From 4 to 8 shows diminishing returns. The consistency model was optimized for low step counts.\n",
    "\n",
    "- **Guidance scale should be low (1.0–2.0).** The augmented PF-ODE already incorporates CFG guidance into the trajectory. Adding more guidance on top is redundant and causes oversaturation — visible as unnaturally vivid colors and harsh contrasts at guidance 4.0.\n",
    "\n",
    "- **The practical sweet spot is 4 steps at guidance 1.5.** This is the combination that balances quality, speed, and prompt fidelity. Most LCM-LoRA workflows should start here.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 4: LCM-LoRA + Style LoRA Composition `[Independent]`\n\nFrom the lesson: LoRA weights are additive. You can load an LCM-LoRA *and* a style LoRA at the same time — the LCM-LoRA captures \"generate in 4 steps\" and the style LoRA captures a visual style. Both bypasses are summed.\n\n**Your task:**\n\n1. Load a fresh SD v1.5 pipeline\n2. Load a style LoRA and generate at 50 steps — this is your style baseline\n3. Load the LCM-LoRA *alongside* the style LoRA, swap to LCMScheduler, and generate at 4 steps\n4. Compare: does the 4-step LCM + style result preserve the style while being fast?\n5. Experiment with different LoRA adapter weight scales to balance the two influences\n\n**Suggested style LoRA:** `\"artificialguybr/pixelartredmond-1-5v-pixel-art-loras-for-sd-1-5\"` (SD v1.5-compatible pixel art LoRA, trigger word: `PixArFK`). Any SD v1.5-compatible style LoRA from the HuggingFace Hub will work.\n\n**Hints:**\n- Use `pipe.load_lora_weights(adapter_name, adapter_name=\"lcm\")` and `pipe.load_lora_weights(style_name, adapter_name=\"style\")` to load multiple LoRAs\n- Use `pipe.set_adapters([\"lcm\", \"style\"], adapter_weights=[1.0, 0.8])` to control the weight of each\n- Generate with the same prompt and seed for fair comparison\n- Try varying the style LoRA weight (0.5, 0.8, 1.0) while keeping LCM weight at 1.0\n- Include the trigger word (`PixArFK`) in your prompt if using the suggested LoRA\n\n**Before running, predict:**\n- Will the composed result look like the style LoRA output, but generated faster?\n- What happens if you set the LCM weight to 0.5? Will the style be stronger but the generation quality drop?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 4: Your code here\n",
    "# ============================================================\n",
    "#\n",
    "# Suggested structure:\n",
    "#\n",
    "# 1. Clean up previous pipeline:\n",
    "#    del pipe_finetune\n",
    "#    torch.cuda.empty_cache()\n",
    "#\n",
    "# 2. Load a fresh SD v1.5 pipeline\n",
    "#\n",
    "# 3. Generate a style baseline:\n",
    "#    a. Load a style LoRA\n",
    "#    b. Generate at 50 steps with standard scheduler and guidance 7.5\n",
    "#    c. Save this image as your \"style baseline\"\n",
    "#\n",
    "# 4. Add LCM-LoRA alongside the style LoRA:\n",
    "#    a. Load LCM-LoRA as a second adapter\n",
    "#    b. Use pipe.set_adapters() to activate both with chosen weights\n",
    "#    c. Swap to LCMScheduler\n",
    "#    d. Generate at 4 steps with guidance 1.5\n",
    "#\n",
    "# 5. Experiment with weight scales:\n",
    "#    - Try style_weight = 0.5, 0.8, 1.0 with lcm_weight = 1.0\n",
    "#    - Observe how the balance shifts between style fidelity and\n",
    "#      generation quality\n",
    "#\n",
    "# 6. Display comparison:\n",
    "#    - Style LoRA alone at 50 steps (baseline)\n",
    "#    - LCM + Style at 4 steps (various weight combinations)\n",
    "#\n",
    "# Remember:\n",
    "# - Same PROMPT and SEED for all generations\n",
    "# - LCM needs guidance_scale around 1.5, not 7.5\n",
    "# - The style baseline at 50 steps uses guidance_scale 7.5\n",
    "# - pipe.set_adapters([\"lcm\", \"style\"], adapter_weights=[lcm_w, style_w])\n",
    "#   controls the balance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\nThe key insight is that LoRA bypasses are additive — you can sum multiple low-rank updates and the model applies all of them. The LCM-LoRA modifies the denoising dynamics (\"generate faster\") while the style LoRA modifies the output aesthetics (\"generate in this style\"). These are largely orthogonal modifications, so they compose well.\n\n```python\n# Clean up\ndel pipe_finetune\ntorch.cuda.empty_cache() if torch.cuda.is_available() else None\n\nPROMPT_4 = \"a lighthouse on a rocky coast during a storm, waves crashing, PixArFK\"\n\n# --- Load a fresh SD v1.5 pipeline ---\npipe_comp = StableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    torch_dtype=dtype,\n    safety_checker=None,\n).to(device)\n\ncomp_scheduler_config = pipe_comp.scheduler.config\n\n# --- Style baseline: style LoRA alone at 50 steps ---\nSTYLE_LORA = \"artificialguybr/pixelartredmond-1-5v-pixel-art-loras-for-sd-1-5\"\n\npipe_comp.load_lora_weights(STYLE_LORA, adapter_name=\"style\")\npipe_comp.set_adapters([\"style\"], adapter_weights=[1.0])\npipe_comp.scheduler = DPMSolverMultistepScheduler.from_config(comp_scheduler_config)\n\nprint(\"Generating: style LoRA alone at 50 steps (baseline)...\")\nimg_style_baseline, t_style = generate_timed(\n    pipe_comp, PROMPT_4, num_inference_steps=50, guidance_scale=7.5\n)\nprint(f\"  Done in {t_style:.1f}s\")\n\n# --- Load LCM-LoRA alongside the style LoRA ---\npipe_comp.load_lora_weights(\n    \"latent-consistency/lcm-lora-sdv1-5\",\n    adapter_name=\"lcm\",\n)\npipe_comp.scheduler = LCMScheduler.from_config(comp_scheduler_config)\n\n# --- Generate with different style weight combinations ---\nimages = [img_style_baseline]\ntitles = [f\"Style LoRA only — 50 steps\\n{t_style:.1f}s | guidance=7.5\"]\n\nfor style_w in [0.5, 0.8, 1.0]:\n    pipe_comp.set_adapters(\n        [\"lcm\", \"style\"],\n        adapter_weights=[1.0, style_w],\n    )\n    img, elapsed = generate_timed(\n        pipe_comp, PROMPT_4, num_inference_steps=4, guidance_scale=1.5\n    )\n    images.append(img)\n    titles.append(f\"LCM + style={style_w} — 4 steps\\n{elapsed:.1f}s\")\n    print(f\"  LCM=1.0, style={style_w}: {elapsed:.1f}s\")\n\nshow_image_row(\n    images,\n    titles,\n    suptitle=f'LoRA Composition: \"{PROMPT_4}\"',\n)\n```\n\n**Why LoRA composition works:** LoRA bypasses are additive to the base weights: $W_{\\text{effective}} = W_{\\text{base}} + \\alpha_1 B_1 A_1 + \\alpha_2 B_2 A_2$. Each LoRA modifies a different \"axis\" of behavior. The LCM-LoRA modifies the denoising dynamics; the style LoRA modifies the output aesthetics. Because they target different aspects of the model's behavior, they compose without significant interference.\n\n**Weight scale effects:**\n- LCM weight below 1.0: the model partially reverts to multi-step behavior. At 4 steps, quality may degrade because the model is not fully in \"LCM mode.\"\n- Style weight below 1.0: the style effect is reduced. At 0.5, you see a faint pixel art influence; at 1.0, the style dominates.\n- The sweet spot is typically LCM weight=1.0 and style weight=0.7–1.0.\n\n**Common mistakes:**\n- Forgetting to swap to LCMScheduler when using LCM-LoRA (generates garbage)\n- Using guidance_scale=7.5 with LCM-LoRA (causes oversaturation)\n- Loading LoRAs without adapter names (makes it impossible to control weights independently)\n- Not using the `peft` library (required for multi-adapter support in diffusers)\n- Forgetting the trigger word (`PixArFK`) in the prompt — the style LoRA may not activate without it\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **LCM-LoRA is a drop-in accelerator.** Three lines of code — load the LoRA, swap the scheduler, reduce the guidance scale — turn any SD v1.5 pipeline from 50-step to 4-step generation with near-baseline quality. The model, the prompt, the VAE, the text encoder all stay the same.\n",
    "\n",
    "2. **One adapter, many models.** The same LCM-LoRA trained on base SD v1.5 works on Dreamshaper, Realistic Vision, and other compatible fine-tunes. It captures the *skill* of fast generation (denoising dynamics), not a specific visual style. This is universality within an architecture family.\n",
    "\n",
    "3. **LCM-LoRA has different sweet spots than standard SD.** Quality plateaus at 4 steps (not 50). Guidance scale should be 1.0–2.0 (not 7.0–7.5). The augmented PF-ODE bakes guidance into the trajectory, making additional guidance redundant and harmful.\n",
    "\n",
    "4. **LoRA composition works.** LCM-LoRA + style LoRA can be loaded simultaneously because LoRA bypasses are additive. The LCM-LoRA handles speed while the style LoRA handles aesthetics. Adjust adapter weights to balance the two influences.\n",
    "\n",
    "5. **Same recipe, bigger kitchen.** Everything you saw here is the consistency distillation from the previous lesson — applied to Stable Diffusion's latent space instead of 2D point clouds. The training procedure is structurally identical. The LoRA variant captures the weight change as a portable adapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}