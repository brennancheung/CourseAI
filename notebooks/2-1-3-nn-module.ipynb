{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# nn.Module: Building Neural Networks\n## Module 2.1, Lesson 3\n\nIn this notebook, you'll build neural networks from scratch using PyTorch's `nn.Module` system.\n\n**What you'll do:**\n1. Create and inspect `nn.Linear` layers — Guided\n2. Verify that `nn.Linear` is just `w @ x + b` — Guided\n3. Build a 2-layer network by subclassing `nn.Module` — Supported\n4. Rewrite it using `nn.Sequential` — Supported\n5. Show why stacking linear layers without activations collapses — Guided\n6. (Stretch) Build a skip-connection module — Independent\n\n**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones — they reveal gaps in your mental model.\n\n---\n\n**Prerequisites:** Tensors & Autograd lesson, basic Python classes"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 1: Create and Inspect nn.Linear Layers (Guided)\n\nAn `nn.Linear(in_features, out_features)` layer performs the operation `y = xW^T + b`.\n\n**Before running, predict:**\n- What shape will the weight matrix be for `nn.Linear(10, 5)`?\n- What shape will the bias be?\n- How many total parameters is that?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear layer: 10 inputs, 5 outputs\n",
    "layer = nn.Linear(10, 5)\n",
    "\n",
    "print('=== nn.Linear(10, 5) ===')\n",
    "print(f'Weight shape: {layer.weight.shape}')  # Predict this first!\n",
    "print(f'Bias shape:   {layer.bias.shape}')     # Predict this first!\n",
    "print(f'Weight numel: {layer.weight.numel()}')\n",
    "print(f'Bias numel:   {layer.bias.numel()}')\n",
    "print(f'Total params: {layer.weight.numel() + layer.bias.numel()}')\n",
    "print()\n",
    "\n",
    "# Verify: weight is (out_features, in_features) = (5, 10)\n",
    "# Why transposed? Because PyTorch computes x @ W^T + b, so W is stored as (out, in)\n",
    "assert layer.weight.shape == (5, 10), f'Expected (5, 10), got {layer.weight.shape}'\n",
    "assert layer.bias.shape == (5,), f'Expected (5,), got {layer.bias.shape}'\n",
    "print('Assertions passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try different sizes. Predict the shapes before running!\n",
    "sizes = [(3, 1), (784, 128), (128, 10), (1, 100)]\n",
    "\n",
    "print(f'{\"Layer\":<20s} {\"Weight Shape\":<18s} {\"Bias Shape\":<14s} {\"Total Params\":>12s}')\n",
    "print('-' * 65)\n",
    "\n",
    "for in_f, out_f in sizes:\n",
    "    l = nn.Linear(in_f, out_f)\n",
    "    total = sum(p.numel() for p in l.parameters())\n",
    "    print(f'Linear({in_f}, {out_f}){\"\":<{13 - len(str(in_f)) - len(str(out_f))}} '\n",
    "          f'{str(tuple(l.weight.shape)):<18s} {str(tuple(l.bias.shape)):<14s} {total:>12,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight:** The parameter count for `nn.Linear(in, out)` is always `in * out + out` (weights + biases). That `784 * 128 = 100,352` is why dense layers on images get expensive fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 2: Verify nn.Linear IS w*x + b (Guided)\n\n`nn.Linear` isn't magic. It computes exactly `x @ weight.T + bias`. Let's prove it.\n\n**Before running, predict:** Will `torch.allclose()` return True or False when comparing `layer(x)` to `x @ layer.weight.T + layer.bias`?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a layer and a random input\n",
    "layer = nn.Linear(10, 5)\n",
    "x = torch.randn(3, 10)  # batch of 3 samples, 10 features each\n",
    "\n",
    "# Method 1: Use the layer directly\n",
    "out_layer = layer(x)\n",
    "\n",
    "# Method 2: Manual computation\n",
    "out_manual = x @ layer.weight.T + layer.bias\n",
    "\n",
    "print(f'Input shape:         {x.shape}')\n",
    "print(f'Layer output shape:  {out_layer.shape}')\n",
    "print(f'Manual output shape: {out_manual.shape}')\n",
    "print()\n",
    "print(f'Layer output:\\n{out_layer}')\n",
    "print(f'\\nManual output:\\n{out_manual}')\n",
    "print()\n",
    "\n",
    "# Are they the same?\n",
    "match = torch.allclose(out_layer, out_manual)\n",
    "print(f'Outputs match: {match}')\n",
    "assert match, 'Outputs should match — nn.Linear is literally x @ W^T + b'\n",
    "print('\\nConfirmed: nn.Linear(x) == x @ weight.T + bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why this matters:** There's no hidden complexity in `nn.Linear`. When you call `layer(x)`, PyTorch computes `x @ weight.T + bias` and tracks gradients. That's it. The `nn.Module` system gives you parameter management, not computation magic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 3: Build a 2-Layer nn.Module Subclass (Supported)\n\nNow build an actual neural network by subclassing `nn.Module`. The architecture:\n\n```\nInput (10) -> Linear(10, 32) -> ReLU -> Linear(32, 1) -> Output (1)\n```\n\n**Task:**\n1. Define layers in `__init__`\n2. Wire them together in `forward`\n\nFill in the TODO sections below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: Define the layers\n",
    "        # self.fc1 = nn.Linear(10, 32)\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.fc2 = nn.Linear(32, 1)\n",
    "        pass  # Remove this line when you add your code\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Wire the layers together\n",
    "        # x = self.fc1(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.fc2(x)\n",
    "        # return x\n",
    "        return x  # Replace this line\n",
    "\n",
    "\n",
    "# Test it\n",
    "model = TwoLayerNet()\n",
    "x = torch.randn(5, 10)  # batch of 5 samples\n",
    "out = model(x)\n",
    "\n",
    "print(f'Input shape:  {x.shape}')    # [5, 10]\n",
    "print(f'Output shape: {out.shape}')   # Should be [5, 1]\n",
    "print()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print('Parameter breakdown:')\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'  {name:<12s} shape={str(tuple(param.shape)):<12s} params={param.numel()}')\n",
    "print(f'  {\"TOTAL\":<12s} {\"\":12s} params={total_params}')\n",
    "print()\n",
    "\n",
    "# Verify\n",
    "expected_params = 10 * 32 + 32 + 32 * 1 + 1  # fc1 weights + bias + fc2 weights + bias\n",
    "print(f'Expected params: {expected_params}')\n",
    "print(f'Actual params:   {total_params}')\n",
    "assert total_params == expected_params, f'Expected {expected_params}, got {total_params}'\n",
    "assert out.shape == (5, 1), f'Expected output shape (5, 1), got {out.shape}'\n",
    "print('\\nAll checks passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\nThe key pattern is: define layers as `self.xxx` in `__init__` (so PyTorch can find them), then call them in sequence in `forward`.\n\n```python\nclass TwoLayerNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(10, 32)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(32, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n```\n\n**Why `super().__init__()`?** This registers the module with PyTorch's parameter tracking system. Without it, `.parameters()` won't find your layers.\n\n**Why assign layers to `self`?** PyTorch uses `__setattr__` magic to detect `nn.Module` attributes. If you create a layer as a local variable, it won't be registered and won't appear in `.parameters()`.\n\n</details>\n\n**Key pattern:** Every `nn.Module` subclass follows the same structure:\n- `__init__`: register layers as attributes (so PyTorch can find their parameters)\n- `forward`: define the computation graph\n- Never call `forward()` directly — call the module like a function: `model(x)`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 4: Convert to nn.Sequential (Supported)\n\nFor simple feed-forward networks where data flows straight through each layer, `nn.Sequential` saves you from writing a `forward` method.\n\n**Task:** Rewrite `TwoLayerNet` as an `nn.Sequential` model. Then verify it produces the same output when given the same weights."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build the same architecture using nn.Sequential\n",
    "# sequential_model = nn.Sequential(\n",
    "#     nn.Linear(10, 32),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(32, 1),\n",
    "# )\n",
    "\n",
    "sequential_model = nn.Sequential()  # Replace this line with the full Sequential\n",
    "\n",
    "print('Sequential model:')\n",
    "print(sequential_model)\n",
    "print()\n",
    "\n",
    "# Verify same parameter count\n",
    "seq_params = sum(p.numel() for p in sequential_model.parameters())\n",
    "print(f'Sequential params: {seq_params}')\n",
    "print(f'TwoLayerNet params: {total_params}')\n",
    "assert seq_params == total_params, 'Parameter counts should match!'\n",
    "print('Parameter counts match!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now prove they compute the same thing with the same weights.\n",
    "# Copy weights from TwoLayerNet into the Sequential model.\n",
    "\n",
    "# The Sequential layers are indexed: 0 = first Linear, 1 = ReLU, 2 = second Linear\n",
    "with torch.no_grad():\n",
    "    sequential_model[0].weight.copy_(model.fc1.weight)\n",
    "    sequential_model[0].bias.copy_(model.fc1.bias)\n",
    "    sequential_model[2].weight.copy_(model.fc2.weight)\n",
    "    sequential_model[2].bias.copy_(model.fc2.bias)\n",
    "\n",
    "# Same input\n",
    "x = torch.randn(5, 10)\n",
    "out_module = model(x)\n",
    "out_sequential = sequential_model(x)\n",
    "\n",
    "print(f'Module output:     {out_module.flatten()[:5]}')\n",
    "print(f'Sequential output: {out_sequential.flatten()[:5]}')\n",
    "print()\n",
    "\n",
    "match = torch.allclose(out_module, out_sequential)\n",
    "print(f'Outputs match: {match}')\n",
    "assert match, 'Same weights + same input should give same output'\n",
    "print('\\nConfirmed: nn.Module subclass and nn.Sequential are interchangeable.')\n",
    "print('Sequential is just syntactic sugar for simple feed-forward architectures.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n`nn.Sequential` takes layers in order and wires them together automatically — no `forward` method needed.\n\n```python\nsequential_model = nn.Sequential(\n    nn.Linear(10, 32),\n    nn.ReLU(),\n    nn.Linear(32, 1),\n)\n```\n\nThe layers are accessed by index: `sequential_model[0]` is the first Linear, `sequential_model[1]` is ReLU, `sequential_model[2]` is the second Linear.\n\n</details>\n\n**When to use which:**\n- `nn.Sequential`: Simple pipelines where data flows straight through\n- `nn.Module` subclass: Anything with branching, skip connections, multiple inputs/outputs, or conditional logic"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 5: Linear Collapse Experiment (Guided)\n\n**Claim:** Stacking linear layers without activations is pointless — the entire stack collapses to a single linear transformation.\n\nMathematically: if `f(x) = W2 @ (W1 @ x + b1) + b2`, that simplifies to `f(x) = (W2 @ W1) @ x + (W2 @ b1 + b2)`. One matrix multiplication, one bias. The depth adds no expressive power.\n\n**Before running, predict:** If we collapse a 3-layer network (no activations) into a single Linear layer, will the outputs be identical?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network WITHOUT activations (should collapse)\n",
    "no_activation = nn.Sequential(\n",
    "    nn.Linear(10, 64),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.Linear(64, 5),\n",
    ")\n",
    "\n",
    "# Network WITH activations (should NOT collapse)\n",
    "with_activation = nn.Sequential(\n",
    "    nn.Linear(10, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 5),\n",
    ")\n",
    "\n",
    "print(f'No-activation params:   {sum(p.numel() for p in no_activation.parameters()):,}')\n",
    "print(f'With-activation params: {sum(p.numel() for p in with_activation.parameters()):,}')\n",
    "print('(Same parameter count — activations have no parameters)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prove the no-activation network collapses to a single linear layer.\n",
    "# Compute the collapsed weight and bias manually.\n",
    "\n",
    "with torch.no_grad():\n",
    "    W1 = no_activation[0].weight  # (64, 10)\n",
    "    b1 = no_activation[0].bias    # (64,)\n",
    "    W2 = no_activation[1].weight  # (64, 64)\n",
    "    b2 = no_activation[1].bias    # (64,)\n",
    "    W3 = no_activation[2].weight  # (5, 64)\n",
    "    b3 = no_activation[2].bias    # (5,)\n",
    "\n",
    "    # Collapse: W_combined = W3 @ W2 @ W1\n",
    "    #           b_combined = W3 @ W2 @ b1 + W3 @ b2 + b3\n",
    "    W_combined = W3 @ W2 @ W1\n",
    "    b_combined = W3 @ W2 @ b1 + W3 @ b2 + b3\n",
    "\n",
    "    print(f'Collapsed weight shape: {W_combined.shape}')  # (5, 10)\n",
    "    print(f'Collapsed bias shape:   {b_combined.shape}')  # (5,)\n",
    "    print()\n",
    "\n",
    "    # Build a single-layer equivalent\n",
    "    collapsed = nn.Linear(10, 5)\n",
    "    collapsed.weight.copy_(W_combined)\n",
    "    collapsed.bias.copy_(b_combined)\n",
    "\n",
    "# Test: do the 3-layer network and the collapsed single layer give the same output?\n",
    "x = torch.randn(20, 10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_3layer = no_activation(x)\n",
    "    out_collapsed = collapsed(x)\n",
    "\n",
    "match = torch.allclose(out_3layer, out_collapsed, atol=1e-5)\n",
    "print(f'3-layer output matches single-layer: {match}')\n",
    "assert match, 'They should match — stacked linears collapse!'\n",
    "print()\n",
    "print('Confirmed: 3 linear layers with no activations = 1 linear layer.')\n",
    "print('All that extra computation and parameters add zero expressive power.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now show that this does NOT work for the network with activations.\n",
    "# ReLU is nonlinear, so the composition cannot be reduced.\n",
    "\n",
    "# Try to approximate the activated network with a single linear layer.\n",
    "# We'll use least squares to find the best single-layer approximation.\n",
    "\n",
    "torch.manual_seed(42)\n",
    "x_test = torch.randn(1000, 10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_activated = with_activation(x_test)\n",
    "\n",
    "# Best linear fit: solve for W, b such that x @ W^T + b ≈ y\n",
    "# Using least squares via pseudoinverse\n",
    "x_augmented = torch.cat([x_test, torch.ones(1000, 1)], dim=1)  # add bias column\n",
    "solution = torch.linalg.lstsq(x_augmented, y_activated).solution  # (11, 5)\n",
    "\n",
    "y_linear_approx = x_augmented @ solution\n",
    "\n",
    "# How close is the approximation?\n",
    "residual = (y_activated - y_linear_approx).pow(2).mean().sqrt()\n",
    "signal = y_activated.pow(2).mean().sqrt()\n",
    "\n",
    "print(f'RMS of activated outputs:        {signal:.4f}')\n",
    "print(f'RMS of linear approximation err: {residual:.4f}')\n",
    "print(f'Relative error:                  {residual / signal:.1%}')\n",
    "print()\n",
    "\n",
    "if residual / signal > 0.01:\n",
    "    print('The activated network CANNOT be collapsed to a single linear layer.')\n",
    "    print('This is the whole point of activation functions: they add nonlinearity.')\n",
    "    print('Without them, depth is an illusion.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** Linear layers stacked without activations collapse to a single linear transformation. Activation functions (ReLU, etc.) are what give deep networks their expressive power. Depth without nonlinearity is meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 6: Build a Skip-Connection Module (Independent)\n\nA **residual block** computes `output = x + f(x)` where `f` is some transformation. The skip connection (`+ x`) lets gradients flow directly through the network, which helps with training deep networks.\n\n**Your task:** Implement `ResidualBlock` where:\n- `f(x) = Linear -> ReLU -> Linear`\n- The forward pass returns `x + f(x)`\n\n**Important:** For the skip connection to work, `f(x)` must output the same shape as `x`. So both Linear layers must preserve the dimension."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"A simple residual block: output = x + f(x)\n",
    "    \n",
    "    f(x) = Linear(dim, dim) -> ReLU -> Linear(dim, dim)\n",
    "    \n",
    "    The skip connection adds the input directly to the output.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        # TODO: Define the transformation f(x)\n",
    "        # self.fc1 = nn.Linear(dim, dim)\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.fc2 = nn.Linear(dim, dim)\n",
    "        pass  # Remove this line when you add your code\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Compute f(x) and add the skip connection\n",
    "        # f_x = self.fc2(self.relu(self.fc1(x)))\n",
    "        # return x + f_x\n",
    "        return x  # Replace this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the ResidualBlock\n",
    "block = ResidualBlock(dim=16)\n",
    "x = torch.randn(4, 16)\n",
    "out = block(x)\n",
    "\n",
    "print(f'Input shape:  {x.shape}')   # [4, 16]\n",
    "print(f'Output shape: {out.shape}')  # Should be [4, 16]\n",
    "assert out.shape == x.shape, f'Output shape should match input: {x.shape}'\n",
    "print()\n",
    "\n",
    "# Verify the skip connection is present.\n",
    "# If we zero out all the weights in the transformation,\n",
    "# the output should equal the input (identity).\n",
    "with torch.no_grad():\n",
    "    for p in block.parameters():\n",
    "        p.zero_()\n",
    "\n",
    "out_zeroed = block(x)\n",
    "is_identity = torch.allclose(out_zeroed, x)\n",
    "print(f'With zeroed weights, output == input: {is_identity}')\n",
    "assert is_identity, 'When f(x)=0, the block should be identity (x + 0 = x)'\n",
    "print()\n",
    "print('Skip connection verified!')\n",
    "print('When the transformation learns nothing, the block defaults to passing input through.')\n",
    "print('This is why residual connections help training: the \"easy\" path is identity, not zero.')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<details>\n<summary>Solution</summary>\n\nThe key insight is that the skip connection (`x + f(x)`) means the block defaults to identity when `f` learns nothing. This is why residual networks are easier to train than plain deep networks.\n\n```python\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(dim, dim)\n\n    def forward(self, x):\n        f_x = self.fc2(self.relu(self.fc1(x)))\n        return x + f_x\n```\n\n**Why `dim -> dim`?** Both Linear layers must preserve the dimension so that `x + f(x)` works. If `f(x)` had a different shape than `x`, the addition would fail.\n\n**Why this matters:** In a plain deep network, each layer can only learn transformations that are \"close to zero\" at initialization. With a skip connection, each block can learn transformations that are \"close to identity\" — a much easier starting point.\n\n</details>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Stack multiple residual blocks into a deeper network\n",
    "deep_residual = nn.Sequential(\n",
    "    nn.Linear(10, 32),\n",
    "    nn.ReLU(),\n",
    "    ResidualBlock(32),\n",
    "    ResidualBlock(32),\n",
    "    ResidualBlock(32),\n",
    "    nn.Linear(32, 1),\n",
    ")\n",
    "\n",
    "x = torch.randn(8, 10)\n",
    "out = deep_residual(x)\n",
    "\n",
    "print(f'Deep residual network:')\n",
    "print(f'  Input:  {x.shape}')\n",
    "print(f'  Output: {out.shape}')\n",
    "print(f'  Params: {sum(p.numel() for p in deep_residual.parameters()):,}')\n",
    "print()\n",
    "print('Each ResidualBlock adds depth without risk of degradation.')\n",
    "print('This pattern is the foundation of ResNets, Transformers, and most modern architectures.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **`nn.Linear` is just `x @ W^T + b`** — no magic, just matrix math with tracked gradients\n",
    "2. **`nn.Module` subclassing** is the standard pattern: define layers in `__init__`, wire them in `forward`\n",
    "3. **`nn.Sequential`** is syntactic sugar for simple pipelines — use `nn.Module` when you need branching or skip connections\n",
    "4. **Stacked linear layers without activations collapse** to a single linear transformation — activations are what give depth its power\n",
    "5. **Skip connections** let a block default to identity, making deep networks trainable\n",
    "\n",
    "**Everything in PyTorch builds on these primitives.** CNNs, RNNs, Transformers — they all use `nn.Module`, define layers in `__init__`, and wire them in `forward`. The only thing that changes is *which* layers and *how* they connect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}