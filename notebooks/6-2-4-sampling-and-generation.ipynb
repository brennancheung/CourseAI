{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling and Generation\n",
    "\n",
    "**Module 6.2, Lesson 4** | CourseAI\n",
    "\n",
    "In this notebook you will compute the reverse step coefficients by hand, trace a single reverse step with real numbers, compare one-shot denoising to iterative denoising, and implement the full DDPM sampling loop.\n",
    "\n",
    "**What you'll do:**\n",
    "- Compute the scaling factor, noise correction, and noise injection coefficients at various timesteps\n",
    "- Trace one reverse step at t=500 numerically\u2014plug in values, compute x_{499}, and visualize the result\n",
    "- Compare one-shot denoising (blurry failure) to multi-step iterative denoising (structured output)\n",
    "- Implement the full sampling loop using a dummy \"oracle\" model and visualize the denoising trajectory\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "**Estimated time:** 20-30 minutes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to import everything and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Setup: Data and Noise Schedule\n",
    "\n",
    "We load Fashion-MNIST and define the cosine noise schedule\u2014the same setup from the Learning to Denoise notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "class_names = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]\n",
    "\n",
    "print(f'Dataset size: {len(dataset)}')\n",
    "print(f'Image shape: {dataset[0][0].shape}')\n",
    "print(f'Pixel range: [{dataset[0][0].min():.1f}, {dataset[0][0].max():.1f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine noise schedule (same as Learning to Denoise notebook)\n",
    "T = 1000\n",
    "\n",
    "def cosine_alpha_bar_schedule(T, s=0.008):\n",
    "    \"\"\"Compute alpha_bar for each timestep using the cosine schedule.\n",
    "    \n",
    "    alpha_bar_t = how much original signal remains at step t.\n",
    "    At t=0, alpha_bar ~ 1 (all signal). At t=T, alpha_bar ~ 0 (all noise).\n",
    "    \"\"\"\n",
    "    steps = torch.arange(T + 1, dtype=torch.float32)\n",
    "    f = torch.cos(((steps / T) + s) / (1 + s) * (math.pi / 2)) ** 2\n",
    "    alpha_bar = f / f[0]\n",
    "    return alpha_bar\n",
    "\n",
    "alpha_bar = cosine_alpha_bar_schedule(T)\n",
    "\n",
    "# Derive the per-step quantities the reverse formula needs:\n",
    "# alpha_t = alpha_bar_t / alpha_bar_{t-1}\n",
    "# beta_t = 1 - alpha_t\n",
    "# sigma_t = sqrt(beta_t)\n",
    "\n",
    "# alpha_bar has T+1 entries (indices 0 through T)\n",
    "# alpha_t is defined for t=1..T as: alpha_t = alpha_bar[t] / alpha_bar[t-1]\n",
    "alpha = torch.zeros(T + 1)\n",
    "alpha[0] = 1.0  # not used in sampling, but keeps indexing clean\n",
    "alpha[1:] = alpha_bar[1:] / alpha_bar[:-1]\n",
    "\n",
    "beta = 1.0 - alpha\n",
    "sigma = torch.sqrt(beta)\n",
    "\n",
    "print(f'alpha_bar shape: {alpha_bar.shape}')\n",
    "print(f'alpha_bar[0]   = {alpha_bar[0]:.4f}   (clean image)')\n",
    "print(f'alpha_bar[500] = {alpha_bar[500]:.4f}  (midpoint)')\n",
    "print(f'alpha_bar[999] = {alpha_bar[999]:.6f}  (near pure noise)')\n",
    "print()\n",
    "print(f'beta[500]  = {beta[500]:.6f}')\n",
    "print(f'sigma[500] = {sigma[500]:.6f}')\n",
    "print(f'alpha[500] = {alpha[500]:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Reverse Step Coefficients (Guided)\n",
    "\n",
    "The DDPM reverse step formula is:\n",
    "\n",
    "$$x_{t-1} = \\underbrace{\\frac{1}{\\sqrt{\\alpha_t}}}_{\\text{scaling factor}} \\left( x_t - \\underbrace{\\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}}}_{\\text{noise correction}} \\cdot \\epsilon_\\theta(x_t, t) \\right) + \\underbrace{\\sigma_t}_{\\text{noise injection}} \\cdot z$$\n",
    "\n",
    "Each of these three coefficients changes across the schedule. Let us compute them at different timesteps and see how the algorithm behaves differently at different noise levels.\n",
    "\n",
    "**Before running, predict:**\n",
    "- At t=950 (near pure noise), is the noise correction coefficient large or small? Is the model making bold changes or tiny refinements?\n",
    "- At t=50 (near clean), is the noise correction large or small?\n",
    "- Does the scaling factor $\\frac{1}{\\sqrt{\\alpha_t}}$ change dramatically across timesteps, or stay close to 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the three reverse step coefficients at several timesteps\n",
    "timesteps_to_check = [950, 800, 500, 200, 50]\n",
    "\n",
    "print(f'{\"t\":>5} | {\"1/sqrt(a_t)\":>12} | {\"beta_t/sqrt(1-ab_t)\":>20} | {\"sigma_t\":>10}')\n",
    "print('-' * 60)\n",
    "\n",
    "scaling_factors = []\n",
    "noise_corrections = []\n",
    "noise_injections = []\n",
    "\n",
    "for t in timesteps_to_check:\n",
    "    # The three coefficients from the reverse step formula\n",
    "    scaling_factor = 1.0 / torch.sqrt(alpha[t])\n",
    "    noise_correction = beta[t] / torch.sqrt(1.0 - alpha_bar[t])\n",
    "    noise_injection = sigma[t]\n",
    "    \n",
    "    scaling_factors.append(scaling_factor.item())\n",
    "    noise_corrections.append(noise_correction.item())\n",
    "    noise_injections.append(noise_injection.item())\n",
    "    \n",
    "    print(f'{t:>5} | {scaling_factor:>12.6f} | {noise_correction:>20.6f} | {noise_injection:>10.6f}')\n",
    "\n",
    "print()\n",
    "print('Observations:')\n",
    "print(f'  Scaling factor range: {min(scaling_factors):.4f} to {max(scaling_factors):.4f} (always close to 1)')\n",
    "print(f'  Noise correction range: {min(noise_corrections):.6f} to {max(noise_corrections):.6f}')\n",
    "print(f'  Noise injection range: {min(noise_injections):.6f} to {max(noise_injections):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how the coefficients change across the full schedule\n",
    "all_t = torch.arange(1, T + 1)\n",
    "all_scaling = 1.0 / torch.sqrt(alpha[1:])\n",
    "all_noise_corr = beta[1:] / torch.sqrt(1.0 - alpha_bar[1:])\n",
    "all_noise_inj = sigma[1:]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(all_t.numpy(), all_scaling.numpy(), color='#c084fc', linewidth=2)\n",
    "axes[0].set_title('Scaling factor\\n$1 / \\\\sqrt{\\\\alpha_t}$', fontsize=11)\n",
    "axes[0].set_xlabel('Timestep t')\n",
    "axes[0].set_ylabel('Value')\n",
    "\n",
    "axes[1].plot(all_t.numpy(), all_noise_corr.numpy(), color='#fca5a5', linewidth=2)\n",
    "axes[1].set_title('Noise correction\\n$\\\\beta_t / \\\\sqrt{1 - \\\\bar{\\\\alpha}_t}$', fontsize=11)\n",
    "axes[1].set_xlabel('Timestep t')\n",
    "axes[1].set_ylabel('Value')\n",
    "\n",
    "axes[2].plot(all_t.numpy(), all_noise_inj.numpy(), color='#86efac', linewidth=2)\n",
    "axes[2].set_title('Noise injection\\n$\\\\sigma_t = \\\\sqrt{\\\\beta_t}$', fontsize=11)\n",
    "axes[2].set_xlabel('Timestep t')\n",
    "axes[2].set_ylabel('Value')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axvline(x=500, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Reverse step coefficients across the schedule', y=1.02, fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You computed the three terms of the reverse step formula across the noise schedule. Key observations:\n",
    "\n",
    "- **Scaling factor** ($1/\\sqrt{\\alpha_t}$): Always close to 1. This compensates for how much the signal was scaled down at each step. Since $\\alpha_t$ is close to 1, this factor barely changes.\n",
    "\n",
    "- **Noise correction** ($\\beta_t / \\sqrt{1 - \\bar{\\alpha}_t}$): This is the interesting one. At high $t$ (near pure noise), the correction is larger\u2014the model is making bold structural decisions. At low $t$ (near clean), the correction is tiny\u2014the model is polishing fine details.\n",
    "\n",
    "- **Noise injection** ($\\sigma_t = \\sqrt{\\beta_t}$): Fresh noise added at each step for exploration. Larger at high $t$ where there is more room for diversity, smaller at low $t$ where the image is nearly committed.\n",
    "\n",
    "Same formula at every step. The coefficients change with the schedule, and the model adapts via the timestep embedding\u2014but the algorithm is identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Trace One Reverse Step at t=500 (Guided)\n",
    "\n",
    "Now let us apply the reverse step formula to an actual image. We will:\n",
    "1. Start with a clean T-shirt image $x_0$\n",
    "2. Use the forward process formula to create a noisy image $x_{500}$\n",
    "3. Pretend we have a perfect \"oracle\" model that knows the exact noise $\\epsilon$ (in training, this was the answer key\u2014in real sampling, the model only has its prediction $\\epsilon_\\theta$)\n",
    "4. Apply one reverse step to get $x_{499}$\n",
    "5. See if $x_{499}$ is slightly cleaner than $x_{500}$\n",
    "\n",
    "**Before running, predict:**\n",
    "- At t=500, the image is heavily noisy. After one reverse step, will x_499 look dramatically cleaner, slightly cleaner, or about the same?\n",
    "- The scaling factor is ~1 and the noise correction is small. What does that tell you about the size of each step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a T-shirt image\n",
    "x_0, label = dataset[0]\n",
    "print(f'Image: {class_names[label]}')\n",
    "\n",
    "# Step 1: Create x_500 using the forward process closed-form formula\n",
    "t = 500\n",
    "torch.manual_seed(42)\n",
    "epsilon = torch.randn_like(x_0)  # the true noise (our \"answer key\")\n",
    "\n",
    "x_t = torch.sqrt(alpha_bar[t]) * x_0 + torch.sqrt(1 - alpha_bar[t]) * epsilon\n",
    "print(f'Created x_{t} using forward formula')\n",
    "print(f'  alpha_bar[{t}] = {alpha_bar[t]:.4f}')\n",
    "print(f'  signal coeff = {torch.sqrt(alpha_bar[t]):.4f}')\n",
    "print(f'  noise coeff  = {torch.sqrt(1 - alpha_bar[t]):.4f}')\n",
    "\n",
    "# Step 2: Apply one reverse step using the ORACLE (true epsilon)\n",
    "# In real sampling, the model would predict epsilon_theta.\n",
    "# Here we cheat and use the true noise to verify the formula works.\n",
    "epsilon_theta = epsilon  # oracle: perfect prediction\n",
    "\n",
    "# The reverse step formula:\n",
    "# x_{t-1} = (1/sqrt(alpha_t)) * (x_t - (beta_t/sqrt(1-alpha_bar_t)) * epsilon_theta) + sigma_t * z\n",
    "torch.manual_seed(123)\n",
    "z = torch.randn_like(x_0)  # fresh noise for stochastic sampling\n",
    "\n",
    "scaling = 1.0 / torch.sqrt(alpha[t])\n",
    "noise_corr = beta[t] / torch.sqrt(1.0 - alpha_bar[t])\n",
    "\n",
    "x_t_minus_1 = scaling * (x_t - noise_corr * epsilon_theta) + sigma[t] * z\n",
    "\n",
    "print(f'\\nReverse step coefficients at t={t}:')\n",
    "print(f'  scaling factor = {scaling:.6f}')\n",
    "print(f'  noise correction = {noise_corr:.6f}')\n",
    "print(f'  noise injection (sigma) = {sigma[t]:.6f}')\n",
    "\n",
    "# Step 3: What SHOULD x_499 look like?\n",
    "# If the formula is correct, x_499 should be approximately what the forward\n",
    "# process would produce at t=499 from the same x_0.\n",
    "x_499_forward = torch.sqrt(alpha_bar[t - 1]) * x_0 + torch.sqrt(1 - alpha_bar[t - 1]) * epsilon\n",
    "\n",
    "# Visualize all three\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 3.5))\n",
    "\n",
    "images = [x_0, x_t, x_t_minus_1, x_499_forward]\n",
    "titles = [\n",
    "    f'x_0\\n(clean)',\n",
    "    f'x_{t}\\n(noisy input)',\n",
    "    f'x_{t-1} (reverse step)\\n(formula output)',\n",
    "    f'x_{t-1} (forward ref)\\n(what t={t-1} should look like)',\n",
    "]\n",
    "\n",
    "for i, (img, title) in enumerate(zip(images, titles)):\n",
    "    axes[i].imshow(img.squeeze(), cmap='gray', vmin=-2, vmax=2)\n",
    "    axes[i].set_title(title, fontsize=9)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('One reverse step: x_500 -> x_499', y=1.05, fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# How similar is the reverse step output to what we expect?\n",
    "# (Not exact because of the stochastic z term)\n",
    "mse_vs_forward = torch.mean((x_t_minus_1 - x_499_forward) ** 2).item()\n",
    "print(f'MSE between reverse step x_499 and forward reference x_499: {mse_vs_forward:.4f}')\n",
    "print(f'(Not zero because of the stochastic noise injection sigma_t * z)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You traced one reverse step of the DDPM sampling algorithm. Here is what each piece did:\n",
    "\n",
    "1. **Created $x_{500}$** from the clean image using the forward process formula (same closed-form teleportation from The Forward Process).\n",
    "2. **Applied the reverse step formula** using the oracle's perfect noise prediction. The formula removed a small amount of the predicted noise and added a small amount of fresh noise.\n",
    "3. **Compared** the result to what the forward process would produce at $t=499$. They are similar but not identical, because the stochastic $\\sigma_t \\cdot z$ term adds fresh randomness.\n",
    "\n",
    "The key insight: **one step barely changes the image**. $x_{499}$ looks almost identical to $x_{500}$. That is by design\u2014each step makes a tiny correction. You need 1,000 of these tiny corrections to go from pure noise to a clean image.\n",
    "\n",
    "Also notice: we used the **oracle** (true $\\epsilon$) here. In real sampling, the model only has its imperfect prediction $\\epsilon_\\theta$. That is why iterative small steps matter\u2014each step's error is tiny, and the model gets a fresh look at a slightly cleaner image next time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: One-Shot vs Multi-Step Denoising (Supported)\n",
    "\n",
    "In the web lesson, you learned that one-shot denoising\u2014jumping directly from $x_t$ to $\\hat{x}_0$ in a single step\u2014fails. The result is a foggy smear because a single imperfect prediction cannot recover all the lost information.\n",
    "\n",
    "Now you will see this firsthand. You will:\n",
    "1. Implement the one-shot formula (rearranging the forward process to solve for $x_0$)\n",
    "2. Implement a multi-step reverse process using the oracle\n",
    "3. Compare the results\n",
    "\n",
    "We use the forward process closed-form formula to simulate a \"perfect oracle\" model: given $x_0$, we can compute the true noise at any timestep. This lets us isolate the effect of iterating vs not iterating, without any model error.\n",
    "\n",
    "**Fill in the two TODO markers.** Each is 1-2 lines.\n",
    "\n",
    "<details>\n",
    "<summary>\ud83d\udca1 Solution</summary>\n",
    "\n",
    "The key insight: one-shot denoising rearranges the forward formula to solve for $x_0$ directly. Multi-step denoising applies the reverse step formula repeatedly. Even with a perfect oracle, the one-shot approach is fine\u2014but with an imperfect model, the multi-step approach is far better because errors at each step are small.\n",
    "\n",
    "```python\n",
    "# TODO 1: One-shot denoising\n",
    "x_0_oneshot = (x_t - torch.sqrt(1 - alpha_bar[start_t]) * epsilon_theta) / torch.sqrt(alpha_bar[start_t])\n",
    "\n",
    "# TODO 2: Apply the reverse step formula\n",
    "x_curr = scaling * (x_curr - noise_corr * eps_pred) + sigma[t] * z\n",
    "```\n",
    "\n",
    "Note: for TODO 2, the reverse step formula is the same one from Exercise 2, just inside a loop.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from a heavily noisy image\n",
    "x_0, label = dataset[0]\n",
    "start_t = 800\n",
    "\n",
    "torch.manual_seed(42)\n",
    "true_epsilon = torch.randn_like(x_0)\n",
    "\n",
    "# Create x_t at t=start_t\n",
    "x_t = torch.sqrt(alpha_bar[start_t]) * x_0 + torch.sqrt(1 - alpha_bar[start_t]) * true_epsilon\n",
    "\n",
    "\n",
    "# --- Method 1: One-shot denoising ---\n",
    "# Rearrange the forward formula: x_0 = (x_t - sqrt(1 - alpha_bar_t) * epsilon) / sqrt(alpha_bar_t)\n",
    "# Use the TRUE epsilon (oracle) for a fair comparison.\n",
    "epsilon_theta = true_epsilon\n",
    "\n",
    "# TODO 1: Compute the one-shot estimate of x_0\n",
    "# Use the rearranged forward formula with the oracle's noise prediction.\n",
    "# x_0_oneshot = ???\n",
    "\n",
    "\n",
    "# --- Method 2: Multi-step reverse process ---\n",
    "# Apply the reverse step formula from t=start_t down to t=1.\n",
    "# For each step, use the oracle to get the true noise at that timestep.\n",
    "x_curr = x_t.clone()\n",
    "num_steps = start_t\n",
    "\n",
    "torch.manual_seed(999)  # fresh seed for the z noise in reverse steps\n",
    "\n",
    "for t in range(start_t, 0, -1):\n",
    "    # Oracle: compute the true noise at timestep t\n",
    "    # Using the closed-form: x_t = sqrt(ab_t) * x_0 + sqrt(1-ab_t) * eps\n",
    "    # So: eps = (x_t_from_x0 - sqrt(ab_t) * x_0) / sqrt(1-ab_t)\n",
    "    # But we are cheating: we just use the forward formula to compute the true noise.\n",
    "    # In reality, the model would predict epsilon from x_curr and t.\n",
    "    eps_pred = (x_curr - torch.sqrt(alpha_bar[t]) * x_0) / torch.sqrt(1 - alpha_bar[t])\n",
    "    \n",
    "    # Reverse step coefficients\n",
    "    scaling = 1.0 / torch.sqrt(alpha[t])\n",
    "    noise_corr = beta[t] / torch.sqrt(1.0 - alpha_bar[t])\n",
    "    \n",
    "    # Sample z (zero at the final step t=1)\n",
    "    z = torch.randn_like(x_curr) if t > 1 else torch.zeros_like(x_curr)\n",
    "    \n",
    "    # TODO 2: Apply the reverse step formula to get x_{t-1}\n",
    "    # x_curr = ???\n",
    "    pass\n",
    "\n",
    "x_0_multistep = x_curr\n",
    "\n",
    "\n",
    "# --- Visualize the comparison ---\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 3.5))\n",
    "\n",
    "images = [x_0, x_t, x_0_oneshot, x_0_multistep]\n",
    "titles = [\n",
    "    'Original x_0',\n",
    "    f'Noisy x_{start_t}',\n",
    "    'One-shot denoising\\n(single formula)',\n",
    "    f'Multi-step denoising\\n({num_steps} reverse steps)',\n",
    "]\n",
    "\n",
    "for i, (img, title) in enumerate(zip(images, titles)):\n",
    "    axes[i].imshow(img.squeeze().clamp(-1, 1), cmap='gray', vmin=-2, vmax=2)\n",
    "    axes[i].set_title(title, fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('One-shot vs multi-step denoising (oracle model)', y=1.05, fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quantitative comparison\n",
    "mse_oneshot = torch.mean((x_0_oneshot - x_0) ** 2).item()\n",
    "mse_multistep = torch.mean((x_0_multistep - x_0) ** 2).item()\n",
    "print(f'MSE (one-shot vs original):    {mse_oneshot:.4f}')\n",
    "print(f'MSE (multi-step vs original):  {mse_multistep:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "With a **perfect oracle**, both methods recover the image well. The one-shot method works here because the oracle knows the exact noise. But remember: in real sampling, the model does NOT know the exact noise\u2014it only predicts $\\epsilon_\\theta$, which is imperfect.\n",
    "\n",
    "Now let us see what happens with an **imperfect** predictor. We will add a small amount of noise to the oracle's prediction, simulating a model that is good but not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate an IMPERFECT model by adding noise to the oracle's prediction\n",
    "# This is the realistic case: the model's prediction is close but not perfect.\n",
    "noise_level = 0.3  # how imperfect the model is\n",
    "\n",
    "# --- One-shot with imperfect model ---\n",
    "torch.manual_seed(77)\n",
    "noisy_pred = true_epsilon + noise_level * torch.randn_like(true_epsilon)\n",
    "x_0_oneshot_noisy = (x_t - torch.sqrt(1 - alpha_bar[start_t]) * noisy_pred) / torch.sqrt(alpha_bar[start_t])\n",
    "\n",
    "# --- Multi-step with imperfect model ---\n",
    "x_curr = x_t.clone()\n",
    "torch.manual_seed(999)\n",
    "\n",
    "for t in range(start_t, 0, -1):\n",
    "    # Oracle prediction with added noise (simulating model imperfection)\n",
    "    true_eps_at_t = (x_curr - torch.sqrt(alpha_bar[t]) * x_0) / torch.sqrt(1 - alpha_bar[t])\n",
    "    eps_pred = true_eps_at_t + noise_level * torch.randn_like(x_curr)\n",
    "    \n",
    "    scaling = 1.0 / torch.sqrt(alpha[t])\n",
    "    noise_corr = beta[t] / torch.sqrt(1.0 - alpha_bar[t])\n",
    "    z = torch.randn_like(x_curr) if t > 1 else torch.zeros_like(x_curr)\n",
    "    \n",
    "    x_curr = scaling * (x_curr - noise_corr * eps_pred) + sigma[t] * z\n",
    "\n",
    "x_0_multistep_noisy = x_curr\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 3.5))\n",
    "\n",
    "images = [x_0, x_t, x_0_oneshot_noisy, x_0_multistep_noisy]\n",
    "titles = [\n",
    "    'Original x_0',\n",
    "    f'Noisy x_{start_t}',\n",
    "    'One-shot (imperfect)\\nBLURRY / BROKEN',\n",
    "    f'Multi-step (imperfect)\\n{start_t} reverse steps',\n",
    "]\n",
    "colors = ['white', 'white', '#fca5a5', '#86efac']\n",
    "\n",
    "for i, (img, title, color) in enumerate(zip(images, titles, colors)):\n",
    "    axes[i].imshow(img.squeeze().clamp(-2, 2), cmap='gray', vmin=-2, vmax=2)\n",
    "    axes[i].set_title(title, fontsize=10, color=color)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Imperfect model: one-shot fails, multi-step recovers', y=1.05, fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mse_oneshot_noisy = torch.mean((x_0_oneshot_noisy - x_0) ** 2).item()\n",
    "mse_multistep_noisy = torch.mean((x_0_multistep_noisy - x_0) ** 2).item()\n",
    "print(f'MSE (one-shot, imperfect):    {mse_oneshot_noisy:.4f}')\n",
    "print(f'MSE (multi-step, imperfect):  {mse_multistep_noisy:.4f}')\n",
    "print()\n",
    "print('With an imperfect model, one-shot denoising amplifies errors catastrophically.')\n",
    "print('Multi-step denoising keeps errors small at each step, and the model gets a')\n",
    "print('fresh look at a slightly cleaner image every time. This is why 1,000 steps.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "This is the core lesson: **one-shot denoising amplifies prediction errors catastrophically.** At $t=800$, $\\bar{\\alpha}_t$ is small, so dividing by $\\sqrt{\\bar{\\alpha}_t}$ in the one-shot formula magnifies any error in the noise prediction. A small mistake in $\\epsilon_\\theta$ becomes a large mistake in $\\hat{x}_0$.\n",
    "\n",
    "Multi-step denoising avoids this because:\n",
    "1. Each step makes a **tiny** correction (the noise correction coefficient is small)\n",
    "2. Errors at each step are proportionally small\n",
    "3. The model gets a **fresh look** at a slightly cleaner image next time, so it can course-correct\n",
    "4. The stochastic noise injection ($\\sigma_t \\cdot z$) prevents the process from committing too early to one interpretation\n",
    "\n",
    "This is why DDPM uses 1,000 steps. Not for elegance\u2014for robustness against imperfect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: The Full Sampling Loop (Independent)\n",
    "\n",
    "Implement the complete DDPM sampling algorithm. Start from pure Gaussian noise $x_T \\sim \\mathcal{N}(0, I)$ and iteratively denoise to produce a generated image.\n",
    "\n",
    "Since we do not have a trained neural network (that comes in Lesson 5), you will use a **dummy oracle model**: at each step, the oracle computes what the noise *would be* if the target image were the T-shirt from the dataset. This simulates a perfectly trained model that always generates one specific image.\n",
    "\n",
    "**Your task:**\n",
    "1. Write a `ddpm_sample()` function that implements the full sampling algorithm\n",
    "2. The function should accept a target image `x_target` (the oracle's \"knowledge\")\n",
    "3. Save snapshots at key timesteps to visualize the denoising trajectory\n",
    "4. Return the final generated image\n",
    "\n",
    "**Algorithm (from the web lesson):**\n",
    "1. Sample $x_T \\sim \\mathcal{N}(0, I)$\n",
    "2. For $t = T, T-1, \\ldots, 1$:\n",
    "   - If $t > 1$, sample $z \\sim \\mathcal{N}(0, I)$. If $t = 1$, set $z = 0$.\n",
    "   - Predict noise: $\\epsilon_\\theta = \\text{oracle}(x_t, t, x_{\\text{target}})$\n",
    "   - Compute: $x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\cdot \\epsilon_\\theta \\right) + \\sigma_t \\cdot z$\n",
    "3. Return $x_0$\n",
    "\n",
    "**Oracle model:** Given $x_t$, timestep $t$, and the target $x_{\\text{target}}$, the oracle computes the noise as:\n",
    "$$\\epsilon_{\\text{oracle}} = \\frac{x_t - \\sqrt{\\bar{\\alpha}_t} \\cdot x_{\\text{target}}}{\\sqrt{1 - \\bar{\\alpha}_t}}$$\n",
    "\n",
    "This is just rearranging the forward formula\u2014the oracle \"knows\" the target and can compute what noise would have been needed.\n",
    "\n",
    "<details>\n",
    "<summary>\ud83d\udca1 Solution</summary>\n",
    "\n",
    "The sampling loop is the reverse step formula applied T times. The oracle provides a stand-in for a trained model. The key details are: (1) start from pure noise, (2) loop from T down to 1, (3) set z=0 at the final step (t=1), and (4) save snapshots at specific timesteps for visualization.\n",
    "\n",
    "```python\n",
    "def ddpm_sample(x_target, alpha_bar, alpha, beta, sigma, T, snapshot_timesteps=None):\n",
    "    \"\"\"Full DDPM sampling loop using an oracle model.\"\"\"\n",
    "    shape = x_target.shape\n",
    "    snapshots = {}\n",
    "    \n",
    "    # Step 1: Start from pure noise\n",
    "    x = torch.randn(shape)\n",
    "    \n",
    "    if snapshot_timesteps and T in snapshot_timesteps:\n",
    "        snapshots[T] = x.clone()\n",
    "    \n",
    "    # Step 2: Loop from T down to 1\n",
    "    for t in range(T, 0, -1):\n",
    "        # Sample z (zero at final step)\n",
    "        z = torch.randn_like(x) if t > 1 else torch.zeros_like(x)\n",
    "        \n",
    "        # Oracle noise prediction\n",
    "        eps_pred = (x - torch.sqrt(alpha_bar[t]) * x_target) / torch.sqrt(1 - alpha_bar[t])\n",
    "        \n",
    "        # Reverse step\n",
    "        scaling = 1.0 / torch.sqrt(alpha[t])\n",
    "        noise_corr = beta[t] / torch.sqrt(1 - alpha_bar[t])\n",
    "        x = scaling * (x - noise_corr * eps_pred) + sigma[t] * z\n",
    "        \n",
    "        if snapshot_timesteps and (t - 1) in snapshot_timesteps:\n",
    "            snapshots[t - 1] = x.clone()\n",
    "    \n",
    "    return x, snapshots\n",
    "```\n",
    "\n",
    "Common mistakes:\n",
    "- Forgetting to set `z = 0` at `t = 1`\u2014this adds noise to the final output\n",
    "- Using `alpha_bar` instead of `alpha` for the scaling factor\u2014they are different quantities\n",
    "- Looping from 0 to T instead of T down to 1\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Implement the ddpm_sample function.\n",
    "# Parameters: x_target, alpha_bar, alpha, beta, sigma, T, snapshot_timesteps (list of timesteps to save)\n",
    "# Returns: (final_image, snapshots_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sampling loop and visualize the trajectory\n",
    "x_target = dataset[0][0]  # T-shirt\n",
    "\n",
    "snapshot_steps = [1000, 900, 800, 600, 400, 200, 100, 50, 0]\n",
    "\n",
    "torch.manual_seed(42)\n",
    "generated, snapshots = ddpm_sample(\n",
    "    x_target, alpha_bar, alpha, beta, sigma, T,\n",
    "    snapshot_timesteps=snapshot_steps\n",
    ")\n",
    "\n",
    "# Visualize the denoising trajectory\n",
    "fig, axes = plt.subplots(1, len(snapshot_steps), figsize=(18, 3))\n",
    "\n",
    "for i, t in enumerate(snapshot_steps):\n",
    "    img = snapshots[t].squeeze().clamp(-2, 2)\n",
    "    axes[i].imshow(img, cmap='gray', vmin=-2, vmax=2)\n",
    "    axes[i].set_title(f't={t}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('The denoising trajectory: pure noise to generated image', y=1.05, fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare final result to original\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "axes[0].imshow(x_target.squeeze(), cmap='gray', vmin=-2, vmax=2)\n",
    "axes[0].set_title('Original T-shirt', fontsize=11)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(generated.squeeze().clamp(-2, 2), cmap='gray', vmin=-2, vmax=2)\n",
    "axes[1].set_title('Generated (oracle sampling)', fontsize=11)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mse = torch.mean((generated - x_target) ** 2).item()\n",
    "print(f'MSE between generated and original: {mse:.4f}')\n",
    "print(f'(Not zero because of the stochastic noise injection at each step)')\n",
    "print(f'Number of \"forward passes\" (steps): {T}')\n",
    "print(f'In real DDPM, each step is a neural network forward pass.')\n",
    "print(f'That is {T} evaluations for ONE image.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You implemented the complete DDPM sampling algorithm. The trajectory shows exactly what the web lesson described:\n",
    "\n",
    "- **t=1000 to t=800:** Pure static begins to show the vaguest hint of structure. The oracle is making coarse, global decisions.\n",
    "- **t=600 to t=400:** A recognizable shape emerges. Edges form, proportions become clear.\n",
    "- **t=200 to t=50:** Fine details appear. Textures, subtle shading, the outline solidifies.\n",
    "- **t=0:** The generated image. Not identical to the original because of the stochastic noise injection at each step\u2014but structurally the same.\n",
    "\n",
    "**The cost is real.** 1,000 steps. In a real diffusion model, each step is a full neural network forward pass through a U-Net. At 28\u00d728, this runs in seconds. At 512\u00d7512 (Stable Diffusion scale), each step takes much longer. That is the pain that motivates accelerated samplers, DDIM, latent diffusion, and everything that follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The reverse step coefficients change with the schedule.** The scaling factor stays close to 1, but the noise correction is larger at high $t$ (bold structural decisions) and tiny at low $t$ (fine detail polishing). Same formula, different regime.\n",
    "\n",
    "2. **One step barely changes the image.** Each reverse step makes a tiny correction. You need 1,000 of them\u2014and that is a strength, not a weakness. Small steps mean small errors per step.\n",
    "\n",
    "3. **One-shot denoising fails with imperfect predictions.** Dividing by $\\sqrt{\\bar{\\alpha}_t}$ amplifies any error in the noise prediction. Multi-step denoising is robust because each step's error is proportionally small and the model gets a fresh look at a slightly cleaner image.\n",
    "\n",
    "4. **The full sampling algorithm is a loop: start from noise, apply the reverse step formula T times, return $x_0$.** Set $z = 0$ at the final step ($t = 1$)\u2014the last step commits, every step before it explores.\n",
    "\n",
    "5. **The denoising trajectory is coarse-to-fine.** Early steps create structure from nothing. Late steps refine details. Not all steps are created equal.\n",
    "\n",
    "**Mental model:** Destruction was easy and known. Creation requires a trained guide. At each step, the guide points toward the clean image, but a small jitter keeps the path from collapsing to a single boring route. 1,000 tiny corrections compose into something that never existed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}