{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Training Loop: Putting It All Together\n",
    "\n",
    "In this notebook, you'll go from understanding the individual pieces (loss functions, optimizers, gradients) to wiring them into a complete PyTorch training loop.\n",
    "\n",
    "**What you'll do:**\n",
    "- Verify that PyTorch's built-in loss matches your manual calculation\n",
    "- Verify that the optimizer does the same thing as a manual parameter update\n",
    "- Write a complete training loop from scratch\n",
    "- Compare SGD vs Adam convergence\n",
    "- Diagnose a common training bug (missing `zero_grad`)\n",
    "- (Stretch) Train a 2-layer network on nonlinear data\n",
    "\n",
    "**Predict-first methodology:** Before running each cell, predict what you think the output will be. This forces active engagement and builds real intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Verify nn.MSELoss Matches Manual MSE\n",
    "\n",
    "**Type: GUIDED** — Follow along, fill in the marked lines.\n",
    "\n",
    "Before using `nn.MSELoss` in a training loop, you should know exactly what it computes. The formula is:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "Your task: compute MSE manually, then verify it matches `nn.MSELoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some fake predictions and targets\n",
    "y_hat = torch.tensor([2.5, 0.0, 1.0, 3.5])\n",
    "y = torch.tensor([3.0, -0.5, 0.0, 2.0])\n",
    "\n",
    "# --- Manual MSE ---\n",
    "# Step 1: Compute the differences\n",
    "diffs = y_hat - y\n",
    "print(f'Differences: {diffs}')\n",
    "\n",
    "# Step 2: Square them\n",
    "squared = diffs ** 2\n",
    "print(f'Squared:     {squared}')\n",
    "\n",
    "# Step 3: Take the mean\n",
    "manual_mse = squared.mean()\n",
    "print(f'Manual MSE:  {manual_mse.item():.6f}')\n",
    "\n",
    "# --- PyTorch MSE ---\n",
    "criterion = nn.MSELoss()\n",
    "pytorch_mse = criterion(y_hat, y)\n",
    "print(f'PyTorch MSE: {pytorch_mse.item():.6f}')\n",
    "\n",
    "# --- Verify they match ---\n",
    "match = torch.allclose(manual_mse, pytorch_mse)\n",
    "print(f'\\nMatch: {match}')\n",
    "\n",
    "if match:\n",
    "    print('nn.MSELoss is doing exactly what you think it does. No magic.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just proved:** `nn.MSELoss` is literally `((y_hat - y)**2).mean()`. There's no hidden complexity. When you see it in a training loop, you know exactly what's happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Verify optimizer.step() Matches Manual Update\n",
    "\n",
    "**Type: GUIDED** — Follow along, compare the results.\n",
    "\n",
    "SGD with learning rate `lr` does exactly one thing per parameter:\n",
    "\n",
    "$$p \\leftarrow p - \\text{lr} \\times \\frac{\\partial L}{\\partial p}$$\n",
    "\n",
    "Let's prove that `optimizer.step()` does the same thing as doing it by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "\n",
    "# --- Method A: Manual update ---\n",
    "torch.manual_seed(0)\n",
    "model_manual = nn.Linear(1, 1)\n",
    "\n",
    "# Record initial parameter values\n",
    "w_before = model_manual.weight.data.clone()\n",
    "b_before = model_manual.bias.data.clone()\n",
    "print(f'Initial weight: {w_before.item():.6f}')\n",
    "print(f'Initial bias:   {b_before.item():.6f}')\n",
    "\n",
    "# Forward pass to get gradients\n",
    "x = torch.tensor([[2.0]])\n",
    "y = torch.tensor([[5.0]])\n",
    "pred = model_manual(x)\n",
    "loss = nn.MSELoss()(pred, y)\n",
    "loss.backward()\n",
    "\n",
    "print(f'\\nWeight grad: {model_manual.weight.grad.item():.6f}')\n",
    "print(f'Bias grad:   {model_manual.bias.grad.item():.6f}')\n",
    "\n",
    "# Manual update: p = p - lr * grad\n",
    "with torch.no_grad():\n",
    "    model_manual.weight -= lr * model_manual.weight.grad\n",
    "    model_manual.bias -= lr * model_manual.bias.grad\n",
    "\n",
    "w_manual = model_manual.weight.data.clone()\n",
    "b_manual = model_manual.bias.data.clone()\n",
    "print(f'\\nAfter manual update:')\n",
    "print(f'  weight: {w_manual.item():.6f}')\n",
    "print(f'  bias:   {b_manual.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Method B: optimizer.step() ---\n",
    "torch.manual_seed(0)  # Same init as above\n",
    "model_optim = nn.Linear(1, 1)\n",
    "optimizer = optim.SGD(model_optim.parameters(), lr=lr)\n",
    "\n",
    "# Same forward pass\n",
    "x = torch.tensor([[2.0]])\n",
    "y = torch.tensor([[5.0]])\n",
    "pred = model_optim(x)\n",
    "loss = nn.MSELoss()(pred, y)\n",
    "loss.backward()\n",
    "\n",
    "# Optimizer update\n",
    "optimizer.step()\n",
    "\n",
    "w_optim = model_optim.weight.data.clone()\n",
    "b_optim = model_optim.bias.data.clone()\n",
    "print(f'After optimizer.step():')\n",
    "print(f'  weight: {w_optim.item():.6f}')\n",
    "print(f'  bias:   {b_optim.item():.6f}')\n",
    "\n",
    "# --- Compare ---\n",
    "print(f'\\nWeights match: {torch.allclose(w_manual, w_optim)}')\n",
    "print(f'Biases match:  {torch.allclose(b_manual, b_optim)}')\n",
    "print('\\noptimizer.step() is just p -= lr * p.grad for every parameter. No magic.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just proved:** `optimizer.step()` with SGD does the exact same subtraction you'd do by hand. It just does it for every parameter in the model at once. The optimizer is a convenience, not a mystery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Train Linear Regression in PyTorch\n",
    "\n",
    "**Type: SUPPORTED** — Template provided, you fill in the training loop.\n",
    "\n",
    "Now wire the pieces together. You'll train a model to learn `y = 3x - 2` from noisy data.\n",
    "\n",
    "The training loop pattern:\n",
    "```\n",
    "for each epoch:\n",
    "    1. Forward pass:  predictions = model(x)\n",
    "    2. Compute loss:  loss = criterion(predictions, y)\n",
    "    3. Zero grads:    optimizer.zero_grad()\n",
    "    4. Backward:      loss.backward()\n",
    "    5. Update:        optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data: y = 3x - 2 + noise\n",
    "torch.manual_seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "X = torch.randn(n_samples, 1) * 3  # Random x values\n",
    "y = 3 * X - 2 + torch.randn(n_samples, 1) * 0.5  # y = 3x - 2 + noise\n",
    "\n",
    "# Visualize the data\n",
    "plt.scatter(X.numpy(), y.numpy(), alpha=0.5, s=20)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Training Data: y = 3x - 2 + noise')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Your training loop ---\n",
    "\n",
    "# Model: a single linear layer (1 input -> 1 output)\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer: SGD with learning rate 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Record initial parameters\n",
    "print(f'Before training:')\n",
    "print(f'  weight = {model.weight.item():.4f} (target: 3.0)')\n",
    "print(f'  bias   = {model.bias.item():.4f} (target: -2.0)')\n",
    "\n",
    "# Training\n",
    "n_epochs = 100\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # 1. Forward pass\n",
    "    predictions = model(X)\n",
    "    \n",
    "    # 2. Compute loss\n",
    "    loss = criterion(predictions, y)\n",
    "    \n",
    "    # 3. Zero gradients (critical! you'll see why in Exercise 5)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Record loss\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Print every 20 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'  Epoch {epoch+1:3d}: loss = {loss.item():.4f}')\n",
    "\n",
    "print(f'\\nAfter training:')\n",
    "print(f'  weight = {model.weight.item():.4f} (target: 3.0)')\n",
    "print(f'  bias   = {model.bias.item():.4f} (target: -2.0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(losses, linewidth=2, color='#ff6b6b')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Final fit\n",
    "x_line = torch.linspace(X.min(), X.max(), 100).unsqueeze(1)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_line)\n",
    "\n",
    "axes[1].scatter(X.numpy(), y.numpy(), alpha=0.4, s=20, label='Data')\n",
    "axes[1].plot(x_line.numpy(), y_pred.numpy(), color='#ff6b6b', linewidth=2,\n",
    "             label=f'Learned: y = {model.weight.item():.2f}x + ({model.bias.item():.2f})')\n",
    "axes[1].plot(x_line.numpy(), 3 * x_line.numpy() - 2, '--', color='#51cf66', linewidth=2,\n",
    "             label='True: y = 3x - 2', alpha=0.7)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Learned Fit vs True Function')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Swap SGD for Adam\n",
    "\n",
    "**Type: SUPPORTED** — One-line change, observe the difference.\n",
    "\n",
    "Adam adapts the learning rate per-parameter using momentum and squared gradient history. In practice, this often means faster convergence.\n",
    "\n",
    "Your task: run the same training as Exercise 3, but swap `optim.SGD` for `optim.Adam`. Compare the loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SGD training ---\n",
    "torch.manual_seed(42)\n",
    "model_sgd = nn.Linear(1, 1)\n",
    "optimizer_sgd = optim.SGD(model_sgd.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "losses_sgd = []\n",
    "for epoch in range(100):\n",
    "    pred = model_sgd(X)\n",
    "    loss = criterion(pred, y)\n",
    "    optimizer_sgd.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_sgd.step()\n",
    "    losses_sgd.append(loss.item())\n",
    "\n",
    "# --- Adam training ---\n",
    "torch.manual_seed(42)\n",
    "model_adam = nn.Linear(1, 1)\n",
    "optimizer_adam = optim.Adam(model_adam.parameters(), lr=0.01)  # <-- The only change\n",
    "\n",
    "losses_adam = []\n",
    "for epoch in range(100):\n",
    "    pred = model_adam(X)\n",
    "    loss = criterion(pred, y)\n",
    "    optimizer_adam.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "    losses_adam.append(loss.item())\n",
    "\n",
    "print(f'SGD  — Final loss: {losses_sgd[-1]:.4f}, weight: {model_sgd.weight.item():.4f}, bias: {model_sgd.bias.item():.4f}')\n",
    "print(f'Adam — Final loss: {losses_adam[-1]:.4f}, weight: {model_adam.weight.item():.4f}, bias: {model_adam.bias.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare loss curves side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(losses_sgd, linewidth=2, color='#ff6b6b', label='SGD')\n",
    "axes[0].plot(losses_adam, linewidth=2, color='#4ecdc4', label='Adam')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Loss Curves: SGD vs Adam')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Zoomed in on early epochs\n",
    "axes[1].plot(losses_sgd[:30], linewidth=2, color='#ff6b6b', label='SGD')\n",
    "axes[1].plot(losses_adam[:30], linewidth=2, color='#4ecdc4', label='Adam')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MSE Loss')\n",
    "axes[1].set_title('First 30 Epochs (Zoomed)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nNotice: Adam typically converges faster in the early epochs.')\n",
    "print('Both reach a similar final loss, but Adam gets there quicker.')\n",
    "print('This is why Adam is the default choice for most deep learning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Diagnose the Accumulation Bug\n",
    "\n",
    "**Type: SUPPORTED** — Predict, observe, fix.\n",
    "\n",
    "The code below has a training loop with a **common bug**. Before running it:\n",
    "\n",
    "1. Read the code carefully\n",
    "2. **Predict** what will go wrong\n",
    "3. Run it and see if you were right\n",
    "4. Fix the bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BUGGY training loop ---\n",
    "# Read this carefully. What's wrong?\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_buggy = nn.Linear(1, 1)\n",
    "optimizer_buggy = optim.SGD(model_buggy.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "losses_buggy = []\n",
    "grad_norms = []  # Track gradient magnitude\n",
    "\n",
    "for epoch in range(100):\n",
    "    pred = model_buggy(X)\n",
    "    loss = criterion(pred, y)\n",
    "    \n",
    "    # BUG: Where's optimizer.zero_grad()?\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # Record gradient magnitude before stepping\n",
    "    grad_norm = model_buggy.weight.grad.norm().item()\n",
    "    grad_norms.append(grad_norm)\n",
    "    \n",
    "    optimizer_buggy.step()\n",
    "    losses_buggy.append(loss.item())\n",
    "\n",
    "print(f'Final loss: {losses_buggy[-1]:.4f}')\n",
    "print(f'Weight: {model_buggy.weight.item():.4f} (target: 3.0)')\n",
    "print(f'Bias:   {model_buggy.bias.item():.4f} (target: -2.0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the bug\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss comparison\n",
    "axes[0].plot(losses_sgd, linewidth=2, color='#51cf66', label='Correct (with zero_grad)')\n",
    "axes[0].plot(losses_buggy, linewidth=2, color='#ff6b6b', label='Buggy (no zero_grad)')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Loss: Correct vs Buggy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Gradient norms\n",
    "axes[1].plot(grad_norms, linewidth=2, color='#ff6b6b')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Gradient Norm')\n",
    "axes[1].set_title('Gradient Magnitude Over Time (no zero_grad)')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nWithout zero_grad(), gradients ACCUMULATE across epochs.')\n",
    "print('Each .backward() ADDS to the existing .grad, it does not replace it.')\n",
    "print('This makes the effective gradient grow every iteration,')\n",
    "print('causing the optimizer to take increasingly large steps.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FIXED training loop ---\n",
    "# Add the missing line and verify it works\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_fixed = nn.Linear(1, 1)\n",
    "optimizer_fixed = optim.SGD(model_fixed.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "losses_fixed = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    pred = model_fixed(X)\n",
    "    loss = criterion(pred, y)\n",
    "    \n",
    "    optimizer_fixed.zero_grad()  # <-- THE FIX: zero gradients before backward\n",
    "    loss.backward()\n",
    "    optimizer_fixed.step()\n",
    "    \n",
    "    losses_fixed.append(loss.item())\n",
    "\n",
    "print(f'Fixed — Final loss: {losses_fixed[-1]:.4f}')\n",
    "print(f'Weight: {model_fixed.weight.item():.4f} (target: 3.0)')\n",
    "print(f'Bias:   {model_fixed.bias.item():.4f} (target: -2.0)')\n",
    "print(f'\\nLesson: always call optimizer.zero_grad() before loss.backward().')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does PyTorch accumulate gradients by default?**\n",
    "\n",
    "It seems like a footgun, but it's actually useful. When your batch is too large to fit in memory, you can split it into mini-batches, call `.backward()` on each, and the gradients accumulate. Then you call `optimizer.step()` once. This is called **gradient accumulation** and it's a real technique used in LLM training.\n",
    "\n",
    "For standard training loops though: always `zero_grad()` first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 6 (Stretch): Train a 2-Layer Network on Nonlinear Data\n",
    "\n",
    "**Type: INDEPENDENT** — Minimal guidance. You have all the pieces.\n",
    "\n",
    "A single `nn.Linear` can only learn straight lines. For `y = x^2`, you need at least one hidden layer with a nonlinear activation.\n",
    "\n",
    "Your task:\n",
    "1. Generate data from `y = x^2 + noise`\n",
    "2. Build a 2-layer network: `Linear(1, 32) -> ReLU -> Linear(32, 1)`\n",
    "3. Train it with the same loop pattern\n",
    "4. Plot the learned curve vs the true curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate nonlinear data ---\n",
    "torch.manual_seed(42)\n",
    "X_nl = torch.linspace(-3, 3, 200).unsqueeze(1)  # 200 points from -3 to 3\n",
    "y_nl = X_nl ** 2 + torch.randn(200, 1) * 0.3    # y = x^2 + noise\n",
    "\n",
    "plt.scatter(X_nl.numpy(), y_nl.numpy(), alpha=0.4, s=15)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Nonlinear Data: y = x² + noise')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build and train your network ---\n",
    "\n",
    "# Model: 2-layer network with ReLU\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(1, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1)\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 500\n",
    "losses_nl = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    pred = net(X_nl)\n",
    "    loss = criterion(pred, y_nl)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses_nl.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'  Epoch {epoch+1}: loss = {loss.item():.4f}')\n",
    "\n",
    "print(f'\\nFinal loss: {losses_nl[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize the result ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(losses_nl, linewidth=2, color='#ff6b6b')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Learned curve vs true curve\n",
    "x_plot = torch.linspace(-3, 3, 300).unsqueeze(1)\n",
    "with torch.no_grad():\n",
    "    y_learned = net(x_plot)\n",
    "\n",
    "axes[1].scatter(X_nl.numpy(), y_nl.numpy(), alpha=0.3, s=10, label='Data')\n",
    "axes[1].plot(x_plot.numpy(), x_plot.numpy() ** 2, '--', color='#51cf66',\n",
    "             linewidth=2, label='True: y = x²')\n",
    "axes[1].plot(x_plot.numpy(), y_learned.numpy(), color='#ff6b6b',\n",
    "             linewidth=2, label='Learned curve')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Learned Curve vs True Function')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('A linear model cannot learn y = x². But Linear + ReLU + Linear can.')\n",
    "print('The ReLU activation is what allows the network to bend.')\n",
    "print('This is why neural networks use nonlinear activations between layers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **`nn.MSELoss` is just `((y_hat - y)**2).mean()`** — no hidden complexity\n",
    "2. **`optimizer.step()` is just `p -= lr * p.grad`** for every parameter — the optimizer is a convenience, not magic\n",
    "3. **The training loop is always the same pattern:** forward -> loss -> zero_grad -> backward -> step\n",
    "4. **Adam converges faster than SGD** for the same learning rate on most problems\n",
    "5. **Always call `zero_grad()` before `backward()`** — PyTorch accumulates gradients by default, which is useful for gradient accumulation but a bug if you forget\n",
    "6. **Nonlinear activations (ReLU) are essential** — without them, stacking linear layers just gives you another linear layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}