{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Linear Regression from Scratch\n\nIn this notebook, you'll implement linear regression with gradient descent using only NumPy.\n\n**What you'll do:**\n- Generate synthetic data and visualize the learning target\n- Implement the training loop step by step (forward → loss → backward → update)\n- Train the model and watch parameters converge in real-time\n- Experiment with learning rates to build intuition for gradient descent\n\n**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones — they reveal gaps in your mental model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nimport time\n\n# Reproducibility\nnp.random.seed(42)\n\n# For nice plots\nplt.style.use('dark_background')\nplt.rcParams['figure.figsize'] = [10, 4]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 1: Generate Synthetic Data (Guided)\n\nWe'll create data that follows the pattern: `y = 1.5x + 2 + noise`\n\nOur goal: learn `w = 1.5` and `b = 2` from the data alone.\n\n**Before running, predict:** What will the scatter plot look like? Will the points form a perfect line, or will there be spread?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# True parameters (what we're trying to learn)\nTRUE_W = 1.5\nTRUE_B = 2.0\n\n# Generate data\nn_samples = 100\n\nX = np.random.uniform(-5, 5, n_samples)\nnoise = np.random.normal(0, 1.5, n_samples)\ny = TRUE_W * X + TRUE_B + noise\n\n# Visualize\nplt.scatter(X, y, alpha=0.6, label='Data')\nplt.plot(X, TRUE_W * X + TRUE_B, 'g--', label=f'True: y = {TRUE_W}x + {TRUE_B}')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.title('Our Training Data')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 2: Initialize and Predict (Guided)\n\nWe start with `w = 0` and `b = 0`. The model knows nothing yet.\n\n**Before running, predict:** With `w = 0` and `b = 0`, what will the model predict for any input x? (Hint: what is `0 * x + 0`?)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "\n",
    "# Learning rate - try changing this!\n",
    "learning_rate = 0.01\n",
    "\n",
    "print(f\"Initial parameters: w = {w}, b = {b}\")\n",
    "print(f\"Learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 3: Define the Training Step (Guided)\n\nEach step follows this pattern:\n\n1. **Forward pass**: Compute predictions `y_hat = wx + b`\n2. **Compute loss**: MSE = mean((y - y_hat)^2)\n3. **Backward pass**: Compute gradients dL/dw and dL/db\n4. **Update**: w = w - lr * dL/dw, b = b - lr * dL/db\n\n**Before running, predict:** If the loss is MSE = mean((y - y_hat)^2), what is the gradient dL/dw? (Hint: apply the chain rule to differentiate with respect to w.)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(X, y, w, b, learning_rate):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent step.\n",
    "    Returns: new_w, new_b, loss\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    \n",
    "    # 1. Forward pass: compute predictions\n",
    "    y_pred = w * X + b\n",
    "    \n",
    "    # 2. Compute loss (MSE)\n",
    "    loss = np.mean((y - y_pred) ** 2)\n",
    "    \n",
    "    # 3. Backward pass: compute gradients\n",
    "    # dL/dw = (1/n) * sum(-2 * x * (y - y_pred))\n",
    "    # dL/db = (1/n) * sum(-2 * (y - y_pred))\n",
    "    dw = np.mean(-2 * X * (y - y_pred))\n",
    "    db = np.mean(-2 * (y - y_pred))\n",
    "    \n",
    "    # 4. Update parameters\n",
    "    new_w = w - learning_rate * dw\n",
    "    new_b = b - learning_rate * db\n",
    "    \n",
    "    return new_w, new_b, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 4: Train the Model (Guided)\n\nRun the training loop and watch the model learn.\n\n**Before running, predict:** After 100 epochs with learning rate 0.01, will `w` and `b` be close to the true values (1.5 and 2.0)? Will they match exactly?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset parameters\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "\n",
    "# Training settings\n",
    "n_epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Track history for plotting\n",
    "loss_history = []\n",
    "w_history = []\n",
    "b_history = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    w, b, loss = train_step(X, y, w, b, learning_rate)\n",
    "    \n",
    "    loss_history.append(loss)\n",
    "    w_history.append(w)\n",
    "    b_history.append(b)\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}: loss = {loss:.4f}, w = {w:.4f}, b = {b:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal: w = {w:.4f} (true: {TRUE_W}), b = {b:.4f} (true: {TRUE_B})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 5: Visualize the Results (Guided)\n\n**Before running, predict:** What will the loss curve look like? Straight line down, or something else?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Plot 1: Loss curve\n",
    "axes[0].plot(loss_history, 'r-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Loss Over Time')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Parameter convergence\n",
    "axes[1].plot(w_history, label=f'w (true: {TRUE_W})', linewidth=2)\n",
    "axes[1].plot(b_history, label=f'b (true: {TRUE_B})', linewidth=2)\n",
    "axes[1].axhline(y=TRUE_W, color='C0', linestyle='--', alpha=0.5)\n",
    "axes[1].axhline(y=TRUE_B, color='C1', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Parameter Value')\n",
    "axes[1].set_title('Parameters Over Time')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Final fit\n",
    "axes[2].scatter(X, y, alpha=0.6, label='Data')\n",
    "x_line = np.linspace(-5, 5, 100)\n",
    "axes[2].plot(x_line, TRUE_W * x_line + TRUE_B, 'g--', \n",
    "             label=f'True: y = {TRUE_W}x + {TRUE_B}', linewidth=2)\n",
    "axes[2].plot(x_line, w * x_line + b, 'r-', \n",
    "             label=f'Learned: y = {w:.2f}x + {b:.2f}', linewidth=2)\n",
    "axes[2].set_xlabel('X')\n",
    "axes[2].set_ylabel('y')\n",
    "axes[2].set_title('Final Fit')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 6: Animated Training (Guided)\n\nWatch the line fit the data in real-time!\n\n**Before running, predict:** Will the line rotate into place, translate into place, or both simultaneously?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset for animation\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "learning_rate = 0.01\n",
    "\n",
    "x_line = np.linspace(-5, 5, 100)\n",
    "\n",
    "for epoch in range(50):\n",
    "    w, b, loss = train_step(X, y, w, b, learning_rate)\n",
    "    \n",
    "    # Update plot\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.scatter(X, y, alpha=0.6, label='Data')\n",
    "    ax.plot(x_line, TRUE_W * x_line + TRUE_B, 'g--', \n",
    "            label='True line', linewidth=2, alpha=0.5)\n",
    "    ax.plot(x_line, w * x_line + b, 'r-', \n",
    "            label=f'Learned: y = {w:.3f}x + {b:.3f}', linewidth=2)\n",
    "    ax.set_xlim(-6, 6)\n",
    "    ax.set_ylim(-12, 15)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'Epoch {epoch+1} | Loss: {loss:.4f}')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Learned: w = {w:.4f} (true: {TRUE_W}), b = {b:.4f} (true: {TRUE_B})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 7: Learning Rate Experiments (Supported)\n\nNow it's your turn. Modify the training loop to test different learning rates and observe the effects.\n\n**Task:**\n1. Set `learning_rate = 0.5` and run 100 epochs. What happens to the loss?\n2. Set `learning_rate = 0.001` and run 100 epochs. Does the model converge?\n3. Plot both loss curves on the same chart to compare.\n\n<details>\n<summary>Solution</summary>\n\nThe key insight is that learning rate controls the step size of each gradient descent update.\n\n```python\nresults = {}\nfor lr in [0.001, 0.01, 0.5]:\n    w, b = 0.0, 0.0\n    losses = []\n    for epoch in range(100):\n        w, b, loss = train_step(X, y, w, b, lr)\n        losses.append(loss)\n    results[lr] = losses\n\nfor lr, losses in results.items():\n    plt.plot(losses, label=f'lr={lr}')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Learning Rate Comparison')\nplt.grid(alpha=0.3)\nplt.show()\n```\n\n- **lr=0.5**: Loss explodes (overshooting — each step is too big)\n- **lr=0.001**: Loss decreases very slowly (steps are too small to converge in 100 epochs)\n- **lr=0.01**: Loss decreases smoothly and converges (the sweet spot for this problem)\n\n</details>\n\n---\n\n## Key Takeaways\n\n1. **The training loop is universal**: forward → loss → backward → update. This pattern scales from linear regression to neural networks with millions of parameters.\n2. **Gradients point toward increasing loss**, so we subtract them to move toward lower loss.\n3. **Learning rate controls step size**: too big = overshoot and diverge, too small = painfully slow convergence.\n4. **Parameters don't converge to exact true values** because of noise in the data — the model finds the best fit given the noisy observations."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}