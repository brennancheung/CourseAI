{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression from Scratch\n",
    "\n",
    "In this notebook, you'll implement linear regression with gradient descent using only NumPy.\n",
    "\n",
    "**What you'll learn:**\n",
    "- The training loop structure (forward → loss → backward → update)\n",
    "- Computing gradients by hand\n",
    "- Watching a model learn in real-time\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# For nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "We'll create data that follows the pattern: `y = 1.5x + 2 + noise`\n",
    "\n",
    "Our goal: learn `w = 1.5` and `b = 2` from the data alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameters (what we're trying to learn)\n",
    "TRUE_W = 1.5\n",
    "TRUE_B = 2.0\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "X = np.random.uniform(-5, 5, n_samples)\n",
    "noise = np.random.normal(0, 1.5, n_samples)\n",
    "y = TRUE_W * X + TRUE_B + noise\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(X, y, alpha=0.6, label='Data')\n",
    "plt.plot(X, TRUE_W * X + TRUE_B, 'g--', label=f'True: y = {TRUE_W}x + {TRUE_B}')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Our Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Parameters\n",
    "\n",
    "We start with `w = 0` and `b = 0`. The model knows nothing yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "\n",
    "# Learning rate - try changing this!\n",
    "learning_rate = 0.01\n",
    "\n",
    "print(f\"Initial parameters: w = {w}, b = {b}\")\n",
    "print(f\"Learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the Training Step\n",
    "\n",
    "Each step follows this pattern:\n",
    "\n",
    "1. **Forward pass**: Compute predictions `ŷ = wx + b`\n",
    "2. **Compute loss**: MSE = mean((y - ŷ)²)\n",
    "3. **Backward pass**: Compute gradients ∂L/∂w and ∂L/∂b\n",
    "4. **Update**: w = w - lr × ∂L/∂w, b = b - lr × ∂L/∂b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(X, y, w, b, learning_rate):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent step.\n",
    "    Returns: new_w, new_b, loss\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    \n",
    "    # 1. Forward pass: compute predictions\n",
    "    y_pred = w * X + b\n",
    "    \n",
    "    # 2. Compute loss (MSE)\n",
    "    loss = np.mean((y - y_pred) ** 2)\n",
    "    \n",
    "    # 3. Backward pass: compute gradients\n",
    "    # dL/dw = (1/n) * sum(-2 * x * (y - y_pred))\n",
    "    # dL/db = (1/n) * sum(-2 * (y - y_pred))\n",
    "    dw = np.mean(-2 * X * (y - y_pred))\n",
    "    db = np.mean(-2 * (y - y_pred))\n",
    "    \n",
    "    # 4. Update parameters\n",
    "    new_w = w - learning_rate * dw\n",
    "    new_b = b - learning_rate * db\n",
    "    \n",
    "    return new_w, new_b, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model!\n",
    "\n",
    "Run the training loop and watch the model learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset parameters\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "\n",
    "# Training settings\n",
    "n_epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Track history for plotting\n",
    "loss_history = []\n",
    "w_history = []\n",
    "b_history = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    w, b, loss = train_step(X, y, w, b, learning_rate)\n",
    "    \n",
    "    loss_history.append(loss)\n",
    "    w_history.append(w)\n",
    "    b_history.append(b)\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}: loss = {loss:.4f}, w = {w:.4f}, b = {b:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal: w = {w:.4f} (true: {TRUE_W}), b = {b:.4f} (true: {TRUE_B})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Plot 1: Loss curve\n",
    "axes[0].plot(loss_history, 'r-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Loss Over Time')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Parameter convergence\n",
    "axes[1].plot(w_history, label=f'w (true: {TRUE_W})', linewidth=2)\n",
    "axes[1].plot(b_history, label=f'b (true: {TRUE_B})', linewidth=2)\n",
    "axes[1].axhline(y=TRUE_W, color='C0', linestyle='--', alpha=0.5)\n",
    "axes[1].axhline(y=TRUE_B, color='C1', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Parameter Value')\n",
    "axes[1].set_title('Parameters Over Time')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Final fit\n",
    "axes[2].scatter(X, y, alpha=0.6, label='Data')\n",
    "x_line = np.linspace(-5, 5, 100)\n",
    "axes[2].plot(x_line, TRUE_W * x_line + TRUE_B, 'g--', \n",
    "             label=f'True: y = {TRUE_W}x + {TRUE_B}', linewidth=2)\n",
    "axes[2].plot(x_line, w * x_line + b, 'r-', \n",
    "             label=f'Learned: y = {w:.2f}x + {b:.2f}', linewidth=2)\n",
    "axes[2].set_xlabel('X')\n",
    "axes[2].set_ylabel('y')\n",
    "axes[2].set_title('Final Fit')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Animated Training (Optional)\n",
    "\n",
    "Watch the line fit the data in real-time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset for animation\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "learning_rate = 0.01\n",
    "\n",
    "x_line = np.linspace(-5, 5, 100)\n",
    "\n",
    "for epoch in range(50):\n",
    "    w, b, loss = train_step(X, y, w, b, learning_rate)\n",
    "    \n",
    "    # Update plot\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.scatter(X, y, alpha=0.6, label='Data')\n",
    "    ax.plot(x_line, TRUE_W * x_line + TRUE_B, 'g--', \n",
    "            label='True line', linewidth=2, alpha=0.5)\n",
    "    ax.plot(x_line, w * x_line + b, 'r-', \n",
    "            label=f'Learned: y = {w:.3f}x + {b:.3f}', linewidth=2)\n",
    "    ax.set_xlim(-6, 6)\n",
    "    ax.set_ylim(-12, 15)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'Epoch {epoch+1} | Loss: {loss:.4f}')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Learned: w = {w:.4f} (true: {TRUE_W}), b = {b:.4f} (true: {TRUE_B})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these experiments:\n",
    "\n",
    "1. **Learning rate too high**: Set `learning_rate = 0.5`. What happens?\n",
    "2. **Learning rate too low**: Set `learning_rate = 0.001`. How many epochs to converge?\n",
    "3. **More noise**: Increase the noise in the data. How does it affect the final loss?\n",
    "4. **Fewer samples**: Try with only 10 data points. What changes?\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways:**\n",
    "- The training loop is universal: forward → loss → backward → update\n",
    "- Gradients point toward increasing loss, so we subtract them\n",
    "- Learning rate controls step size: too big = overshoot, too small = slow\n",
    "- This exact pattern scales to neural networks with millions of parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
