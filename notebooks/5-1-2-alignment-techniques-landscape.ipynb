{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Alignment Techniques Landscape\n",
    "\n",
    "In this notebook, you'll explore three axes of the preference optimization design space through small-scale, hands-on exercises.\n",
    "\n",
    "**What you'll do:**\n",
    "- Convert preference data between paired comparison format and single-response format, seeing what information is gained and lost\n",
    "- Compute log-probability ratios between a policy model and a reference model, visualizing the KL-penalty mechanism that prevents forgetting\n",
    "- Observe how training data becomes \"stale\" when a policy updates, demonstrating why online methods exist\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones ‚Äî they reveal gaps in your mental model.\n",
    "\n",
    "**Important:** These exercises demonstrate *design axes* from the lesson, not full training runs. We use small-scale proxies (short text, GPT-2) to make the concepts concrete and fast to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup ‚Äî self-contained for Google Colab\n",
    "# transformers and torch are pre-installed in Colab\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Load GPT-2 (small, ~500MB) ‚Äî our proxy for a \"language model\"\n",
    "# In real alignment work, this would be a much larger model.\n",
    "# GPT-2 is enough to demonstrate the mechanisms.\n",
    "print(\"Loading GPT-2...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}.\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared helper: compute per-token log probabilities for a response given a prompt\n",
    "\n",
    "def get_log_probs(model, tokenizer, prompt: str, response: str) -> torch.Tensor:\n",
    "    \"\"\"Compute per-token log probabilities of `response` given `prompt`.\n",
    "\n",
    "    Returns a 1-D tensor of log-probs, one per response token.\n",
    "    This is the fundamental quantity in DPO, KTO, and all preference\n",
    "    optimization methods ‚Äî they all manipulate these log-probs.\n",
    "    \"\"\"\n",
    "    prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    full_ids = tokenizer.encode(prompt + response, return_tensors=\"pt\").to(device)\n",
    "    response_start = prompt_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(full_ids)\n",
    "        # logits shape: [1, seq_len, vocab_size]\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # For each response position, get the log-prob of the actual next token\n",
    "    # logits[0, t] predicts token t+1, so we shift by 1\n",
    "    log_probs_all = F.log_softmax(logits[0], dim=-1)\n",
    "    response_token_ids = full_ids[0, response_start:]\n",
    "    # Gather the log-prob of each actual response token\n",
    "    token_log_probs = log_probs_all[response_start - 1 : -1]  # shifted: position t predicts t+1\n",
    "    token_log_probs = token_log_probs.gather(1, response_token_ids.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    return token_log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Preference Data Format Conversion (Guided)\n",
    "\n",
    "The **data format axis** is the most practical constraint in real alignment work. DPO requires paired comparisons (\"Response A is better than Response B\"). KTO works with single responses labeled good or bad (thumbs-up/thumbs-down). These aren't interchangeable ‚Äî converting between them reveals what information is gained and lost.\n",
    "\n",
    "In this exercise, you'll see the same human feedback expressed in both formats and observe what happens when you convert from one to the other.\n",
    "\n",
    "**Before running, predict:**\n",
    "- If you have 3 paired comparisons, how many single-response labels can you extract from them?\n",
    "- If you have 6 single-response labels (thumbs-up/down), can you reconstruct the original pairs? What information is lost?\n",
    "- Why would KTO exist if paired data is strictly more informative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preference data in PAIRED COMPARISON format (for DPO) ---\n",
    "# Each entry: a prompt, two responses, and which one a human preferred.\n",
    "# This is the format from RLHF & Alignment ‚Äî \"A is better than B.\"\n",
    "\n",
    "paired_data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain quantum computing in one sentence.\",\n",
    "        \"response_a\": \"Quantum computing uses qubits that can be 0, 1, or both at once, letting it explore many solutions simultaneously.\",\n",
    "        \"response_b\": \"Quantum computing is a type of computing that is very fast and uses quantum mechanics.\",\n",
    "        \"preferred\": \"a\",  # Human chose response A\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What causes rain?\",\n",
    "        \"response_a\": \"God makes it rain when he's sad.\",\n",
    "        \"response_b\": \"Water evaporates, rises, cools into droplets in clouds, and falls when droplets grow heavy enough.\",\n",
    "        \"preferred\": \"b\",  # Human chose response B\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Is it safe to eat raw cookie dough?\",\n",
    "        \"response_a\": \"Yes, raw cookie dough is perfectly safe and delicious. Enjoy as much as you want!\",\n",
    "        \"response_b\": \"Raw cookie dough can contain raw eggs and uncooked flour, both of which carry a small risk of bacterial contamination. It's safer to use heat-treated flour and pasteurized eggs if you want to eat it raw.\",\n",
    "        \"preferred\": \"b\",  # Human chose response B\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PAIRED COMPARISON DATA (DPO format)\")\n",
    "print(\"=\" * 70)\n",
    "for i, pair in enumerate(paired_data):\n",
    "    print(f\"\\nPair {i+1}: {pair['prompt']}\")\n",
    "    print(f\"  Response A: {pair['response_a'][:80]}...\" if len(pair['response_a']) > 80 else f\"  Response A: {pair['response_a']}\")\n",
    "    print(f\"  Response B: {pair['response_b'][:80]}...\" if len(pair['response_b']) > 80 else f\"  Response B: {pair['response_b']}\")\n",
    "    print(f\"  Preferred:  {'A' if pair['preferred'] == 'a' else 'B'}\")\n",
    "\n",
    "print(f\"\\nTotal paired comparisons: {len(paired_data)}\")\n",
    "print(f\"Total individual responses: {len(paired_data) * 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert paired data ‚Üí single-response labels (KTO format) ---\n",
    "# In KTO format, each response gets an independent label: \"good\" or \"bad\".\n",
    "# No pairing, no \"better than\" ‚Äî just thumbs-up or thumbs-down.\n",
    "\n",
    "def pairs_to_singles(paired_data):\n",
    "    \"\"\"Convert paired comparison data to single-response labels.\n",
    "    \n",
    "    The preferred response becomes \"good\" (thumbs-up).\n",
    "    The dispreferred response becomes \"bad\" (thumbs-down).\n",
    "    \n",
    "    This is the conversion that KTO makes possible: if you only\n",
    "    HAVE single-response labels, you can still train. But if you\n",
    "    START with paired data and convert, you lose information.\n",
    "    \"\"\"\n",
    "    singles = []\n",
    "    for pair in paired_data:\n",
    "        if pair[\"preferred\"] == \"a\":\n",
    "            good_response = pair[\"response_a\"]\n",
    "            bad_response = pair[\"response_b\"]\n",
    "        else:\n",
    "            good_response = pair[\"response_b\"]\n",
    "            bad_response = pair[\"response_a\"]\n",
    "        \n",
    "        singles.append({\n",
    "            \"prompt\": pair[\"prompt\"],\n",
    "            \"response\": good_response,\n",
    "            \"label\": \"good\",  # thumbs-up\n",
    "        })\n",
    "        singles.append({\n",
    "            \"prompt\": pair[\"prompt\"],\n",
    "            \"response\": bad_response,\n",
    "            \"label\": \"bad\",  # thumbs-down\n",
    "        })\n",
    "    return singles\n",
    "\n",
    "\n",
    "single_data = pairs_to_singles(paired_data)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SINGLE-RESPONSE DATA (KTO format)\")\n",
    "print(\"=\" * 70)\n",
    "for i, item in enumerate(single_data):\n",
    "    label_icon = \"üëç\" if item[\"label\"] == \"good\" else \"üëé\"\n",
    "    print(f\"\\n{i+1}. [{label_icon} {item['label'].upper()}] {item['prompt']}\")\n",
    "    resp = item['response']\n",
    "    print(f\"   {resp[:90]}...\" if len(resp) > 90 else f\"   {resp}\")\n",
    "\n",
    "print(f\"\\nTotal single-response labels: {len(single_data)}\")\n",
    "print(f\"  Good (thumbs-up):  {sum(1 for s in single_data if s['label'] == 'good')}\")\n",
    "print(f\"  Bad (thumbs-down): {sum(1 for s in single_data if s['label'] == 'bad')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- What information was lost in the conversion? ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"WHAT'S LOST IN CONVERSION: Pairs ‚Üí Singles\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "From paired data, you know:\n",
    "  ‚úì Response A is BETTER THAN Response B (relative ranking)\n",
    "  ‚úì Both responses answer the SAME prompt (they're comparable)\n",
    "  ‚úì The quality gap is implicit (human chose one over the other)\n",
    "\n",
    "From single-response data, you know:\n",
    "  ‚úì This response is good / this response is bad (absolute label)\n",
    "  ‚úó You do NOT know which responses were compared to each other\n",
    "  ‚úó You do NOT know the relative quality gap\n",
    "  ‚úó A \"bad\" response might still be decent ‚Äî just worse than its pair\n",
    "\"\"\")\n",
    "\n",
    "# Concrete example of the information loss\n",
    "print(\"CONCRETE EXAMPLE of information loss:\")\n",
    "print(\"-\" * 50)\n",
    "print(f'Pair 1, Response B (marked \"bad\"):')\n",
    "print(f'  \"{paired_data[0][\"response_b\"]}\"')\n",
    "print(f\"\\n  This response is vague but not WRONG. It's 'bad' only because\")\n",
    "print(f\"  Response A was more specific. In KTO format, it gets the same\")\n",
    "print(f\"  thumbs-down as Pair 2's Response A (which IS factually wrong).\")\n",
    "print(f\"  The relative quality information is gone.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NOW REVERSE: Can we go from singles BACK to pairs?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "Given 6 single-response labels, can we reconstruct the 3 original pairs?\n",
    "\n",
    "NO. We have 6 independent items. We don't know:\n",
    "  - Which responses were originally compared to each other\n",
    "  - Whether two responses even answer the same prompt\n",
    "    (multiple prompts might look similar)\n",
    "  - The DEGREE of preference (slight vs overwhelming)\n",
    "\n",
    "We could guess (match by prompt, pair good with bad), but we'd be\n",
    "manufacturing the relative signal that was never collected.\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"WHY KTO EXISTS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "KTO exists because most real-world feedback IS already in single-response\n",
    "format. App users click üëç or üëé on ONE response ‚Äî they don't compare two.\n",
    "\n",
    "Collecting paired comparisons requires showing users two responses and\n",
    "asking \"which is better?\" This is:\n",
    "  - 2x the generation cost (two responses per prompt)\n",
    "  - More cognitive load for annotators\n",
    "  - Slower to collect at scale\n",
    "\n",
    "KTO trades information richness (relative ranking) for data availability\n",
    "(use the thumbs-up/down you already have). This is the DATA FORMAT axis\n",
    "of the design space: paired data is more informative per example, but\n",
    "single-response data is cheaper and more abundant.\n",
    "\n",
    "The design space tradeoff: DPO needs fewer examples but more expensive\n",
    "ones. KTO needs more examples but cheaper ones.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just observed:** Converting paired comparisons to single-response labels is a lossy transformation. The relative ranking (\"A is better than B\") collapses into absolute labels (\"A is good, B is bad\"), losing the quality gap information and the pairing structure.\n",
    "\n",
    "This is why KTO exists ‚Äî not because single-response labels are *better* than pairs, but because they're *what you actually have*. Most user feedback comes as thumbs-up/thumbs-down on individual responses. Forcing that into paired format would require manufacturing comparisons that were never made. KTO meets the data where it is.\n",
    "\n",
    "The insight maps directly to the **data format axis** from the lesson's design space: paired comparisons (DPO) vs single responses (KTO). Each format constrains which methods are viable. The constraint is practical (what data can you collect?), not theoretical (which loss function is better?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Reference Model Drift Visualization (Supported)\n",
    "\n",
    "The **reference model axis** is about stability. In DPO, IPO, and KTO, a frozen copy of the SFT model acts as an anchor ‚Äî the KL penalty prevents the policy from drifting too far from what it knew before alignment. This is the \"continuous version of 'freeze the backbone'\" from the RLHF lesson.\n",
    "\n",
    "In this exercise, you'll compute the log-probability ratio between a \"policy\" model and a \"reference\" model on several outputs. The ratio tells you how much the policy has diverged from the reference on each response. In real DPO training, this ratio IS the implicit KL constraint.\n",
    "\n",
    "We'll simulate this by comparing GPT-2 (our \"reference\") against a version with slightly modified logits (our \"drifted policy\"). You'll fill in the key computation.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "The log-probability ratio for a single token is:\n",
    "```\n",
    "ratio = log_prob_policy(token) - log_prob_reference(token)\n",
    "```\n",
    "\n",
    "For a full response, sum the per-token ratios. A positive sum means the policy assigns higher probability to this response than the reference does. A negative sum means lower probability.\n",
    "\n",
    "The KL divergence between policy and reference is related to the *expected* value of this ratio. Large positive or negative ratios indicate drift ‚Äî the policy's distribution has moved away from the reference.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simulating policy drift from a reference model ---\n",
    "#\n",
    "# In real alignment training:\n",
    "#   - Reference model = frozen copy of the SFT model (before alignment)\n",
    "#   - Policy model = the model being trained (drifts during alignment)\n",
    "#\n",
    "# We'll simulate this by using GPT-2 as the reference and creating a\n",
    "# \"drifted\" version by adding noise to its logits. This is a proxy ‚Äî\n",
    "# real drift comes from gradient updates, not noise ‚Äî but it lets us\n",
    "# see the same quantities that DPO's loss function operates on.\n",
    "\n",
    "def get_log_probs_with_drift(model, tokenizer, prompt, response, drift_scale=0.0):\n",
    "    \"\"\"Compute log-probs with optional logit perturbation to simulate drift.\n",
    "    \n",
    "    drift_scale=0.0 ‚Üí reference model (no drift)\n",
    "    drift_scale>0.0 ‚Üí \"policy\" that has drifted from the reference\n",
    "    \"\"\"\n",
    "    prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    full_ids = tokenizer.encode(prompt + response, return_tensors=\"pt\").to(device)\n",
    "    response_start = prompt_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(full_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        if drift_scale > 0:\n",
    "            # Add consistent noise (seeded by token position for reproducibility)\n",
    "            torch.manual_seed(123)\n",
    "            noise = torch.randn_like(logits) * drift_scale\n",
    "            logits = logits + noise\n",
    "\n",
    "    log_probs_all = F.log_softmax(logits[0], dim=-1)\n",
    "    response_token_ids = full_ids[0, response_start:]\n",
    "    token_log_probs = log_probs_all[response_start - 1 : -1]\n",
    "    token_log_probs = token_log_probs.gather(1, response_token_ids.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    return token_log_probs\n",
    "\n",
    "\n",
    "# Responses to evaluate ‚Äî a mix of styles\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "responses = [\n",
    "    \" Paris, which is known for the Eiffel Tower.\",          # Factual, natural\n",
    "    \" definitely absolutely certainly Paris without doubt.\",  # Overly confident filler\n",
    "    \" London, the largest city in England.\",                  # Wrong but fluent\n",
    "    \" Paris. Paris is located in northern France.\",           # Factual, repetitive\n",
    "    \" unknown to me, I cannot answer that question.\",         # Refusal\n",
    "]\n",
    "\n",
    "response_labels = [\n",
    "    \"Factual, natural\",\n",
    "    \"Overly confident filler\",\n",
    "    \"Wrong but fluent\",\n",
    "    \"Factual, repetitive\",\n",
    "    \"Refusal\",\n",
    "]\n",
    "\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"\\nResponses to evaluate:\")\n",
    "for i, (resp, label) in enumerate(zip(responses, response_labels)):\n",
    "    print(f\"  {i+1}. [{label}] \\\"{resp.strip()}\\\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute log-probability ratios at increasing drift levels ---\n",
    "#\n",
    "# drift_scale controls how much the \"policy\" has diverged from the reference.\n",
    "# At drift_scale=0, the policy IS the reference (no drift).\n",
    "# As drift_scale increases, the policy assigns different probabilities.\n",
    "\n",
    "drift_levels = [0.0, 0.5, 1.0, 2.0, 4.0]\n",
    "\n",
    "# For each response, compute the sum of per-token log-prob ratios at each drift level\n",
    "all_ratios = {}  # response_index -> list of ratios (one per drift level)\n",
    "\n",
    "for resp_idx, response in enumerate(responses):\n",
    "    ratios_for_response = []\n",
    "\n",
    "    # Reference model log-probs (drift_scale=0, always the same)\n",
    "    ref_log_probs = get_log_probs_with_drift(model, tokenizer, prompt, response, drift_scale=0.0)\n",
    "\n",
    "    for drift in drift_levels:\n",
    "        # TODO: Compute the policy log-probs at this drift level\n",
    "        # Use get_log_probs_with_drift() with the current drift value\n",
    "        # YOUR CODE HERE (1 line)\n",
    "        policy_log_probs = None  # REPLACE THIS\n",
    "\n",
    "        # TODO: Compute the per-token log-probability ratio: policy - reference\n",
    "        # Then sum across all tokens to get the total ratio for this response.\n",
    "        # A positive sum means the policy assigns MORE probability to this response.\n",
    "        # A negative sum means the policy assigns LESS probability.\n",
    "        # YOUR CODE HERE (1 line)\n",
    "        total_ratio = 0.0  # REPLACE THIS\n",
    "\n",
    "        ratios_for_response.append(total_ratio)\n",
    "\n",
    "    all_ratios[resp_idx] = ratios_for_response\n",
    "\n",
    "# Display the ratios\n",
    "print(\"Log-probability ratios (policy - reference), summed over tokens:\")\n",
    "print(f\"{'Response':<28} | \" + \" | \".join(f\"drift={d:.1f}\" for d in drift_levels))\n",
    "print(\"-\" * 90)\n",
    "for idx, label in enumerate(response_labels):\n",
    "    vals = \" | \".join(f\"{r:>9.2f}\" for r in all_ratios[idx])\n",
    "    print(f\"{label:<28} | {vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize the drift ---\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = ['#6366f1', '#f59e0b', '#ef4444', '#06b6d4', '#10b981']\n",
    "\n",
    "# Left plot: log-prob ratios across drift levels\n",
    "ax = axes[0]\n",
    "for idx, label in enumerate(response_labels):\n",
    "    ax.plot(drift_levels, all_ratios[idx], 'o-', color=colors[idx], label=label, linewidth=2, markersize=6)\n",
    "ax.axhline(y=0, color='white', linestyle='--', alpha=0.3, linewidth=1)\n",
    "ax.set_xlabel('Drift Scale (simulated training steps)', fontsize=11)\n",
    "ax.set_ylabel('Log-Prob Ratio (policy ‚àí reference)', fontsize=11)\n",
    "ax.set_title('How Policy Drift Affects Different Responses', fontsize=12)\n",
    "ax.legend(fontsize=8, loc='best')\n",
    "ax.grid(alpha=0.2)\n",
    "\n",
    "# Right plot: absolute ratio magnitude (how much each response has shifted)\n",
    "ax2 = axes[1]\n",
    "for idx, label in enumerate(response_labels):\n",
    "    abs_ratios = [abs(r) for r in all_ratios[idx]]\n",
    "    ax2.plot(drift_levels, abs_ratios, 'o-', color=colors[idx], label=label, linewidth=2, markersize=6)\n",
    "ax2.set_xlabel('Drift Scale (simulated training steps)', fontsize=11)\n",
    "ax2.set_ylabel('|Log-Prob Ratio| (magnitude of drift)', fontsize=11)\n",
    "ax2.set_title('Magnitude of Drift Per Response', fontsize=12)\n",
    "ax2.legend(fontsize=8, loc='best')\n",
    "ax2.grid(alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAt drift_scale=0, all ratios are 0 (policy = reference, no drift).\")\n",
    "print(\"As drift increases, different responses are affected differently.\")\n",
    "print(\"\\nThe KL penalty in DPO/IPO/KTO penalizes LARGE ratios ‚Äî it says:\")\n",
    "print('  \"You can adjust probabilities, but not TOO far from the reference.\"')\n",
    "print(\"Without this constraint, the model could collapse: assigning all\")\n",
    "print(\"probability to one response style and zero to everything else.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The log-probability ratio is the core quantity in preference optimization. For each token, it measures how much the policy has diverged from the reference.\n",
    "\n",
    "```python\n",
    "# Compute policy log-probs at this drift level\n",
    "policy_log_probs = get_log_probs_with_drift(model, tokenizer, prompt, response, drift_scale=drift)\n",
    "\n",
    "# Per-token ratio summed across the response\n",
    "total_ratio = (policy_log_probs - ref_log_probs).sum().item()\n",
    "```\n",
    "\n",
    "**Why this matters:** In DPO's loss function, the key quantity is exactly this ratio: `log_prob_policy(response) - log_prob_reference(response)`. DPO computes this for both the preferred and dispreferred responses, then pushes the ratio up for preferred and down for dispreferred ‚Äî but only within the bound set by the implicit KL penalty.\n",
    "\n",
    "The reference model acts as a \"memory\" of what the model knew before alignment. Large ratios mean the policy has moved far from its starting point. The KL penalty says: move, but not too far. This prevents the catastrophic forgetting that would happen if the model abandoned everything it learned during pretraining and SFT.\n",
    "\n",
    "**Common mistake:** Computing `ref_log_probs - policy_log_probs` (reversed). The convention is policy minus reference ‚Äî positive means the policy assigns *more* probability than the reference.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just observed:** As the policy drifts from the reference, different responses are affected by different amounts. The log-probability ratio measures this divergence per response ‚Äî and it is exactly the quantity that DPO, IPO, and KTO use in their loss functions.\n",
    "\n",
    "The **reference model constraint** (the KL penalty) prevents these ratios from growing too large. Without it, alignment training could collapse: the model might assign all probability mass to responses that superficially satisfy the preference signal while forgetting how to be a generally capable language model. This is the same \"reward hacking\" problem from RLHF ‚Äî the KL penalty is the continuous version of \"freeze the backbone.\"\n",
    "\n",
    "ORPO removes the reference model entirely. That means it does not have this ratio to constrain. Instead, it uses an odds ratio computed from the model's own probabilities ‚Äî a different stability mechanism. The lesson's insight applies: **simplification in one dimension creates complexity in another.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Online vs Offline Distribution Mismatch (Supported)\n",
    "\n",
    "The **online vs offline axis** is about *when* training data is generated. Offline methods (DPO by default) train on a static dataset collected before training. Online methods (PPO, online DPO) generate fresh data from the current model during training.\n",
    "\n",
    "The problem with offline training: as the policy updates, the pre-collected data becomes \"stale.\" The responses in the dataset were generated by an older version of the model. The policy is being evaluated on a distribution it no longer produces.\n",
    "\n",
    "In this exercise, you'll simulate this: generate text from a \"base\" model, then compare the base model's output distribution to what the model would produce after a simulated update. You'll observe the distribution shift.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "To see distribution mismatch, compare the token probability distributions at the same prompt position between the \"old\" policy (reference) and the \"updated\" policy (drifted). If the distributions are different, then training data generated by the old policy is \"off-distribution\" for the new policy.\n",
    "\n",
    "Use `F.softmax(logits, dim=-1)` to get probability distributions, and compare the top-k tokens between the two models.\n",
    "\n",
    "The KL divergence between the two distributions quantifies the mismatch: `KL = sum(p_new * log(p_new / p_old))` where p_new is the updated policy and p_old is the original.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simulating the online vs offline distribution mismatch ---\n",
    "#\n",
    "# Scenario:\n",
    "#   1. A \"base\" model generates responses (this is the offline training data)\n",
    "#   2. The model gets updated during training (the policy changes)\n",
    "#   3. The old training data no longer reflects what the updated model produces\n",
    "#\n",
    "# We simulate step 2 by perturbing the logits (same technique as Exercise 2).\n",
    "\n",
    "prompt = \"The best way to learn machine learning is\"\n",
    "\n",
    "# Step 1: Get the \"base\" model's next-token distribution at this prompt\n",
    "prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(prompt_ids)\n",
    "    base_logits = outputs.logits[0, -1]  # logits at the last position\n",
    "\n",
    "base_probs = F.softmax(base_logits, dim=-1)\n",
    "\n",
    "# Show the base model's top-10 next tokens\n",
    "top_k = 10\n",
    "top_probs, top_indices = torch.topk(base_probs, top_k)\n",
    "\n",
    "print(f'Prompt: \"{prompt}\"')\n",
    "print(f\"\\nBase model's top-{top_k} next tokens:\")\n",
    "print(f\"{'Token':<20} {'Probability':>12}\")\n",
    "print(\"-\" * 35)\n",
    "for prob, idx in zip(top_probs, top_indices):\n",
    "    token = tokenizer.decode([idx.item()])\n",
    "    print(f\"{repr(token):<20} {prob.item():>12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Simulate policy updates at increasing drift levels\n",
    "# and measure how the distribution shifts\n",
    "\n",
    "drift_levels = [0.0, 0.5, 1.0, 2.0, 4.0, 8.0]\n",
    "kl_divergences = []\n",
    "top1_changes = []  # Track whether the most-likely token changes\n",
    "\n",
    "for drift in drift_levels:\n",
    "    # Simulate a policy update by adding noise to logits\n",
    "    torch.manual_seed(456)  # Same noise pattern for each drift level, scaled\n",
    "    noise = torch.randn_like(base_logits) * drift\n",
    "    updated_logits = base_logits + noise\n",
    "\n",
    "    # TODO: Compute the updated probability distribution from updated_logits\n",
    "    # Use F.softmax() just like we did for base_probs above\n",
    "    # YOUR CODE HERE (1 line)\n",
    "    updated_probs = None  # REPLACE THIS\n",
    "\n",
    "    # TODO: Compute the KL divergence from base_probs to updated_probs\n",
    "    # KL(updated || base) = sum(updated_probs * log(updated_probs / base_probs))\n",
    "    # Use torch.sum() and torch.log(). Add a small epsilon (1e-10) to avoid log(0).\n",
    "    # This measures how \"stale\" the base model's data is relative to the updated policy.\n",
    "    # YOUR CODE HERE (1 line)\n",
    "    kl = 0.0  # REPLACE THIS\n",
    "\n",
    "    kl_divergences.append(kl)\n",
    "\n",
    "    # Track top-1 token change\n",
    "    updated_top1 = tokenizer.decode([updated_probs.argmax().item()])\n",
    "    base_top1 = tokenizer.decode([base_probs.argmax().item()])\n",
    "    top1_changes.append(updated_top1)\n",
    "\n",
    "# Display results\n",
    "print(f\"Distribution mismatch as policy updates:\")\n",
    "print(f\"{'Drift Level':<15} {'KL Divergence':>15} {'Top-1 Token':>15} {'Changed?':>10}\")\n",
    "print(\"-\" * 60)\n",
    "base_top1 = tokenizer.decode([base_probs.argmax().item()])\n",
    "for drift, kl, top1 in zip(drift_levels, kl_divergences, top1_changes):\n",
    "    changed = \"YES\" if top1 != base_top1 else \"no\"\n",
    "    print(f\"{drift:<15.1f} {kl:>15.4f} {repr(top1):>15} {changed:>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize the distribution shift ---\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: KL divergence vs drift level\n",
    "ax = axes[0]\n",
    "ax.plot(drift_levels, kl_divergences, 'o-', color='#f59e0b', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Policy Drift (simulated training steps)', fontsize=11)\n",
    "ax.set_ylabel('KL Divergence (nats)', fontsize=11)\n",
    "ax.set_title('Distribution Mismatch Grows With Policy Updates', fontsize=12)\n",
    "ax.grid(alpha=0.2)\n",
    "\n",
    "# Add annotation\n",
    "ax.annotate(\n",
    "    'Offline training data\\nbecomes stale here',\n",
    "    xy=(drift_levels[-2], kl_divergences[-2]),\n",
    "    xytext=(drift_levels[-2] - 2, kl_divergences[-2] + (max(kl_divergences) * 0.2)),\n",
    "    fontsize=9,\n",
    "    color='#f59e0b',\n",
    "    arrowprops=dict(arrowstyle='->', color='#f59e0b', lw=1.5),\n",
    ")\n",
    "\n",
    "# Right: Probability comparison for top tokens at different drift levels\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Compare base vs heavily drifted model's top-10 token probs\n",
    "torch.manual_seed(456)\n",
    "heavy_drift = 4.0\n",
    "noise = torch.randn_like(base_logits) * heavy_drift\n",
    "heavy_probs = F.softmax(base_logits + noise, dim=-1)\n",
    "\n",
    "# Get union of top-8 tokens from both distributions\n",
    "base_top8 = set(torch.topk(base_probs, 8).indices.tolist())\n",
    "heavy_top8 = set(torch.topk(heavy_probs, 8).indices.tolist())\n",
    "union_tokens = sorted(base_top8 | heavy_top8)\n",
    "union_tokens = union_tokens[:12]  # Cap for readability\n",
    "\n",
    "token_labels = [repr(tokenizer.decode([t])) for t in union_tokens]\n",
    "base_vals = [base_probs[t].item() for t in union_tokens]\n",
    "heavy_vals = [heavy_probs[t].item() for t in union_tokens]\n",
    "\n",
    "x = np.arange(len(union_tokens))\n",
    "width = 0.35\n",
    "ax2.bar(x - width/2, base_vals, width, label=f'Base (drift=0)', color='#6366f1', alpha=0.8)\n",
    "ax2.bar(x + width/2, heavy_vals, width, label=f'Updated (drift={heavy_drift})', color='#f59e0b', alpha=0.8)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(token_labels, rotation=45, ha='right', fontsize=8)\n",
    "ax2.set_ylabel('Probability', fontsize=11)\n",
    "ax2.set_title(f'Token Probabilities: Base vs Updated Policy', fontsize=12)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(alpha=0.2, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: KL divergence grows as the policy updates. Offline training data\")\n",
    "print(\"becomes increasingly stale ‚Äî it was generated by an older policy.\")\n",
    "print(\"\\nRight: The probability distribution over next tokens shifts. Tokens\")\n",
    "print(\"that were likely under the base model may become unlikely under the\")\n",
    "print(\"updated policy, and vice versa.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- The online vs offline tradeoff ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"THE ONLINE VS OFFLINE TRADEOFF\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "What you just observed: as the policy updates during training, the\n",
    "pre-collected (offline) data becomes stale. The KL divergence between\n",
    "the current policy and the policy that generated the training data grows.\n",
    "\n",
    "This is the DISTRIBUTION MISMATCH problem:\n",
    "  - Offline data was generated by policy_v0\n",
    "  - After training, the model is policy_v1\n",
    "  - policy_v1 would generate DIFFERENT responses than policy_v0\n",
    "  - But it's being trained on policy_v0's responses\n",
    "\n",
    "OFFLINE methods (DPO, default):\n",
    "  ‚úì Cheaper ‚Äî no generation during training\n",
    "  ‚úì Simpler ‚Äî standard supervised training loop\n",
    "  ‚úó Data becomes stale as the policy changes\n",
    "  ‚úó Training on off-distribution examples\n",
    "\n",
    "ONLINE methods (PPO, online DPO):\n",
    "  ‚úì Fresh data from the current policy\n",
    "  ‚úì No distribution mismatch\n",
    "  ‚úó N forward passes per training step (expensive)\n",
    "  ‚úó Cold-start: early generations are low quality\n",
    "\n",
    "ITERATIVE (compromise):\n",
    "  ‚úì Run offline DPO, then generate new data, then repeat\n",
    "  ‚úì Multiple discrete rounds, not continuous online generation\n",
    "  ‚úì Cheaper than fully online, fresher than fully offline\n",
    "\n",
    "The practical reality: for many teams, the performance gap between\n",
    "online and offline is small when the offline data is high quality.\n",
    "Most deployed models use offline methods because the cost/quality\n",
    "tradeoff favors it.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The two key computations are straightforward probability operations:\n",
    "\n",
    "```python\n",
    "# Updated probability distribution\n",
    "updated_probs = F.softmax(updated_logits, dim=-1)\n",
    "\n",
    "# KL divergence: how different is the updated distribution from the base?\n",
    "kl = torch.sum(updated_probs * torch.log((updated_probs + 1e-10) / (base_probs + 1e-10))).item()\n",
    "```\n",
    "\n",
    "**Why this matters:** The KL divergence quantifies how \"stale\" the offline training data has become. When the policy was first trained, the data came from the same distribution the model produces (KL = 0). As training progresses and the policy changes, the KL grows ‚Äî the model is learning from examples it would no longer generate itself.\n",
    "\n",
    "Online methods solve this by regenerating data from the *current* policy at each training step. The data is always on-distribution because it comes from the model that is being trained. The cost: generating responses requires forward passes through the model during training, which is expensive for large models.\n",
    "\n",
    "**Common mistake:** Computing KL(base || updated) instead of KL(updated || base). The direction matters: we want to measure how surprising the base data is to the updated model, which is KL(updated || base).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just observed:** As the policy updates during training, the KL divergence between the current policy and the policy that generated the training data grows. Offline training data becomes increasingly stale ‚Äî the model is learning from responses it would no longer produce.\n",
    "\n",
    "This is the fundamental tension on the **online vs offline axis**: offline methods are cheaper but train on stale data; online methods generate fresh data but are expensive. The choice depends on your compute budget and how much the policy changes during training.\n",
    "\n",
    "The insight connects back to the design space: online vs offline is *orthogonal* to the choice of loss function (DPO vs IPO vs KTO). You can run any of those methods either online or offline. The axis is independent ‚Äî which is what makes it an *axis* and not a *feature*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The data format axis is a practical constraint, not a theoretical one.** Converting paired comparisons to single-response labels is lossy ‚Äî you lose relative ranking and pairing structure. KTO exists because most real feedback is already in single-response format (thumbs-up/down). The method fits the data you have, not the data you wish you had.\n",
    "\n",
    "2. **The reference model is a stability mechanism, not just extra memory cost.** The log-probability ratio between policy and reference IS the KL constraint. It prevents the policy from drifting too far and forgetting what it learned in pretraining and SFT. Removing it (as ORPO does) requires a different stability mechanism.\n",
    "\n",
    "3. **Offline training data becomes stale as the policy updates.** The distribution mismatch grows with each training step. Online methods regenerate data from the current policy to avoid this, at the cost of additional compute. Most teams use offline methods because the cost/quality tradeoff favors it.\n",
    "\n",
    "4. **The axes are independent.** Data format, reference model, and online vs offline are orthogonal choices. You can combine them in different ways, which is why the design space has many methods ‚Äî each represents a different combination of tradeoffs.\n",
    "\n",
    "5. **Alignment techniques are points in a design space, not steps on a ladder.** No method dominates all axes. The right choice depends on your constraints: what data you have, how much memory you can spare, and how much compute you can afford."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}