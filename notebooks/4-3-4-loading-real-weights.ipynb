{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Real Weights\n",
    "## Module 4.3, Lesson 4 â€” Load pretrained GPT-2 into your own architecture\n",
    "\n",
    "**What you'll do:**\n",
    "1. Download GPT-2 weights from HuggingFace and inspect the state dict structure\n",
    "2. Compare your nanoGPT model's parameter names to GPT-2's parameter names\n",
    "3. Write the weight mapping function to translate between naming conventions\n",
    "4. Load the mapped weights and verify correctness with logit comparison\n",
    "5. Generate text with different prompts and compare to HuggingFace's built-in pipeline\n",
    "\n",
    "**Methodology:** For each exercise, PREDICT the output before running the cell. Wrong predictions are more valuable than correct ones â€” they reveal gaps in your mental model.\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:** Building nanoGPT (Module 4.3, Lesson 1), Pretraining (Lesson 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers tiktoken\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Your nanoGPT Architecture\n",
    "\n",
    "This is the GPT model you built in Building nanoGPT. We need it here so we can load real weights into it. Read through it â€” this is your code, just collected into one cell for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # Combined Q, K, V projection\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        # Scaled dot-product attention with causal mask\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # Weight tying: embedding and output projection share the same tensor\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.size()\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "# Create our model with GPT-2 config\n",
    "config = GPTConfig()\n",
    "model_ours = GPT(config)\n",
    "model_ours.to(device)\n",
    "print(f'Our model: {sum(p.numel() for p in model_ours.parameters()):,} parameters')\n",
    "print(f'Weight tying active: {model_ours.transformer.wte.weight is model_ours.lm_head.weight}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Download and Inspect GPT-2 Weights `[Guided]`\n",
    "\n",
    "We use HuggingFace as a weight download tool â€” nothing more. Two lines of code give us the official GPT-2 weights and a reference implementation to compare against.\n",
    "\n",
    "**Before running, predict:**\n",
    "- How many keys will the HuggingFace state dict have?\n",
    "- How many keys will your model's state dict have?\n",
    "- Will they be the same number? If not, why?\n",
    "- What will the first few key names look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Download the official GPT-2 weights (~500MB)\n",
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model_hf.to(device)\n",
    "model_hf.eval()\n",
    "\n",
    "print(f'HuggingFace model: {sum(p.numel() for p in model_hf.parameters()):,} parameters')\n",
    "print()\n",
    "\n",
    "# Inspect the state dicts\n",
    "sd_hf = model_hf.state_dict()\n",
    "sd_ours = model_ours.state_dict()\n",
    "\n",
    "print(f'HuggingFace state dict: {len(sd_hf)} keys')\n",
    "print(f'Our state dict:         {len(sd_ours)} keys')\n",
    "print()\n",
    "\n",
    "# Show first 15 keys from each\n",
    "print('=== First 15 HuggingFace keys ===')\n",
    "for i, k in enumerate(sd_hf.keys()):\n",
    "    if i >= 15:\n",
    "        break\n",
    "    print(f'  {k:<50s} {str(tuple(sd_hf[k].shape)):>20s}')\n",
    "\n",
    "print()\n",
    "print('=== First 15 of our keys ===')\n",
    "for i, k in enumerate(sd_ours.keys()):\n",
    "    if i >= 15:\n",
    "        break\n",
    "    print(f'  {k:<50s} {str(tuple(sd_ours[k].shape)):>20s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you should notice:**\n",
    "\n",
    "- HuggingFace has **148 keys**; your model has **147 keys**. The difference: HuggingFace stores `lm_head.weight` as a separate entry, but in your model it's the same tensor as `transformer.wte.weight` (weight tying). One tensor, two names â€” but only one entry in your state dict because they share the same memory.\n",
    "- Some key names look very similar between the two models (both use `transformer.h.0.ln_1.weight` etc.)\n",
    "- But there are also some attention mask buffers in HuggingFace's state dict (`attn.bias`, `attn.masked_bias`) that we don't have â€” these are registered buffers, not learned parameters.\n",
    "\n",
    "The key names are close, but not identical. A naive `load_state_dict()` will fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Compare Parameter Names Side by Side `[Guided]`\n",
    "\n",
    "Now let's look more carefully at the differences. We'll filter to just the *learned parameters* (skip buffers like attention masks) and compare names and shapes.\n",
    "\n",
    "**Before running, predict:**\n",
    "- Will the names match exactly, or will some differ?\n",
    "- Will the shapes all match? If not, which parameters will have different shapes, and why?\n",
    "- Hint: HuggingFace uses Conv1D for linear projections. Conv1D stores weights as `(in_features, out_features)`. Your nn.Linear stores them as `(out_features, in_features)`. Which layers use Conv1D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter HuggingFace state dict to just the learned parameters\n",
    "# (skip attention mask buffers that aren't real weights)\n",
    "skip_buffers = {'attn.masked_bias', 'attn.bias'}\n",
    "\n",
    "hf_params = {k: v for k, v in sd_hf.items()\n",
    "             if not any(k.endswith(s) for s in skip_buffers)}\n",
    "\n",
    "print(f'HuggingFace learned params: {len(hf_params)} keys')\n",
    "print(f'Our model params:           {len(sd_ours)} keys')\n",
    "print()\n",
    "\n",
    "# Compare parameter-by-parameter for the first transformer block\n",
    "print('=== Block 0: Side-by-side comparison ===')\n",
    "print(f'{\"HuggingFace Key\":<50s} {\"Shape\":>15s}   {\"Our Key\":<50s} {\"Shape\":>15s}   Match?')\n",
    "print('-' * 145)\n",
    "\n",
    "block_keys_hf = sorted([k for k in hf_params if k.startswith('transformer.h.0.')])\n",
    "block_keys_ours = sorted([k for k in sd_ours if k.startswith('transformer.h.0.')])\n",
    "\n",
    "for hf_key, our_key in zip(block_keys_hf, block_keys_ours):\n",
    "    hf_shape = tuple(hf_params[hf_key].shape)\n",
    "    our_shape = tuple(sd_ours[our_key].shape)\n",
    "    shapes_match = hf_shape == our_shape\n",
    "    match_str = 'YES' if shapes_match else f'NO  (transposed: {hf_shape} vs {our_shape})'\n",
    "    print(f'{hf_key:<50s} {str(hf_shape):>15s}   {our_key:<50s} {str(our_shape):>15s}   {match_str}')\n",
    "\n",
    "print()\n",
    "\n",
    "# Identify all transposed parameters across the full model\n",
    "transposed_count = 0\n",
    "direct_count = 0\n",
    "for our_key in sd_ours:\n",
    "    if our_key not in hf_params:\n",
    "        continue\n",
    "    if sd_ours[our_key].shape == hf_params[our_key].shape:\n",
    "        direct_count += 1\n",
    "    else:\n",
    "        transposed_count += 1\n",
    "\n",
    "print(f'Parameters that copy directly: {direct_count}')\n",
    "print(f'Parameters that need transposing: {transposed_count}')\n",
    "print()\n",
    "print('The pattern: every 2D weight in attention (c_attn, c_proj) and FFN (c_fc, c_proj) needs transposing.')\n",
    "print('Everything else â€” embeddings, layer norms, biases â€” copies directly.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why the shapes differ for some parameters:**\n",
    "\n",
    "HuggingFace's GPT-2 uses a custom **Conv1D** class for the linear projections â€” a historical artifact from OpenAI's original release. Conv1D stores weights as `(in_features, out_features)`, which is **transposed** relative to `nn.Linear`'s `(out_features, in_features)`.\n",
    "\n",
    "| Component | HuggingFace (Conv1D) | Your model (nn.Linear) | Need `.t()`? |\n",
    "|-----------|---------------------|----------------------|-------------|\n",
    "| `c_attn.weight` | (768, 2304) | (2304, 768) | Yes |\n",
    "| `c_proj.weight` | (768, 768) | (768, 768) | Yes* |\n",
    "| `c_fc.weight` | (768, 3072) | (3072, 768) | Yes |\n",
    "| `mlp.c_proj.weight` | (3072, 768) | (768, 3072) | Yes |\n",
    "| Layer norms, biases, embeddings | same | same | No |\n",
    "\n",
    "\\*`c_proj` is square (768x768), so the shape *looks* the same, but the values are still transposed. The fix is the same: `.t()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Write the Weight Mapping Function `[Supported]`\n",
    "\n",
    "Now build the function that copies HuggingFace weights into your model. The function needs to:\n",
    "1. Iterate over your model's state dict keys\n",
    "2. Skip `lm_head.weight` (handled by weight tying)\n",
    "3. Transpose 2D weights from Conv1D layers\n",
    "4. Copy everything else directly\n",
    "\n",
    "**The rule is simple:** if the parameter name ends with one of `attn.c_attn.weight`, `attn.c_proj.weight`, `mlp.c_fc.weight`, or `mlp.c_proj.weight`, transpose it. Everything else copies directly.\n",
    "\n",
    "Fill in the TODO sections below.\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The key insight is that you iterate over *your* model's keys (not HuggingFace's), skip the tied weight, and use `endswith()` to detect which weights need transposing. The `copy_()` method modifies the tensor in place â€” since `sd_ours` holds references to the model's actual parameter tensors, this directly updates the model.\n",
    "\n",
    "```python\n",
    "# TODO 1: Define the set of suffixes that need transposing\n",
    "transposed = {\n",
    "    'attn.c_attn.weight',\n",
    "    'attn.c_proj.weight',\n",
    "    'mlp.c_fc.weight',\n",
    "    'mlp.c_proj.weight',\n",
    "}\n",
    "\n",
    "# TODO 2: Check if this key needs transposing and copy accordingly\n",
    "needs_transpose = any(key.endswith(t) for t in transposed)\n",
    "\n",
    "if needs_transpose:\n",
    "    assert sd_hf[key].shape[::-1] == sd_ours[key].shape, \\\n",
    "        f'Shape mismatch (transposed) for {key}: HF {sd_hf[key].shape} vs ours {sd_ours[key].shape}'\n",
    "    with torch.no_grad():\n",
    "        sd_ours[key].copy_(sd_hf[key].t())\n",
    "else:\n",
    "    assert sd_hf[key].shape == sd_ours[key].shape, \\\n",
    "        f'Shape mismatch for {key}: HF {sd_hf[key].shape} vs ours {sd_ours[key].shape}'\n",
    "    with torch.no_grad():\n",
    "        sd_ours[key].copy_(sd_hf[key])\n",
    "```\n",
    "\n",
    "Common mistake: forgetting that `shape[::-1]` reverses the tuple for the transposed shape assertion. If you wrote `shape.t()`, that's a tensor operation â€” you need tuple reversal here.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hf_weights(model_ours, model_hf):\n",
    "    \"\"\"Load HuggingFace GPT-2 weights into our model.\"\"\"\n",
    "    sd_hf = model_hf.state_dict()\n",
    "    sd_ours = model_ours.state_dict()\n",
    "\n",
    "    # Keys to skip: weight tying means lm_head shares with wte\n",
    "    skip = {'lm_head.weight'}\n",
    "\n",
    "    # HuggingFace buffers that aren't learned parameters\n",
    "    skip_buffers = {'attn.masked_bias', 'attn.bias'}\n",
    "\n",
    "    # TODO 1: Define the set of Conv1D weight suffixes that need transposing.\n",
    "    # These are the 2D weight matrices in attention and FFN layers.\n",
    "    # Hint: there are 4 suffixes â€” the same ones from Exercise 2.\n",
    "    transposed = {\n",
    "        # YOUR CODE HERE (4 strings)\n",
    "    }\n",
    "\n",
    "    loaded = 0\n",
    "    transposed_count = 0\n",
    "\n",
    "    for key in sd_ours:\n",
    "        if key in skip:\n",
    "            continue\n",
    "\n",
    "        # TODO 2: Check if this key needs transposing.\n",
    "        # Then copy the weight â€” transposing with .t() if needed.\n",
    "        # Use assert to verify shapes match before copying.\n",
    "        # Use torch.no_grad() and copy_() for in-place assignment.\n",
    "        needs_transpose = False  # REPLACE THIS LINE\n",
    "\n",
    "        if needs_transpose:\n",
    "            # YOUR CODE HERE: assert shape match (reversed), then copy with .t()\n",
    "            transposed_count += 1\n",
    "            pass\n",
    "        else:\n",
    "            # YOUR CODE HERE: assert shape match, then copy directly\n",
    "            pass\n",
    "\n",
    "        loaded += 1\n",
    "\n",
    "    print(f'Loaded {loaded} parameters ({transposed_count} transposed)')\n",
    "    return loaded, transposed_count\n",
    "\n",
    "\n",
    "# Run it\n",
    "loaded, transposed_count = load_hf_weights(model_ours, model_hf)\n",
    "\n",
    "# Verify\n",
    "assert loaded == len(sd_ours) - 1, f'Expected {len(sd_ours) - 1} loaded, got {loaded} (one skipped for weight tying)'\n",
    "assert transposed_count > 0, 'Should have transposed some weights!'\n",
    "print(f'\\nAll {loaded} parameters loaded successfully.')\n",
    "print(f'Skipped lm_head.weight (weight tying handles it).')\n",
    "\n",
    "# Verify weight tying is still intact\n",
    "print(f'\\nWeight tying still active: {model_ours.transformer.wte.weight.data_ptr() == model_ours.lm_head.weight.data_ptr()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why `copy_()` works without `load_state_dict()`:** `sd_ours` holds references to the model's actual parameter tensors (not copies). When you call `sd_ours[key].copy_(value)`, you're writing directly into the model's memory. No need to call `model_ours.load_state_dict(sd_ours)` afterward â€” the model is already updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Verify with Logit Comparison `[Supported]`\n",
    "\n",
    "Text comparison is subjective and stochastic (sampling adds randomness). The gold standard test: feed the **same input tokens** to both models and compare the **output logits**.\n",
    "\n",
    "If `torch.allclose()` returns True, every component â€” every projection, every layer norm, every residual connection â€” produces the same output as the reference. This is stronger than parameter counting (shapes) and stronger than text comparison (subjective).\n",
    "\n",
    "Fill in the TODO sections to run the verification.\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The key insight is that both models must be in `eval()` mode (disables dropout), and you compare logits â€” the raw output before sampling. HuggingFace wraps its output in a named tuple, so you access `.logits` on the HuggingFace output.\n",
    "\n",
    "```python\n",
    "# TODO 1: Put both models in eval mode\n",
    "model_ours.eval()\n",
    "model_hf.eval()\n",
    "\n",
    "# TODO 2: Get logits from both models\n",
    "logits_ours = model_ours(input_ids)\n",
    "logits_hf = model_hf(input_ids).logits\n",
    "\n",
    "# TODO 3: Compare with torch.allclose\n",
    "match = torch.allclose(logits_ours, logits_hf, atol=1e-5)\n",
    "```\n",
    "\n",
    "If `allclose` fails at `atol=1e-5` but passes at `atol=1e-3`, that's floating-point noise from different operation ordering â€” not a bug. The top-k tokens and their relative probabilities are essentially identical.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a test prompt\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = enc.encode(\"The capital of France is\")\n",
    "input_ids = torch.tensor([tokens], device=device)\n",
    "\n",
    "print(f'Input tokens: {tokens}')\n",
    "print(f'Input text: \"{enc.decode(tokens)}\"')\n",
    "print(f'Input shape: {input_ids.shape}')\n",
    "print()\n",
    "\n",
    "# TODO 1: Put both models in eval mode (disables dropout)\n",
    "# YOUR CODE HERE (2 lines)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # TODO 2: Get logits from both models.\n",
    "    # For your model: logits_ours = model_ours(input_ids)\n",
    "    # For HuggingFace: logits_hf = model_hf(input_ids).logits\n",
    "    #   (HuggingFace wraps output in a named tuple â€” .logits gets the raw tensor)\n",
    "    logits_ours = None  # REPLACE THIS LINE\n",
    "    logits_hf = None    # REPLACE THIS LINE\n",
    "\n",
    "print(f'Our logits shape: {logits_ours.shape}')\n",
    "print(f'HF logits shape:  {logits_hf.shape}')\n",
    "print()\n",
    "\n",
    "# TODO 3: Compare logits with torch.allclose\n",
    "match = False  # REPLACE THIS LINE\n",
    "\n",
    "print(f'Logits match (atol=1e-5): {match}')\n",
    "\n",
    "# Also check with a slightly relaxed tolerance\n",
    "match_relaxed = torch.allclose(logits_ours, logits_hf, atol=1e-3)\n",
    "print(f'Logits match (atol=1e-3): {match_relaxed}')\n",
    "print()\n",
    "\n",
    "# Show the max difference\n",
    "max_diff = (logits_ours - logits_hf).abs().max().item()\n",
    "print(f'Max absolute difference: {max_diff:.2e}')\n",
    "\n",
    "# Verify both models predict the same next token\n",
    "next_token_ours = logits_ours[0, -1].argmax().item()\n",
    "next_token_hf = logits_hf[0, -1].argmax().item()\n",
    "print(f'\\nOur model predicts next token:  \"{enc.decode([next_token_ours])}\" (id={next_token_ours})')\n",
    "print(f'HuggingFace predicts next token: \"{enc.decode([next_token_hf])}\" (id={next_token_hf})')\n",
    "print(f'Same prediction: {next_token_ours == next_token_hf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The verification chain:**\n",
    "- In Building nanoGPT, parameter counting verified your architecture had the right **structure**.\n",
    "- Now, logit comparison verifies your architecture computes the right **function**.\n",
    "- Together: right structure AND right computation.\n",
    "\n",
    "If you see a small numerical difference (e.g., `1e-6`) but `allclose` passes, that's floating-point noise from different operation ordering â€” not a bug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Generate and Compare `[Independent]`\n",
    "\n",
    "You have a verified GPT-2 implementation. Now put it to use:\n",
    "\n",
    "1. Generate text from your model using several prompts\n",
    "2. Generate text from HuggingFace's pipeline for the same prompts\n",
    "3. Compare the quality â€” your model should produce coherent, knowledgeable text\n",
    "\n",
    "Write the generation code from scratch. Use your model's `.generate()` method with reasonable sampling parameters (temperature around 0.8, top_k around 50). For HuggingFace, use `transformers.pipeline(\"text-generation\", model=model_hf, tokenizer=enc)` or generate manually.\n",
    "\n",
    "Try at least these prompts:\n",
    "- `\"The meaning of life is\"`\n",
    "- `\"The capital of France is\"`\n",
    "- `\"Once upon a time in a land far away\"`\n",
    "- `\"The transformer architecture consists of\"`\n",
    "\n",
    "Think about: How does this compare to the gibberish from the untrained model in Building nanoGPT? The architecture didn't change â€” the weights changed.\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The key here is straightforward use of your model's `generate()` method. The interesting part is seeing the quality difference: your code, OpenAI's knowledge, producing real coherent English. The architecture is the vessel; the weights are the knowledge.\n",
    "\n",
    "```python\n",
    "prompts = [\n",
    "    \"The meaning of life is\",\n",
    "    \"The capital of France is\",\n",
    "    \"Once upon a time in a land far away\",\n",
    "    \"The transformer architecture consists of\",\n",
    "]\n",
    "\n",
    "model_ours.eval()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"YOUR GPT-2 (your code + OpenAI's weights)\")\n",
    "print(\"=\" * 60)\n",
    "for prompt in prompts:\n",
    "    tokens = enc.encode(prompt)\n",
    "    idx = torch.tensor([tokens], device=device)\n",
    "    generated = model_ours.generate(idx, max_new_tokens=50, temperature=0.8, top_k=50)\n",
    "    text = enc.decode(generated[0].tolist())\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Output: {text}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"HUGGINGFACE GPT-2 (reference implementation)\")\n",
    "print(\"=\" * 60)\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "generator = hf_pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_hf,\n",
    "    tokenizer=\"gpt2\",\n",
    "    device=device,\n",
    ")\n",
    "for prompt in prompts:\n",
    "    result = generator(prompt, max_new_tokens=50, temperature=0.8, top_k=50, do_sample=True)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Output: {result[0]['generated_text']}\")\n",
    "```\n",
    "\n",
    "Note: Because of sampling randomness, the two models won't produce the same text even with the same prompt. That's expected â€” the logits are the same (verified in Exercise 4), but different sampling implementations draw different random tokens. Both should be coherent English.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Generate text from your model and from HuggingFace's pipeline.\n",
    "# Compare the quality across multiple prompts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The full arc of Module 4.3:**\n",
    "\n",
    "- **Random weights** â†’ gibberish\n",
    "- **Trained on Shakespeare** â†’ recognizable English\n",
    "- **Real GPT-2 weights** â†’ coherent, knowledgeable text\n",
    "\n",
    "Your code did not change. The weights changed. That is what pretraining buys.\n",
    "\n",
    "The architecture is the vessel. The weights are the knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The weight mapping IS the verification.** Every shape match is a component verified. Every matching logit is a computation confirmed. If real weights work in your model, your implementation is correct.\n",
    "\n",
    "2. **Conv1D vs nn.Linear: same parameters, different layout.** HuggingFace's GPT-2 uses Conv1D `(in_features, out_features)`. Your model uses nn.Linear `(out_features, in_features)`. The fix is `.t()` â€” one transpose per 2D weight in attention and FFN.\n",
    "\n",
    "3. **Logit comparison is the gold standard.** Same input, same weights, same computation â€” if `torch.allclose` returns True, the models are functionally identical. Stronger than parameter counting (shapes). Stronger than text comparison (subjective).\n",
    "\n",
    "4. **The architecture is the vessel. The weights are the knowledge.** Your code defines what the model can compute. The pretrained weights encode what it has learned about language. Same code, different weights, dramatically different behavior.\n",
    "\n",
    "5. **Weight tying: one tensor, two names.** `wte.weight` and `lm_head.weight` point to the same memory. Loading it once updates both. This is not a shortcut â€” it is how the architecture works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}