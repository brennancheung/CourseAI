{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Tuning (SFT)\n",
    "\n",
    "In this notebook, you'll perform supervised finetuning (SFT) on a base language model and observe the behavioral transformation from text completer to instruction follower.\n",
    "\n",
    "**What you'll do:**\n",
    "- Load a base model and observe that it does NOT follow instructions â€” it completes text\n",
    "- Prepare an instruction-following dataset in chat format (system/user/assistant turns)\n",
    "- Implement the SFT training loop with loss masking on prompt tokens\n",
    "- Compare before/after model responses on held-out prompts\n",
    "- Experiment with dataset quality â€” noisy vs clean data\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones â€” they reveal gaps in your mental model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup â€” self-contained for Google Colab\n",
    "!pip install -q transformers datasets accelerate peft\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import copy\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "\n",
    "print('\\nSetup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Base Model Behavior â€” Text Completion, Not Instruction Following (Guided)\n",
    "\n",
    "The lesson showed the core insight: a base model is a text completer. It predicts the next token. When you give it an instruction, it does not *follow* it â€” it *continues* it as if it were part of a document.\n",
    "\n",
    "We'll load GPT-2 (a base model) and send it several instruction-style prompts.\n",
    "\n",
    "**Before running, predict:** When you send GPT-2 the prompt `\"Write a haiku about machine learning\"`, what will it generate? Will it produce a haiku, or something else? What about `\"What is the capital of France?\"`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 â€” a base model (not instruction-tuned)\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# GPT-2 has no pad token by default â€” set it to eos\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in base_model.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_new_tokens=80):\n",
    "    \"\"\"Generate text from a prompt. Returns only the NEW tokens (not the prompt).\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Greedy for reproducibility\n",
    "            temperature=1.0,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode only the generated part (not the prompt)\n",
    "    generated_ids = outputs[0][prompt_length:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Test prompts â€” things you'd ask an instruction-following model\n",
    "test_prompts = [\n",
    "    \"Write a haiku about machine learning.\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain why the sky is blue in one sentence.\",\n",
    "    \"List three benefits of exercise.\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASE MODEL (GPT-2) RESPONSES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = generate_response(base_model, tokenizer, prompt)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Response: {response[:200]}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just observed:** GPT-2 does not follow instructions. It treats each prompt as a document fragment and continues it. \"Write a haiku\" becomes part of an article *about* haiku writing. \"What is the capital of France?\" becomes a quiz question in a textbook.\n",
    "\n",
    "The model **has knowledge** (it knows Paris is the capital of France) but it does not have the **behavior** of answering questions directly. It is a text completer, not an instruction follower.\n",
    "\n",
    "SFT will change this behavior â€” using the exact same model architecture and loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Prepare an Instruction Dataset in Chat Format (Guided)\n",
    "\n",
    "SFT data is instruction-response pairs. But the model needs to know where the instruction ends and the response begins. That's what **chat templates** and **special tokens** do â€” they are structural delimiters the model learns during SFT.\n",
    "\n",
    "We'll load a real instruction dataset (Alpaca format) and convert each example into chat-template format with special tokens.\n",
    "\n",
    "**Before running, predict:** An Alpaca-format example has fields `instruction`, `input`, and `output`. When we convert it to chat format with `<|im_start|>` and `<|im_end|>` tokens, what will the resulting string look like? How many special tokens will there be per example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small instruction dataset\n",
    "# tatsu-lab/alpaca is the classic 52K instruction dataset from Stanford\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "print(f\"Fields: {list(dataset[0].keys())}\")\n",
    "\n",
    "# Look at a few raw examples\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RAW ALPACA EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(3):\n",
    "    ex = dataset[i]\n",
    "    print(f\"\\n--- Example {i} ---\")\n",
    "    print(f\"instruction: {ex['instruction'][:100]}\")\n",
    "    print(f\"input:       {ex['input'][:100] if ex['input'] else '(none)'}\")\n",
    "    print(f\"output:      {ex['output'][:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Alpaca format to ChatML format with special tokens\n",
    "#\n",
    "# ChatML uses:\n",
    "#   <|im_start|>role\\ncontent<|im_end|>\n",
    "#\n",
    "# The model learns that after <|im_start|>assistant\\n it should generate a response.\n",
    "\n",
    "CHAT_TEMPLATE = \"\"\"<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{response}<|im_end|>\"\"\"\n",
    "\n",
    "\n",
    "def format_example(example):\n",
    "    \"\"\"Convert an Alpaca example to ChatML format.\"\"\"\n",
    "    instruction = example[\"instruction\"]\n",
    "    if example[\"input\"]:\n",
    "        instruction = f\"{instruction}\\n\\n{example['input']}\"\n",
    "    return CHAT_TEMPLATE.format(\n",
    "        instruction=instruction,\n",
    "        response=example[\"output\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# Format a few examples and inspect\n",
    "print(\"FORMATTED EXAMPLE (ChatML):\")\n",
    "print(\"=\" * 60)\n",
    "formatted = format_example(dataset[0])\n",
    "print(formatted)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count special tokens in the formatted string\n",
    "n_im_start = formatted.count(\"<|im_start|>\")\n",
    "n_im_end = formatted.count(\"<|im_end|>\")\n",
    "print(f\"\\nSpecial tokens per example: {n_im_start} <|im_start|>, {n_im_end} <|im_end|>\")\n",
    "print(f\"Roles: system, user, assistant â€” three turns\")\n",
    "print(f\"\\nThe model will learn: after '<|im_start|>assistant\\\\n', generate a response.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just built:** A function that converts raw instruction/response pairs into the ChatML template format. The special tokens `<|im_start|>` and `<|im_end|>` are structural delimiters â€” the model will learn during SFT that they mark role boundaries.\n",
    "\n",
    "These special tokens have **no pretrained meaning**. They did not exist in GPT-2's pretraining data. They will acquire meaning entirely from the SFT training data, where they consistently appear as boundaries between roles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Implement the SFT Training Loop (Supported)\n",
    "\n",
    "Now the core exercise: implement SFT. Remember from the lesson â€” the training loop is the **same heartbeat** as pretraining and classification finetuning:\n",
    "\n",
    "1. Forward pass\n",
    "2. Compute loss (cross-entropy on next-token prediction)\n",
    "3. Zero gradients\n",
    "4. Backward\n",
    "5. Step\n",
    "\n",
    "The **one new mechanical concept** is **loss masking**: compute loss only on response tokens (not prompt tokens). Prompt tokens get label `-100`, which `CrossEntropyLoss` ignores.\n",
    "\n",
    "You'll fill in the TODOs for:\n",
    "- Adding special tokens to the tokenizer and resizing the model embeddings\n",
    "- Implementing the loss masking logic\n",
    "- Writing the training loop\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The key insights:\n",
    "\n",
    "1. **Adding special tokens** requires both extending the tokenizer vocabulary AND resizing the model's embedding matrix. The new token embeddings are initialized randomly and will learn their meaning during SFT.\n",
    "\n",
    "2. **Loss masking** sets labels to -100 for all prompt tokens. The boundary is where `<|im_start|>assistant\\n` ends â€” everything before that is prompt, everything after is response. PyTorch's `CrossEntropyLoss` ignores -100 indices by default.\n",
    "\n",
    "3. **The training loop** is identical to what you've written before. Forward, loss, zero_grad, backward, step. The only difference is the data going in.\n",
    "\n",
    "```python\n",
    "# Adding special tokens:\n",
    "special_tokens = {\"additional_special_tokens\": [\"<|im_start|>\", \"<|im_end|>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Loss masking â€” find where the assistant response starts:\n",
    "assistant_marker = \"<|im_start|>assistant\\n\"\n",
    "marker_ids = tokenizer.encode(assistant_marker, add_special_tokens=False)\n",
    "# Find marker position in token_ids, set labels[:marker_end] = -100\n",
    "\n",
    "# Training loop:\n",
    "for step, batch in enumerate(dataloader):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    outputs = model(input_ids=input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "Common mistake: forgetting to resize embeddings after adding special tokens. The model will crash because token IDs exceed the embedding matrix dimensions.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Prepare the model and tokenizer for SFT ---\n",
    "\n",
    "# Fresh copy of GPT-2 for SFT (keep the original base_model for comparison)\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "sft_tokenizer.pad_token = sft_tokenizer.eos_token\n",
    "\n",
    "# TODO: Add the ChatML special tokens to the tokenizer.\n",
    "# The tokens are: \"<|im_start|>\" and \"<|im_end|>\"\n",
    "# Use sft_tokenizer.add_special_tokens() with the key \"additional_special_tokens\"\n",
    "special_tokens = {\"additional_special_tokens\": [\"<|im_start|>\", \"<|im_end|>\"]}\n",
    "num_added = sft_tokenizer.add_special_tokens(special_tokens)\n",
    "print(f\"Added {num_added} special tokens to vocabulary\")\n",
    "\n",
    "# TODO: Resize the model's embedding matrix to accommodate the new tokens.\n",
    "# Use sft_model.resize_token_embeddings(len(sft_tokenizer))\n",
    "sft_model.resize_token_embeddings(len(sft_tokenizer))\n",
    "print(f\"New vocabulary size: {len(sft_tokenizer)}\")\n",
    "\n",
    "# Verify the new tokens have IDs\n",
    "im_start_id = sft_tokenizer.convert_tokens_to_ids(\"<|im_start|>\")\n",
    "im_end_id = sft_tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "print(f\"<|im_start|> token ID: {im_start_id}\")\n",
    "print(f\"<|im_end|> token ID: {im_end_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Tokenize with loss masking ---\n",
    "\n",
    "# Loss masking: we compute loss ONLY on the response tokens.\n",
    "# The prompt (system + user turns) gets label = -100.\n",
    "# The response (assistant turn) gets the actual next-token targets.\n",
    "\n",
    "MAX_LENGTH = 256  # Keep sequences short for training speed\n",
    "\n",
    "\n",
    "def tokenize_with_labels(formatted_text, tokenizer, max_length=MAX_LENGTH):\n",
    "    \"\"\"Tokenize a ChatML-formatted example and create labels with loss masking.\n",
    "\n",
    "    Returns:\n",
    "        input_ids: token IDs for the full sequence\n",
    "        labels: same as input_ids but with -100 for prompt tokens\n",
    "    \"\"\"\n",
    "    # Tokenize the full formatted text\n",
    "    encoding = tokenizer(\n",
    "        formatted_text,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"][0]\n",
    "\n",
    "    # For next-token prediction, labels are the input shifted by 1.\n",
    "    # HuggingFace models handle this shift internally, so labels = input_ids.\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # TODO: Find where the assistant response starts and mask everything before it.\n",
    "    #\n",
    "    # Strategy: find \"<|im_start|>assistant\\n\" in the formatted text,\n",
    "    # tokenize just the prompt portion to get its length in tokens,\n",
    "    # then set labels[:prompt_length] = -100.\n",
    "    #\n",
    "    # Hint: Use formatted_text.find() to locate the assistant marker,\n",
    "    # then tokenize formatted_text[:marker_end] to count prompt tokens.\n",
    "\n",
    "    assistant_marker = \"<|im_start|>assistant\\n\"\n",
    "    marker_pos = formatted_text.find(assistant_marker)\n",
    "    prompt_end = marker_pos + len(assistant_marker)\n",
    "\n",
    "    # Tokenize just the prompt to find where to mask\n",
    "    prompt_tokens = tokenizer(\n",
    "        formatted_text[:prompt_end],\n",
    "        return_tensors=\"pt\",\n",
    "    )[\"input_ids\"][0]\n",
    "    prompt_length = len(prompt_tokens)\n",
    "\n",
    "    # Mask the prompt tokens â€” these do NOT contribute to the loss\n",
    "    labels[:prompt_length] = -100\n",
    "\n",
    "    return input_ids, labels\n",
    "\n",
    "\n",
    "# Test it on one example\n",
    "test_formatted = format_example(dataset[0])\n",
    "test_ids, test_labels = tokenize_with_labels(test_formatted, sft_tokenizer)\n",
    "\n",
    "# Show the masking\n",
    "n_total = len(test_ids)\n",
    "n_masked = (test_labels == -100).sum().item()\n",
    "n_active = n_total - n_masked\n",
    "\n",
    "print(f\"Total tokens: {n_total}\")\n",
    "print(f\"Masked (prompt, label=-100): {n_masked}\")\n",
    "print(f\"Active (response, loss computed): {n_active}\")\n",
    "print(f\"\\nFirst few labels: {test_labels[:15].tolist()}\")\n",
    "print(f\"Last few labels:  {test_labels[-15:].tolist()}\")\n",
    "print(f\"\\n-100 = masked (prompt tokens). Other values = target token IDs (response tokens).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Create the training dataset ---\n",
    "\n",
    "# Use a small subset for fast training (SFT is data-efficient!)\n",
    "NUM_TRAIN = 500  # 500 examples â€” enough to see behavioral change\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, raw_dataset, tokenizer, num_examples, max_length=MAX_LENGTH):\n",
    "        self.examples = []\n",
    "        skipped = 0\n",
    "\n",
    "        for i in range(min(num_examples, len(raw_dataset))):\n",
    "            formatted = format_example(raw_dataset[i])\n",
    "            input_ids, labels = tokenize_with_labels(formatted, tokenizer, max_length)\n",
    "\n",
    "            # Skip examples where the response is entirely truncated\n",
    "            if (labels != -100).sum() < 5:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            self.examples.append({\"input_ids\": input_ids, \"labels\": labels})\n",
    "\n",
    "        print(f\"Created dataset: {len(self.examples)} examples (skipped {skipped} too-long)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pad examples to the same length within a batch.\"\"\"\n",
    "    max_len = max(len(ex[\"input_ids\"]) for ex in batch)\n",
    "\n",
    "    input_ids_padded = []\n",
    "    labels_padded = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for ex in batch:\n",
    "        pad_len = max_len - len(ex[\"input_ids\"])\n",
    "        pad_id = sft_tokenizer.pad_token_id\n",
    "\n",
    "        input_ids_padded.append(\n",
    "            torch.cat([ex[\"input_ids\"], torch.full((pad_len,), pad_id)])\n",
    "        )\n",
    "        labels_padded.append(\n",
    "            torch.cat([ex[\"labels\"], torch.full((pad_len,), -100)])\n",
    "        )\n",
    "        attention_masks.append(\n",
    "            torch.cat([torch.ones(len(ex[\"input_ids\"])), torch.zeros(pad_len)])\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.stack(input_ids_padded).long(),\n",
    "        \"labels\": torch.stack(labels_padded).long(),\n",
    "        \"attention_mask\": torch.stack(attention_masks).long(),\n",
    "    }\n",
    "\n",
    "\n",
    "train_dataset = InstructionDataset(dataset, sft_tokenizer, NUM_TRAIN)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: The SFT Training Loop ---\n",
    "\n",
    "# Same heartbeat as pretraining and classification finetuning:\n",
    "#   forward -> loss -> zero_grad -> backward -> step\n",
    "#\n",
    "# The ONLY difference: the data is instruction-response pairs with loss masking.\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "optimizer = torch.optim.AdamW(sft_model.parameters(), lr=LEARNING_RATE)\n",
    "sft_model.train()\n",
    "\n",
    "losses = []\n",
    "step_count = 0\n",
    "\n",
    "print(f\"Training for {NUM_EPOCHS} epochs on {len(train_dataset)} examples...\")\n",
    "print(f\"Batch size: 4, Steps per epoch: ~{len(train_dataloader)}\")\n",
    "print()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        # TODO: Complete the training loop.\n",
    "        # 1. Forward pass: outputs = sft_model(input_ids=..., labels=..., attention_mask=...)\n",
    "        # 2. Get loss: loss = outputs.loss\n",
    "        # 3. Zero gradients\n",
    "        # 4. Backward\n",
    "        # 5. Step\n",
    "\n",
    "        outputs = sft_model(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "        step_count += 1\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if step_count % 25 == 0:\n",
    "            print(f\"  Step {step_count:4d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS} â€” Avg loss: {avg_loss:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Training complete! {step_count} total steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, linewidth=1.5, color='#34d399', alpha=0.6, label='Per-step loss')\n",
    "\n",
    "# Smoothed version\n",
    "window = 20\n",
    "if len(losses) > window:\n",
    "    smoothed = [sum(losses[max(0,i-window):i+1]) / len(losses[max(0,i-window):i+1]) for i in range(len(losses))]\n",
    "    plt.plot(smoothed, linewidth=2, color='#34d399', label=f'Smoothed ({window}-step)')\n",
    "\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('SFT Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Loss decreased from {losses[0]:.4f} to {losses[-1]:.4f}\")\n",
    "print(\"The model is learning to predict response tokens given instruction prompts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you just implemented:** The complete SFT pipeline â€” the same training loop heartbeat (forward, loss, zero_grad, backward, step), but with instruction-response pairs and loss masking. No new architecture. No new loss function. The only change is the **data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Compare Before and After (Supported)\n",
    "\n",
    "Now the payoff: compare the original base model's responses to the SFT model's responses on the same prompts.\n",
    "\n",
    "The model needs prompts formatted with the chat template it was trained on. If we use the wrong template (or no template), the model will not recognize the structural boundary â€” as the lesson's \"Wrong Template\" checkpoint explained.\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The key insight: the SFT model was trained on ChatML-formatted data, so at inference time we must format the prompt the same way. The model learned that `<|im_start|>assistant\\n` means \"start generating a response here.\"\n",
    "\n",
    "```python\n",
    "def format_inference_prompt(user_message):\n",
    "    return (\n",
    "        \"<|im_start|>system\\n\"\n",
    "        \"You are a helpful assistant.<|im_end|>\\n\"\n",
    "        \"<|im_start|>user\\n\"\n",
    "        f\"{user_message}<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "```\n",
    "\n",
    "We stop at `<|im_start|>assistant\\n` â€” the model generates the rest. Without this template, the SFT model would not know where to start its response.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a function that formats a user message into a ChatML prompt\n",
    "# for inference. The prompt should include the system message and user turn,\n",
    "# ending with \"<|im_start|>assistant\\n\" so the model knows to generate a response.\n",
    "\n",
    "def format_inference_prompt(user_message):\n",
    "    \"\"\"Format a user message into ChatML for inference.\n",
    "    Includes system + user turns, ending at the assistant's turn start.\"\"\"\n",
    "    return (\n",
    "        \"<|im_start|>system\\n\"\n",
    "        \"You are a helpful assistant.<|im_end|>\\n\"\n",
    "        \"<|im_start|>user\\n\"\n",
    "        f\"{user_message}<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Evaluation prompts â€” a mix of tasks the model may or may not have seen\n",
    "eval_prompts = [\n",
    "    \"Write a haiku about machine learning.\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain why the sky is blue in one sentence.\",\n",
    "    \"List three benefits of exercise.\",\n",
    "    \"What is the difference between a list and a tuple in Python?\",\n",
    "]\n",
    "\n",
    "sft_model.eval()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BEFORE vs AFTER SFT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt_text in eval_prompts:\n",
    "    # Base model: raw prompt (no template â€” base model was never trained on templates)\n",
    "    base_response = generate_response(base_model, tokenizer, prompt_text, max_new_tokens=100)\n",
    "\n",
    "    # SFT model: formatted with chat template\n",
    "    chat_prompt = format_inference_prompt(prompt_text)\n",
    "    sft_response = generate_response(sft_model, sft_tokenizer, chat_prompt, max_new_tokens=100)\n",
    "\n",
    "    print(f\"\\nPrompt: {prompt_text}\")\n",
    "    print(f\"\\n  BASE MODEL (text completer):\")\n",
    "    print(f\"  {base_response[:200]}\")\n",
    "    print(f\"\\n  SFT MODEL (instruction follower):\")\n",
    "    print(f\"  {sft_response[:200]}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you should observe:** The SFT model's responses should show a shift toward instruction-following behavior. With only 500 examples and 2 epochs on tiny GPT-2, don't expect ChatGPT quality â€” but you should see a clear difference in **format**. The base model continues text. The SFT model attempts to answer.\n",
    "\n",
    "This is the lesson's central insight in action: **SFT teaches format, not knowledge.** The knowledge was already in the base model. SFT changed how the model expresses it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Data Quality Experiment â€” Noisy vs Clean (Independent)\n",
    "\n",
    "The lesson mentioned that data quality matters more than quantity for SFT â€” LIMA showed that 1,000 carefully curated examples can match datasets 50x larger.\n",
    "\n",
    "**Your task:** Train two SFT models from the same base:\n",
    "1. One on **clean** instruction-response pairs (well-formed, correct responses)\n",
    "2. One on **noisy** data (same instructions but with corrupted/garbled responses)\n",
    "\n",
    "Compare their outputs on the same evaluation prompts. Does the noise in the training data show up in the model's behavior?\n",
    "\n",
    "**Specification:**\n",
    "- Create a `corrupt_response()` function that degrades response quality (shuffle words, add random characters, truncate, etc.)\n",
    "- Build two datasets of 200 examples each: one clean, one noisy\n",
    "- Train two models (same hyperparameters) for 1 epoch each\n",
    "- Compare responses on 3-5 evaluation prompts\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ’¡ Solution</summary>\n",
    "\n",
    "The reasoning: if SFT teaches format and not knowledge, then noisy SFT teaches noisy format. The model will still attempt to respond to instructions (it learned the instruction-response pattern), but the quality of its responses will reflect the quality of the training data.\n",
    "\n",
    "This is why the LIMA paper's result makes sense: format is a relatively simple pattern, so a small number of high-quality examples is enough. But low-quality examples teach low-quality format.\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "def corrupt_response(text):\n",
    "    \"\"\"Degrade a response: shuffle words, add noise, truncate.\"\"\"\n",
    "    words = text.split()\n",
    "    # Shuffle word order\n",
    "    random.shuffle(words)\n",
    "    # Truncate to random length\n",
    "    keep = max(3, len(words) // 2)\n",
    "    words = words[:keep]\n",
    "    # Add random characters\n",
    "    noisy_words = []\n",
    "    for w in words:\n",
    "        if random.random() < 0.3:\n",
    "            w = w + \"xxx\"\n",
    "        noisy_words.append(w)\n",
    "    return \" \".join(noisy_words)\n",
    "\n",
    "# Build noisy dataset\n",
    "noisy_examples = []\n",
    "for i in range(200):\n",
    "    ex = dataset[i]\n",
    "    noisy_ex = {\n",
    "        \"instruction\": ex[\"instruction\"],\n",
    "        \"input\": ex[\"input\"],\n",
    "        \"output\": corrupt_response(ex[\"output\"]),\n",
    "    }\n",
    "    noisy_examples.append(noisy_ex)\n",
    "\n",
    "# Train both models with the same loop, then compare on eval prompts.\n",
    "```\n",
    "\n",
    "Common alternative: Instead of shuffling, you could replace responses with random text entirely. This tests whether the model even learns the format at all vs learning bad format.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Your data quality experiment ---\n",
    "# Implement the experiment described above.\n",
    "# Create corrupt_response(), build clean and noisy datasets,\n",
    "# train two models, and compare outputs.\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **SFT teaches format, not knowledge.** The base model already has vast knowledge from pretraining. SFT on instruction-response pairs teaches it to express that knowledge in an instruction-following format â€” a much simpler pattern to learn.\n",
    "\n",
    "2. **No new architecture, no new loss function.** The training loop is the same heartbeat: forward, cross-entropy loss, zero_grad, backward, step. The only change is the data â€” formatted instruction-response pairs instead of web text.\n",
    "\n",
    "3. **Loss masking focuses training on responses.** Prompt tokens get label `-100` so the model learns to *generate* responses, not to predict instruction tokens it already has. This is the one genuinely new mechanical concept.\n",
    "\n",
    "4. **Chat templates are functional structure, not cosmetic formatting.** Special tokens like `<|im_start|>` and `<|im_end|>` are structural delimiters the model learns to recognize. Using the wrong template at inference time breaks the model's ability to find the response boundary.\n",
    "\n",
    "5. **Data quality matters more than quantity.** A small number of clean, well-formed instruction-response pairs teaches better format than a large number of noisy ones. Format is a simple pattern â€” it needs clarity, not volume."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}