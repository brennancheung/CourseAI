{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Speed Landscape\n",
    "\n",
    "**Module 7.3, Lesson 3** | CourseAI\n",
    "\n",
    "You have learned six ways to generate images faster across three modules. Each was taught as a response to a limitation of the previous approach. This notebook puts them head-to-head so you can see the tradeoffs for yourself.\n",
    "\n",
    "**What you will do:**\n",
    "- Generate the same image with DPM-Solver++ at different step counts and time each one—see where the \"free speedup\" ends and quality loss begins\n",
    "- Compare DPM-Solver++ (Level 1) against LCM-LoRA (Level 3a) on the same model and prompt—a head-to-head acceleration showdown\n",
    "- Compose LCM-LoRA with a style LoRA and verify that speed + style combine as the framework predicts\n",
    "- Build your own decision cheat sheet by analyzing scenarios and verifying one choice with code\n",
    "\n",
    "**For each exercise, PREDICT the output before running the cell.**\n",
    "\n",
    "Every concept in this notebook comes from the lesson. The four-dimensional framework (speed, quality, flexibility, composability), the quality-speed curve, the composability rules. No new theory—just hands-on comparison of approaches you already understand.\n",
    "\n",
    "**Estimated time:** 35-50 minutes. All exercises use pre-trained models (no training). Requires a GPU runtime for reasonable generation times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install dependencies and configure the environment.\n",
    "\n",
    "**Important:** Switch to a GPU runtime in Colab (Runtime > Change runtime type > T4 GPU). Generation works on CPU but is very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate safetensors peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import (\n",
    "    StableDiffusionPipeline,\n",
    "    DDPMScheduler,\n",
    "    LCMScheduler,\n",
    "    DPMSolverMultistepScheduler,\n",
    ")\n",
    "from IPython.display import display\n",
    "\n",
    "# Reproducible results\n",
    "SEED = 42\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [14, 5]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(f'Device: {device}')\n",
    "print(f'Dtype: {dtype}')\n",
    "if device.type == 'cpu':\n",
    "    print('WARNING: No GPU detected. Generation will be very slow.')\n",
    "    print('Switch to a GPU runtime: Runtime > Change runtime type > T4 GPU')\n",
    "print()\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Helpers\n",
    "\n",
    "Utility functions for timing generation and displaying image comparisons. Run this cell now—these are used across all four exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timed(pipe, prompt, num_inference_steps, guidance_scale, seed=SEED):\n",
    "    \"\"\"Generate an image and return (image, elapsed_seconds).\"\"\"\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    start = time.time()\n",
    "    result = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator,\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    return result.images[0], elapsed\n",
    "\n",
    "\n",
    "def show_image_row(images, titles, suptitle=None, figsize=None):\n",
    "    \"\"\"Display a row of PIL images with titles.\"\"\"\n",
    "    n = len(images)\n",
    "    fig_w = figsize[0] if figsize else max(4 * n, 12)\n",
    "    fig_h = figsize[1] if figsize else 5\n",
    "    fig, axes = plt.subplots(1, n, figsize=(fig_w, fig_h))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle, fontsize=13, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_timing_table(labels, times):\n",
    "    \"\"\"Print a formatted timing comparison table.\"\"\"\n",
    "    max_label = max(len(l) for l in labels)\n",
    "    print(\"Timing comparison:\")\n",
    "    for label, t in zip(labels, times):\n",
    "        print(f\"  {label:<{max_label}}  {t:.2f}s\")\n",
    "    if len(times) >= 2:\n",
    "        print(f\"  Speedup (first vs last): {times[0] / times[-1]:.1f}x\")\n",
    "\n",
    "\n",
    "print('Helpers defined: generate_timed, show_image_row, print_timing_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: The Sampler Swap `[Guided]`\n",
    "\n",
    "Level 1 acceleration is the simplest: swap the scheduler to DPM-Solver++, a higher-order ODE solver that reads trajectory curvature. No model changes, no adapter, no retraining. One line of code.\n",
    "\n",
    "From the lesson: the quality-speed curve is **nonlinear**. The first 50x speedup (1000 to 20 steps) costs nothing—it just removes redundant computation. The next reduction (20 to 10 steps) starts showing quality loss.\n",
    "\n",
    "We will generate the same image at three step counts to see this curve in action:\n",
    "1. **DDPM at 50 steps**—the baseline (many small steps, Markov chain)\n",
    "2. **DPM-Solver++ at 20 steps**—Level 1 acceleration (higher-order ODE solver)\n",
    "3. **DPM-Solver++ at 10 steps**—pushing Level 1 further\n",
    "\n",
    "**Before running, predict:**\n",
    "- How much quality difference will you see between DDPM at 50 steps and DPM-Solver++ at 20 steps? (Hint: from the lesson, the first speedup is \"free.\")\n",
    "- What about DPM-Solver++ at 10 steps—will you start to see degradation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1: Load SD v1.5 and compare sampler step counts\n",
    "# ============================================================\n",
    "\n",
    "PROMPT = \"a small cottage on a hillside at sunset, warm golden light, detailed\"\n",
    "\n",
    "# --- Step 1: Load SD v1.5 ---\n",
    "print(\"Loading SD v1.5...\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    torch_dtype=dtype,\n",
    "    safety_checker=None,\n",
    ").to(device)\n",
    "\n",
    "# Save the original scheduler config so we can swap schedulers\n",
    "original_scheduler_config = pipe.scheduler.config\n",
    "print(\"SD v1.5 loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Generate with DDPM at 50 steps (baseline) ---\n",
    "pipe.scheduler = DDPMScheduler.from_config(original_scheduler_config)\n",
    "\n",
    "print(\"Generating: DDPM at 50 steps (baseline)...\")\n",
    "img_ddpm_50, time_ddpm_50 = generate_timed(\n",
    "    pipe, PROMPT, num_inference_steps=50, guidance_scale=7.5\n",
    ")\n",
    "print(f\"  Done in {time_ddpm_50:.2f}s\")\n",
    "\n",
    "# --- Step 3: Generate with DPM-Solver++ at 20 steps ---\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(original_scheduler_config)\n",
    "\n",
    "print(\"Generating: DPM-Solver++ at 20 steps...\")\n",
    "img_dpm_20, time_dpm_20 = generate_timed(\n",
    "    pipe, PROMPT, num_inference_steps=20, guidance_scale=7.5\n",
    ")\n",
    "print(f\"  Done in {time_dpm_20:.2f}s\")\n",
    "\n",
    "# --- Step 4: Generate with DPM-Solver++ at 10 steps ---\n",
    "print(\"Generating: DPM-Solver++ at 10 steps...\")\n",
    "img_dpm_10, time_dpm_10 = generate_timed(\n",
    "    pipe, PROMPT, num_inference_steps=10, guidance_scale=7.5\n",
    ")\n",
    "print(f\"  Done in {time_dpm_10:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Compare all three side by side ---\n",
    "show_image_row(\n",
    "    [img_ddpm_50, img_dpm_20, img_dpm_10],\n",
    "    [\n",
    "        f\"DDPM | 50 steps\\n{time_ddpm_50:.1f}s | guidance=7.5\",\n",
    "        f\"DPM-Solver++ | 20 steps\\n{time_dpm_20:.1f}s | guidance=7.5\",\n",
    "        f\"DPM-Solver++ | 10 steps\\n{time_dpm_10:.1f}s | guidance=7.5\",\n",
    "    ],\n",
    "    suptitle=f'Level 1 Acceleration: \"{PROMPT}\"',\n",
    ")\n",
    "\n",
    "print_timing_table(\n",
    "    [\"DDPM (50 steps):\", \"DPM-Solver++ (20 steps):\", \"DPM-Solver++ (10 steps):\"],\n",
    "    [time_ddpm_50, time_dpm_20, time_dpm_10],\n",
    ")\n",
    "print()\n",
    "print(\"What changed between images:\")\n",
    "print(\"  - DDPM to DPM-Solver++ at 20: scheduler swap only. Same model, same weights.\")\n",
    "print(\"    Quality should be nearly identical. This is the 'free speedup' from the lesson.\")\n",
    "print(\"  - DPM-Solver++ 20 to 10: same scheduler, fewer steps.\")\n",
    "print(\"    You may see softening in textures or loss of fine detail.\")\n",
    "print(\"    This is where Level 1 starts hitting its floor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You saw Level 1 acceleration in action—the simplest and most accessible speedup:\n",
    "\n",
    "- **DDPM at 50 steps vs DPM-Solver++ at 20 steps: nearly identical quality.** The DPM-Solver++ higher-order ODE solver reads trajectory curvature to take larger, more accurate steps. It removes the *redundancy* in DDPM's many small steps—this is wasted computation eliminated, not quality sacrificed. This is the \"free speedup\" region from the lesson's quality-speed curve.\n",
    "\n",
    "- **DPM-Solver++ at 10 steps: quality starts to degrade.** You may notice softening in fine textures, loss of small details, or slightly less coherent composition. The solver is now taking steps large enough that even its higher-order corrections cannot fully compensate. This is the boundary of the \"free\" region—beyond this, Level 1 needs help from Level 2 or Level 3.\n",
    "\n",
    "- **Nothing changed about the model.** Same U-Net weights, same VAE, same text encoder. The only change was the scheduler class and step count. This is the defining property of Level 1: **it is an inference-time choice that works with any diffusion model**.\n",
    "\n",
    "- **The timing confirms the speedup is roughly proportional to step count.** Each step costs about the same amount of compute (one forward pass through the U-Net). Fewer steps = proportionally less time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: LCM-LoRA vs DPM-Solver++ `[Guided]`\n",
    "\n",
    "Level 1 (DPM-Solver++ at 20 steps) gave us a free speedup. But what if we want to go faster—say, 4 steps? That is where Level 3a comes in.\n",
    "\n",
    "From the lesson: LCM-LoRA captures the consistency distillation as a 4 MB LoRA adapter. It is a different *kind* of acceleration—not a better ODE solver, but a direct mapping that bypasses the trajectory entirely.\n",
    "\n",
    "We will compare them head-to-head on the same model and prompt:\n",
    "1. **DPM-Solver++ at 20 steps**—Level 1 (our best result from Exercise 1)\n",
    "2. **LCM-LoRA at 8 steps**—Level 3a (moderate step count)\n",
    "3. **LCM-LoRA at 4 steps**—Level 3a (the typical sweet spot)\n",
    "\n",
    "**Before running, predict:**\n",
    "- LCM-LoRA at 4 steps uses the consistency objective—it was trained to produce good results in few steps. Will it match DPM-Solver++ at 20 steps in quality?\n",
    "- What about at 8 steps? More steps with LCM-LoRA means more consistency refinement cycles. Will 8 steps look better than 4?\n",
    "- Which approach will be faster: DPM-Solver++ at 20 steps or LCM-LoRA at 4 steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 2: Head-to-head—DPM-Solver++ vs LCM-LoRA\n",
    "# ============================================================\n",
    "\n",
    "# We reuse the pipeline from Exercise 1.\n",
    "# If you restarted the runtime, re-run Exercise 1 first.\n",
    "\n",
    "# --- Step 1: DPM-Solver++ at 20 steps (Level 1 best) ---\n",
    "# We already have this from Exercise 1, but let's regenerate for a clean comparison.\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(original_scheduler_config)\n",
    "\n",
    "print(\"Generating: DPM-Solver++ at 20 steps (Level 1)...\")\n",
    "img_level1, time_level1 = generate_timed(\n",
    "    pipe, PROMPT, num_inference_steps=20, guidance_scale=7.5\n",
    ")\n",
    "print(f\"  Done in {time_level1:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Load LCM-LoRA and generate at 8 steps and 4 steps ---\n",
    "print(\"Loading LCM-LoRA adapter...\")\n",
    "pipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n",
    "pipe.scheduler = LCMScheduler.from_config(original_scheduler_config)\n",
    "print(\"LCM-LoRA loaded. Scheduler swapped to LCMScheduler.\")\n",
    "\n",
    "# Note: LCM uses a reduced guidance scale (1.5 instead of 7.5).\n",
    "# The augmented PF-ODE already incorporates guidance into the trajectory.\n",
    "\n",
    "print(\"Generating: LCM-LoRA at 8 steps (Level 3a)...\")\n",
    "img_lcm_8, time_lcm_8 = generate_timed(\n",
    "    pipe, PROMPT, num_inference_steps=8, guidance_scale=1.5\n",
    ")\n",
    "print(f\"  Done in {time_lcm_8:.2f}s\")\n",
    "\n",
    "print(\"Generating: LCM-LoRA at 4 steps (Level 3a)...\")\n",
    "img_lcm_4, time_lcm_4 = generate_timed(\n",
    "    pipe, PROMPT, num_inference_steps=4, guidance_scale=1.5\n",
    ")\n",
    "print(f\"  Done in {time_lcm_4:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Compare all three side by side ---\n",
    "show_image_row(\n",
    "    [img_level1, img_lcm_8, img_lcm_4],\n",
    "    [\n",
    "        f\"Level 1: DPM-Solver++\\n20 steps | {time_level1:.1f}s | cfg=7.5\",\n",
    "        f\"Level 3a: LCM-LoRA\\n8 steps | {time_lcm_8:.1f}s | cfg=1.5\",\n",
    "        f\"Level 3a: LCM-LoRA\\n4 steps | {time_lcm_4:.1f}s | cfg=1.5\",\n",
    "    ],\n",
    "    suptitle=f'Level 1 vs Level 3a: \"{PROMPT}\"',\n",
    ")\n",
    "\n",
    "print_timing_table(\n",
    "    [\"DPM-Solver++ (20 steps):\", \"LCM-LoRA (8 steps):\", \"LCM-LoRA (4 steps):\"],\n",
    "    [time_level1, time_lcm_8, time_lcm_4],\n",
    ")\n",
    "print()\n",
    "print(\"These are two fundamentally different acceleration strategies:\")\n",
    "print(\"  - DPM-Solver++ (Level 1): a better ODE solver stepping along the trajectory.\")\n",
    "print(\"    Works with any model. No adapter needed. 20 steps, high quality.\")\n",
    "print(\"  - LCM-LoRA (Level 3a): a consistency model that bypasses the trajectory.\")\n",
    "print(\"    Needs the 4 MB adapter + LCMScheduler. 4 steps, good quality.\")\n",
    "print()\n",
    "print(\"The framework from the lesson evaluates this across four dimensions:\")\n",
    "print(\"  Speed:         LCM-LoRA wins (4 steps vs 20)\")\n",
    "print(\"  Quality:       DPM-Solver++ has an edge (20 ODE steps vs 4 consistency steps)\")\n",
    "print(\"  Flexibility:   DPM-Solver++ wins (no adapter needed, works on anything)\")\n",
    "print(\"  Composability: LCM-LoRA wins (composable with style LoRAs and ControlNet)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened\n",
    "\n",
    "You saw Level 1 and Level 3a compared directly—the same model, same prompt, same seed, different acceleration strategies:\n",
    "\n",
    "- **DPM-Solver++ at 20 steps produces the highest quality.** It is a mathematical ODE solver that takes carefully computed steps along the diffusion trajectory. At 20 steps, it has enough steps to capture the trajectory curvature accurately. But it takes 20 forward passes through the U-Net.\n",
    "\n",
    "- **LCM-LoRA at 4 steps is faster with a small quality tradeoff.** The consistency model does not follow the trajectory step-by-step—it maps directly from noisy input to clean output. The quality is good but may show slight softening compared to 20-step DPM-Solver++. This is the \"cheap speedup\" region of the quality-speed curve.\n",
    "\n",
    "- **LCM-LoRA at 8 steps shows diminishing returns.** The improvement from 4 to 8 steps is modest—the consistency model was optimized for very few steps. Going beyond 4 steps provides marginal gains, unlike DPM-Solver++ which benefits more linearly from additional steps.\n",
    "\n",
    "- **Neither approach is universally better.** This is the core insight from the lesson: the decision has four dimensions. If you want maximum quality with no setup, DPM-Solver++ at 20 steps wins. If you want fast previews and plan to compose with other LoRAs, LCM-LoRA at 4 steps wins. The \"right\" answer depends on your constraints.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cleanup: unload LCM-LoRA so we start fresh for Exercise 3 ---\n",
    "pipe.unload_lora_weights()\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(original_scheduler_config)\n",
    "print(\"LCM-LoRA unloaded. Scheduler reset to DPM-Solver++.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: The Composability Test `[Supported]`\n",
    "\n",
    "From the lesson: approaches **compose across levels** but **conflict within levels**. LCM-LoRA (Level 3a) and a style LoRA are both LoRA adapters, but they target different behaviors—speed and style respectively. The framework predicts they should compose cleanly.\n",
    "\n",
    "Your task: verify this prediction by generating three comparison images:\n",
    "1. **Style LoRA alone** at 20 steps with DPM-Solver++—the style baseline\n",
    "2. **LCM-LoRA alone** at 4 steps—the speed baseline (from Exercise 2)\n",
    "3. **LCM-LoRA + style LoRA** at 4 steps—the composition\n",
    "\n",
    "If the composition works, image 3 should look like image 1's style but generated in image 2's time.\n",
    "\n",
    "Fill in the TODO markers to complete the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 3: Composability—LCM-LoRA + style LoRA\n",
    "# ============================================================\n",
    "\n",
    "STYLE_LORA = \"artificialguybr/pixelartredmond-1-5v-pixel-art-loras-for-sd-1-5\"\n",
    "PROMPT_3 = \"a lighthouse on a rocky coast during a storm, waves crashing, PixArFK\"\n",
    "\n",
    "# --- Step 1: Style LoRA alone at 20 steps (style baseline) ---\n",
    "# Load the style LoRA with a named adapter so we can manage multiple LoRAs.\n",
    "print(\"Loading style LoRA...\")\n",
    "pipe.load_lora_weights(STYLE_LORA, adapter_name=\"style\")\n",
    "pipe.set_adapters([\"style\"], adapter_weights=[1.0])\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(original_scheduler_config)\n",
    "\n",
    "print(\"Generating: style LoRA alone, DPM-Solver++ at 20 steps...\")\n",
    "img_style_only, time_style_only = generate_timed(\n",
    "    pipe, PROMPT_3, num_inference_steps=20, guidance_scale=7.5\n",
    ")\n",
    "print(f\"  Done in {time_style_only:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: LCM-LoRA alone at 4 steps (speed baseline) ---\n",
    "\n",
    "# TODO: Load the LCM-LoRA as a second named adapter (adapter_name=\"lcm\").\n",
    "# Use pipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\", adapter_name=\"lcm\")\n",
    "raise NotImplementedError(\n",
    "    \"TODO: Load the LCM-LoRA with adapter_name='lcm'. See the hint above.\"\n",
    ")\n",
    "\n",
    "# TODO: Activate ONLY the LCM adapter (deactivate the style adapter).\n",
    "# Use pipe.set_adapters([\"lcm\"], adapter_weights=[1.0])\n",
    "raise NotImplementedError(\n",
    "    \"TODO: Set only the 'lcm' adapter active with weight 1.0.\"\n",
    ")\n",
    "\n",
    "# TODO: Swap the scheduler to LCMScheduler.\n",
    "# Use pipe.scheduler = LCMScheduler.from_config(original_scheduler_config)\n",
    "raise NotImplementedError(\n",
    "    \"TODO: Swap scheduler to LCMScheduler.\"\n",
    ")\n",
    "\n",
    "print(\"Generating: LCM-LoRA alone at 4 steps...\")\n",
    "img_lcm_only, time_lcm_only = generate_timed(\n",
    "    pipe, PROMPT_3, num_inference_steps=4, guidance_scale=1.5\n",
    ")\n",
    "print(f\"  Done in {time_lcm_only:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: LCM-LoRA + style LoRA composed at 4 steps ---\n",
    "\n",
    "# TODO: Activate BOTH adapters with appropriate weights.\n",
    "# Use pipe.set_adapters([\"lcm\", \"style\"], adapter_weights=[1.0, 1.0])\n",
    "# Keep the LCMScheduler from Step 2—we are generating with LCM.\n",
    "raise NotImplementedError(\n",
    "    \"TODO: Activate both 'lcm' and 'style' adapters. See the hint above.\"\n",
    ")\n",
    "\n",
    "print(\"Generating: LCM-LoRA + style LoRA composed at 4 steps...\")\n",
    "img_composed, time_composed = generate_timed(\n",
    "    pipe, PROMPT_3, num_inference_steps=4, guidance_scale=1.5\n",
    ")\n",
    "print(f\"  Done in {time_composed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Compare all three ---\n",
    "show_image_row(\n",
    "    [img_style_only, img_lcm_only, img_composed],\n",
    "    [\n",
    "        f\"Style LoRA only\\n20 steps | {time_style_only:.1f}s | cfg=7.5\",\n",
    "        f\"LCM-LoRA only\\n4 steps | {time_lcm_only:.1f}s | cfg=1.5\",\n",
    "        f\"LCM + Style composed\\n4 steps | {time_composed:.1f}s | cfg=1.5\",\n",
    "    ],\n",
    "    suptitle=f'Composability Test: \"{PROMPT_3}\"',\n",
    ")\n",
    "\n",
    "print_timing_table(\n",
    "    [\"Style LoRA alone (20 steps):\", \"LCM-LoRA alone (4 steps):\", \"LCM + Style (4 steps):\"],\n",
    "    [time_style_only, time_lcm_only, time_composed],\n",
    ")\n",
    "print()\n",
    "print(\"Compare the three images:\")\n",
    "print(\"  - Left (style only): the style target. Should show pixel art aesthetics.\")\n",
    "print(\"  - Middle (LCM only): the speed target. Fast generation, no style adapter.\")\n",
    "print(\"  - Right (composed): should combine pixel art style with 4-step speed.\")\n",
    "print()\n",
    "print(\"If the composition works, the right image has the style of the left\")\n",
    "print(\"and the speed of the middle. This confirms the lesson's prediction:\")\n",
    "print(\"LoRA bypasses are additive, speed and style are orthogonal skills.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The key insight is that LoRA bypasses are additive. LCM-LoRA modifies the denoising dynamics (\"generate faster\") while the style LoRA modifies the output aesthetics (\"generate in pixel art style\"). They target different aspects of the model's behavior, so they compose without significant interference.\n",
    "\n",
    "```python\n",
    "# Step 2: LCM-LoRA alone\n",
    "pipe.load_lora_weights(\n",
    "    \"latent-consistency/lcm-lora-sdv1-5\",\n",
    "    adapter_name=\"lcm\",\n",
    ")\n",
    "pipe.set_adapters([\"lcm\"], adapter_weights=[1.0])\n",
    "pipe.scheduler = LCMScheduler.from_config(original_scheduler_config)\n",
    "\n",
    "# Step 3: Both adapters composed\n",
    "pipe.set_adapters([\"lcm\", \"style\"], adapter_weights=[1.0, 1.0])\n",
    "```\n",
    "\n",
    "**Why it works:** Both LoRAs modify the U-Net weights via low-rank bypasses: $W_{\\text{effective}} = W_{\\text{base}} + \\alpha_1 B_1 A_1 + \\alpha_2 B_2 A_2$. The LCM-LoRA was trained to collapse the denoising trajectory into 4 steps. The style LoRA was trained to produce pixel art aesthetics. Because they target different \"axes\" of behavior, the sum of both bypasses achieves both goals simultaneously.\n",
    "\n",
    "**Common mistakes:**\n",
    "- Forgetting to swap to `LCMScheduler` when using LCM-LoRA (produces garbage at 4 steps)\n",
    "- Using `guidance_scale=7.5` with LCM-LoRA (causes oversaturation—the augmented PF-ODE already incorporates guidance)\n",
    "- Loading LoRAs without `adapter_name` (makes it impossible to control weights independently)\n",
    "- Not including the trigger word (`PixArFK`) in the prompt—the style LoRA may not activate without it\n",
    "\n",
    "</details>\n",
    "\n",
    "### What Just Happened\n",
    "\n",
    "You verified the composability prediction from the lesson's framework:\n",
    "\n",
    "- **LCM-LoRA + style LoRA compose.** The composed image should show the pixel art style from the style LoRA at the 4-step speed of LCM-LoRA. Two LoRA adapters, targeting different behaviors (speed and style), combined successfully.\n",
    "\n",
    "- **This works because the adapters are at different \"levels\" in the framework.** LCM-LoRA is a Level 3a acceleration technique. The style LoRA is not an acceleration technique at all—it is a content/style modification. They do not conflict because they address different aspects of generation.\n",
    "\n",
    "- **The scheduler must match the acceleration strategy.** When using LCM-LoRA, you need LCMScheduler and low guidance (1.5). When using only the style LoRA with DPM-Solver++, you use standard guidance (7.5). Mixing these up produces poor results—not because the approaches conflict, but because the configuration is wrong.\n",
    "\n",
    "- **This is the \"speed as a skill\" insight from Latent Consistency & Turbo.** The LCM-LoRA adds the skill of fast generation. The style LoRA adds the skill of pixel art. Skills are composable when they target different behaviors.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Build Your Decision Cheat Sheet `[Independent]`\n",
    "\n",
    "The lesson gave you a four-dimensional framework: **speed, quality, flexibility, composability**. This exercise asks you to apply it.\n",
    "\n",
    "Below are three generation scenarios. For each one:\n",
    "1. **Write** which acceleration approach you would choose\n",
    "2. **Explain** why, evaluating across all four dimensions\n",
    "3. **Pick one** scenario and verify your choice by implementing it and comparing with an alternative\n",
    "\n",
    "The reasoning matters more than the code. The framework should guide your choice.\n",
    "\n",
    "### Scenario A: The Concept Artist\n",
    "\n",
    "A concept artist wants to rapidly explore compositions for a client pitch. They use a **vanilla SD v1.5 model** (no custom fine-tune), need to generate **50+ variations** in an afternoon, and do not need production-quality output—they just need to see if a composition idea works.\n",
    "\n",
    "### Scenario B: The Game Studio\n",
    "\n",
    "A small game studio has a **custom SD 1.5 fine-tune** trained on their game's art style. They want to generate **character portraits** for their wiki at **good quality**. They also use a **ControlNet pose model** for consistent character poses. Speed matters but quality matters more.\n",
    "\n",
    "### Scenario C: The A/B Tester\n",
    "\n",
    "A product team wants to **A/B test** generated hero images on their landing page. They need to generate images **on every page load** (real-time), using a **fixed prompt template** with no custom models or adapters. Latency must be **under 1 second**. Slight quality reduction is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 4: Your decision cheat sheet\n",
    "# ============================================================\n",
    "#\n",
    "# Step 1: For each scenario, write your choice and reasoning\n",
    "#          in the markdown cell below (or as comments here).\n",
    "#\n",
    "# Step 2: Pick ONE scenario and verify your choice.\n",
    "#          - Implement the approach you chose\n",
    "#          - Implement an alternative approach\n",
    "#          - Compare quality, speed, and any other relevant dimension\n",
    "#\n",
    "# You have access to the pipeline from earlier exercises.\n",
    "# Clean up LoRA adapters first:\n",
    "\n",
    "pipe.unload_lora_weights()\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(original_scheduler_config)\n",
    "print(\"Pipeline reset. Ready for your implementation.\")\n",
    "print()\n",
    "print(\"Available tools:\")\n",
    "print(\"  - pipe: SD v1.5 pipeline (currently no LoRA, DPM-Solver++ scheduler)\")\n",
    "print(\"  - generate_timed(pipe, prompt, steps, guidance): returns (image, time)\")\n",
    "print(\"  - show_image_row(images, titles): display comparison\")\n",
    "print(\"  - LCMScheduler, DPMSolverMultistepScheduler: scheduler classes\")\n",
    "print(\"  - LCM-LoRA: 'latent-consistency/lcm-lora-sdv1-5'\")\n",
    "print()\n",
    "print(\"Write your scenario analysis in the next markdown cell,\")\n",
    "print(\"then implement your verification in the code cell after that.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Analysis\n",
    "\n",
    "**Scenario A—The Concept Artist:**\n",
    "\n",
    "*Your choice:* [Which approach?]\n",
    "\n",
    "*Your reasoning (evaluate all 4 dimensions):*\n",
    "\n",
    "- Speed:\n",
    "- Quality:\n",
    "- Flexibility:\n",
    "- Composability:\n",
    "\n",
    "---\n",
    "\n",
    "**Scenario B—The Game Studio:**\n",
    "\n",
    "*Your choice:* [Which approach?]\n",
    "\n",
    "*Your reasoning (evaluate all 4 dimensions):*\n",
    "\n",
    "- Speed:\n",
    "- Quality:\n",
    "- Flexibility:\n",
    "- Composability:\n",
    "\n",
    "---\n",
    "\n",
    "**Scenario C—The A/B Tester:**\n",
    "\n",
    "*Your choice:* [Which approach?]\n",
    "\n",
    "*Your reasoning (evaluate all 4 dimensions):*\n",
    "\n",
    "- Speed:\n",
    "- Quality:\n",
    "- Flexibility:\n",
    "- Composability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Your verification: pick one scenario and implement it\n",
    "# ============================================================\n",
    "#\n",
    "# Generate with your chosen approach AND an alternative.\n",
    "# Compare the results.\n",
    "#\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "There is no single correct answer—the point is applying the framework. Here is how each scenario evaluates:\n",
    "\n",
    "**Scenario A—The Concept Artist:**\n",
    "\n",
    "**Choose: DPM-Solver++ at 15-20 steps (Level 1).**\n",
    "- Speed: 15-20 steps is fast enough for batch exploration. 2-3x faster than the default 50 steps.\n",
    "- Quality: No quality loss at 20 steps—\"free speedup\" region.\n",
    "- Flexibility: Works immediately with vanilla SD 1.5. No downloading adapters.\n",
    "- Composability: Not relevant—no custom adapters needed.\n",
    "\n",
    "Why not LCM-LoRA? It would be faster (4 steps vs 20), but requires downloading the adapter. For quick exploration with a vanilla model, Level 1 has zero friction.\n",
    "\n",
    "**Scenario B—The Game Studio:**\n",
    "\n",
    "**Choose: LCM-LoRA at 4-8 steps (Level 3a).**\n",
    "- Speed: 4-8 steps is fast. Good for batch generation.\n",
    "- Quality: At 8 steps, quality is good. \"Cheap speedup\" region.\n",
    "- Flexibility: LCM-LoRA works with their custom SD 1.5 fine-tune—same architecture.\n",
    "- Composability: LCM-LoRA composes with ControlNet for pose control. This is the decisive factor.\n",
    "\n",
    "Why not DPM-Solver++ at 20 steps? It would work too, but the studio wants speed AND ControlNet. LCM-LoRA at 8 steps with ControlNet gives both.\n",
    "\n",
    "**Scenario C—The A/B Tester:**\n",
    "\n",
    "**Choose: LCM-LoRA at 4 steps (Level 3a) or SDXL Turbo at 1-4 steps (Level 3b) if latency is critical.**\n",
    "- Speed: Must be under 1 second. LCM-LoRA at 4 steps on a GPU should hit this. SDXL Turbo at 1 step is even faster.\n",
    "- Quality: Slight reduction acceptable. Both approaches deliver at 4 steps.\n",
    "- Flexibility: Fixed prompt template, no custom models—no flexibility needed.\n",
    "- Composability: No adapters needed—composability is not a factor.\n",
    "\n",
    "For this scenario, if sub-500ms is truly needed, SDXL Turbo's 1-step generation is the strongest option. The locked-model tradeoff does not matter because they do not need custom models or adapters. (We cannot test SDXL Turbo in this notebook due to VRAM constraints, but we can verify LCM-LoRA at 4 steps.)\n",
    "\n",
    "**Example verification for Scenario A:**\n",
    "\n",
    "```python\n",
    "PROMPT_VERIFY = \"a futuristic cityscape with flying cars, neon lights, wide angle\"\n",
    "\n",
    "# Approach 1: DPM-Solver++ at 20 steps (our choice)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(original_scheduler_config)\n",
    "img_choice, time_choice = generate_timed(\n",
    "    pipe, PROMPT_VERIFY, num_inference_steps=20, guidance_scale=7.5\n",
    ")\n",
    "\n",
    "# Alternative: DPM-Solver++ at 50 steps (slower baseline)\n",
    "img_alt, time_alt = generate_timed(\n",
    "    pipe, PROMPT_VERIFY, num_inference_steps=50, guidance_scale=7.5\n",
    ")\n",
    "\n",
    "show_image_row(\n",
    "    [img_choice, img_alt],\n",
    "    [\n",
    "        f\"Our choice: DPM-Solver++ 20 steps\\n{time_choice:.1f}s\",\n",
    "        f\"Alternative: DPM-Solver++ 50 steps\\n{time_alt:.1f}s\",\n",
    "    ],\n",
    "    suptitle=\"Scenario A Verification\",\n",
    ")\n",
    "\n",
    "print(f\"Speedup: {time_alt / time_choice:.1f}x faster\")\n",
    "print(\"Quality comparison: nearly identical. The 'free speedup' confirmed.\")\n",
    "print(\"For 50+ variations in an afternoon, this time savings adds up.\")\n",
    "```\n",
    "\n",
    "The important thing is not which specific code you wrote, but whether your analysis correctly evaluated the scenario across all four dimensions and reached a well-reasoned conclusion.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The first speedup is free.** Going from 50 to 20 steps with DPM-Solver++ costs zero quality. This is the \"free speedup\" region—redundant computation removed by a better ODE solver. Every diffusion user should take this speedup.\n",
    "\n",
    "2. **Level 1 and Level 3a are different tools, not different versions.** DPM-Solver++ (Level 1) is a better solver you swap in instantly. LCM-LoRA (Level 3a) is a consistency model adapter that bypasses the trajectory. Neither replaces the other—they serve different scenarios.\n",
    "\n",
    "3. **Composability is a real dimension.** LCM-LoRA composes with style LoRAs because they target orthogonal behaviors (speed and style). This is not theoretical—you verified it produces a styled image at 4-step speed.\n",
    "\n",
    "4. **The decision framework has four dimensions, not one.** Speed alone does not determine the best approach. Quality, flexibility (works with your model?), and composability (works with your adapters?) often matter more in practice.\n",
    "\n",
    "5. **The right approach depends on your constraints.** A concept artist exploring compositions needs Level 1 (zero friction). A game studio with a custom model and ControlNet needs LCM-LoRA (composability). A real-time app needs maximum speed (Level 3a or 3b). The framework tells you which."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_extensions": {
   "jupyter_kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   }
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}