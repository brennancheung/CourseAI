{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Features: Seeing What CNNs See\n",
    "\n",
    "**Module 3.3, Lesson 1** | CourseAI\n",
    "\n",
    "In this notebook you will open a pretrained ResNet-18 and see what it actually learned. Three techniques, each answering a different question:\n",
    "\n",
    "1. **Filter Visualization** — What does this layer look for?\n",
    "2. **Activation Maps** — What did this layer find in this image?\n",
    "3. **Grad-CAM** — What in this image mattered for this prediction?\n",
    "\n",
    "Boilerplate for image loading and display is provided. Your job is to write the visualization code at the `# TODO` markers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to import everything and set up display utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained ResNet-18\n",
    "\n",
    "The same model you used in Transfer Learning. Load it and put it in eval mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "# ImageNet class labels\n",
    "weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "categories = weights.meta['categories']\n",
    "\n",
    "print(f'Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters')\n",
    "print(f'Number of classes: {len(categories)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Loading Utilities\n",
    "\n",
    "These helpers load and preprocess images for ResNet-18. Nothing to modify here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard ImageNet preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "def load_image_from_url(url):\n",
    "    \"\"\"Download an image from a URL and return both PIL and tensor versions.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    img_pil = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    img_tensor = preprocess(img_pil).unsqueeze(0).to(device)  # [1, 3, 224, 224]\n",
    "    return img_pil, img_tensor\n",
    "\n",
    "def show_image(img_pil, title=None):\n",
    "    \"\"\"Display a PIL image.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img_pil)\n",
    "    if title:\n",
    "        plt.title(title, fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def predict_top_k(img_tensor, k=5):\n",
    "    \"\"\"Get top-k predictions for an image tensor.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        top_probs, top_indices = probs.topk(k, dim=1)\n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        results.append((categories[top_indices[0, i].item()], top_probs[0, i].item()))\n",
    "    return results\n",
    "\n",
    "print('Utilities loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Sample Image\n",
    "\n",
    "We'll use a photo of a golden retriever. Feel free to try your own images later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample image: golden retriever\n",
    "img_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/b/bd/Golden_Retriever_Dukedestiny01_dv.jpg/800px-Golden_Retriever_Dukedestiny01_dv.jpg'\n",
    "img_pil, img_tensor = load_image_from_url(img_url)\n",
    "\n",
    "# Show the image and top predictions\n",
    "show_image(img_pil, 'Sample Image')\n",
    "print('Top-5 predictions:')\n",
    "for label, prob in predict_top_k(img_tensor):\n",
    "    print(f'  {label}: {prob:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Filter Visualization\n",
    "\n",
    "**Question this answers:** *What does this layer look for?*\n",
    "\n",
    "ResNet-18's first layer (`model.conv1`) has 64 filters of size 7x7x3. Each filter operates on RGB input, so we can display each one as a tiny 7x7 color image.\n",
    "\n",
    "### TODO: Visualize conv1 Filters\n",
    "\n",
    "Access `model.conv1.weight.data`, normalize the filters to [0, 1], and display them in an 8x8 grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get the conv1 filter weights\n",
    "# Hint: model.conv1.weight.data has shape [64, 3, 7, 7]\n",
    "filters = model.conv1.weight.data.clone().cpu()\n",
    "\n",
    "# TODO: Normalize each filter to [0, 1] for display\n",
    "# Hint: subtract the min, then divide by the max\n",
    "# Be careful — normalize each filter independently, not all at once\n",
    "for i in range(filters.shape[0]):\n",
    "    f = filters[i]\n",
    "    filters[i] = (f - f.min()) / (f.max() - f.min() + 1e-8)\n",
    "\n",
    "# Display as 8x8 grid\n",
    "fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < filters.shape[0]:\n",
    "        # TODO: Display filter i\n",
    "        # Hint: filters[i] has shape [3, 7, 7] — you need to permute to [7, 7, 3] for matplotlib\n",
    "        ax.imshow(filters[i].permute(1, 2, 0).numpy())\n",
    "    ax.axis('off')\n",
    "\n",
    "fig.suptitle('Conv1 Filters (64 filters, each 7x7x3)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Do You See?\n",
    "\n",
    "Look at the grid above. You should see:\n",
    "- **Oriented edges** at different angles (horizontal, vertical, diagonal)\n",
    "- **Color gradients** — transitions between complementary colors\n",
    "- **High-frequency patterns** — alternating light/dark stripes\n",
    "\n",
    "Notice what you do **NOT** see: no cats, no dogs, no objects. A 7x7 patch cannot contain a recognizable object. These are the \"questions\" the first layer asks at every spatial position: *\"Is there a vertical edge here? A color boundary? A diagonal gradient?\"*\n",
    "\n",
    "This confirms what you learned in Transfer Learning: conv1 features are **universal**. Edge detectors work for any image domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Activation Maps via Hooks\n",
    "\n",
    "**Question this answers:** *What did this layer find in this image?*\n",
    "\n",
    "To see what a layer produces for a specific image, we need to capture its output during a forward pass. PyTorch **hooks** let us do this without modifying the model.\n",
    "\n",
    "### Learn: Forward Hooks\n",
    "\n",
    "A forward hook is a callback that fires every time a layer completes its forward pass. It receives the layer's output, which we can store for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: capture layer2's output\n",
    "activation = {}\n",
    "\n",
    "def hook_fn(name):\n",
    "    \"\"\"Return a hook function that stores the output under the given name.\"\"\"\n",
    "    def hook(module, input, output):\n",
    "        activation[name] = output.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "# Register the hook\n",
    "hook_handle = model.layer2.register_forward_hook(hook_fn('layer2'))\n",
    "\n",
    "# Run a forward pass — the hook fires automatically\n",
    "with torch.no_grad():\n",
    "    _ = model(img_tensor)\n",
    "\n",
    "# Check what we captured\n",
    "print(f'Captured activation shape: {activation[\"layer2\"].shape}')\n",
    "print('Expected: [1, 128, 28, 28] — 128 channels at 28x28 spatial resolution')\n",
    "\n",
    "# Clean up\n",
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Capture Activations at Three Depths\n",
    "\n",
    "Register hooks on `model.conv1`, `model.layer2`, and `model.layer4`. Run a single forward pass. Then display a grid of activation maps from each layer.\n",
    "\n",
    "**Expected shapes:**\n",
    "- conv1: [1, 64, 112, 112]\n",
    "- layer2: [1, 128, 28, 28]\n",
    "- layer4: [1, 512, 7, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Register hooks on three layers\n",
    "activations = {}\n",
    "\n",
    "def make_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        activations[name] = output.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "hooks = [\n",
    "    model.conv1.register_forward_hook(make_hook('conv1')),\n",
    "    model.layer2.register_forward_hook(make_hook('layer2')),\n",
    "    model.layer4.register_forward_hook(make_hook('layer4')),\n",
    "]\n",
    "\n",
    "# Forward pass — all hooks fire\n",
    "with torch.no_grad():\n",
    "    _ = model(img_tensor)\n",
    "\n",
    "# Verify shapes\n",
    "for name, act in activations.items():\n",
    "    print(f'{name}: {act.shape}')\n",
    "\n",
    "# Remove hooks\n",
    "for h in hooks:\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Activation Maps\n",
    "\n",
    "Now visualize a selection of activation maps from each layer. We'll show 16 channels (4x4 grid) from each depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_activation_grid(act_tensor, layer_name, num_channels=16):\n",
    "    \"\"\"Display a grid of activation maps from a layer.\n",
    "    \n",
    "    act_tensor: shape [1, C, H, W]\n",
    "    \"\"\"\n",
    "    act = act_tensor[0]  # remove batch dim -> [C, H, W]\n",
    "    num_channels = min(num_channels, act.shape[0])\n",
    "    cols = 4\n",
    "    rows = (num_channels + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, 3 * rows))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_channels:\n",
    "            ax.imshow(act[i].numpy(), cmap='viridis')\n",
    "            ax.set_title(f'Channel {i}', fontsize=9)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    spatial = f'{act.shape[1]}x{act.shape[2]}'\n",
    "    fig.suptitle(f'{layer_name} — {act.shape[0]} channels at {spatial}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show activation maps at each depth\n",
    "show_activation_grid(activations['conv1'], 'conv1 (edges, gradients)')\n",
    "show_activation_grid(activations['layer2'], 'layer2 (textures, patterns)')\n",
    "show_activation_grid(activations['layer4'], 'layer4 (abstract features)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret What You See\n",
    "\n",
    "Compare the three grids:\n",
    "\n",
    "- **conv1** (112x112): Sharp, spatially detailed. You can see the edges and boundaries of the original image. Different channels respond to different edge orientations.\n",
    "\n",
    "- **layer2** (28x28): Less spatially precise, more abstract. Channels respond to textures and local patterns rather than simple edges.\n",
    "\n",
    "- **layer4** (7x7): Abstract blobs. Individual channels are **not** recognizable as objects. The representation is distributed across all 512 channels — no single channel encodes \"dog\" or \"grass.\"\n",
    "\n",
    "This is the feature hierarchy in action: **spatial resolution shrinks, channel count grows, representations go from concrete to abstract**. Exactly what you were told — now confirmed by observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Grad-CAM\n",
    "\n",
    "**Question this answers:** *What in this image mattered for this prediction?*\n",
    "\n",
    "Activation maps are class-agnostic — they show the same thing regardless of which class the model predicts. Grad-CAM adds **class specificity** by using gradients to weight the activation maps.\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "1. Forward pass → capture last conv layer activations\n",
    "2. Backward pass → compute gradients of class score w.r.t. those activations  \n",
    "3. Global average pool the gradients → one weight per channel\n",
    "4. Weighted sum of activation maps → single spatial heatmap\n",
    "5. ReLU → keep only positive contributions\n",
    "6. Upsample and overlay on the input image\n",
    "\n",
    "### TODO: Implement Grad-CAM\n",
    "\n",
    "Fill in the TODO sections below. The scaffold handles hooks and display — you write the core computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(model, img_tensor, target_class=None):\n",
    "    \"\"\"Compute Grad-CAM for a given image and target class.\n",
    "    \n",
    "    Args:\n",
    "        model: pretrained model (in eval mode)\n",
    "        img_tensor: preprocessed image tensor [1, 3, 224, 224]\n",
    "        target_class: class index (int). If None, uses the predicted class.\n",
    "    \n",
    "    Returns:\n",
    "        cam: numpy array of shape [224, 224], values in [0, 1]\n",
    "        predicted_class: the class index used\n",
    "    \"\"\"\n",
    "    # Storage for activations and gradients\n",
    "    stored = {}\n",
    "    \n",
    "    def forward_hook(module, input, output):\n",
    "        stored['activations'] = output\n",
    "    \n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        stored['gradients'] = grad_output[0]\n",
    "    \n",
    "    # Register hooks on the last conv layer (layer4)\n",
    "    fhook = model.layer4.register_forward_hook(forward_hook)\n",
    "    bhook = model.layer4.register_full_backward_hook(backward_hook)\n",
    "    \n",
    "    # --- Step 1: Forward pass ---\n",
    "    output = model(img_tensor)\n",
    "    \n",
    "    # Determine target class\n",
    "    if target_class is None:\n",
    "        target_class = output.argmax(dim=1).item()\n",
    "    \n",
    "    # --- Step 2: Backward pass ---\n",
    "    # TODO: Zero gradients, then backward from the target class score\n",
    "    # Hint: output[0, target_class].backward()\n",
    "    model.zero_grad()\n",
    "    class_score = output[0, target_class]\n",
    "    class_score.backward()\n",
    "    \n",
    "    # --- Step 3: Global average pool the gradients ---\n",
    "    # TODO: Compute channel weights by averaging gradients over spatial dimensions\n",
    "    # stored['gradients'] has shape [1, 512, 7, 7]\n",
    "    # You want one weight per channel -> shape [1, 512]\n",
    "    # Hint: .mean(dim=[2, 3])\n",
    "    gradients = stored['gradients']  # [1, 512, 7, 7]\n",
    "    weights = gradients.mean(dim=[2, 3])  # [1, 512]\n",
    "    \n",
    "    # --- Step 4: Weighted sum of activation maps ---\n",
    "    # TODO: Multiply each activation map by its weight and sum across channels\n",
    "    # stored['activations'] has shape [1, 512, 7, 7]\n",
    "    # Result should be shape [1, 1, 7, 7]\n",
    "    # Hint: (weights.unsqueeze(-1).unsqueeze(-1) * activations).sum(dim=1, keepdim=True)\n",
    "    activations = stored['activations']  # [1, 512, 7, 7]\n",
    "    cam = (weights.unsqueeze(-1).unsqueeze(-1) * activations).sum(dim=1, keepdim=True)\n",
    "    \n",
    "    # --- Step 5: ReLU ---\n",
    "    # TODO: Apply ReLU to keep only positive contributions\n",
    "    cam = F.relu(cam)\n",
    "    \n",
    "    # --- Step 6: Upsample to input size ---\n",
    "    cam = F.interpolate(cam, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    cam = cam.squeeze().detach().cpu().numpy()\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    if cam.max() > 0:\n",
    "        cam = cam / cam.max()\n",
    "    \n",
    "    # Clean up hooks\n",
    "    fhook.remove()\n",
    "    bhook.remove()\n",
    "    \n",
    "    return cam, target_class\n",
    "\n",
    "print('grad_cam function defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Utility: Heatmap Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_grad_cam(img_pil, cam, class_name, ax=None):\n",
    "    \"\"\"Overlay Grad-CAM heatmap on the original image.\"\"\"\n",
    "    # Resize PIL image to 224x224 for overlay\n",
    "    img_resized = img_pil.copy()\n",
    "    img_resized = img_resized.resize((224, 224))\n",
    "    img_np = np.array(img_resized).astype(np.float32) / 255.0\n",
    "    \n",
    "    # Create heatmap\n",
    "    heatmap = plt.cm.jet(cam)[:, :, :3]  # RGB from colormap\n",
    "    \n",
    "    # Overlay: blend original image with heatmap\n",
    "    overlay = 0.5 * img_np + 0.5 * heatmap\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    \n",
    "    ax.imshow(overlay)\n",
    "    ax.set_title(f'Grad-CAM: {class_name}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "print('Display utility loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Grad-CAM on the Sample Image\n",
    "\n",
    "Apply Grad-CAM for the top predicted class. The heatmap should highlight the dog, not the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM for the top predicted class\n",
    "cam, pred_class = grad_cam(model, img_tensor)\n",
    "class_name = categories[pred_class]\n",
    "print(f'Predicted class: {class_name} (index {pred_class})')\n",
    "\n",
    "# Show the result\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(img_pil.resize((224, 224)))\n",
    "axes[0].set_title('Original', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "show_grad_cam(img_pil, cam, class_name, ax=axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Class-Specific Heatmaps\n",
    "\n",
    "Grad-CAM is class-specific. For the same image, compute heatmaps for the **top-2 predicted classes** and display them side by side. The heatmaps should focus on different regions.\n",
    "\n",
    "For example, if the top prediction is \"golden retriever\" and the second is \"tennis ball,\" the heatmaps should highlight different parts of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top-2 predictions\n",
    "top_preds = predict_top_k(img_tensor, k=2)\n",
    "print(f'Top-2: {top_preds}')\n",
    "\n",
    "# TODO: Compute Grad-CAM for each of the top-2 classes\n",
    "# Hint: use the target_class parameter of grad_cam()\n",
    "# You need to find the class index from the category name\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(img_pil.resize((224, 224)))\n",
    "axes[0].set_title('Original', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Grad-CAM for each top prediction\n",
    "for i, (label, prob) in enumerate(top_preds):\n",
    "    class_idx = categories.index(label)\n",
    "    cam_i, _ = grad_cam(model, img_tensor, target_class=class_idx)\n",
    "    show_grad_cam(img_pil, cam_i, f'{label} ({prob:.1%})', ax=axes[i + 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the Results\n",
    "\n",
    "Compare the two heatmaps. For different class labels, Grad-CAM highlights **different spatial regions** of the same image. This is because the gradients change — different class scores produce different gradient signals, which weight the activation maps differently.\n",
    "\n",
    "This is the key difference from activation maps: activation maps are the same regardless of the predicted class. Grad-CAM is class-specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Try Multiple Images\n",
    "\n",
    "Apply Grad-CAM to different images to build your intuition. Pay attention to where the model focuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few diverse images to try\n",
    "image_urls = {\n",
    "    'cat': 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg',\n",
    "    'car': 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/2019_Toyota_Corolla_Hybrid_1.8.jpg/1280px-2019_Toyota_Corolla_Hybrid_1.8.jpg',\n",
    "    'bird': 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Eopsaltria_australis_-_Mogo_Campground.jpg/1280px-Eopsaltria_australis_-_Mogo_Campground.jpg',\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(len(image_urls), 2, figsize=(12, 6 * len(image_urls)))\n",
    "\n",
    "for row, (name, url) in enumerate(image_urls.items()):\n",
    "    try:\n",
    "        pil_img, tensor_img = load_image_from_url(url)\n",
    "        cam_result, pred_idx = grad_cam(model, tensor_img)\n",
    "        pred_name = categories[pred_idx]\n",
    "        \n",
    "        axes[row, 0].imshow(pil_img.resize((224, 224)))\n",
    "        axes[row, 0].set_title(f'{name} (original)', fontsize=12)\n",
    "        axes[row, 0].axis('off')\n",
    "        \n",
    "        show_grad_cam(pil_img, cam_result, pred_name, ax=axes[row, 1])\n",
    "    except Exception as e:\n",
    "        print(f'Could not load {name}: {e}')\n",
    "        axes[row, 0].text(0.5, 0.5, f'Failed to load {name}', ha='center', va='center')\n",
    "        axes[row, 0].axis('off')\n",
    "        axes[row, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 5: Context vs Object — Where Does the Model Focus?\n\nThis is the most practically important part of the notebook.\n\nA classic failure mode called **shortcut learning**: a model achieves high accuracy by focusing on **spurious correlations** in the data rather than the actual object features. Grad-CAM reveals this.\n\n### The Husky vs Wolf Problem (Conceptual)\n\nIn a famous example, a model trained to classify \"wolf\" vs \"husky\" achieved 90% accuracy — but Grad-CAM revealed it was looking at the *background* (snow = husky, forest = wolf), not the animal. The training data happened to correlate background with label. The accuracy was real; the reasoning was broken.\n\nWe cannot reproduce that exact failure with a pretrained ImageNet model (it was trained on diverse data, not biased wolf/husky photos). But we **can** use Grad-CAM to ask an important question about any image: **is the model looking at the object, or at the context?** The answer might surprise you."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Two images where background context is prominent:\n# A husky by a lake, and a dog in a very busy scene\ntest_urls = {\n    'husky (with lake background)': 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Le%C3%AFka_au_bord_du_Lac_de_Laffrey.jpg/1280px-Le%C3%AFka_au_bord_du_Lac_de_Laffrey.jpg',\n    'bird (with foliage background)': 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Eopsaltria_australis_-_Mogo_Campground.jpg/1280px-Eopsaltria_australis_-_Mogo_Campground.jpg',\n}\n\nfor name, url in test_urls.items():\n    try:\n        pil_img, tensor_img = load_image_from_url(url)\n        \n        # Get predictions\n        preds = predict_top_k(tensor_img, k=5)\n        print(f'\\n{name}')\n        print('Top-5 predictions:')\n        for label, prob in preds:\n            print(f'  {label}: {prob:.1%}')\n        \n        # Grad-CAM for top prediction\n        cam_result, pred_idx = grad_cam(model, tensor_img)\n        \n        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n        axes[0].imshow(pil_img.resize((224, 224)))\n        axes[0].set_title(f'{name}', fontsize=12)\n        axes[0].axis('off')\n        show_grad_cam(pil_img, cam_result, categories[pred_idx], ax=axes[1])\n        plt.suptitle('Does the model focus on the animal or the background?', fontsize=14)\n        plt.tight_layout()\n        plt.show()\n    except Exception as e:\n        print(f'Could not load {name}: {e}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Key Takeaway\n\nLook carefully at where the heatmaps landed. Did the model always focus on the animal? Or did it sometimes attend to the background context (water, foliage, ground)?\n\nA pretrained ImageNet model is trained on diverse data, so it generally does a good job focusing on the object. But even here you may see some attention to context. For models trained on **smaller, biased datasets**, shortcut learning is far more likely — and far more dangerous.\n\n**Correct prediction does not mean correct reasoning.** Visualization is a debugging tool — use it to verify that your model learned what you intended, not a shortcut. In the next lesson, you will fine-tune your own model and use Grad-CAM to check what it actually learned."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stretch: Grad-CAM on a Wrong Prediction\n",
    "\n",
    "If you have extra time, try finding an image where the model is **wrong**. Run Grad-CAM and see if the heatmap reveals why the model made the mistake. This is the most realistic use case — debugging production failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (stretch): Find an image the model gets wrong and run Grad-CAM on it.\n",
    "# Try unusual angles, occluded objects, or ambiguous scenes.\n",
    "# Does the heatmap explain the mistake?\n",
    "\n",
    "# your_url = '...'\n",
    "# your_pil, your_tensor = load_image_from_url(your_url)\n",
    "# cam_result, pred_idx = grad_cam(model, your_tensor)\n",
    "# show_grad_cam(your_pil, cam_result, categories[pred_idx])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You now have three tools for understanding CNN behavior:\n",
    "\n",
    "| Technique | Question | Scope |\n",
    "|-----------|----------|-------|\n",
    "| **Filter Visualization** | What does this layer look for? | Model-level (same for all images) |\n",
    "| **Activation Maps** | What did this layer find in this image? | Input-specific, class-agnostic |\n",
    "| **Grad-CAM** | What in this image mattered for this prediction? | Input-specific AND class-specific |\n",
    "\n",
    "The feature hierarchy is real — you saw it. Visualization is a debugging tool — not just a pretty picture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}