{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tensors: NumPy → PyTorch\n\nThis notebook helps you build muscle memory with PyTorch tensors.\n\n**What you'll do:**\n1. Create your first PyTorch tensor and compare dtypes to NumPy — Guided\n2. Create training data as PyTorch tensors (translate your NumPy code) — Supported\n3. Check `.shape`, `.dtype`, and `.device` for each tensor — Guided\n4. Move your data to GPU and verify the device changed — Supported\n5. Compute the forward pass: `y_hat = X @ w + b` — Supported\n6. Reshape a batch of 28x28 images into 784-element vectors — Supported\n\n**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones — they reveal gaps in your mental model.\n\nRemember the **shape/dtype/device trinity** — every tensor has all three, and most bugs come from one of them being wrong."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Reproducibility\ntorch.manual_seed(42)\n\n# For nice plots\nplt.style.use('dark_background')\nplt.rcParams['figure.figsize'] = [10, 4]"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Exercise 1: Your First PyTorch Tensor (Guided)\n\nLet's start by creating tensors and seeing how PyTorch differs from NumPy.\n\n**Before running, predict:** What dtype will PyTorch choose for a list of Python integers `[1, 2, 3]`? What about a list of Python floats `[1.0, 2.0, 3.0]`?",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create tensors from Python lists\nint_tensor = torch.tensor([1, 2, 3])\nfloat_tensor = torch.tensor([1.0, 2.0, 3.0])\n\nprint(f'From ints:   value={int_tensor}, dtype={int_tensor.dtype}')\nprint(f'From floats: value={float_tensor}, dtype={float_tensor.dtype}')\nprint()\n\n# Compare to NumPy\nint_array = np.array([1, 2, 3])\nfloat_array = np.array([1.0, 2.0, 3.0])\n\nprint(f'NumPy ints:   dtype={int_array.dtype}')\nprint(f'NumPy floats: dtype={float_array.dtype}')\nprint()\nprint('Key difference: PyTorch defaults to float32, NumPy defaults to float64.')\nprint('float32 is the standard for deep learning — less memory, fast on GPUs.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 2: Create Training Data as PyTorch Tensors (Supported)\n\nYou know how to generate data with NumPy. Now translate it to PyTorch.\n\n**Task:** Create a simple linear regression dataset using PyTorch tensors.\n\n- 100 samples\n- `X` values between -3 and 3\n- True relationship: `y = 2.5 * X - 1.0 + noise`\n\nFill in the blanks below. The NumPy equivalent is shown in comments."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "# NumPy version:  X = np.random.uniform(-3, 3, (n_samples, 1))\n",
    "# PyTorch version: torch.rand gives values in [0, 1), so scale and shift\n",
    "X = ____  # FILL IN: 100x1 tensor with values in [-3, 3)\n",
    "\n",
    "# True parameters\n",
    "true_w = 2.5\n",
    "true_b = -1.0\n",
    "\n",
    "# NumPy version:  noise = np.random.normal(0, 0.5, (n_samples, 1))\n",
    "# PyTorch version: torch.randn gives standard normal, so scale it\n",
    "noise = ____  # FILL IN: 100x1 tensor of Gaussian noise with std=0.5\n",
    "\n",
    "# Compute y\n",
    "y = true_w * X + true_b + noise\n",
    "\n",
    "# Verify\n",
    "print(f'X shape: {X.shape}')  # Should be [100, 1]\n",
    "print(f'y shape: {y.shape}')  # Should be [100, 1]\n",
    "print(f'X range: [{X.min():.2f}, {X.max():.2f}]')\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X.numpy(), y.numpy(), alpha=0.6, s=20)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Training Data (PyTorch tensors → NumPy for plotting)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\nThe key insight is that PyTorch's random functions use different conventions than NumPy. `torch.rand` gives uniform [0, 1), so you scale and shift. `torch.randn` gives standard normal, so you multiply by the desired std.\n\n```python\nX = torch.rand(n_samples, 1) * 6 - 3      # [0, 1) -> [0, 6) -> [-3, 3)\nnoise = torch.randn(n_samples, 1) * 0.5    # standard normal * 0.5 = std of 0.5\n```\n\n**Key differences from NumPy:**\n- `torch.rand` = uniform [0, 1) vs `np.random.uniform(low, high, shape)`\n- `torch.randn` = standard normal vs `np.random.normal(mean, std, shape)`\n- Shape is passed as separate args `(100, 1)` not a tuple `((100, 1),)`\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 3: Check Shape, Dtype, and Device (Guided)\n\nEvery tensor has three fundamental properties: **shape**, **dtype**, and **device**. This is the trinity. When something goes wrong in deep learning, one of these three is usually the culprit.\n\n**Before running, predict:** For each tensor below, what will the shape, dtype, and device be? Write your predictions as comments in the code cell, then run to check."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a few different tensors\n",
    "a = torch.tensor([1, 2, 3])                        # From a Python list of ints\n",
    "b = torch.tensor([1.0, 2.0, 3.0])                  # From a Python list of floats\n",
    "c = torch.zeros(3, 4)                               # 3x4 zeros\n",
    "d = torch.tensor([[True, False], [False, True]])    # Boolean tensor\n",
    "\n",
    "# YOUR PREDICTIONS (fill these in BEFORE running):\n",
    "#   a -> shape: ____  dtype: ____  device: ____\n",
    "#   b -> shape: ____  dtype: ____  device: ____\n",
    "#   c -> shape: ____  dtype: ____  device: ____\n",
    "#   d -> shape: ____  dtype: ____  device: ____\n",
    "\n",
    "for name, tensor in [('a', a), ('b', b), ('c', c), ('d', d)]:\n",
    "    print(f'{name} -> shape: {str(tensor.shape):<16} dtype: {str(tensor.dtype):<16} device: {tensor.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\n```\na -> shape: torch.Size([3])    dtype: torch.int64    device: cpu\nb -> shape: torch.Size([3])    dtype: torch.float32  device: cpu\nc -> shape: torch.Size([3, 4]) dtype: torch.float32  device: cpu\nd -> shape: torch.Size([2, 2]) dtype: torch.bool     device: cpu\n```\n\n**Key insights:**\n- Python `int` list → `torch.int64`. Python `float` list → `torch.float32`.\n- `torch.zeros` defaults to `float32` — the standard dtype for neural network weights.\n- Everything starts on `cpu` unless you explicitly move it.\n- Boolean tensors exist and are useful for masking.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now inspect your training data from Exercise 2:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the trinity for your training data\n",
    "for name, tensor in [('X', X), ('y', y)]:\n",
    "    print(f'{name} -> shape: {str(tensor.shape):<16} dtype: {str(tensor.dtype):<16} device: {tensor.device}')\n",
    "\n",
    "# Quick quiz: why is X float32 and not float64?\n",
    "# (Hint: what does torch.rand return by default?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 4: Move Data to GPU (Supported)\n\nThe `.to(device)` pattern is how you move tensors between CPU and GPU. This is something you'll do at the start of every training script.\n\n**Task:** Move your training data to GPU (if available) and verify the device changed. Fill in the blanks below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Detect available device\n",
    "# This pattern works on any machine: GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Step 2: Move tensors to the device\n",
    "X_device = ____  # FILL IN: move X to `device`\n",
    "y_device = ____  # FILL IN: move y to `device`\n",
    "\n",
    "# Step 3: Verify\n",
    "print(f'X_device is on: {X_device.device}')\n",
    "print(f'y_device is on: {y_device.device}')\n",
    "\n",
    "# Step 4: Check that the original tensors are unchanged\n",
    "print(f'\\nOriginal X is still on: {X.device}')\n",
    "print(f'Original y is still on: {y.device}')\n",
    "print('\\n.to() returns a NEW tensor. It does not modify in place.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\nThe key concept is that `.to(device)` returns a new tensor — it does not modify the original in place.\n\n```python\nX_device = X.to(device)\ny_device = y.to(device)\n```\n\n**Key points:**\n- `.to(device)` returns a new tensor on the target device.\n- If you're already on the target device, `.to()` returns the same tensor (no copy).\n- A common mistake: forgetting to reassign. `X.to(device)` alone does nothing because `.to()` doesn't modify in place.\n- In training scripts, you'll see this pattern: `X, y = X.to(device), y.to(device)`\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Interop: The CPU Requirement\n",
    "\n",
    "One thing that trips people up: NumPy only works with CPU tensors. Try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works (CPU tensor)\n",
    "cpu_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f'CPU tensor to numpy: {cpu_tensor.numpy()}')\n",
    "\n",
    "# If you have a GPU tensor, you need .cpu() first\n",
    "gpu_tensor = cpu_tensor.to(device)\n",
    "try:\n",
    "    gpu_tensor.numpy()  # This will fail if device is cuda\n",
    "    print(f'GPU tensor to numpy: worked (you are on CPU, so .to(device) was a no-op)')\n",
    "except RuntimeError as e:\n",
    "    print(f'GPU tensor to numpy: FAILED \\u2014 {e}')\n",
    "    print(f'Fix: gpu_tensor.cpu().numpy() = {gpu_tensor.cpu().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 5: Compute the Forward Pass (Supported)\n\nA linear model's forward pass is: **`y_hat = X @ w + b`**\n\nThe `@` operator is matrix multiplication. This single line is the entire prediction step for linear regression.\n\n**Task:**\n1. Create a weight vector `w` of shape `(1, 1)` and a bias `b` of shape `(1,)`\n2. Compute `y_hat = X @ w + b` using your training data\n3. Verify that `y_hat` has the same shape as `y`\n4. Plot the predictions against the true values\n\n**Hints:**\n- Use `torch.randn` for random initialization\n- X is `(100, 1)`, so w needs to be `(1, 1)` for the matrix multiply to work\n- Think about what shapes result from `(100, 1) @ (1, 1) + (1,)`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize w and b with random values\n",
    "# w = ...\n",
    "# b = ...\n",
    "\n",
    "# TODO: Compute the forward pass\n",
    "# y_hat = ...\n",
    "\n",
    "# TODO: Verify shapes match\n",
    "# print(f'X shape: {X.shape}')\n",
    "# print(f'w shape: {w.shape}')\n",
    "# print(f'y_hat shape: {y_hat.shape}')\n",
    "# print(f'y shape: {y.shape}')\n",
    "\n",
    "# TODO: Plot predictions vs true values\n",
    "# Sort X for a clean line plot\n",
    "# sorted_idx = X.squeeze().argsort()\n",
    "# plt.scatter(X.numpy(), y.numpy(), alpha=0.4, s=20, label='True data')\n",
    "# plt.plot(X[sorted_idx].numpy(), y_hat[sorted_idx].detach().numpy(), 'r-', linewidth=2, label='Predictions (random w, b)')\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('y')\n",
    "# plt.legend()\n",
    "# plt.title('Forward Pass with Random Parameters')\n",
    "# plt.grid(alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\nThe key insight is shape arithmetic: `(100, 1) @ (1, 1)` gives `(100, 1)`, and adding `(1,)` broadcasts to `(100, 1)`.\n\n```python\n# Initialize random parameters\nw = torch.randn(1, 1)\nb = torch.randn(1)\n\n# Forward pass\ny_hat = X @ w + b\n\n# Verify shapes\nprint(f'X shape: {X.shape}')         # [100, 1]\nprint(f'w shape: {w.shape}')         # [1, 1]\nprint(f'y_hat shape: {y_hat.shape}') # [100, 1]\nprint(f'y shape: {y.shape}')         # [100, 1]\n\n# Plot\nsorted_idx = X.squeeze().argsort()\nplt.scatter(X.numpy(), y.numpy(), alpha=0.4, s=20, label='True data')\nplt.plot(X[sorted_idx].numpy(), y_hat[sorted_idx].detach().numpy(), 'r-', linewidth=2, label='Predictions (random w, b)')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.title('Forward Pass with Random Parameters')\nplt.grid(alpha=0.3)\nplt.show()\n```\n\n**Why the line is wrong:** `w` and `b` are random, so the predictions are garbage. Training adjusts these parameters to minimize the error. But the *shape* is correct, and that's what matters here.\n\n**Shape arithmetic:** `(100, 1) @ (1, 1)` = `(100, 1)`, then `+ (1,)` broadcasts to `(100, 1)`. Broadcasting is PyTorch automatically expanding the `(1,)` bias to match.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercise 6: Reshape a Batch of Images (Supported)\n\nIn image classification, you often need to flatten 2D images into 1D vectors. For example, MNIST images are 28x28 pixels, and a dense (fully-connected) layer expects a flat vector of 784 values.\n\n**Task:**\n1. Create a fake batch of 16 grayscale 28x28 images\n2. Reshape the batch from `(16, 1, 28, 28)` to `(16, 784)` using two different methods\n3. Verify the total number of elements didn't change\n\n**Hints:**\n- `tensor.view(new_shape)` reshapes without copying data\n- `tensor.reshape(new_shape)` also works (copies if needed)\n- You can use `-1` in one dimension to let PyTorch infer the size\n- Think about what `(16, 1, 28, 28)` means: 16 images, 1 channel, 28 height, 28 width"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fake batch of images: 16 images, 1 channel (grayscale), 28x28 pixels\n",
    "images = torch.randn(16, 1, 28, 28)\n",
    "print(f'Original shape: {images.shape}')\n",
    "print(f'Total elements: {images.numel()}')\n",
    "\n",
    "# TODO: Method 1 - Use .view() to flatten each image to 784 elements\n",
    "# flat_v = ...\n",
    "# print(f'After view:    {flat_v.shape}')   # Should be [16, 784]\n",
    "\n",
    "# TODO: Method 2 - Use .reshape() to do the same thing\n",
    "# flat_r = ...\n",
    "# print(f'After reshape: {flat_r.shape}')   # Should be [16, 784]\n",
    "\n",
    "# TODO: Verify nothing was lost\n",
    "# print(f'Elements preserved: {flat_v.numel() == images.numel()}')\n",
    "\n",
    "# Bonus: visualize what flattening does to one image\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "# axes[0].imshow(images[0, 0].numpy(), cmap='gray')\n",
    "# axes[0].set_title(f'2D image ({images[0, 0].shape})')\n",
    "# axes[1].plot(flat_v[0].numpy(), linewidth=0.5)\n",
    "# axes[1].set_title(f'Flattened vector ({flat_v[0].shape})')\n",
    "# axes[1].set_xlabel('Index')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\nThe key insight is the `-1` trick: PyTorch computes the missing dimension automatically (`total elements / 16 = 12544 / 16 = 784`).\n\n```python\n# Method 1: .view()\nflat_v = images.view(16, -1)   # -1 means \"infer this dimension\" -> 1*28*28 = 784\nprint(f'After view:    {flat_v.shape}')   # [16, 784]\n\n# Method 2: .reshape()\nflat_r = images.reshape(16, -1)\nprint(f'After reshape: {flat_r.shape}')   # [16, 784]\n\n# Verify nothing was lost\nprint(f'Elements preserved: {flat_v.numel() == images.numel()}')\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(10, 3))\naxes[0].imshow(images[0, 0].numpy(), cmap='gray')\naxes[0].set_title(f'2D image ({images[0, 0].shape})')\naxes[1].plot(flat_v[0].numpy(), linewidth=0.5)\naxes[1].set_title(f'Flattened vector ({flat_v[0].shape})')\naxes[1].set_xlabel('Index')\nplt.tight_layout()\nplt.show()\n```\n\n**`.view()` vs `.reshape()`:**\n- `.view()` requires the tensor to be contiguous in memory. It never copies data.\n- `.reshape()` works on any tensor. It returns a view if possible, copies if not.\n- In practice, `.view()` is more common in PyTorch code because it's explicit about not copying.\n\n**The `-1` trick:** Using `.view(16, -1)` is safer than hardcoding 784 because it adapts to different image sizes.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **The shape/dtype/device trinity**: Every tensor has all three. Most bugs come from one being wrong.\n",
    "- **NumPy interop**: `torch.from_numpy()` and `.numpy()` convert between the two. GPU tensors need `.cpu()` first.\n",
    "- **GPU transfer**: `.to(device)` returns a new tensor. It does not modify in place.\n",
    "- **Matrix multiply with `@`**: The forward pass `y_hat = X @ w + b` is the foundation of every neural network.\n",
    "- **Reshaping**: `.view()` and `.reshape()` rearrange dimensions without changing the data. Use `-1` to infer a dimension.\n",
    "- **PyTorch defaults**: `float32` for most operations, `cpu` for device. Both are intentional choices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}