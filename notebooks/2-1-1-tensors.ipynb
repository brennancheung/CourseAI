{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors: NumPy \u2192 PyTorch\n",
    "\n",
    "This notebook helps you build muscle memory with PyTorch tensors. For each exercise, **predict the output before running the cell.**\n",
    "\n",
    "**Exercises:**\n",
    "1. Create training data as PyTorch tensors (translate your NumPy code) \u2014 GUIDED\n",
    "2. Check `.shape`, `.dtype`, and `.device` for each tensor \u2014 GUIDED\n",
    "3. Move your data to GPU and verify the device changed \u2014 GUIDED\n",
    "4. Compute the forward pass: `y_hat = X @ w + b` \u2014 SUPPORTED\n",
    "5. Reshape a batch of 28\u00d728 images into 784-element vectors \u2014 SUPPORTED\n",
    "\n",
    "Remember the **shape/dtype/device trinity** \u2014 every tensor has all three, and most bugs come from one of them being wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Create Training Data as PyTorch Tensors (GUIDED)\n",
    "\n",
    "You know how to generate data with NumPy. Now translate it to PyTorch.\n",
    "\n",
    "**Task:** Create a simple linear regression dataset using PyTorch tensors.\n",
    "\n",
    "- 100 samples\n",
    "- `X` values between -3 and 3\n",
    "- True relationship: `y = 2.5 * X - 1.0 + noise`\n",
    "\n",
    "Fill in the blanks below. The NumPy equivalent is shown in comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "# NumPy version:  X = np.random.uniform(-3, 3, (n_samples, 1))\n",
    "# PyTorch version: torch.rand gives values in [0, 1), so scale and shift\n",
    "X = ____  # FILL IN: 100x1 tensor with values in [-3, 3)\n",
    "\n",
    "# True parameters\n",
    "true_w = 2.5\n",
    "true_b = -1.0\n",
    "\n",
    "# NumPy version:  noise = np.random.normal(0, 0.5, (n_samples, 1))\n",
    "# PyTorch version: torch.randn gives standard normal, so scale it\n",
    "noise = ____  # FILL IN: 100x1 tensor of Gaussian noise with std=0.5\n",
    "\n",
    "# Compute y\n",
    "y = true_w * X + true_b + noise\n",
    "\n",
    "# Verify\n",
    "print(f'X shape: {X.shape}')  # Should be [100, 1]\n",
    "print(f'y shape: {y.shape}')  # Should be [100, 1]\n",
    "print(f'X range: [{X.min():.2f}, {X.max():.2f}]')\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X.numpy(), y.numpy(), alpha=0.6, s=20)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Training Data (PyTorch tensors \u2192 NumPy for plotting)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "X = torch.rand(n_samples, 1) * 6 - 3      # [0, 1) -> [0, 6) -> [-3, 3)\n",
    "noise = torch.randn(n_samples, 1) * 0.5    # standard normal * 0.5 = std of 0.5\n",
    "```\n",
    "\n",
    "**Key differences from NumPy:**\n",
    "- `torch.rand` = uniform [0, 1) &nbsp;&nbsp;vs&nbsp;&nbsp; `np.random.uniform(low, high, shape)`\n",
    "- `torch.randn` = standard normal &nbsp;&nbsp;vs&nbsp;&nbsp; `np.random.normal(mean, std, shape)`\n",
    "- Shape is passed as separate args `(100, 1)` not a tuple `((100, 1),)`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Check Shape, Dtype, and Device (GUIDED)\n",
    "\n",
    "Every tensor has three fundamental properties: **shape**, **dtype**, and **device**. This is the trinity. When something goes wrong in deep learning, one of these three is usually the culprit.\n",
    "\n",
    "**Task:** Before running the cell below, predict the output for each tensor. Write your predictions as comments, then run the cell to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a few different tensors\n",
    "a = torch.tensor([1, 2, 3])                        # From a Python list of ints\n",
    "b = torch.tensor([1.0, 2.0, 3.0])                  # From a Python list of floats\n",
    "c = torch.zeros(3, 4)                               # 3x4 zeros\n",
    "d = torch.tensor([[True, False], [False, True]])    # Boolean tensor\n",
    "\n",
    "# YOUR PREDICTIONS (fill these in BEFORE running):\n",
    "#   a -> shape: ____  dtype: ____  device: ____\n",
    "#   b -> shape: ____  dtype: ____  device: ____\n",
    "#   c -> shape: ____  dtype: ____  device: ____\n",
    "#   d -> shape: ____  dtype: ____  device: ____\n",
    "\n",
    "for name, tensor in [('a', a), ('b', b), ('c', c), ('d', d)]:\n",
    "    print(f'{name} -> shape: {str(tensor.shape):<16} dtype: {str(tensor.dtype):<16} device: {tensor.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```\n",
    "a -> shape: torch.Size([3])    dtype: torch.int64    device: cpu\n",
    "b -> shape: torch.Size([3])    dtype: torch.float32  device: cpu\n",
    "c -> shape: torch.Size([3, 4]) dtype: torch.float32  device: cpu\n",
    "d -> shape: torch.Size([2, 2]) dtype: torch.bool     device: cpu\n",
    "```\n",
    "\n",
    "**Key insights:**\n",
    "- Python `int` list \u2192 `torch.int64`. Python `float` list \u2192 `torch.float32`.\n",
    "- `torch.zeros` defaults to `float32` \u2014 the standard dtype for neural network weights.\n",
    "- Everything starts on `cpu` unless you explicitly move it.\n",
    "- Boolean tensors exist and are useful for masking.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now inspect your training data from Exercise 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the trinity for your training data\n",
    "for name, tensor in [('X', X), ('y', y)]:\n",
    "    print(f'{name} -> shape: {str(tensor.shape):<16} dtype: {str(tensor.dtype):<16} device: {tensor.device}')\n",
    "\n",
    "# Quick quiz: why is X float32 and not float64?\n",
    "# (Hint: what does torch.rand return by default?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Move Data to GPU (GUIDED)\n",
    "\n",
    "The `.to(device)` pattern is how you move tensors between CPU and GPU. This is something you'll do at the start of every training script.\n",
    "\n",
    "**Task:** Move your training data to GPU (if available) and verify the device changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Detect available device\n",
    "# This pattern works on any machine: GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Step 2: Move tensors to the device\n",
    "X_device = ____  # FILL IN: move X to `device`\n",
    "y_device = ____  # FILL IN: move y to `device`\n",
    "\n",
    "# Step 3: Verify\n",
    "print(f'X_device is on: {X_device.device}')\n",
    "print(f'y_device is on: {y_device.device}')\n",
    "\n",
    "# Step 4: Check that the original tensors are unchanged\n",
    "print(f'\\nOriginal X is still on: {X.device}')\n",
    "print(f'Original y is still on: {y.device}')\n",
    "print('\\n.to() returns a NEW tensor. It does not modify in place.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "X_device = X.to(device)\n",
    "y_device = y.to(device)\n",
    "```\n",
    "\n",
    "**Key points:**\n",
    "- `.to(device)` returns a new tensor on the target device.\n",
    "- If you're already on the target device, `.to()` returns the same tensor (no copy).\n",
    "- A common mistake: forgetting to reassign. `X.to(device)` alone does nothing because `.to()` doesn't modify in place.\n",
    "- In training scripts, you'll see this pattern: `X, y = X.to(device), y.to(device)`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Interop: The CPU Requirement\n",
    "\n",
    "One thing that trips people up: NumPy only works with CPU tensors. Try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works (CPU tensor)\n",
    "cpu_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f'CPU tensor to numpy: {cpu_tensor.numpy()}')\n",
    "\n",
    "# If you have a GPU tensor, you need .cpu() first\n",
    "gpu_tensor = cpu_tensor.to(device)\n",
    "try:\n",
    "    gpu_tensor.numpy()  # This will fail if device is cuda\n",
    "    print(f'GPU tensor to numpy: worked (you are on CPU, so .to(device) was a no-op)')\n",
    "except RuntimeError as e:\n",
    "    print(f'GPU tensor to numpy: FAILED \\u2014 {e}')\n",
    "    print(f'Fix: gpu_tensor.cpu().numpy() = {gpu_tensor.cpu().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Compute the Forward Pass (SUPPORTED)\n",
    "\n",
    "A linear model's forward pass is: **`y_hat = X @ w + b`**\n",
    "\n",
    "The `@` operator is matrix multiplication. This single line is the entire prediction step for linear regression.\n",
    "\n",
    "**Task:**\n",
    "1. Create a weight vector `w` of shape `(1, 1)` and a bias `b` of shape `(1,)`\n",
    "2. Compute `y_hat = X @ w + b` using your training data\n",
    "3. Verify that `y_hat` has the same shape as `y`\n",
    "4. Plot the predictions against the true values\n",
    "\n",
    "**Hints:**\n",
    "- Use `torch.randn` for random initialization\n",
    "- X is `(100, 1)`, so w needs to be `(1, 1)` for the matrix multiply to work\n",
    "- Think about what shapes result from `(100, 1) @ (1, 1) + (1,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize w and b with random values\n",
    "# w = ...\n",
    "# b = ...\n",
    "\n",
    "# TODO: Compute the forward pass\n",
    "# y_hat = ...\n",
    "\n",
    "# TODO: Verify shapes match\n",
    "# print(f'X shape: {X.shape}')\n",
    "# print(f'w shape: {w.shape}')\n",
    "# print(f'y_hat shape: {y_hat.shape}')\n",
    "# print(f'y shape: {y.shape}')\n",
    "\n",
    "# TODO: Plot predictions vs true values\n",
    "# Sort X for a clean line plot\n",
    "# sorted_idx = X.squeeze().argsort()\n",
    "# plt.scatter(X.numpy(), y.numpy(), alpha=0.4, s=20, label='True data')\n",
    "# plt.plot(X[sorted_idx].numpy(), y_hat[sorted_idx].detach().numpy(), 'r-', linewidth=2, label='Predictions (random w, b)')\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('y')\n",
    "# plt.legend()\n",
    "# plt.title('Forward Pass with Random Parameters')\n",
    "# plt.grid(alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Initialize random parameters\n",
    "w = torch.randn(1, 1)\n",
    "b = torch.randn(1)\n",
    "\n",
    "# Forward pass\n",
    "y_hat = X @ w + b\n",
    "\n",
    "# Verify shapes\n",
    "print(f'X shape: {X.shape}')         # [100, 1]\n",
    "print(f'w shape: {w.shape}')         # [1, 1]\n",
    "print(f'y_hat shape: {y_hat.shape}') # [100, 1]\n",
    "print(f'y shape: {y.shape}')         # [100, 1]\n",
    "\n",
    "# Plot\n",
    "sorted_idx = X.squeeze().argsort()\n",
    "plt.scatter(X.numpy(), y.numpy(), alpha=0.4, s=20, label='True data')\n",
    "plt.plot(X[sorted_idx].numpy(), y_hat[sorted_idx].detach().numpy(), 'r-', linewidth=2, label='Predictions (random w, b)')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Forward Pass with Random Parameters')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Why the line is wrong:** `w` and `b` are random, so the predictions are garbage. Training adjusts these parameters to minimize the error. But the *shape* is correct, and that's what matters here.\n",
    "\n",
    "**Shape arithmetic:** `(100, 1) @ (1, 1)` = `(100, 1)`, then `+ (1,)` broadcasts to `(100, 1)`. Broadcasting is PyTorch automatically expanding the `(1,)` bias to match.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Reshape a Batch of Images (SUPPORTED)\n",
    "\n",
    "In image classification, you often need to flatten 2D images into 1D vectors. For example, MNIST images are 28\u00d728 pixels, and a dense (fully-connected) layer expects a flat vector of 784 values.\n",
    "\n",
    "**Task:**\n",
    "1. Create a fake batch of 16 grayscale 28\u00d728 images\n",
    "2. Reshape the batch from `(16, 1, 28, 28)` to `(16, 784)` using two different methods\n",
    "3. Verify the total number of elements didn't change\n",
    "\n",
    "**Hints:**\n",
    "- `tensor.view(new_shape)` reshapes without copying data\n",
    "- `tensor.reshape(new_shape)` also works (copies if needed)\n",
    "- You can use `-1` in one dimension to let PyTorch infer the size\n",
    "- Think about what `(16, 1, 28, 28)` means: 16 images, 1 channel, 28 height, 28 width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fake batch of images: 16 images, 1 channel (grayscale), 28x28 pixels\n",
    "images = torch.randn(16, 1, 28, 28)\n",
    "print(f'Original shape: {images.shape}')\n",
    "print(f'Total elements: {images.numel()}')\n",
    "\n",
    "# TODO: Method 1 - Use .view() to flatten each image to 784 elements\n",
    "# flat_v = ...\n",
    "# print(f'After view:    {flat_v.shape}')   # Should be [16, 784]\n",
    "\n",
    "# TODO: Method 2 - Use .reshape() to do the same thing\n",
    "# flat_r = ...\n",
    "# print(f'After reshape: {flat_r.shape}')   # Should be [16, 784]\n",
    "\n",
    "# TODO: Verify nothing was lost\n",
    "# print(f'Elements preserved: {flat_v.numel() == images.numel()}')\n",
    "\n",
    "# Bonus: visualize what flattening does to one image\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "# axes[0].imshow(images[0, 0].numpy(), cmap='gray')\n",
    "# axes[0].set_title(f'2D image ({images[0, 0].shape})')\n",
    "# axes[1].plot(flat_v[0].numpy(), linewidth=0.5)\n",
    "# axes[1].set_title(f'Flattened vector ({flat_v[0].shape})')\n",
    "# axes[1].set_xlabel('Index')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (click to expand)</summary>\n",
    "\n",
    "```python\n",
    "# Method 1: .view()\n",
    "flat_v = images.view(16, -1)   # -1 means \"infer this dimension\" -> 1*28*28 = 784\n",
    "print(f'After view:    {flat_v.shape}')   # [16, 784]\n",
    "\n",
    "# Method 2: .reshape()\n",
    "flat_r = images.reshape(16, -1)\n",
    "print(f'After reshape: {flat_r.shape}')   # [16, 784]\n",
    "\n",
    "# Verify nothing was lost\n",
    "print(f'Elements preserved: {flat_v.numel() == images.numel()}')\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "axes[0].imshow(images[0, 0].numpy(), cmap='gray')\n",
    "axes[0].set_title(f'2D image ({images[0, 0].shape})')\n",
    "axes[1].plot(flat_v[0].numpy(), linewidth=0.5)\n",
    "axes[1].set_title(f'Flattened vector ({flat_v[0].shape})')\n",
    "axes[1].set_xlabel('Index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**`.view()` vs `.reshape()`:**\n",
    "- `.view()` requires the tensor to be contiguous in memory. It never copies data.\n",
    "- `.reshape()` works on any tensor. It returns a view if possible, copies if not.\n",
    "- In practice, `.view()` is more common in PyTorch code because it's explicit about not copying.\n",
    "\n",
    "**The `-1` trick:** When you write `.view(16, -1)`, PyTorch computes the missing dimension: `total elements / 16 = 12544 / 16 = 784`. This is safer than hardcoding 784 because it adapts to different image sizes.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **The shape/dtype/device trinity**: Every tensor has all three. Most bugs come from one being wrong.\n",
    "- **NumPy interop**: `torch.from_numpy()` and `.numpy()` convert between the two. GPU tensors need `.cpu()` first.\n",
    "- **GPU transfer**: `.to(device)` returns a new tensor. It does not modify in place.\n",
    "- **Matrix multiply with `@`**: The forward pass `y_hat = X @ w + b` is the foundation of every neural network.\n",
    "- **Reshaping**: `.view()` and `.reshape()` rearrange dimensions without changing the data. Use `-1` to infer a dimension.\n",
    "- **PyTorch defaults**: `float32` for most operations, `cpu` for device. Both are intentional choices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}