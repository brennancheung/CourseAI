{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Long Context & Efficient Attention\n\nIn this notebook, you'll build and explore the three architectural innovations that enable transformers to scale from 1K to 100K+ token contexts. No pretrained models or GPUs needed—everything runs in seconds on CPU with toy-scale tensors.\n\n**What you'll do:**\n- Build the RoPE rotation matrix for a 2D subspace, apply it to Q/K pairs at different positions, and verify that the dot product depends on relative position—not absolute position\n- Build full causal and sliding window attention masks, visualize them as heatmaps, and compare the number of computed attention scores\n- Implement a Grouped Query Attention (GQA) forward pass with `n_heads=8` and `n_kv_heads=2`, and compare KV cache sizes for MHA vs GQA\n- Build an attention cost calculator: compute total FLOPs and KV cache memory for different model configurations (GPT-2, LLaMA 2 70B with MHA vs GQA, with/without sliding window) and see how the three optimizations compound\n\n**For each exercise, PREDICT the output before running the cell.** Wrong predictions are more valuable than correct ones—they reveal gaps in your mental model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup—self-contained for Google Colab\n",
    "# No extra pip installs needed—torch and matplotlib are in Colab by default.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Reproducible results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Implement RoPE Rotation (Guided)\n",
    "\n",
    "RoPE encodes position in the Q/K *dot product*, not the embedding. The mechanism: rotate Q and K vectors by an angle proportional to their position. When you compute `q_i^T k_j`, the rotation causes the result to depend on the *relative* position `(i - j)`, not the absolute positions `i` and `j`.\n",
    "\n",
    "In this exercise, you'll:\n",
    "1. Build the 2D rotation matrix `R(theta)`—the fundamental building block of RoPE\n",
    "2. Rotate Q and K vectors at different absolute positions but the same relative distance\n",
    "3. Compute dot products and verify the relative position property\n",
    "4. Visualize the rotations in the 2D plane for several relative distances\n",
    "\n",
    "**Before running, predict:**\n",
    "- If `q` is at position 3 and `k` is at position 7 (relative distance 4), and then we shift both to positions 1003 and 1007 (still relative distance 4), will the dot product `q_rot^T k_rot` change?\n",
    "- What about if we keep `q` at position 3 but move `k` to position 8 (relative distance 5)? Will the dot product be the same as before or different?\n",
    "- The rotation angle is `position * theta`. If `theta = 0.5`, what angle is applied at position 3? At position 1003?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- The 2D rotation matrix: the building block of RoPE ---\n",
    "#\n",
    "# RoPE pairs consecutive dimensions of Q and K vectors into 2D subspaces.\n",
    "# Each 2D subspace gets its own rotation at a different frequency.\n",
    "# Here we start with a single 2D subspace to build intuition.\n",
    "\n",
    "def rotation_matrix_2d(angle: float) -> torch.Tensor:\n",
    "    \"\"\"Build a 2x2 rotation matrix for the given angle (radians).\n",
    "    \n",
    "    R(angle) = [[cos(angle), -sin(angle)],\n",
    "                [sin(angle),  cos(angle)]]\n",
    "    \"\"\"\n",
    "    c = math.cos(angle)\n",
    "    s = math.sin(angle)\n",
    "    return torch.tensor([[c, -s], [s, c]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "def rotate_vector(v: torch.Tensor, position: int, theta: float) -> torch.Tensor:\n",
    "    \"\"\"Rotate a 2D vector by angle = position * theta.\n",
    "    \n",
    "    This is what RoPE does to each 2D subspace of Q and K:\n",
    "    the rotation angle is proportional to the token's position.\n",
    "    \"\"\"\n",
    "    angle = position * theta\n",
    "    R = rotation_matrix_2d(angle)\n",
    "    return R @ v\n",
    "\n",
    "\n",
    "# Choose a base frequency (one \"hand\" of the clock)\n",
    "theta = 0.5  # radians per position\n",
    "\n",
    "# Create a query vector and a key vector (2D for visualization)\n",
    "q = torch.tensor([1.0, 0.3])  # some arbitrary q vector\n",
    "k = torch.tensor([0.8, 0.6])  # some arbitrary k vector\n",
    "\n",
    "print(f'Original q: {q.tolist()}')\n",
    "print(f'Original k: {k.tolist()}')\n",
    "print(f'theta (frequency): {theta} radians/position')\n",
    "print()\n",
    "\n",
    "# --- Test 1: Same relative distance, different absolute positions ---\n",
    "# Pair A: positions 3 and 7 (relative distance = 4)\n",
    "q_rot_A = rotate_vector(q, position=3, theta=theta)\n",
    "k_rot_A = rotate_vector(k, position=7, theta=theta)\n",
    "dot_A = torch.dot(q_rot_A, k_rot_A).item()\n",
    "\n",
    "# Pair B: positions 1003 and 1007 (relative distance = 4)\n",
    "q_rot_B = rotate_vector(q, position=1003, theta=theta)\n",
    "k_rot_B = rotate_vector(k, position=1007, theta=theta)\n",
    "dot_B = torch.dot(q_rot_B, k_rot_B).item()\n",
    "\n",
    "# Pair C: positions 50000 and 50004 (relative distance = 4)\n",
    "q_rot_C = rotate_vector(q, position=50000, theta=theta)\n",
    "k_rot_C = rotate_vector(k, position=50004, theta=theta)\n",
    "dot_C = torch.dot(q_rot_C, k_rot_C).item()\n",
    "\n",
    "print('=== Same Relative Distance (4), Different Absolute Positions ===')\n",
    "print(f'  Positions  3 & 7     -> dot product = {dot_A:.6f}')\n",
    "print(f'  Positions 1003 & 1007 -> dot product = {dot_B:.6f}')\n",
    "print(f'  Positions 50000 & 50004 -> dot product = {dot_C:.6f}')\n",
    "print(f'  All equal? {abs(dot_A - dot_B) < 1e-5 and abs(dot_A - dot_C) < 1e-5}')\n",
    "print()\n",
    "\n",
    "# --- Test 2: Different relative distance ---\n",
    "# Pair D: positions 3 and 8 (relative distance = 5)\n",
    "q_rot_D = rotate_vector(q, position=3, theta=theta)\n",
    "k_rot_D = rotate_vector(k, position=8, theta=theta)\n",
    "dot_D = torch.dot(q_rot_D, k_rot_D).item()\n",
    "\n",
    "# Pair E: positions 3 and 3 (relative distance = 0)\n",
    "q_rot_E = rotate_vector(q, position=3, theta=theta)\n",
    "k_rot_E = rotate_vector(k, position=3, theta=theta)\n",
    "dot_E = torch.dot(q_rot_E, k_rot_E).item()\n",
    "\n",
    "print('=== Different Relative Distances ===')\n",
    "print(f'  Relative distance 4 -> dot product = {dot_A:.6f}')\n",
    "print(f'  Relative distance 5 -> dot product = {dot_D:.6f}')\n",
    "print(f'  Relative distance 0 -> dot product = {dot_E:.6f}')\n",
    "print(f'  Distance 0 == unrotated dot(q, k)? {abs(dot_E - torch.dot(q, k).item()) < 1e-5}')\n",
    "print()\n",
    "\n",
    "# --- Why does this work? ---\n",
    "# The rotation for q at position i is R(i*theta).\n",
    "# The rotation for k at position j is R(j*theta).\n",
    "# The dot product: rotate(q, i)^T @ rotate(k, j)\n",
    "#   = q^T @ R(i*theta)^T @ R(j*theta) @ k\n",
    "#   = q^T @ R((j-i)*theta) @ k         (rotation matrices compose)\n",
    "# The result depends on (j - i), not on i or j individually.\n",
    "print('=== The Math ===')\n",
    "print('rotate(q, i)^T @ rotate(k, j) = q^T @ R((j-i)*theta) @ k')\n",
    "print('The dot product depends on RELATIVE distance (j - i), not absolute positions.')\n",
    "print(f'  R(4 * {theta}) is the same whether positions are (3,7) or (1003,1007) or (50000,50004).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize: RoPE rotations in the 2D plane ---\n",
    "# Show how the same q and k vectors look at different positions,\n",
    "# and how the angular DIFFERENCE stays constant for the same relative distance.\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "q_color = '#a78bfa'  # violet\n",
    "k_color = '#f59e0b'  # amber\n",
    "angle_color = '#34d399'  # emerald\n",
    "\n",
    "pairs = [\n",
    "    (3, 7, 'Positions 3 & 7\\n(rel. distance = 4)'),\n",
    "    (1003, 1007, 'Positions 1003 & 1007\\n(rel. distance = 4)'),\n",
    "    (3, 8, 'Positions 3 & 8\\n(rel. distance = 5)'),\n",
    "]\n",
    "\n",
    "for ax, (pos_q, pos_k, title) in zip(axes, pairs):\n",
    "    q_r = rotate_vector(q, pos_q, theta)\n",
    "    k_r = rotate_vector(k, pos_k, theta)\n",
    "    dot_val = torch.dot(q_r, k_r).item()\n",
    "    \n",
    "    # Draw unit circle for reference\n",
    "    circle_angles = np.linspace(0, 2*np.pi, 100)\n",
    "    radius = max(q.norm().item(), k.norm().item()) * 1.1\n",
    "    ax.plot(radius * np.cos(circle_angles), radius * np.sin(circle_angles),\n",
    "            color='#334155', linewidth=0.5, linestyle='--')\n",
    "    \n",
    "    # Draw rotated q vector\n",
    "    ax.annotate('', xy=(q_r[0].item(), q_r[1].item()), xytext=(0, 0),\n",
    "                arrowprops=dict(arrowstyle='->', color=q_color, linewidth=2))\n",
    "    ax.text(q_r[0].item() * 1.15, q_r[1].item() * 1.15,\n",
    "            f'q (pos {pos_q})', color=q_color, fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Draw rotated k vector\n",
    "    ax.annotate('', xy=(k_r[0].item(), k_r[1].item()), xytext=(0, 0),\n",
    "                arrowprops=dict(arrowstyle='->', color=k_color, linewidth=2))\n",
    "    ax.text(k_r[0].item() * 1.15, k_r[1].item() * 1.15,\n",
    "            f'k (pos {pos_k})', color=k_color, fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Draw angle arc between them\n",
    "    q_angle = math.atan2(q_r[1].item(), q_r[0].item())\n",
    "    k_angle = math.atan2(k_r[1].item(), k_r[0].item())\n",
    "    arc_r = radius * 0.35\n",
    "    arc_angles = np.linspace(min(q_angle, k_angle), max(q_angle, k_angle), 50)\n",
    "    ax.plot(arc_r * np.cos(arc_angles), arc_r * np.sin(arc_angles),\n",
    "            color=angle_color, linewidth=2, linestyle='--')\n",
    "    \n",
    "    ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "    ax.set_xlim(-radius * 1.4, radius * 1.4)\n",
    "    ax.set_ylim(-radius * 1.4, radius * 1.4)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axhline(0, color='#334155', linewidth=0.5)\n",
    "    ax.axvline(0, color='#334155', linewidth=0.5)\n",
    "    ax.text(0.02, -0.12, f'dot = {dot_val:.4f}', transform=ax.transAxes,\n",
    "            fontsize=10, color=angle_color, fontweight='bold')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "fig.suptitle('RoPE: Same Relative Distance → Same Dot Product',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nLeft and center: same relative distance (4), different absolute positions → same dot product.')\n",
    "print('Right: different relative distance (5) → different dot product.')\n",
    "print('Position enters the dot product through rotation, and the result depends on relative distance.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dot product as a function of relative distance ---\n",
    "# Sweep relative distances from 0 to 20 and plot the dot product.\n",
    "# This shows the \"fingerprint\" of relative position in the attention score.\n",
    "\n",
    "rel_distances = list(range(0, 21))\n",
    "dot_products = []\n",
    "\n",
    "base_pos = 100  # arbitrary absolute position for q\n",
    "for d in rel_distances:\n",
    "    q_r = rotate_vector(q, base_pos, theta)\n",
    "    k_r = rotate_vector(k, base_pos + d, theta)\n",
    "    dot_products.append(torch.dot(q_r, k_r).item())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(rel_distances, dot_products, color='#34d399', linewidth=2, marker='o', markersize=5)\n",
    "ax.set_xlabel('Relative Distance (j - i)', fontsize=12)\n",
    "ax.set_ylabel('Dot Product (attention score contribution)', fontsize=12)\n",
    "ax.set_title('RoPE: Dot Product vs Relative Distance (single 2D subspace)',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.axhline(0, color='#334155', linewidth=0.5)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('The dot product oscillates with relative distance—like a wave.')\n",
    "print('This is ONE dimension pair at ONE frequency.')\n",
    "print('In full RoPE, many dimension pairs at different frequencies combine')\n",
    "print('(\"clock with many hands\"), creating a richer position signature.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** The dot product between RoPE-rotated Q and K vectors depends *only* on relative position. Positions (3, 7), (1003, 1007), and (50000, 50004) all produce the same dot product because they all have relative distance 4. Change the relative distance to 5, and the dot product changes.\n",
    "\n",
    "This is the fundamental property that enables context extension. A model trained at 4K context learns what the dot product looks like for relative distances 1, 5, 100, 2000. At inference on a 32K document, those same relative distances produce the same dot products—the model's learned attention patterns transfer. With learned PE, position 4097 literally has no embedding. With RoPE, position 4097 is just another rotation angle.\n",
    "\n",
    "The oscillating dot product curve shows how a single 2D subspace encodes relative distance. Full RoPE uses many subspaces at different frequencies—the \"clock with many hands\" from sinusoidal PE. Some hands rotate fast (local position), others rotate slow (global position). Together they create a unique fingerprint for each relative distance.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 2: Build Sliding Window Attention Mask (Supported)\n",
    "\n",
    "The second long-context barrier is quadratic compute. Full causal attention computes `O(n^2)` dot products—every token with every previous token. Sliding window attention restricts each token to attend only to its nearest `w` predecessors, reducing cost to `O(n * w)` where `w << n`.\n",
    "\n",
    "In this exercise, you'll:\n",
    "1. Build a full causal attention mask (lower triangular)\n",
    "2. Build a sliding window attention mask (diagonal band within the causal triangle)\n",
    "3. Visualize both as heatmaps and compare the number of computed attention scores\n",
    "\n",
    "The causal mask is provided. You'll implement the sliding window mask.\n",
    "\n",
    "**Before running, predict:**\n",
    "- For a sequence of length 32, how many attention scores does full causal attention compute? (Hint: lower triangular including diagonal)\n",
    "- With a sliding window of `w = 8`, how many scores does each token compute? How many total?\n",
    "- What fraction of the full causal computation does the sliding window require?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Full causal mask (provided) ---\n",
    "# The lower-triangular mask you built in the decoder-only transformers lesson:\n",
    "# each token can attend to itself and all previous tokens.\n",
    "\n",
    "seq_len = 32\n",
    "window_size = 8\n",
    "\n",
    "def build_causal_mask(n: int) -> torch.Tensor:\n",
    "    \"\"\"Full causal attention mask. 1 = can attend, 0 = masked.\n",
    "    Lower triangular: token i can attend to tokens 0..i.\n",
    "    \"\"\"\n",
    "    return torch.tril(torch.ones(n, n))\n",
    "\n",
    "\n",
    "# --- Sliding window mask (you implement this) ---\n",
    "\n",
    "def build_sliding_window_mask(n: int, w: int) -> torch.Tensor:\n",
    "    \"\"\"Sliding window attention mask. 1 = can attend, 0 = masked.\n",
    "    Token i can attend to tokens max(0, i-w+1)..i (the nearest w tokens).\n",
    "    \n",
    "    This is the causal mask PLUS a further restriction: tokens more than\n",
    "    w-1 positions before the current token are also masked out.\n",
    "    \n",
    "    Args:\n",
    "        n: sequence length\n",
    "        w: window size (each token attends to at most w tokens)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape (n, n) with 1s and 0s\n",
    "    \"\"\"\n",
    "    # TODO: Build the mask. Two approaches:\n",
    "    #\n",
    "    # Approach 1 (combine two masks):\n",
    "    #   1. Start with the causal mask: torch.tril(torch.ones(n, n))\n",
    "    #   2. Create a \"too far away\" mask: torch.triu(torch.ones(n, n), diagonal=-(w-1))\n",
    "    #      This keeps entries where col >= row - (w-1), i.e., within the window.\n",
    "    #   3. Combine: element-wise AND (multiply) the two masks.\n",
    "    #\n",
    "    # Approach 2 (direct band):\n",
    "    #   1. Create a matrix where entry (i,j) is 1 if:\n",
    "    #      j <= i  (causal)  AND  i - j < w  (within window)\n",
    "    #   2. Use: torch.ones(n,n) and mask with conditions.\n",
    "    #\n",
    "    # Either approach is fine. The result should be a diagonal band of 1s.\n",
    "    \n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "\n",
    "# --- Build and compare ---\n",
    "causal_mask = build_causal_mask(seq_len)\n",
    "window_mask = build_sliding_window_mask(seq_len, window_size)\n",
    "\n",
    "causal_count = int(causal_mask.sum().item())\n",
    "window_count = int(window_mask.sum().item())\n",
    "\n",
    "print(f'Sequence length: {seq_len}')\n",
    "print(f'Window size: {window_size}')\n",
    "print()\n",
    "print(f'Full causal attention scores: {causal_count}')\n",
    "print(f'Sliding window attention scores: {window_count}')\n",
    "print(f'Reduction: {window_count / causal_count:.1%} of full causal ({causal_count / window_count:.1f}x fewer)')\n",
    "print()\n",
    "print(f'Full causal formula: n(n+1)/2 = {seq_len}*{seq_len+1}/2 = {seq_len*(seq_len+1)//2}')\n",
    "print(f'Sliding window formula: ~n*w = {seq_len}*{window_size} = {seq_len*window_size}')\n",
    "print(f'(Exact window count may differ slightly at the start of the sequence where fewer than w tokens precede.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The key insight is that the sliding window mask is the intersection of two constraints: (1) causal—can only attend to earlier tokens, and (2) window—can only attend to the nearest `w` tokens. Both constraints are expressed as triangular matrices.\n",
    "\n",
    "```python\n",
    "def build_sliding_window_mask(n: int, w: int) -> torch.Tensor:\n",
    "    causal = torch.tril(torch.ones(n, n))  # lower triangular\n",
    "    window = torch.triu(torch.ones(n, n), diagonal=-(w - 1))  # keep entries within w\n",
    "    return causal * window  # element-wise AND\n",
    "```\n",
    "\n",
    "`torch.triu(..., diagonal=-(w-1))` creates an upper-triangular matrix shifted down by `w-1` rows. The intersection with the causal mask gives a diagonal band of width `w` within the lower triangle. Entry `(i, j)` is 1 when `j <= i` (causal) AND `i - j < w` (within window).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper: Working Sliding Window Mask\n",
    "\n",
    "**Run the cell below** to get a working `build_sliding_window_mask` for the visualization and remaining exercises. If your implementation above works correctly, this just redefines the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reference implementation for remaining exercises ---\n",
    "\n",
    "def build_causal_mask(n: int) -> torch.Tensor:\n",
    "    \"\"\"Full causal attention mask.\"\"\"\n",
    "    return torch.tril(torch.ones(n, n))\n",
    "\n",
    "def build_sliding_window_mask(n: int, w: int) -> torch.Tensor:\n",
    "    \"\"\"Sliding window attention mask.\"\"\"\n",
    "    causal = torch.tril(torch.ones(n, n))\n",
    "    window = torch.triu(torch.ones(n, n), diagonal=-(w - 1))\n",
    "    return causal * window\n",
    "\n",
    "# Rebuild masks with reference implementation\n",
    "causal_mask = build_causal_mask(seq_len)\n",
    "window_mask = build_sliding_window_mask(seq_len, window_size)\n",
    "\n",
    "causal_count = int(causal_mask.sum().item())\n",
    "window_count = int(window_mask.sum().item())\n",
    "\n",
    "print(f'Full causal scores: {causal_count}')\n",
    "print(f'Sliding window scores: {window_count}')\n",
    "print(f'Reduction: {window_count / causal_count:.1%} of full causal')\n",
    "print('Reference masks ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize: full causal vs sliding window as heatmaps ---\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5.5))\n",
    "\n",
    "# Full causal\n",
    "im0 = axes[0].imshow(causal_mask.numpy(), cmap='YlOrRd', interpolation='nearest',\n",
    "                      origin='upper', vmin=0, vmax=1)\n",
    "axes[0].set_title(f'Full Causal Attention\\n{causal_count} scores—O(n²)',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Key position (j)', fontsize=10)\n",
    "axes[0].set_ylabel('Query position (i)', fontsize=10)\n",
    "\n",
    "# Sliding window\n",
    "im1 = axes[1].imshow(window_mask.numpy(), cmap='YlGn', interpolation='nearest',\n",
    "                      origin='upper', vmin=0, vmax=1)\n",
    "axes[1].set_title(f'Sliding Window (w={window_size})\\n{window_count} scores—O(n·w)',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Key position (j)', fontsize=10)\n",
    "axes[1].set_ylabel('Query position (i)', fontsize=10)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks(range(0, seq_len, 8))\n",
    "    ax.set_yticks(range(0, seq_len, 8))\n",
    "\n",
    "fig.suptitle(f'Attention Masks: Sequence Length {seq_len}',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Full causal: every query attends to ALL preceding tokens.')\n",
    "print(f'Sliding window: every query attends to at most the {window_size} nearest tokens.')\n",
    "print(f'The window mask is a diagonal band within the causal triangle.')\n",
    "print(f'\\nAt 128K tokens with w=4096:')\n",
    "print(f'  Full causal: ~{128_000 * 128_001 // 2:,} scores')\n",
    "print(f'  Sliding window: ~{128_000 * 4096:,} scores')\n",
    "print(f'  That is a {128_000 * 128_001 // 2 / (128_000 * 4096):.0f}x reduction.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** The full causal mask is a solid lower triangle—every token attends to every previous token. The sliding window mask is a narrow diagonal band—each token attends to only its `w` nearest predecessors. The visual difference makes the compute savings obvious: the colored area (computed scores) is dramatically smaller.\n",
    "\n",
    "At the scale that matters—128K tokens with `w = 4096`—full causal computes ~8.2 billion scores. Sliding window computes ~524 million. A 16x reduction.\n",
    "\n",
    "\"But what about long-range dependencies?\" Remember from the lesson: stacked layers of local attention propagate information across the full context through the residual stream. Layer 1 propagates information from position 1 to ~4096, layer 2 from ~4096 to ~8192, and by layer 13 information from position 1 has reached position 50,000.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 3: GQA Forward Pass (Supported)\n",
    "\n",
    "The third long-context barrier is KV cache memory. In standard multi-head attention (MHA), every head has its own K and V projections—and its own K/V cache during generation. GQA shares K/V across groups of Q heads: 8 Q heads might share 2 K/V heads, giving a 4x reduction in KV cache memory while preserving query diversity.\n",
    "\n",
    "In this exercise, you'll:\n",
    "1. Implement a GQA forward pass with `n_heads=8` and `n_kv_heads=2`\n",
    "2. Run it on a sample input and verify the output shape\n",
    "3. Compare KV cache sizes: MHA (8 KV heads) vs GQA (2 KV heads)\n",
    "\n",
    "The projection matrices and the Q computation are provided. You'll implement the K/V head expansion (repeating shared K/V to match Q heads) and the attention computation.\n",
    "\n",
    "**Before running, predict:**\n",
    "- With `n_heads=8` and `n_kv_heads=2`, how many Q heads share each K/V head?\n",
    "- What is the output shape of GQA compared to standard MHA? Same or different?\n",
    "- If MHA uses 8 separate K and V caches, and GQA uses 2, what is the memory reduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GQA Implementation ---\n",
    "#\n",
    "# In standard MHA: n_heads Q heads, n_heads K/V heads.\n",
    "# In GQA: n_heads Q heads, n_kv_heads K/V heads (n_kv_heads < n_heads).\n",
    "# Multiple Q heads share the same K/V head.\n",
    "#\n",
    "# The key operation: \"expand\" the K/V from n_kv_heads to n_heads by repeating\n",
    "# each K/V head for (n_heads // n_kv_heads) Q heads.\n",
    "\n",
    "# --- Configuration ---\n",
    "d_model = 64\n",
    "n_heads = 8\n",
    "n_kv_heads = 2\n",
    "d_head = d_model // n_heads  # 64 // 8 = 8\n",
    "seq_len_ex3 = 16  # short sequence for this exercise\n",
    "heads_per_kv_group = n_heads // n_kv_heads  # 8 // 2 = 4 Q heads per KV head\n",
    "\n",
    "print(f'Model config:')\n",
    "print(f'  d_model = {d_model}')\n",
    "print(f'  n_heads (Q) = {n_heads}')\n",
    "print(f'  n_kv_heads (K/V) = {n_kv_heads}')\n",
    "print(f'  d_head = {d_head}')\n",
    "print(f'  Q heads per KV group = {heads_per_kv_group}')\n",
    "print()\n",
    "\n",
    "# --- Projection matrices ---\n",
    "# Q projections: one per Q head (8 heads)\n",
    "W_Q = nn.Linear(d_model, n_heads * d_head, bias=False)\n",
    "# K and V projections: one per KV head (only 2 heads!)\n",
    "W_K = nn.Linear(d_model, n_kv_heads * d_head, bias=False)\n",
    "W_V = nn.Linear(d_model, n_kv_heads * d_head, bias=False)\n",
    "# Output projection\n",
    "W_O = nn.Linear(n_heads * d_head, d_model, bias=False)\n",
    "\n",
    "print(f'W_Q shape: ({d_model}, {n_heads * d_head}) —projects to {n_heads} Q heads')\n",
    "print(f'W_K shape: ({d_model}, {n_kv_heads * d_head}) —projects to {n_kv_heads} K heads')\n",
    "print(f'W_V shape: ({d_model}, {n_kv_heads * d_head}) —projects to {n_kv_heads} V heads')\n",
    "print(f'W_O shape: ({n_heads * d_head}, {d_model}) —combines all {n_heads} head outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GQA forward pass ---\n",
    "\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(seq_len_ex3, d_model)  # (seq_len, d_model)\n",
    "\n",
    "# Step 1: Project Q, K, V\n",
    "Q = W_Q(x)  # (seq_len, n_heads * d_head)\n",
    "K = W_K(x)  # (seq_len, n_kv_heads * d_head)\n",
    "V = W_V(x)  # (seq_len, n_kv_heads * d_head)\n",
    "\n",
    "print('After projection:')\n",
    "print(f'  Q shape: {Q.shape} —{n_heads} Q heads x {d_head} dims')\n",
    "print(f'  K shape: {K.shape} —{n_kv_heads} K heads x {d_head} dims')\n",
    "print(f'  V shape: {V.shape} —{n_kv_heads} V heads x {d_head} dims')\n",
    "print()\n",
    "\n",
    "# Step 2: Reshape to separate heads\n",
    "Q = Q.view(seq_len_ex3, n_heads, d_head).transpose(0, 1)       # (n_heads, seq_len, d_head)\n",
    "K = K.view(seq_len_ex3, n_kv_heads, d_head).transpose(0, 1)    # (n_kv_heads, seq_len, d_head)\n",
    "V = V.view(seq_len_ex3, n_kv_heads, d_head).transpose(0, 1)    # (n_kv_heads, seq_len, d_head)\n",
    "\n",
    "print('After reshape to separate heads:')\n",
    "print(f'  Q: {Q.shape} —{n_heads} heads, each attending over {seq_len_ex3} tokens')\n",
    "print(f'  K: {K.shape} —only {n_kv_heads} KV heads!')\n",
    "print(f'  V: {V.shape} —only {n_kv_heads} KV heads!')\n",
    "print()\n",
    "\n",
    "# Step 3: Expand K and V to match Q heads\n",
    "#\n",
    "# TODO: Repeat each KV head to serve `heads_per_kv_group` Q heads.\n",
    "#\n",
    "# K is shape (n_kv_heads, seq_len, d_head)—we need (n_heads, seq_len, d_head).\n",
    "# Each of the 2 KV heads needs to be repeated 4 times (heads_per_kv_group = 4).\n",
    "#\n",
    "# Use torch.repeat_interleave(K, repeats=heads_per_kv_group, dim=0)\n",
    "# This repeats along dim=0: [KV_0, KV_1] -> [KV_0, KV_0, KV_0, KV_0, KV_1, KV_1, KV_1, KV_1]\n",
    "# Result: (n_heads, seq_len, d_head)\n",
    "#\n",
    "# Do the same for V.\n",
    "\n",
    "K_expanded = None  # TODO: expand K from (n_kv_heads, ...) to (n_heads, ...)\n",
    "V_expanded = None  # TODO: expand V from (n_kv_heads, ...) to (n_heads, ...)\n",
    "\n",
    "print(f'After KV expansion:')\n",
    "print(f'  K_expanded: {K_expanded.shape} —now matches Q\\'s {n_heads} heads')\n",
    "print(f'  V_expanded: {V_expanded.shape} —now matches Q\\'s {n_heads} heads')\n",
    "print()\n",
    "\n",
    "# Step 4: Compute attention (standard scaled dot-product)\n",
    "#\n",
    "# TODO: Compute attention scores, apply causal mask, softmax, weighted V.\n",
    "#   1. scores = Q @ K_expanded^T / sqrt(d_head)    shape: (n_heads, seq_len, seq_len)\n",
    "#   2. Apply causal mask: set upper triangle to -inf\n",
    "#   3. weights = softmax(scores, dim=-1)\n",
    "#   4. output = weights @ V_expanded               shape: (n_heads, seq_len, d_head)\n",
    "\n",
    "# Compute attention scores\n",
    "scores = None  # TODO: Q @ K_expanded.transpose(-2, -1) / math.sqrt(d_head)\n",
    "\n",
    "# Apply causal mask\n",
    "causal = torch.tril(torch.ones(seq_len_ex3, seq_len_ex3))\n",
    "scores = scores.masked_fill(causal == 0, float('-inf'))\n",
    "\n",
    "# Softmax and weighted sum\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "attn_output = None  # TODO: weights @ V_expanded\n",
    "\n",
    "print(f'Attention output per head: {attn_output.shape}')\n",
    "\n",
    "# Step 5: Concatenate heads and project\n",
    "# Transpose back: (n_heads, seq_len, d_head) -> (seq_len, n_heads, d_head)\n",
    "# Then reshape: (seq_len, n_heads * d_head)\n",
    "concat = attn_output.transpose(0, 1).contiguous().view(seq_len_ex3, n_heads * d_head)\n",
    "final_output = W_O(concat)  # (seq_len, d_model)\n",
    "\n",
    "print(f'Final GQA output: {final_output.shape}')\n",
    "print(f'Same shape as input? {final_output.shape == x.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The key insight is that GQA \"expands\" the K/V heads to match the Q heads by repeating. Each KV head is shared across `heads_per_kv_group` Q heads. The attention computation itself is identical to standard MHA—the only change is where the K and V come from.\n",
    "\n",
    "```python\n",
    "# Step 3: Expand K and V\n",
    "K_expanded = torch.repeat_interleave(K, repeats=heads_per_kv_group, dim=0)\n",
    "V_expanded = torch.repeat_interleave(V, repeats=heads_per_kv_group, dim=0)\n",
    "\n",
    "# Step 4: Attention\n",
    "scores = Q @ K_expanded.transpose(-2, -1) / math.sqrt(d_head)\n",
    "# (causal mask applied after this line)\n",
    "attn_output = weights @ V_expanded\n",
    "```\n",
    "\n",
    "The `repeat_interleave` is the GQA mechanism in one line: take 2 KV heads, repeat each 4 times to get 8, so each Q head can index its matching KV head. In production, this is often done without explicit expansion (using gather/index operations), but the logic is the same.\n",
    "\n",
    "Note: the *computation* after expansion is identical to MHA. The savings come from the *cache*: during generation, you store only 2 K/V caches instead of 8.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper: Working GQA Implementation\n",
    "\n",
    "**Run the cell below** to execute a complete GQA forward pass and see the KV cache comparison. This ensures the output and analysis are correct regardless of your implementation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Complete GQA forward pass (reference) ---\n",
    "\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(seq_len_ex3, d_model)\n",
    "\n",
    "# Project\n",
    "Q = W_Q(x).view(seq_len_ex3, n_heads, d_head).transpose(0, 1)\n",
    "K = W_K(x).view(seq_len_ex3, n_kv_heads, d_head).transpose(0, 1)\n",
    "V = W_V(x).view(seq_len_ex3, n_kv_heads, d_head).transpose(0, 1)\n",
    "\n",
    "# Expand K/V to match Q heads\n",
    "K_expanded = torch.repeat_interleave(K, repeats=heads_per_kv_group, dim=0)\n",
    "V_expanded = torch.repeat_interleave(V, repeats=heads_per_kv_group, dim=0)\n",
    "\n",
    "# Attention\n",
    "scores = Q @ K_expanded.transpose(-2, -1) / math.sqrt(d_head)\n",
    "causal = torch.tril(torch.ones(seq_len_ex3, seq_len_ex3))\n",
    "scores = scores.masked_fill(causal == 0, float('-inf'))\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "attn_output = weights @ V_expanded\n",
    "\n",
    "# Concatenate and project\n",
    "concat = attn_output.transpose(0, 1).contiguous().view(seq_len_ex3, n_heads * d_head)\n",
    "final_output = W_O(concat)\n",
    "\n",
    "print(f'GQA output shape: {final_output.shape}')\n",
    "print(f'Input shape: {x.shape}')\n",
    "print(f'Same shape (drop-in replacement)? {final_output.shape == x.shape}')\n",
    "print()\n",
    "\n",
    "# --- KV Cache Comparison ---\n",
    "print('=== KV Cache Size Comparison ===')\n",
    "print()\n",
    "\n",
    "# Per-head KV cache: stores K and V for all sequence positions\n",
    "# Shape per head: (seq_len, d_head) for K + (seq_len, d_head) for V\n",
    "# In bytes (float32): 2 * seq_len * d_head * 4 bytes\n",
    "\n",
    "# For this toy model:\n",
    "bytes_per_kv_head = 2 * seq_len_ex3 * d_head * 4  # K + V, float32\n",
    "\n",
    "mha_kv_cache = n_heads * bytes_per_kv_head\n",
    "gqa_kv_cache = n_kv_heads * bytes_per_kv_head\n",
    "\n",
    "print(f'Toy model (seq_len={seq_len_ex3}, d_head={d_head}, float32):')\n",
    "print(f'  MHA ({n_heads} KV heads): {mha_kv_cache:,} bytes')\n",
    "print(f'  GQA ({n_kv_heads} KV heads): {gqa_kv_cache:,} bytes')\n",
    "print(f'  Reduction: {n_heads // n_kv_heads}x')\n",
    "print()\n",
    "\n",
    "# Scale to LLaMA 2 70B numbers:\n",
    "print('--- Scaled to LLaMA 2 70B at 128K context ---')\n",
    "llama_n_heads = 64\n",
    "llama_n_kv_heads = 8\n",
    "llama_d_head = 128\n",
    "llama_seq_len = 128_000\n",
    "llama_n_layers = 80\n",
    "bytes_per_element = 2  # bf16\n",
    "\n",
    "# Per-head per-layer KV cache (K + V)\n",
    "llama_bytes_per_kv_head = 2 * llama_seq_len * llama_d_head * bytes_per_element\n",
    "\n",
    "llama_mha_cache = llama_n_heads * llama_n_layers * llama_bytes_per_kv_head\n",
    "llama_gqa_cache = llama_n_kv_heads * llama_n_layers * llama_bytes_per_kv_head\n",
    "\n",
    "print(f'  MHA ({llama_n_heads} KV heads): {llama_mha_cache / 1e9:.1f} GB')\n",
    "print(f'  GQA ({llama_n_kv_heads} KV heads): {llama_gqa_cache / 1e9:.1f} GB')\n",
    "print(f'  Reduction: {llama_n_heads // llama_n_kv_heads}x')\n",
    "print(f'  Savings: {(llama_mha_cache - llama_gqa_cache) / 1e9:.1f} GB')\n",
    "print()\n",
    "print(f'  LLaMA 2 70B model weights: ~140 GB')\n",
    "print(f'  MHA KV cache at 128K: {llama_mha_cache / 1e9:.1f} GB—larger than the model!')\n",
    "print(f'  GQA KV cache at 128K: {llama_gqa_cache / 1e9:.1f} GB—manageable.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize: MHA vs GQA architecture ---\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "q_color = '#a78bfa'   # violet\n",
    "kv_color = '#f59e0b'  # amber\n",
    "\n",
    "configs = [\n",
    "    ('MHA (Multi-Head)', 8, 8),\n",
    "    ('GQA (Grouped Query)', 8, 2),\n",
    "    ('MQA (Multi-Query)', 8, 1),\n",
    "]\n",
    "\n",
    "for ax, (title, n_q, n_kv) in zip(axes, configs):\n",
    "    # Draw Q heads\n",
    "    for i in range(n_q):\n",
    "        y = 0.9 - i * 0.1\n",
    "        ax.add_patch(plt.Rectangle((0.05, y - 0.03), 0.2, 0.06,\n",
    "                     facecolor=q_color, alpha=0.6, edgecolor='white', linewidth=0.8))\n",
    "        ax.text(0.15, y, f'Q{i}', ha='center', va='center', fontsize=7,\n",
    "                color='white', fontweight='bold')\n",
    "    \n",
    "    # Draw KV heads\n",
    "    kv_spacing = 0.8 / max(n_kv, 1)\n",
    "    for i in range(n_kv):\n",
    "        y = 0.9 - i * (0.8 / n_kv) - (0.8 / n_kv - 0.06) / 2\n",
    "        height = 0.8 / n_kv - 0.02\n",
    "        ax.add_patch(plt.Rectangle((0.65, y - height/2), 0.25, height,\n",
    "                     facecolor=kv_color, alpha=0.5, edgecolor='white', linewidth=0.8))\n",
    "        ax.text(0.775, y, f'KV{i}', ha='center', va='center', fontsize=7,\n",
    "                color='white', fontweight='bold')\n",
    "        \n",
    "        # Draw connection lines from Q heads to this KV head\n",
    "        q_per_kv = n_q // n_kv\n",
    "        for j in range(q_per_kv):\n",
    "            q_idx = i * q_per_kv + j\n",
    "            q_y = 0.9 - q_idx * 0.1\n",
    "            ax.plot([0.25, 0.65], [q_y, y], color='#94a3b8', linewidth=0.8, alpha=0.5)\n",
    "    \n",
    "    ax.set_title(f'{title}\\n{n_q} Q heads, {n_kv} KV heads\\nKV cache: {n_kv}/{n_q} = {n_kv/n_q:.0%}',\n",
    "                fontsize=10, fontweight='bold')\n",
    "    ax.set_xlim(-0.05, 1.0)\n",
    "    ax.set_ylim(0.0, 1.05)\n",
    "    ax.axis('off')\n",
    "\n",
    "fig.suptitle('The MHA → GQA → MQA Spectrum: Same Q Diversity, Less KV Memory',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('All three architectures keep the same 8 Q heads with independent W_Q projections.')\n",
    "print('The only change: how many independent K/V heads exist.')\n",
    "print('GQA is the sweet spot: significant KV cache savings with minimal quality loss.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 4: Attention Cost Calculator (Independent)\n\nThe lesson presented three barriers to long context: position encoding limits, quadratic compute, and KV cache memory. Each has a targeted solution: RoPE, sparse attention, and GQA. In this exercise, you'll build a calculator that quantifies all three costs and shows how the solutions compound.\n\n**Your task:** Build a function `attention_cost(config)` that computes:\n1. **Attention FLOPs per layer**—the compute cost of the QK^T matrix multiplication and the attention-weighted V sum\n2. **KV cache memory**—the total bytes needed to store K and V tensors across all layers during generation\n\nThen compute costs for five configurations:\n- **(a) GPT-2** at 1K context: 12 heads, d_model=768, d_head=64, 12 layers, float32\n- **(b) LLaMA 2 70B with MHA** at 4K context: 64 Q heads, 64 KV heads, d_model=8192, d_head=128, 80 layers, bf16\n- **(c) LLaMA 2 70B with GQA** at 4K context: 64 Q heads, 8 KV heads (same otherwise)\n- **(d) LLaMA 2 70B with GQA** at 128K context\n- **(e) LLaMA 2 70B with GQA + sliding window (w=4096)** at 128K context\n\nPresent results in a table. The insight: RoPE enables the position encoding (not computed here, but enables configs d and e), GQA reduces KV cache, and sliding window reduces FLOPs. Their benefits compound.\n\n**Formulas:**\n- Attention FLOPs per layer ≈ `2 * n_q_heads * seq_len * seq_len * d_head` (for QK^T) + `2 * n_q_heads * seq_len * seq_len * d_head` (for attn @ V). With sliding window, replace the second `seq_len` with `min(seq_len, window_size)`.\n- KV cache per layer = `2 * n_kv_heads * seq_len * d_head * bytes_per_element` (2 for K and V)\n- Total KV cache = KV cache per layer × n_layers\n\n**Before running, predict:**\n- How many times more expensive is 128K vs 4K in FLOPs? (Hint: quadratic scaling)\n- How much does GQA reduce KV cache for LLaMA 2 70B (64 KV heads → 8)?\n- Does sliding window affect KV cache size, or only FLOPs?\n\n**No skeleton is provided.** Design the function and table yourself."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Your attention cost calculator here.\n#\n# Suggested structure:\n#\n# 1. Define a config dataclass or dict with fields:\n#    name, n_q_heads, n_kv_heads, d_head, n_layers,\n#    seq_len, bytes_per_element, window_size (None = full attention)\n#\n# 2. Write attention_cost(config) that returns:\n#    - flops_per_layer: attention FLOPs for one layer\n#    - total_flops: flops_per_layer * n_layers\n#    - kv_cache_per_layer: bytes for K + V in one layer\n#    - total_kv_cache: kv_cache_per_layer * n_layers\n#\n# 3. Define the five configurations (a)-(e)\n#\n# 4. Compute costs for each and print a formatted table\n#\n# 5. Add a comparison section showing:\n#    - FLOPs ratio: (b) vs (c) (GQA doesn't change FLOPs)\n#    - KV cache ratio: (b) vs (c) (GQA reduces cache)\n#    - FLOPs ratio: (d) vs (e) (sliding window reduces FLOPs)\n#    - KV cache ratio: (d) vs (e) (sliding window doesn't change cache)\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Solution</summary>\n\nThe key insight is that GQA and sliding window address *different* costs. GQA reduces KV cache memory (fewer KV heads to store) but does not reduce attention FLOPs (the Q heads still compute the same number of dot products). Sliding window reduces FLOPs (each query attends to at most `w` keys instead of all previous keys) but does not reduce KV cache size (the full K/V sequence is still stored for all layers). They are complementary.\n\n```python\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass AttentionConfig:\n    name: str\n    n_q_heads: int\n    n_kv_heads: int\n    d_head: int\n    n_layers: int\n    seq_len: int\n    bytes_per_element: int  # 4 for float32, 2 for bf16\n    window_size: int = 0    # 0 means full attention (no sliding window)\n\n\ndef attention_cost(cfg: AttentionConfig) -> dict:\n    \"\"\"Compute attention FLOPs and KV cache memory for a given config.\"\"\"\n\n    # Effective attention span per query token\n    effective_span = cfg.seq_len\n    if cfg.window_size > 0:\n        effective_span = min(cfg.seq_len, cfg.window_size)\n\n    # Attention FLOPs per layer:\n    # QK^T: each Q head computes (seq_len, d_head) @ (d_head, effective_span)\n    #   = 2 * seq_len * effective_span * d_head per head (multiply-add = 2 ops)\n    # attn @ V: each Q head computes (seq_len, effective_span) @ (effective_span, d_head)\n    #   = 2 * seq_len * effective_span * d_head per head\n    # Total per layer = n_q_heads * (QK^T FLOPs + attn@V FLOPs)\n    flops_qk = 2 * cfg.n_q_heads * cfg.seq_len * effective_span * cfg.d_head\n    flops_av = 2 * cfg.n_q_heads * cfg.seq_len * effective_span * cfg.d_head\n    flops_per_layer = flops_qk + flops_av\n    total_flops = flops_per_layer * cfg.n_layers\n\n    # KV cache: store K and V for all sequence positions, per KV head, per layer\n    # K: (seq_len, d_head) and V: (seq_len, d_head) per KV head\n    kv_cache_per_layer = 2 * cfg.n_kv_heads * cfg.seq_len * cfg.d_head * cfg.bytes_per_element\n    total_kv_cache = kv_cache_per_layer * cfg.n_layers\n\n    return {\n        'flops_per_layer': flops_per_layer,\n        'total_flops': total_flops,\n        'kv_cache_per_layer': kv_cache_per_layer,\n        'total_kv_cache': total_kv_cache,\n    }\n\n\ndef format_flops(flops: float) -> str:\n    \"\"\"Human-readable FLOPs string.\"\"\"\n    if flops >= 1e15:\n        return f'{flops / 1e15:.1f} PFLOPs'\n    if flops >= 1e12:\n        return f'{flops / 1e12:.1f} TFLOPs'\n    if flops >= 1e9:\n        return f'{flops / 1e9:.1f} GFLOPs'\n    return f'{flops / 1e6:.1f} MFLOPs'\n\n\ndef format_bytes(b: float) -> str:\n    \"\"\"Human-readable bytes string.\"\"\"\n    if b >= 1e9:\n        return f'{b / 1e9:.1f} GB'\n    if b >= 1e6:\n        return f'{b / 1e6:.1f} MB'\n    return f'{b / 1e3:.1f} KB'\n\n\n# --- Define the five configurations ---\n\nconfigs = [\n    AttentionConfig(\n        name='(a) GPT-2, 1K',\n        n_q_heads=12, n_kv_heads=12, d_head=64,\n        n_layers=12, seq_len=1024,\n        bytes_per_element=4,  # float32\n    ),\n    AttentionConfig(\n        name='(b) LLaMA 70B MHA, 4K',\n        n_q_heads=64, n_kv_heads=64, d_head=128,\n        n_layers=80, seq_len=4096,\n        bytes_per_element=2,  # bf16\n    ),\n    AttentionConfig(\n        name='(c) LLaMA 70B GQA, 4K',\n        n_q_heads=64, n_kv_heads=8, d_head=128,\n        n_layers=80, seq_len=4096,\n        bytes_per_element=2,\n    ),\n    AttentionConfig(\n        name='(d) LLaMA 70B GQA, 128K',\n        n_q_heads=64, n_kv_heads=8, d_head=128,\n        n_layers=80, seq_len=128_000,\n        bytes_per_element=2,\n    ),\n    AttentionConfig(\n        name='(e) LLaMA 70B GQA+SW, 128K',\n        n_q_heads=64, n_kv_heads=8, d_head=128,\n        n_layers=80, seq_len=128_000,\n        bytes_per_element=2,\n        window_size=4096,\n    ),\n]\n\n# --- Compute and display ---\n\nresults = [(cfg, attention_cost(cfg)) for cfg in configs]\n\nprint(f'{\"Config\":<32} {\"FLOPs/Layer\":>14} {\"Total FLOPs\":>14} {\"KV Cache\":>12}')\nprint('-' * 76)\nfor cfg, cost in results:\n    print(f'{cfg.name:<32} '\n          f'{format_flops(cost[\"flops_per_layer\"]):>14} '\n          f'{format_flops(cost[\"total_flops\"]):>14} '\n          f'{format_bytes(cost[\"total_kv_cache\"]):>12}')\n\n# --- Comparison analysis ---\nprint('\\n=== Key Comparisons ===\\n')\n\n_, cost_b = results[1]  # MHA at 4K\n_, cost_c = results[2]  # GQA at 4K\n_, cost_d = results[3]  # GQA at 128K\n_, cost_e = results[4]  # GQA + sliding window at 128K\n\nprint('MHA vs GQA (same model, same context length):')\nprint(f'  FLOPs: {cost_b[\"total_flops\"] / cost_c[\"total_flops\"]:.1f}x '\n      f'(GQA does NOT reduce FLOPs—same Q heads, same dot products)')\nprint(f'  KV cache: {cost_b[\"total_kv_cache\"] / cost_c[\"total_kv_cache\"]:.0f}x reduction '\n      f'(GQA reduces KV heads from 64 to 8)')\n\nprint(f'\\n4K vs 128K context (same architecture):')\nprint(f'  FLOPs: {cost_d[\"total_flops\"] / cost_c[\"total_flops\"]:.0f}x increase '\n      f'(quadratic: (128K/4K)² = {(128_000/4096)**2:.0f}x)')\nprint(f'  KV cache: {cost_d[\"total_kv_cache\"] / cost_c[\"total_kv_cache\"]:.1f}x increase '\n      f'(linear: 128K/4K = {128_000/4096:.1f}x)')\n\nprint(f'\\nFull attention vs sliding window at 128K:')\nprint(f'  FLOPs: {cost_d[\"total_flops\"] / cost_e[\"total_flops\"]:.1f}x reduction '\n      f'(window limits attention span)')\nprint(f'  KV cache: {cost_d[\"total_kv_cache\"] / cost_e[\"total_kv_cache\"]:.1f}x '\n      f'(sliding window does NOT reduce KV cache—full sequence still stored)')\n\nprint(f'\\nEnd-to-end: MHA at 4K (b) vs GQA+SW at 128K (e):')\nprint(f'  FLOPs: {cost_e[\"total_flops\"] / cost_b[\"total_flops\"]:.1f}x')\nprint(f'  KV cache: {cost_e[\"total_kv_cache\"] / cost_b[\"total_kv_cache\"]:.1f}x')\nprint(f'  Context: 32x longer—and the costs are manageable because')\nprint(f'  GQA compressed the cache and sliding window compressed the compute.')\n\nprint('\\n=== The Three Barriers, Quantified ===')\nprint('Position: RoPE makes configs (d) and (e) possible at all (learned PE stops at 4K)')\nprint(f'Compute: sliding window cuts 128K FLOPs by {cost_d[\"total_flops\"] / cost_e[\"total_flops\"]:.0f}x')\nprint(f'Memory: GQA cuts KV cache by {cost_b[\"total_kv_cache\"] / cost_c[\"total_kv_cache\"]:.0f}x')\nprint('Three barriers, three solutions, compounding benefits.')\n```\n\n**Design choices explained:**\n- The function separates FLOPs and KV cache because they respond to *different* optimizations. GQA reduces cache but not FLOPs. Sliding window reduces FLOPs but not cache. This separation makes the \"three independent barriers\" framework concrete.\n- FLOPs formula counts both QK^T and attn@V multiplications. Each is a matrix multiply: 2 ops per multiply-add.\n- KV cache counts K and V separately (factor of 2), for all KV heads, for all layers. The seq_len dimension is the full context length even with sliding window—the cache stores all positions because different layers may need different windows.\n- The comparison section explicitly calls out what each optimization does and does not affect. This is the lesson's core message: three independent bottlenecks, three independent solutions.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **RoPE encodes position in the dot product via rotation, making attention scores depend on relative distance.** The dot product between rotated Q and K vectors is the same for positions (3, 7) and (1003, 1007)—both have relative distance 4. This mathematical property is what enables context extension: patterns learned at training length transfer to longer sequences.\n",
    "\n",
    "2. **Sparse attention (sliding window) restricts which token pairs compute scores, reducing O(n^2) to O(n*w).** The sliding window mask is a narrow diagonal band within the causal triangle. At 128K tokens with w=4096, this is a 16x compute reduction. Information still flows across the full context through stacked layers and the residual stream.\n",
    "\n",
    "3. **GQA shares K/V heads across groups of Q heads, preserving query diversity while cutting KV cache memory.** With 8 Q heads and 2 KV heads, the cache is 4x smaller. At LLaMA 2 70B scale (64 Q, 8 KV), GQA reduces the 128K-context KV cache from ~335 GB to ~42 GB—the difference between \"larger than the model\" and \"fits in memory.\"\n",
    "\n",
    "4. **These three innovations address three independent bottlenecks.** RoPE fixes position generalization. Sparse attention fixes quadratic compute. GQA fixes KV cache memory. They combine, not compete—LLaMA and Mistral use all three together.\n",
    "\n",
    "5. **Position in the handshake, not the nametag. Compute where attention concentrates, not everywhere. Cache what's needed, not everything.** Three barriers, three targeted solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}