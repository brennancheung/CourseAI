# Module 5.3: Scaling Architecture -- Record

## Concept Index

| Concept | Depth | Lesson | Notes |
|---------|-------|--------|-------|
| Conditional computation (not every parameter activates for every token, decoupling total parameters from per-token compute) | DEVELOPED | mixture-of-experts | The paradigm shift: every model the student studied previously activates all parameters for every input. MoE breaks this by activating only a subset of FFN parameters per token. The student can explain why this targets the FFN specifically (2/3 of parameters, knowledge store) and articulate the core insight: total parameters and active parameters are different quantities. |
| Router mechanism (single linear layer + softmax + top-k selecting which experts activate) | DEVELOPED | mixture-of-experts | W_router @ hidden_state -> softmax -> top-k. Connected to attention's dot-product + softmax pattern: router rows are "expert embeddings," dot product measures token-expert relevance, softmax produces probability distribution, top-k selects. Demystified by showing it is simpler than a single attention head. The student can trace how a token flows from hidden state through the router to selected experts and back to the residual stream. |
| MoE architecture (replacing monolithic FFN with N expert FFNs + router within the transformer block) | DEVELOPED | mixture-of-experts | Only the FFN sub-layer changes. Attention is identical. Residual stream is identical. Each expert is a standard two-layer FFN (expansion, GELU) with its own weights. Selected expert outputs are weighted by router probabilities and summed, then added to the residual stream. The student understands MoE as a targeted modification of the "writes" half of "attention reads, FFN writes." |
| Parameter-compute decoupling (total parameters >> active parameters, per-token compute scales with active not total) | DEVELOPED | mixture-of-experts | Concretized with Mixtral 8x7B: ~47B total parameters, ~13B active per token. Three-way comparison table (13B dense, Mixtral 8x7B, 47B dense) makes the decoupling tangible. Connected to Chinchilla: MoE adds a third scaling dimension -- scale total parameters without scaling per-token compute. Memory tradeoff explicitly noted: all parameters must be loaded even though only a subset activates. |
| Expert specialization (emergent, per-token, not designed per-topic) | INTRODUCED | mixture-of-experts | Different tokens in the same sentence route to different experts. Function words cluster toward syntax-oriented experts; domain vocabulary routes to other experts. Specialization is learned from training data, not imposed. Boundaries do not map cleanly to human categories. Research on Mixtral confirms preferences but not clean topic boundaries; early layers show syntactic specialization, later layers show semantic patterns. |
| Load balancing / auxiliary loss (preventing router collapse where one expert dominates) | INTRODUCED | mixture-of-experts | Without intervention, a positive feedback loop causes one expert to dominate: slightly better expert gets more tokens, gets more gradient updates, gets even better. Other experts atrophy. The model degenerates to approximately a dense model. Solution: auxiliary loss measuring per-expert token fraction and average router probability -- their product penalizes concentration. The student knows the problem (collapse), the mechanism (positive feedback loop), and the concept of the solution (product-based penalty pushing toward balanced utilization), but not the mathematical formula. |
| Mixtral 8x7B as a concrete MoE architecture | INTRODUCED | mixture-of-experts | 8 experts per MoE layer, top-2 routing, ~47B total parameters, ~13B active per token. Competitive with models 3-4x its active parameter count. Named as the primary real-world example throughout the lesson. |
| DeepSeek-V3 as a frontier MoE architecture | INTRODUCED | mixture-of-experts | Named in references as a recent frontier MoE model using auxiliary-loss-free load balancing and multi-head latent attention. Not developed beyond name recognition. |
| Memory tradeoff of MoE (all parameters must be loaded, only subset activates) | INTRODUCED | mixture-of-experts | Mixtral uses ~3.5x more memory than its active parameter count suggests. MoE models need more memory to store inactive experts, motivating distribution across multiple GPUs. "More knowledge, same speed, more memory." |
| "Unconstrained optimization finds degenerate solutions" pattern (connecting MoE collapse to RLHF reward hacking) | INTRODUCED | mixture-of-experts | In RLHF, the policy exploits the reward model without a KL constraint. In MoE, gradient descent concentrates tokens on one expert without a balancing constraint. Different mechanisms, same pattern: unconstrained optimization of a local signal leads to collapse. The auxiliary loss constrains the optimization. |
| Three barriers to long context (position encoding limits, quadratic compute, KV cache memory) | DEVELOPED | long-context-and-efficient-attention | Framed as three independent bottlenecks requiring three independent solutions. Position: learned PE stops at max_seq_len, sinusoidal PE degrades on unseen lengths. Compute: attention FLOPs grow O(n^2), flash attention fixes memory not compute. Memory: KV cache grows linearly per head, multiplied by num_heads. Concrete numbers: 128K tokens is 16,384x more expensive than 1K; KV cache for 70B model at 128K is ~332 GB (larger than model weights). ThreeBarrierDiagram (inline SVG) provides visual map. |
| RoPE mechanism (rotation in 2D subspaces, relative position encoding in Q/K dot product) | DEVELOPED | long-context-and-efficient-attention | Rotate Q and K vectors by position-proportional angles in paired 2D subspaces. The dot product q_rot^T k_rot depends on relative distance (i-j), not absolute positions. Different dimension pairs rotate at different frequencies (same multi-frequency structure as sinusoidal PE, "clock with many hands" callback). Taught via nametag-vs-handshake analogy (additive PE encodes position in the nametag/embedding; RoPE encodes position in the handshake/dot product). RoPERotationDiagram (inline SVG) shows same angular difference at positions 3/7 and 100/104. Code block shows apply_rope implementation. Formal rotation matrix R(m, theta_d) presented after code (concrete before abstract). |
| Context extension via RoPE (relative position patterns transfer to longer sequences) | INTRODUCED | long-context-and-efficient-attention | RoPE makes context extension possible (not automatic). Rotation for relative distance d is always R(d*theta)--patterns learned at training length transfer. Distances beyond training use unseen rotations; NTK-aware scaling and YaRN mentioned as extension techniques but not developed. Key point: RoPE gives something to extend; learned PE gives nothing. |
| Quadratic attention bottleneck (concrete cost at increasing sequence lengths) | DEVELOPED | long-context-and-efficient-attention | Attention FLOPs ~2*n^2*d_model per layer. Concrete table: 1K (~1.5B FLOPs), 4K (~24B, 16x), 32K (~1.5T, 1024x), 128K (~24T, 16,384x). Flash attention makes this fit in memory but does not reduce computation. Student can compute the cost at different sequence lengths and explain why flash attention is insufficient. |
| Sliding window attention (each token attends to at most w preceding tokens, O(n*w) compute) | DEVELOPED | long-context-and-efficient-attention | Same masking mechanism as causal attention (set entries to -inf before softmax), but diagonal band pattern instead of lower-triangular. At w=4096 and n=128K, 32x cheaper than full attention. Used by Mistral-7B. AttentionPatternDiagram (inline SVG) shows full causal vs sliding window vs dilated side by side with computed score counts. |
| Stacked-layer information propagation (local attention approximates global through residual stream across layers) | DEVELOPED | long-context-and-efficient-attention | Token at position 50K cannot directly attend to position 1 through window of 4096. But through residual stream: layer 1 propagates info to ~4096, layer 2 to ~8192, by layer 13 info from position 1 reaches 50K. Addresses misconception that sparse attention loses too much information. Connected to student's existing GPT-2 residual stream understanding. |
| Dilated attention (attend to every kth token within a wider range, same compute budget as window w but covers kw span) | DEVELOPED | long-context-and-efficient-attention | Described on its own terms without reference to dilated convolutions (which are untaught). Captures longer-range patterns with same compute as sliding window. Often combined with sliding window in different heads or layers. Shown in AttentionPatternDiagram alongside full causal and sliding window. |
| Linear attention (kernel trick reformulation, O(n) compute concept) | INTRODUCED | long-context-and-efficient-attention | Replace softmax with feature map phi, change order of operations: instead of forming n*n score matrix first (phi(Q) * phi(K)^T * V), compute K/V summary first (phi(Q) * (phi(K)^T V)). Summary is d_k x d_v, independent of sequence length. Cost drops from O(n^2*d_k) to O(n*d_k^2). Tradeoff: phi approximates softmax kernel, introducing approximation error. Full softmax remains more expressive for sharp position-dependent patterns. Taught with plain-language motivation first ("compute a summary of K and V first, then let each Q token query that summary"), then symbolic confirmation. |
| GQA mechanism (sharing K/V heads across groups of Q heads, reducing KV cache while preserving query diversity) | DEVELOPED | long-context-and-efficient-attention | Multiple Q heads share a single K/V pair. Queries still need diversity (different W_Q^i per head), but K/V show significant redundancy. All Q heads remain, only K/V heads are shared. Comparison table: MHA (32Q/32KV), GQA (32Q/8KV, 4x cache reduction), MQA (32Q/1KV, 32x cache reduction), fewer heads (4Q/4KV, reduced diversity). Misconception addressed: GQA is NOT just fewer heads. ComparisonRow with LLaMA 2 70B numbers: MHA ~332 GB vs GQA ~42 GB KV cache (8x reduction). Connected to "split, not multiplied" from multi-head attention. |
| MHA-GQA-MQA spectrum (smooth tradeoff between KV cache memory and K/V independence) | DEVELOPED | long-context-and-efficient-attention | All three architectures keep same number of Q heads with same per-head dimension. Only change is how many independent K/V heads exist. MHA: full independence. GQA: grouped sharing. MQA: all queries share one K/V. Presented as a spectrum, not discrete choices. LLaMA 2 70B uses GQA with 8 KV groups (64 Q heads). |
| NTK-aware scaling and YaRN (context extension techniques beyond basic RoPE) | MENTIONED | long-context-and-efficient-attention | Named as techniques that adjust the frequency base to smooth the transition when extending beyond training context length. Not developed--mentioned to acknowledge that RoPE context extension is not automatic and that engineering is required. |
| ALiBi (Attention with Linear Biases) | MENTIONED | long-context-and-efficient-attention | Named as an alternative positional encoding approach. Not developed. |
| Data parallelism (replicate full model on each GPU, split training data, all-reduce gradients) | DEVELOPED | training-and-serving-at-scale | Each GPU holds the full model and processes a different batch. After backward pass, gradients are averaged across GPUs via all-reduce. N GPUs = N times throughput. Requires that the full model fits on one GPU. Concrete example: GPT-2 124M on 4 GPUs. Misconception explicitly addressed: "parallelism is just data parallelism" -- data parallelism fails when the model does not fit on a single GPU (70B at 840 GB vs 80 GB A100). |
| Tensor parallelism (split weight matrices within layers across GPUs, both GPUs compute simultaneously on the same input) | DEVELOPED | training-and-serving-at-scale | Split a linear layer's weight matrix column-wise across GPUs. Each GPU stores half the columns, computes half the output, then all-reduce reconstructs the full output. Communication happens within each layer (high frequency, fine-grained). Connected to the student's nn.Linear -- "the same layer you built, split across two chips." Expert parallelism for MoE (different experts on different GPUs) MENTIONED as a specialized form. |
| Pipeline parallelism (assign different layers to different GPUs, microbatch flow through stages) | DEVELOPED | training-and-serving-at-scale | GPU 0 runs blocks 0-11, GPU 1 runs blocks 12-23, etc. Communication happens only between stages (lower frequency). Assembly line analogy: each station processes one step, inefficient for one item but fast when the line is full. Pipeline bubble negative example: with 4 stages and 1 microbatch, 75% of GPUs are idle. Solution: microbatching fills the pipeline. Gradient accumulation MENTIONED as related. |
| Communication overhead as the central constraint shaping all parallelism design | DEVELOPED | training-and-serving-at-scale | A100 computes at ~312 TFLOPS (bf16), NVLink bandwidth ~600 GB/s. Transferring 1 GB takes ~1.7 ms, during which the GPU could have done ~530 billion FLOPs. At 175B scale with naive tensor parallelism, 40-60% of time is spent waiting for communication. Misconception explicitly addressed via rose GradientCard: "communication between GPUs is fast because they are in the same machine." Connected to the delivery truck analogy from scaling-and-efficiency -- "the delivery truck now drives between buildings, not between floors." |
| ZeRO optimizer state sharding (each GPU stores only a fraction of optimizer states) | INTRODUCED | training-and-serving-at-scale | Recall from lora-and-quantization: optimizer states are 2/3 of training memory. ZeRO Stage 1: each GPU stores full weights and gradients but only 1/N of optimizer states. 70B on 8 GPUs: per-GPU memory drops from ~840 GB (impossible) to ~350 GB (still needs tensor/pipeline parallelism but the gap is much smaller). ZeRO has three stages (shard optimizer states, shard gradients, shard parameters) -- only Stage 1 developed. DeepSpeed and FSDP MENTIONED as frameworks implementing ZeRO. |
| Speculative decoding (draft-verify loop: small model drafts K tokens, large model verifies all K in one forward pass) | DEVELOPED | training-and-serving-at-scale | Upgraded from MENTIONED (scaling-and-efficiency 4.3.3). Key insight: the large model's forward pass costs roughly the same for 1 or 5 tokens because matmul is compute-bound (d_model and d_vocab dominate, not batch dimension). Connected to compute-bound/memory-bound framework from scaling-and-efficiency. Draft model (7B) generates K candidates autoregressively. Large model (70B) verifies all K in one forward pass. Accept matching tokens, reject from first disagreement. Worked example: "The capital of France is" -- 5 drafted, 3 accepted = 3 tokens from 1 large forward pass instead of 3 separate passes. Connected to generate() loop from building-nanogpt. Misconception addressed: speed comes from parallel verification, not from the small model being fast. |
| Continuous batching (dynamically fill completed request slots with new requests from queue) | DEVELOPED | training-and-serving-at-scale | Upgraded from MENTIONED (scaling-and-efficiency 4.3.3). Static batching: batch of 8 requests runs until the longest finishes, wasting compute on completed requests. Continuous batching: when a request completes, its slot is immediately filled with the next request from the queue. Batch size stays constant, contents change dynamically. Restaurant waitlist analogy: as a table opens, the next party is seated immediately. Concrete waste calculation: 8 requests with varying lengths (20 to 200 tokens), 22.5% compute wasted in static vs near 0% in continuous. Pseudocode: InferenceServer class with slot management. Misconception addressed: continuous batching is NOT just using a bigger batch size -- the improvement is in slot utilization. |
| Expert parallelism (placing different MoE experts on different GPUs) | MENTIONED | training-and-serving-at-scale | Named as a specialized form of tensor parallelism where the router determines which GPU does the work per token. Not developed beyond the concept. |
| Ring attention / sequence parallelism | MENTIONED | training-and-serving-at-scale | Named as further parallelism techniques for long-context models. Not developed. |
| vLLM, TGI (inference serving frameworks) | MENTIONED | training-and-serving-at-scale | Named as frameworks implementing continuous batching and other serving optimizations. Not developed. |
| Gradient accumulation (accumulate gradients over microbatches before optimizer step) | MENTIONED | training-and-serving-at-scale | Named as related to microbatching in pipeline parallelism. Not developed. |

## Per-Lesson Summaries

### Lesson 1: Mixture of Experts (mixture-of-experts)

**Concepts taught:**
- Conditional computation -- not every parameter activates for every token (DEVELOPED)
- Router mechanism -- single linear layer + softmax + top-k, connected to attention's dot-product-softmax pattern (DEVELOPED)
- MoE architecture -- replacing monolithic FFN with N expert FFNs + router within the transformer block (DEVELOPED)
- Parameter-compute decoupling -- total parameters >> active parameters, Mixtral 8x7B as concrete example (DEVELOPED)
- Expert specialization -- emergent and per-token, not designed and per-topic (INTRODUCED)
- Load balancing / auxiliary loss -- positive feedback loop causing collapse, product-based penalty as solution concept (INTRODUCED)
- Mixtral 8x7B and DeepSeek-V3 as MoE examples (INTRODUCED)
- Memory tradeoff of MoE models (INTRODUCED)
- "Unconstrained optimization finds degenerate solutions" cross-concept pattern (INTRODUCED)

**Mental models established:**
- "Attention reads, the right experts write" -- extends the core transformer block mental model. MoE modifies only the "writes" half. Attention is untouched. The modification is targeted and minimal.
- "The library with specialist librarians" -- a dense FFN is one librarian who has read every book. An MoE FFN is a library with 8 specialist librarians + a front desk clerk (router) who directs questions to the 2 most relevant librarians. More total knowledge because each librarian only needs deep expertise in their specialty. The front desk clerk does a quick lookup (one matrix multiply + softmax), not deep analysis.
- "The dense transformer activates all knowledge for every token. MoE activates only the relevant knowledge. More total knowledge, same compute per token. The library got bigger, but you still only talk to two librarians." -- the summary mental model echo.

**Analogies used:**
- Library with specialist librarians (dense FFN = one generalist librarian, MoE = specialist librarians + front desk clerk/router)
- "Attention reads, the right experts write" (extends "attention reads, FFN writes")
- Router as the same dot-product + softmax pattern from attention, selecting experts instead of tokens
- "Scale parameters without scaling compute" (extends Chinchilla's "scale both, not just one")

**How concepts were taught:**
- **Recap:** Three facts from The Transformer Block reconnected: (1) "Attention reads, FFN writes," (2) FFN has ~2/3 of parameters, (3) FFN neurons as key-value memories (Geva et al.). Connected: "If 2/3 of the model's capacity is stored knowledge, and most stored knowledge is irrelevant to any given token, what happens during a forward pass?" Brief reconnection to Chinchilla: "Scale both, not just one. But what if you could scale parameters without proportionally scaling compute?"
- **Hook (waste problem):** GPT-3 activates all ~117B FFN parameters for every token. "The" fires the same parameters encoding French cooking, JavaScript syntax, and medieval history. Softened framing: "Consider how much of the FFN's stored knowledge could be relevant to any given token." GradientCard: "What if the model could activate only the knowledge it needs?"
- **Explain Part 1 (MoE Architecture):** Step-by-step walkthrough: same attention (unchanged), same residual stream (unchanged), FFN changes to N expert FFNs + router + top-k selection + weighted output. MoEArchitectureDiagram (inline SVG): side-by-side standard block vs MoE block, color-coded (shared violet, router amber, active experts emerald, inactive gray). Misconception 1 addressed immediately: "MoE models are NOT ensembles." Library analogy introduced.
- **Explain Part 2 (Router Mechanism):** Pseudocode showing router is one linear layer + softmax + top-k. Connected to attention: "This is dot-product + softmax + weighted sum." Misconception 4 addressed: "The router is simpler than a single attention head." "Of course" beat: three established facts combine into MoE insight.
- **Explain Part 3 (Parameter-Compute Decoupling):** Misconception 3 addressed: "More parameters does not always mean proportionally more compute." Mixtral 8x7B walkthrough: 8 experts, top-2, ~47B total, ~13B active. Three-way comparison table (13B dense, Mixtral, 47B dense). Connection to Chinchilla: "MoE adds a third option -- scale total parameters without scaling per-token compute." Memory tradeoff: all 47B must be loaded.
- **Check 1 (predict-and-verify):** 8 experts, top-1 routing, French translation prompt. Predict: how many experts per token, do all tokens activate same expert, what happens to other 7, compute comparison. Details/summary reveal.
- **Explain Part 4 (Expert Specialization):** PerTokenRoutingDiagram (inline SVG): "The mitochondria is the powerhouse of the cell" with per-token expert assignments. Function words cluster (E2), science words cluster (E5), domain word routes differently (E7). Misconception 2 addressed: specialization is emergent and per-token, not designed and per-topic.
- **Elaborate (Load Balancing):** RouterCollapseDiagram (inline SVG): positive feedback loop visualization (Expert 3 starts slightly better -> gets more tokens -> gets more gradients -> gets even better -> collapse). Misconception 5 addressed: load imbalance is a training collapse problem, not just an efficiency problem. Auxiliary loss conceptual description: measures per-expert token fraction and average router probability, product-based penalty. Connection to RLHF: "unconstrained optimization finds degenerate solutions" pattern.
- **Check 2 (transfer question):** Company choosing between 13B dense and 47B MoE for 1T tokens, fixed compute budget. Predict: compute, knowledge, performance, tradeoff. Details/summary reveal.

**Visual elements:**
- MoEArchitectureDiagram (inline SVG): Side-by-side standard transformer block vs MoE block. Color-coded: shared components (violet), router (amber), active experts (emerald), inactive experts (gray dashed). Dashed vertical divider. Legend at bottom.
- PerTokenRoutingDiagram (inline SVG): "The mitochondria is the powerhouse of the cell" with tokens color-coded by expert assignment. Observation boxes analyzing routing patterns (function words -> E2, science -> E5, domain -> E7). Bottom annotation: "Specialization is emergent and per-token, not designed and per-topic."
- RouterCollapseDiagram (inline SVG): Three-step positive feedback loop (starts better -> gets more tokens -> gets more gradients) with curved feedback arrow. Result box: "Expert 3 handles 80%+ of tokens."
- GradientCards: waste problem (orange), misconception 1 "just ensembles" (rose), library analogy (cyan), misconception 4 "complex router" (rose), "of course" beat (violet), misconception 3 "more params = more compute" (rose), misconception 2 "topic specialists" (rose), misconception 5 "just efficiency" (rose), mental model echo (violet), both checkpoints (emerald)
- ComparisonRow: Dense Scaling (amber) vs MoE Scaling (emerald), 4 items each
- Three-way comparison table: 13B dense, Mixtral 8x7B, 47B dense (active params, total params, per-token compute)
- CodeBlock: Router pseudocode (W_router @ hidden_state -> softmax -> top_k -> weighted sum)

**What is NOT covered:**
- Implementing MoE in code (notebook exercises use toy routing on small proxies)
- Training an MoE model from scratch
- Token dropping or capacity factors in detail
- Switch Transformer or other historical MoE variants in depth
- Communication overhead of distributing experts across GPUs (deferred to Lesson 3: training-and-serving-at-scale)
- Comparing specific benchmark results
- DeepSeek-V3 architecture in detail (named in references only)

**Notebook:** `notebooks/5-3-1-mixture-of-experts.ipynb` (4 exercises)
- Exercise 1 (Guided): Implement a simple router. Build a router for 4 experts on d_model=64. Apply softmax, select top-2, visualize router probabilities as bar chart for 5 inputs. Insight: router naturally produces different selections for different inputs.
- Exercise 2 (Supported): MoE forward pass. Build complete MoE layer: 4 expert FFNs (d_model=64, d_ff=256) + router. Forward pass for batch of 8 tokens. Compare to dense FFN. Count active vs total parameters. Helper cell provides complete MoELayer after solution reveal for use by later exercises.
- Exercise 3 (Supported): Visualize expert routing on real text. Tokenize sentences, visualize per-token expert assignments with color coding. Look for patterns in function vs domain words.
- Exercise 4 (Independent): Router collapse experiment. Train toy MoE with and without auxiliary load-balancing loss. Track expert utilization over training steps. Plot token distribution at step 0, 100, 500, 1000. Observe collapse without loss, stability with it.

**Review:** Passed at iteration 2/3. Iteration 1 had 1 critical (notebook Exercise 4 dependency on Exercise 2's unfinished implementation), 4 improvement (objective block repeats header, auxiliary loss detail missing for INTRODUCED depth, unsourced "10-20%" claim in hook, reward hacking connection too loose), and 3 polish (notebook spaced em dashes, PerTokenRoutingDiagram clean specialty labels, identical checkpoint reveal prompts). All critical and improvement findings resolved. Iteration 2 had 0 critical, 0 improvement, 2 polish (notebook code comments still use spaced em dashes, diagram observation box specialty labels subtly contradict "emergent not designed" message). Clean pass.

### Lesson 2: Long Context & Efficient Attention (long-context-and-efficient-attention)

**Concepts taught:**
- Three barriers to long context--position encoding limits, quadratic compute, KV cache memory (DEVELOPED)
- RoPE mechanism--rotation in 2D subspaces encoding relative position in the Q/K dot product (DEVELOPED)
- Context extension via RoPE--relative position patterns transfer to longer sequences (INTRODUCED)
- Quadratic attention bottleneck--concrete cost calculations at increasing sequence lengths (DEVELOPED)
- Sliding window attention--each token attends to at most w preceding tokens, O(n*w) compute (DEVELOPED)
- Stacked-layer information propagation--local attention approximates global through residual stream across layers (DEVELOPED)
- Dilated attention--attend to every kth token, same compute budget covering wider range (DEVELOPED)
- Linear attention--kernel trick reformulation, O(n) compute concept (INTRODUCED)
- GQA--sharing K/V heads across Q head groups, KV cache compression (DEVELOPED)
- MHA-GQA-MQA spectrum--smooth tradeoff between KV cache memory and K/V independence (DEVELOPED)
- NTK-aware scaling and YaRN context extension techniques (MENTIONED)
- ALiBi as alternative positional encoding (MENTIONED)

**Mental models established:**
- "Position in the handshake, not the nametag"--sinusoidal/learned PE puts position on the nametag (the embedding, added before attention). RoPE encodes position in the handshake (the Q/K dot product, the interaction between two tokens). Two people 3 seats apart feel the same handshake regardless of which seats they are in. The handshake depends on relative distance, not absolute position.
- "Compute where attention concentrates, not everywhere"--most attention weight is local in trained models. Sparse attention skips distant computations that receive near-zero weight. The same conditional computation principle from MoE applied to attention.
- "Cache what's needed, not everything"--K/V across heads are often similar enough to share. GQA preserves query diversity while reducing KV memory. Not fewer heads, but selective sharing of the KV component.
- "Three barriers, three targeted solutions"--position (RoPE), compute (sparse attention), memory (GQA). Independent bottlenecks, independent solutions, combine in practice (LLaMA, Mistral).

**Analogies used:**
- Nametag vs handshake (additive PE vs RoPE: position in the embedding vs position in the dot product)
- "Clock with many hands" callback from sinusoidal PE--RoPE uses the same multi-frequency structure, but the hands rotate Q/K vectors instead of adding to the embedding
- "Split, not multiplied" extension--GQA takes this further from multi-head attention: split Q into many heads for diversity, share K/V across groups for memory efficiency
- Conditional computation callback from MoE--"not every token pair needs to compute an attention score" parallels "not every parameter needs to activate for every token"

**How concepts were taught:**
- **Recap:** Three facts from prior lessons reconnected: (1) attention formula "every token computes a score with every other token" (4.2.1), (2) positional encoding: sinusoidal adds position to embeddings, learned PE can't generalize beyond training length, DNA transfer question callback (4.1.3), (3) flash attention fixes memory not compute, KV caching costs memory per head per token (4.3.3). Connect: "What happens when we push attention to 128K tokens? Three things break."
- **Hook (three walls):** ThreeBarrierDiagram (inline SVG) showing three bottlenecks with solutions. Wall 1 Position: learned PE has no embedding for position 4097 at max_seq_len=4096. Wall 2 Compute: concrete FLOPs table showing 16,384x increase from 1K to 128K. Wall 3 Memory: KV cache for 70B model at 128K is ~332 GB, larger than 140 GB model weights. GradientCard: "Three walls. Three solutions."
- **Explain Part 1 (RoPE):** Problem-first: existing PE adds position before Q/K projection. What if position entered the dot product directly? Key insight: rotate Q and K by position-proportional angles, dot product depends on relative position. Nametag-vs-handshake analogy. RoPERotationDiagram (inline SVG): positions 3/7 vs 100/104, same angular difference 4theta. Code block (apply_rope function) presented before rotation matrix formula (concrete before abstract). Multi-frequency structure connected to "clock with many hands." Misconception addressed: "RoPE is not just another PE scheme"--additive vs multiplicative, input vs dot product. Context extension: RoPE gives something to extend, learned PE gives nothing.
- **Check 1 (predict-and-verify):** Two models (learned PE vs RoPE), both trained at 4K, asked to process 8K document. Predict what happens at position 4097 for each. Details/summary reveal.
- **Explain Part 2 (Sparse Attention):** Transition from MoE conditional computation. AttentionPatternDiagram (inline SVG): full causal, sliding window, dilated side by side with score counts. Sliding window: same masking mechanism as causal (set to -inf before softmax), diagonal band pattern, O(n*w). "Of course" beat grounded in head specialization from Module 4.2. Misconception addressed: stacked-layer information propagation through residual stream (13 layers * 4096 window = ~53K reach). Dilated attention described on its own terms. Linear attention: plain-language motivation first ("compute a summary of K and V first"), then symbolic confirmation, tradeoff stated.
- **Explain Part 3 (GQA):** Transition from compute to memory wall. Recall multi-head attention: each head stores own K/V cache. GQA insight: Q needs diversity, K/V can share. Misconception addressed: "GQA is not just fewer heads"--all Q heads remain, only K/V sharing changes. Four-row comparison table (MHA, GQA, MQA, fewer heads). ComparisonRow: LLaMA 2 70B MHA ~332 GB vs GQA ~42 GB (8x reduction). MHA-GQA-MQA spectrum as smooth tradeoff. Connection to "split, not multiplied."
- **Check 2 (transfer question):** Design architecture changes for 7B model to handle 64K legal documents (learned PE at 4K, full MHA, no sparse attention). Student applies three-barrier framework. Details/summary reveal.

**Visual elements:**
- ThreeBarrierDiagram (inline SVG): Three barriers (position/amber, compute/rose, memory/blue) with solutions below (RoPE, sparse attention, GQA/emerald). Roadmap for the lesson.
- RoPERotationDiagram (inline SVG): Two unit circles showing Q/K vectors rotated at positions 3/7 (left) and 100/104 (right). Same angular difference 4theta in both. Equals sign between. Bottom box: "Same angular difference -> same dot product contribution from position."
- AttentionPatternDiagram (inline SVG): Three 10x10 grids showing full causal (55 scores), sliding window w=3 (24 scores), and dilated (varies). Axis labels for query/key position. Summary box with concrete 128K comparison.
- GradientCards: Three walls (amber, rose, blue), three-walls summary (emerald), nametag-vs-handshake analogy (cyan), RoPE misconception (rose), "of course" beat (violet), sparse attention misconception (rose), GQA misconception (rose), both checkpoints (emerald), mental model echo (violet)
- ComparisonRow: MHA 64 KV heads (amber) vs GQA 8 KV groups (emerald), with concrete LLaMA 2 70B cache calculations
- Four-row comparison table: MHA, GQA, MQA, fewer heads (Q heads, KV heads, KV cache, query diversity)
- Concrete FLOPs table: 1K, 4K, 32K, 128K tokens with attention FLOPs per layer and factor vs 1K
- CodeBlock: apply_rope Python implementation
- BlockMath: RoPE rotation matrix R(m, theta_d), relative position dot product property q_rot^T k_rot = q^T R((i-j)*theta) k

**What is NOT covered:**
- Implementing RoPE, sparse attention, or GQA in production code (notebook demos concepts on small proxies)
- NTK-aware scaling or YaRN extension formulas in detail (mentioned only)
- ALiBi in depth (named as alternative only)
- Ring attention or sequence parallelism (deferred to Lesson 3: training-and-serving-at-scale)
- Flash attention implementation details (already INTRODUCED in 4.3.3)
- State space models (SSMs) or Mamba
- Multi-Head Latent Attention (MLA) in detail
- Benchmarking or specific model performance comparisons

**Notebook:** `notebooks/5-3-2-long-context-and-efficient-attention.ipynb` (4 exercises)
- Exercise 1 (Guided): RoPE rotation on 2D vectors. Implement 2D rotation matrix. Rotate pairs of vectors at different absolute positions but same relative distance. Compute dot products and verify relative position property. Predict-before-run: "Will the dot product change if we shift both positions by 1000?" Insight: position enters the dot product through rotation, and the result depends on relative distance.
- Exercise 2 (Supported): Sparse attention mask patterns. Implement full causal, sliding window, and dilated attention masks for 64-token sequence. Visualize as heatmaps. Compare computed entry counts. Extend to 256 tokens and plot compute scaling. Insight: sparse patterns dramatically reduce computed attention scores while preserving local structure.
- Exercise 3 (Supported): GQA forward pass. Implement GQA layer with h_Q=8, h_KV=2 (4 Q heads per KV group). Forward pass on batch of 16 tokens, d_model=64. Compare output shape to standard MHA. Count KV cache parameters: MHA (8 KV heads) vs GQA (2 KV groups). Insight: GQA produces same output shape as MHA with 4x fewer KV parameters.
- Exercise 4 (Independent): Attention cost calculator. Build function computing total attention FLOPs and KV cache memory for given model configuration. Compute costs for GPT-2 at 1K, LLaMA 2 70B with MHA at 4K, with GQA at 4K, with GQA at 128K, and with sliding window at 128K. Present results in table. Insight: three optimizations address different bottlenecks and their benefits compound.

**Review:** Passed at iteration 2/3. Iteration 1 had 0 critical, 5 improvement (linear attention drops student with cold formula, notebook Exercise 4 does not match plan, Exercise 4 solution references wrong functions, dilated attention references untaught dilated convolutions, RoPE formula before code violates concrete-before-abstract), and 4 polish (spaced em dashes in TSX SVG text, notebook markdown, component comments, notebook intro mismatch). All improvement findings resolved. Iteration 2 had 0 critical, 0 improvement, 2 polish (Exercise 4 solution inside markdown details block without runnable cell, Exercise 4 solution duplicates takeaways). Clean pass.

### Lesson 3: Training & Serving at Scale (training-and-serving-at-scale)

**Concepts taught:**
- Data parallelism -- replicate full model on each GPU, split training data, all-reduce gradients (DEVELOPED)
- Tensor parallelism -- split weight matrices within layers across GPUs, both GPUs compute simultaneously (DEVELOPED)
- Pipeline parallelism -- assign different layers to different GPUs, microbatch flow through stages (DEVELOPED)
- Communication overhead as the central constraint shaping all parallelism design (DEVELOPED)
- ZeRO optimizer state sharding -- each GPU stores only a fraction of optimizer states (INTRODUCED)
- Speculative decoding -- draft-verify loop, upgraded from MENTIONED to DEVELOPED
- Continuous batching -- dynamically fill completed request slots, upgraded from MENTIONED to DEVELOPED
- Expert parallelism, ring attention, sequence parallelism, vLLM, TGI, gradient accumulation (MENTIONED)

**Mental models established:**
- "Communication is the constraint" -- every parallelism strategy is a tradeoff between computation distribution and communication cost. Extends the "delivery truck" analogy from scaling-and-efficiency to inter-GPU scale: "the delivery truck now drives between buildings, not between floors."
- "The math is elegant; the engineering makes it work" (module-level echo) -- every technique in this lesson answers the same question: how do you distribute work across devices when communication is expensive? Data parallelism distributes data. Tensor parallelism distributes computation. Pipeline parallelism distributes layers. ZeRO distributes memory. Speculative decoding distributes generation steps. Continuous batching distributes serving capacity. The bottleneck determines the solution.
- "Two walls" framing -- training: the model does not fit on one GPU. Inference: the model generates one token at a time. Different problems, different solutions. This framing organizes the entire lesson into two halves.

**Analogies used:**
- "Delivery truck between buildings" -- extends the compute-bound/memory-bound delivery truck analogy to inter-GPU communication. NVLink is fast in absolute terms but orders of magnitude slower than on-chip compute.
- "Assembly line" for pipeline parallelism -- each station processes one step; inefficient for one item, fast when the line is full. Caveat: if stages are unbalanced, some GPUs wait (pipeline bubble).
- "Rough draft and editor" for speculative decoding -- fast writer produces draft, careful editor verifies in parallel. Speed comes from parallel verification, not from the draft being fast.
- "Restaurant with a waitlist" for continuous batching -- as a table opens, the next party is seated immediately rather than waiting for all tables to clear.

**How concepts were taught:**
- **Recap:** Three facts reconnected: (1) training memory breakdown from lora-and-quantization (7B model needs ~84 GB, optimizer states are 2/3), (2) generate() method from building-nanogpt (each token is one forward pass, sequential by nature), (3) compute-bound vs memory-bound from scaling-and-efficiency (moving data is slower than computing on it). Connected: "What happens at 70B parameters? 70B x 12 bytes = 840 GB. Your A100 has 80 GB. You cannot even start training."
- **Hook (two walls):** Concrete arithmetic: 70B model, training memory = 840 GB vs 80 GB GPU. Second punch: even if trained, serving means sequential generation, 1000 concurrent users at 100 tokens each = 100,000 sequential forward passes. GradientCard (orange): "Two walls. Training: the model does not fit on one GPU. Inference: the model generates one token at a time."
- **Explain Part 1 (Training Parallelism):** Data parallelism first (GPT-2 124M on 4 GPUs, all-reduce pseudocode). Rose GradientCard addressing Misconception 1 (data parallelism requires full model on every GPU). Communication constraint pivot with concrete numbers (312 TFLOPS vs 600 GB/s NVLink, 530 billion wasted ops per 1 GB transfer). Rose GradientCard addressing Misconception 4 (40-60% time waiting for communication at 175B scale). "Of course" callback to delivery truck analogy. Tensor parallelism (split linear layer column-wise, expert parallelism MENTIONED). Pipeline parallelism (assembly line analogy, pipeline bubble negative example with 4 stages at 75% idle, microbatching solution). Side-by-side comparison table (5 properties across 3 strategies). Rose GradientCard addressing Misconception 2 (tensor vs pipeline are different granularity, different communication patterns).
- **Check 1 (predict-and-verify):** 30B dense model on 4 A100s. Can you use data parallelism? Can you use pipeline parallelism with 4 stages? What combination works?
- **Explain Part 2 (ZeRO):** Transition from "data parallelism requires full model, but 70B doesn't fit." ZeRO Stage 1 arithmetic: 70B on 8 GPUs, per-GPU memory drops from ~840 GB to ~350 GB. ComparisonRow: without ZeRO vs with ZeRO Stage 1. Three stages MENTIONED. DeepSpeed and FSDP MENTIONED.
- **Explain Part 3 (Inference):** Speculative decoding developed from MENTIONED. Callback to generate() method. Key insight grounded in compute-bound/memory-bound framework: forward pass cost is dominated by weight matrix dimensions (d_model, d_vocab), not batch dimension. Draft-verify mechanism with pseudocode. Worked example: "The capital of France is" with 5 drafted, 3 accepted. Rose GradientCard addressing Misconception 3. Continuous batching developed from MENTIONED. Static batching waste calculation (8 requests, varying lengths, 22.5% wasted). Restaurant waitlist analogy. Mechanism paragraph bridging analogy to pseudocode. InferenceServer class pseudocode. Rose GradientCard addressing Misconception 5.
- **Check 2 (transfer question):** Mixtral 8x7B for customer service chatbot on 8 A100s. Why does MoE make parallelism important? Would speculative decoding work? What role does continuous batching play?
- **Elaborate (frontier model combinations):** LLaMA 2 70B training: tensor + pipeline + data + ZeRO. Mixtral serving: expert parallelism + continuous batching + speculative decoding. Key insight: each technique addresses a specific bottleneck, the art is identifying which bottleneck dominates.
- **Module completion:** ModuleCompleteBlock summarizing the three-lesson arc from MoE through long-context to training/serving infrastructure.

**Visual elements:**
- ParallelismComparisonDiagram (inline SVG): Three-panel showing data parallelism (4 GPUs with full models, split data, all-reduce), tensor parallelism (2 GPUs with split layers, all-reduce within layer), pipeline parallelism (4 GPUs with sequential stages, microbatch flow). Color-coded: emerald (data), violet (tensor), amber (pipeline), rose (communication).
- PipelineBubbleDiagram (inline SVG): Two-panel timeline. Left: 1 microbatch, 4 stages, 75% idle (gray). Right: 4 microbatches color-coded filling the pipeline, near-full utilization.
- SpeculativeDecodingDiagram (inline SVG): Draft model producing 5 tokens, large model verifying in one forward pass, accept/reject visualization with emerald (accepted) and rose (rejected) coloring.
- ContinuousBatchingDiagram (inline SVG): Two-panel timeline. Static batching (top): 8 slots, completed requests shown as gray wasted space. Continuous batching (bottom): completed slots immediately filled with new requests from queue.
- GradientCards: two walls hook (orange), data parallelism limitation (rose, Misconception 1), communication constraint (rose, Misconception 4), "of course" callback (violet), tensor vs pipeline distinction (rose, Misconception 2), ZeRO insight (violet), speculative decoding mechanism (rose, Misconception 3), continuous batching mechanism (rose, Misconception 5), mental model echo (violet), both checkpoints (emerald)
- ComparisonRow: Without ZeRO (amber) vs With ZeRO Stage 1 (emerald), 4 items each
- Side-by-side comparison table: 5 properties across data, tensor, and pipeline parallelism
- CodeBlock: all-reduce pseudocode, speculative decoding draft-verify loop, InferenceServer class pseudocode

**What is NOT covered:**
- Implementing any parallelism strategy in code (no PyTorch distributed, FSDP, DeepSpeed)
- NCCL, MPI, or communication primitives
- Specific hardware interconnect details beyond the essential insight (NVLink bandwidth as a number for context, not a deep dive)
- Ring attention or sequence parallelism in depth (MENTIONED only)
- Expert parallelism for MoE in detail (MENTIONED only)
- Model architecture search or optimal parallelism configuration
- Quantized inference serving (already INTRODUCED in lora-and-quantization)
- vLLM, TGI, or specific serving frameworks in depth (MENTIONED only)
- Gradient accumulation in depth (MENTIONED as related to microbatching)

**Notebook:** `notebooks/5-3-3-training-and-serving-at-scale.ipynb` (4 exercises)
- Exercise 1 (Guided): Training memory calculator. Build a function computing training memory for a given model configuration (num_params, precision, optimizer). Apply to GPT-2 (124M), LLaMA 7B, LLaMA 70B. Compare single-GPU requirement to A100 capacity. Calculate ZeRO Stage 1 savings across 1, 2, 4, 8 GPUs. Predict-before-run: "Will ZeRO Stage 1 alone make 70B fit on 8 A100s?" Insight: optimizer sharding alone is not enough for very large models, but dramatically extends what data parallelism can handle.
- Exercise 2 (Supported): Speculative decoding simulation. Simulate the draft-and-verify mechanism with configurable match probability. Measure acceptance rate at K=1, 3, 5, 8. Plot acceptance rate vs K. Measure wall-clock time per accepted token. Insight: acceptance rate decreases with K, and there is an optimal draft length balancing verification overhead against expected accepted tokens.
- Exercise 3 (Supported): Continuous batching simulation. Simulate an inference server: queue of 50 requests with varying target lengths (mean 80, std 60, min 10, max 300). Implement static batching (batch_size=8, wait for all to finish) and continuous batching (batch_size=8, fill completed slots). Measure total time, average latency, GPU utilization. Plot utilization over time for both strategies. Insight: continuous batching achieves near-100% utilization while static batching degrades as short requests complete.
- Exercise 4 (Independent): Parallelism strategy advisor. Given model size, number of GPUs, and GPU memory, determine which parallelism strategy or combination is needed. Build a function that outputs: can use data parallelism alone? Minimum tensor parallelism degree? Pipeline parallelism stages? ZeRO stage recommendation? Test on 5 configurations from GPT-2 (124M, 1 GPU) to hypothetical 175B (64 GPUs). Insight: the choice of parallelism strategy is determined by whether the bottleneck is compute, memory, or communication.

**Review:** Passed at iteration 2/3. Iteration 1 had 1 critical (Misconception 4 "communication between GPUs is fast" not addressed with explicit misconception framing), 4 improvement (Misconception 1 not explicit, speculative decoding forward pass cost not grounded in compute-bound framework, continuous batching mechanism paragraph too brief before pseudocode, notebook Exercise 2 description in TSX says GPT-2 models but notebook uses simulation), and 3 polish (spaced em dashes in SVG text, notebook double hyphens in markdown, CONSOLIDATE Lesson internal terminology in aside). All critical and improvement findings resolved. Iteration 2 had 0 critical, 0 improvement, 2 polish (exercises aside still references "small GPT-2 models" for Exercise 2, notebook code cells use double hyphens in print statements). Clean pass.
