# Lesson Plan: Chain-of-Thought Reasoning

**Module:** 5.2 (Reasoning & In-Context Learning)
**Position:** Lesson 3 of 4
**Slug:** `chain-of-thought`
**Status:** Planning complete

---

## Phase 1: Orient (Student State)

### Relevant Concepts the Student Has

| Concept | Depth | Source | Notes |
|---------|-------|--------|-------|
| Autoregressive generation as feedback loop (sample, append, repeat) | DEVELOPED | what-is-a-language-model (4.1.1), building-nanogpt (4.3.1) | Core prerequisite. The student understands the loop: model generates one token, that token is appended to the context, the model runs another forward pass on the extended context. Implemented as the `generate()` method in code. This is the mechanical basis for why CoT works: each generated token feeds back as input for the next forward pass. |
| Attention as data-dependent weighted average (weights computed from input, not fixed parameters) | DEVELOPED | the-problem-attention-solves (4.2.1), reinforced in in-context-learning (5.2.1) | The student understands that attention weights are computed freshly from each input. Relevant because intermediate reasoning tokens change the context, which changes the attention pattern for subsequent tokens. |
| In-context learning as gradient-free task learning from examples in the prompt | DEVELOPED | in-context-learning (5.2.1) | The student understands that examples in the prompt steer the model's behavior through attention without weight updates. CoT can be prompted via few-shot examples showing reasoning steps. |
| "The prompt is a program; attention is the interpreter" | DEVELOPED (mental model) | in-context-learning (5.2.1), extended in prompt-engineering (5.2.2) | Central mental model. Extends naturally: CoT is writing a program that executes step by step, with each step feeding back as additional context for the next. |
| Prompt engineering as structured programming (systematic prompt construction) | DEVELOPED | prompt-engineering (5.2.2) | The student can construct structured prompts with components (role, format, examples, context). CoT is a specific prompting technique that fits into this framework. |
| Transformer block: "Attention reads, FFN writes" with fixed architecture per layer | DEVELOPED | the-transformer-block (4.2.5) | Each forward pass runs through the same N transformer blocks with the same operations. The computation per token is architecturally fixed: same number of layers, same dimension, same operations. This is the mechanical constraint CoT addresses. |
| Full GPT architecture end-to-end (embedding -> N blocks -> output projection) | DEVELOPED | decoder-only-transformers (4.2.6), building-nanogpt (4.3.1) | The student built the full architecture in code. They can trace a forward pass. This grounds the "fixed computation per forward pass" insight: the same 12 (or N) blocks run for every single token prediction, regardless of problem difficulty. |
| Causal masking (lower-triangular attention, each token sees only past) | DEVELOPED | decoder-only-transformers (4.2.6) | Relevant because intermediate reasoning tokens generated by CoT become part of the context for subsequent tokens. Each new token can attend to all previous reasoning tokens. |
| ICL limitations: ordering sensitivity, format fragility, context window constraints | INTRODUCED | in-context-learning (5.2.1) | Student knows ICL is powerful but fragile. CoT introduces a different kind of limitation: the quality of the reasoning chain depends on each intermediate step being correct. |
| Format specification and output constraints | DEVELOPED | prompt-engineering (5.2.2) | Student understands that format tokens in the prompt constrain output. Relevant because CoT often involves structured reasoning formats (e.g., "Step 1: ..., Step 2: ..."). |

### Established Mental Models and Analogies

- "The prompt is a program; attention is the interpreter. Prompt engineering is writing better programs." (5.2.1, 5.2.2)
- "Attention is data-dependent. Examples are data. Of course examples influence the output." (5.2.1)
- "Between retrieval and comprehension"---ICL is not memorization and not understanding; it is attention-based computation (5.2.1)
- "Autoregressive generation is a feedback loop: outputs become inputs" (4.1.1)
- "Attention reads, FFN writes" (4.2.5)
- "SFT teaches format, not knowledge" (4.4.2)
- "The formula IS the code"---transformer block forward() directly implements the formula (4.3.1)
- "Temperature changes sampling, not knowledge" (4.1.1)

### What Was Explicitly NOT Covered

- Chain-of-thought or step-by-step reasoning was explicitly deferred from both in-context-learning (5.2.1) and prompt-engineering (5.2.2)
- The "Next Step" section of prompt-engineering explicitly set this up: "there is a class of problems where even the best-structured prompt fails: problems that require more computation than a single forward pass provides"
- Reasoning models and test-time compute were deferred to Lesson 4 (reasoning-models)
- Process supervision vs outcome supervision has not been discussed
- The concept of "tokens as computation" (each generated token giving the model another forward pass worth of compute) has not been introduced

### Readiness Assessment

The student is well prepared. They have the two critical prerequisites at DEVELOPED depth: (1) autoregressive generation as a feedback loop---each generated token becomes input for the next forward pass, and (2) the fixed architecture of the transformer---every token prediction runs through the same N blocks regardless of problem difficulty. They have never been asked to connect these two ideas: that the feedback loop is not just a generation mechanism but a *computational amplifier* that gives the model more forward passes per problem. The lesson's core insight---that each intermediate token generated during CoT provides an additional forward pass worth of computation---is genuinely new but will click naturally because the student already understands both pieces. The prompt-engineering lesson's "Next Step" section explicitly motivated this lesson by naming the limitation CoT addresses: "problems that require more computation than a single forward pass provides."

---

## Phase 2: Analyze

### Target Concept

This lesson teaches the student to explain why chain-of-thought prompting works mechanically---that intermediate reasoning tokens give the model additional forward passes worth of computation, expanding its effective computational capacity beyond what a single forward pass provides---and to identify when CoT helps vs when it does not.

### Prerequisites Table

| Concept | Required Depth | Actual Depth | Source Lesson | Status | Reasoning |
|---------|---------------|-------------|---------------|--------|-----------|
| Autoregressive generation as feedback loop (sample, append, repeat) | DEVELOPED | DEVELOPED | what-is-a-language-model (4.1.1), building-nanogpt (4.3.1) | OK | The core mechanism of CoT. Each reasoning token is generated, appended, and becomes input for the next forward pass. The student must deeply understand this loop to see that intermediate tokens give the model more computation. They implemented generate() in code. |
| Transformer block as fixed-architecture computation (N blocks, same operations per token) | DEVELOPED | DEVELOPED | the-transformer-block (4.2.5), building-nanogpt (4.3.1) | OK | The student must understand that the same number of layers runs for every token prediction. This is the constraint CoT addresses: the model gets the same compute budget per token regardless of problem difficulty. |
| In-context learning (examples in prompt steer behavior through attention) | INTRODUCED | DEVELOPED | in-context-learning (5.2.1) | OK | CoT can be prompted via few-shot ICL (show reasoning examples). The student needs to understand ICL at INTRODUCED depth to grasp few-shot CoT prompting. They have it at DEVELOPED. |
| Attention as data-dependent computation | INTRODUCED | DEVELOPED | the-problem-attention-solves (4.2.1) | OK | Intermediate tokens change the context, changing attention patterns for subsequent tokens. The student must understand that attention operates on the entire context including previously generated tokens. |
| Prompt engineering as structured programming | INTRODUCED | DEVELOPED | prompt-engineering (5.2.2) | OK | CoT is a prompting technique. The student should situate it within the "prompt is a program" framework. Only INTRODUCED depth needed; they have DEVELOPED. |
| Causal masking | INTRODUCED | DEVELOPED | decoder-only-transformers (4.2.6) | OK | Each generated reasoning token can attend to all previous reasoning tokens but not future ones. This explains why the order of reasoning steps matters. |

All prerequisites are met at sufficient depth. No gaps.

### Gap Resolution

No gaps to resolve. All prerequisites are at sufficient or above-sufficient depth.

### Misconceptions Table

| Misconception | Why They'd Think This | Negative Example That Disproves It | Where to Address |
|---------------|----------------------|-----------------------------------|-----------------|
| **"CoT works because the model 'thinks' or 'reasons' when generating intermediate tokens"** | The anthropomorphic framing is almost irresistible. The model appears to "think through" the problem. The output reads like human reasoning. The student has been trained to resist anthropomorphic framing (ICL lesson addressed "the model understands"), but CoT makes it harder because the output explicitly looks like thinking. | The model generates each token via the same fixed forward pass: embedding -> N transformer blocks -> output projection. There is no "thinking mode" that activates. The model predicts the next token given the context, and each predicted token happens to be a reasoning step because that is what is most likely given the prompt and context so far. If you replace the correct intermediate steps with plausible-sounding but logically wrong steps, the model will continue from the wrong steps and produce a wrong answer---it is not "checking its reasoning," it is continuing from whatever context exists. | Section 5 (Explain), immediately after presenting the core mechanism. Name the misconception explicitly before it takes hold. |
| **"CoT helps because the model is 'showing its work,' not because it changes the computation"** | The student may think CoT is about transparency/interpretability---the model is exposing the reasoning it would have done internally anyway. This frames CoT as a display technique, not a computational one. By analogy: a student who writes out their work on an exam vs one who does it in their head---same reasoning, different visibility. | If CoT were just showing internal reasoning, the model should get the same answer with or without CoT. It does not. For multi-step arithmetic (e.g., 17 x 24), asking for the direct answer often produces a wrong result, while asking for step-by-step reasoning produces the correct result. The intermediate tokens are not exposing hidden computation---they are enabling computation that could not happen in a single forward pass. The model with direct answer has exactly N transformer blocks of computation; with CoT, it has N blocks times the number of intermediate tokens. | Section 5 (Explain), as part of the core argument. The arithmetic example is the clearest demonstration: same model, same weights, dramatically different accuracy based solely on whether intermediate tokens are generated. |
| **"CoT always helps---you should always ask for step-by-step reasoning"** | The student has seen CoT improve performance on reasoning tasks. Natural overgeneralization: if it helps sometimes, it helps always. Popular prompting advice reinforces this ("always add 'let's think step by step'"). | CoT does NOT help (and can hurt) on tasks that fit comfortably within a single forward pass: simple factual recall ("What is the capital of France?"), sentiment classification, basic text completion. Adding "let's think step by step" to "What is the capital of France?" produces unnecessary reasoning tokens that waste context window and can introduce errors through overthinking. CoT helps when the problem requires more computation than one forward pass provides---multi-step reasoning, arithmetic, planning, constraint satisfaction. The criterion is computational complexity, not difficulty in the human sense. | Section 7 (Elaborate), after the core mechanism is established. Show CoT hurting or not helping on a simple task. |
| **"Longer chains of thought are always better---more tokens = more computation = better answers"** | If each token provides an additional forward pass, then more tokens should always be better. Monotonic improvement with chain length. | Show an example where a very long, verbose chain of thought introduces errors, goes off track, or includes irrelevant reasoning that dilutes the attention to the relevant intermediate results. A concise 3-step chain can outperform a rambling 15-step chain because the quality of intermediate tokens matters, not just the quantity. Each token must provide useful intermediate results that subsequent tokens can build on. Random or irrelevant tokens do not help even though they technically add forward passes---the additional context they create is noise, not signal. Connected to the ICL finding that more examples do not always help and the RAG finding that irrelevant context dilutes attention. | Section 7 (Elaborate), alongside misconception 3. Both address the "more = better" overgeneralization from different angles. |
| **"CoT is a fundamentally new capability that emerged in large models"** | The student may think CoT is a mysterious emergent ability that appears at scale. Popular narrative frames CoT as an "emergence" phenomenon. This framing makes CoT feel magical rather than mechanistic. | CoT is not a new mechanism. It is the autoregressive feedback loop the student already understands, applied to reasoning. The same generate() method runs. The same forward pass happens. The "emergence" is that large models generate coherent intermediate reasoning steps (because they have learned reasoning patterns from training data), and those coherent steps happen to provide useful context for subsequent forward passes. Small models fail at CoT not because they lack the mechanism (they have the same autoregressive loop) but because they cannot generate useful intermediate steps. The mechanism is universal; the usefulness depends on the model's ability to generate helpful intermediate tokens. | Section 5 or 7, after the mechanism is clear. Callback to "the formula has not changed; the context has" from in-context-learning. |

### Examples Planned

| Example | Type | Purpose | Why This Example |
|---------|------|---------|-----------------|
| **Multi-step arithmetic: 17 x 24 with and without CoT** | Positive | First example: simplest possible demonstration that intermediate tokens change the answer. Direct answer often wrong; step-by-step (17 x 20 = 340, 17 x 4 = 68, 340 + 68 = 408) gets it right. | Arithmetic is unambiguous---the answer is either right or wrong. No subjectivity. The student can verify the answer themselves. The multi-step nature makes it clear that a single forward pass (which must map "17 x 24" directly to "408") is asking one forward pass to do what humans need multiple steps for. The decomposition into sub-problems is visible. |
| **Word problem requiring multi-step reasoning** (e.g., "If a store has 3 shelves with 8 books each and removes 5 books, how many are left?") | Positive | Second example: shows CoT on a slightly more complex problem that requires extracting information, performing multiple operations, and combining results. Confirms the pattern generalizes beyond pure arithmetic. | Different enough from arithmetic to prevent the student from thinking CoT only works for math. Requires reading comprehension + extraction + computation---multiple cognitive steps that each benefit from an intermediate token. The correct chain (3 x 8 = 24, 24 - 5 = 19) makes the "each step feeds the next" pattern vivid. |
| **CoT on a simple factual question: "What is the capital of France? Let's think step by step."** | Negative | Shows that CoT does NOT help on tasks that fit within a single forward pass. The model may produce unnecessary reasoning ("France is a country in Western Europe... its capital is Paris") but arrives at the same answer with or without CoT, wasting tokens. | Defines the boundary. Prevents the "always use CoT" overgeneralization. The contrast with the arithmetic example makes the criterion clear: the problem must require more computation than one forward pass provides. Factual recall is a single lookup in the model's parameters---additional forward passes add nothing useful. |
| **CoT with an error in an intermediate step propagating to the final answer** | Negative (stretch) | Shows that CoT quality matters, not just CoT quantity. If step 2 of a reasoning chain contains an error, the model continues from the error and produces a wrong final answer. The model does not "catch" its mistakes. | Demonstrates that the model is not "reasoning" in the human sense---it is generating tokens that feed back as context. If the context contains an error, subsequent forward passes build on the error. This disproves the "model thinks/reasons" misconception and the "longer is always better" misconception simultaneously. Connected to the broader "between retrieval and comprehension" framing from ICL. |

---

## Phase 3: Design

### Narrative Arc

The student just finished prompt engineering and can now construct structured prompts that reliably control model behavior. But the prompt-engineering lesson ended with a deliberate cliffhanger: "there is a class of problems where even the best-structured prompt fails: problems that require more computation than a single forward pass provides." This lesson delivers on that promise.

Here is the problem. Consider 17 x 24. A human would decompose this: 17 x 20 = 340, 17 x 4 = 68, 340 + 68 = 408. Multiple steps, each building on the last. Now consider what happens when a transformer tries to answer "17 x 24 = ?" in a single shot. The model gets exactly one forward pass---the same N transformer blocks that run for every token, regardless of whether the question is "2 + 2" or "17 x 24." All the computation that maps the input tokens to the answer must happen in that one pass through the network. For simple problems, one pass is enough. For multi-step problems, it is not.

The breakthrough of chain-of-thought prompting (Wei et al. 2022) is that you can give the model more computation by asking it to generate intermediate tokens. Each intermediate token triggers another forward pass. "17 x 20 = 340" is generated, then "17 x 4 = 68" is generated, then "340 + 68 = 408" is generated---and each of these steps has the full context of the previous steps as input. The model does not "think harder." It runs more forward passes, each building on the context created by previous ones. CoT does not work because the model reasons. It works because each token generated feeds back as input for the next forward pass, giving the model more computation per problem.

This is not a new mechanism. The student already knows the autoregressive feedback loop from Module 4.1---"outputs become inputs." They already know the fixed architecture from Module 4.2---same N blocks per token. CoT is simply the intersection of these two facts: if the model needs more computation than one forward pass provides, generate intermediate tokens that feed back as additional context for subsequent forward passes. The mechanism is the same generate() method the student implemented in Module 4.3. The insight is recognizing that this loop is not just a generation mechanism---it is a computational amplifier.

### Modalities Planned

| Modality | What Specifically | Why This Modality for This Concept |
|----------|------------------|------------------------------------|
| **Concrete example** | Three worked examples: (1) 17 x 24 with direct answer vs step-by-step, (2) word problem requiring multi-step reasoning, (3) simple factual recall where CoT adds nothing. Each shows the input prompt and the model's output with and without CoT. | CoT is an empirical phenomenon. The student must SEE that intermediate tokens change the answer before understanding why. The arithmetic example is the cleanest: unambiguous right/wrong, clear decomposition, visible intermediate steps. The factual recall negative example defines the boundary. |
| **Visual** | Inline SVG diagram showing the "computation per token" concept. Left side: direct answer---one forward pass from "17 x 24 =" to answer. Right side: CoT---multiple forward passes, each taking the entire context (including previously generated reasoning tokens) as input. Show the context growing with each step, and the forward pass happening for each token. Color-code the intermediate tokens to show they are both output of one forward pass and input to the next. | The core insight is that more tokens = more forward passes. The diagram makes this spatial and countable: the student can literally count the forward passes and see that CoT gets 3-4x more computation than the direct answer. The feedback loop is visible. Connects to the Mermaid diagram from what-is-a-language-model (4.1.1). |
| **Verbal/Analogy** | "A forward pass is the model's thinking budget per token. CoT is spending more budget by generating more tokens." Extended: "Asking a model to answer 17 x 24 in one token is like asking a human to multiply two-digit numbers without writing anything down. Some people can do it; most cannot. Writing intermediate results is not 'trying harder'---it is using external memory to decompose a problem that does not fit in working memory." The scratchpad analogy: intermediate tokens are a scratchpad, and the model's context window is its working memory. | Maps directly to the student's experience. Everyone knows that writing intermediate steps helps with mental math---not because writing makes you smarter, but because it externalizes state. The "thinking budget" framing is precise: each forward pass is a fixed computational budget (N blocks, d_model dimensions), and CoT is spending more of that budget. |
| **Symbolic/Code** | Show the generate() method from building-nanogpt (4.3.1) and annotate it: each iteration of the loop is one forward pass. For direct answer, the loop runs once (one output token). For CoT, the loop runs many times (many output tokens). The code is identical---the only difference is how many tokens are generated. Connect to the transformer block formula: x' = x + MHA(LayerNorm(x)), output = x' + FFN(LayerNorm(x'))---this runs N times per token, and CoT makes the model predict more tokens. | The student wrote this code. Seeing the existing code re-annotated with the CoT insight makes the concept feel like a natural extension of what they already built, not a new mechanism. "Look at your own generate() method. Each iteration is a forward pass. CoT just means more iterations." |
| **Intuitive** | The "of course" beat: "You already knew this. Autoregressive generation is a feedback loop---outputs become inputs. Each token gets a full forward pass through N transformer blocks. If you generate more tokens, you get more forward passes. Of course intermediate reasoning tokens help: they give the model more computation AND provide useful intermediate results in the context." | Collapses the perceived novelty. CoT should feel obvious once the student connects "more tokens = more forward passes" to "more forward passes = more computation." The "of course" beat works because both pieces are DEVELOPED concepts the student already has. |

### Cognitive Load Assessment

- **New concepts in this lesson:** 2
  1. Intermediate tokens as computation: each generated token provides an additional forward pass worth of compute, expanding the model's effective computational capacity (the "tokens as computation" insight)
  2. Process supervision vs outcome supervision: two approaches to training/evaluating reasoning chains---reward each step (process) vs reward only the final answer (outcome)
- **Previous lesson load:** BUILD. Prompt-engineering was a practical, application-oriented lesson systematizing techniques the student already understood.
- **This lesson's load:** STRETCH. The "tokens as computation" insight is a genuinely new way of seeing the autoregressive loop. It reframes a mechanism the student has used (generation) as a computational resource. This is not learning a new mechanism---it is seeing a familiar mechanism from a new angle, which is a different kind of cognitive demand. The second concept (process vs outcome supervision) is lighter and more practical.
- **Load is appropriate:** STRETCH after BUILD follows the module plan's intended trajectory (STRETCH-BUILD-STRETCH-BUILD). The student has had one BUILD lesson (prompt-engineering) to consolidate after the STRETCH of in-context-learning. The new conceptual demand (tokens as computation) is grounded in two DEVELOPED concepts (autoregressive loop, fixed architecture) that make the insight feel like a natural connection rather than a new paradigm.

### Connections to Prior Concepts

- **Autoregressive generation as feedback loop (4.1.1, 4.3.1):** THE foundational connection. The student implemented generate(): sample a token, append it, run another forward pass. CoT exploits this loop: intermediate reasoning tokens are not decorative---they trigger additional forward passes with enriched context. "You already built this. Each iteration of the generate() loop is a forward pass. CoT just generates more tokens."
- **Fixed transformer architecture (4.2.5, 4.2.6, 4.3.1):** The constraint CoT addresses. Every token prediction runs through the same N blocks---same 12 layers, same 768 dimensions, same operations. The model cannot "try harder" on difficult problems. The only way to get more computation is to run more forward passes, which means generating more tokens. "The model's thinking budget per token is fixed by architecture. CoT is a way to spend more budget."
- **"The prompt is a program; attention is the interpreter" (5.2.1, 5.2.2):** Extends naturally. A CoT prompt is a program that instructs the model to execute step-by-step. The intermediate tokens become additional context that attention operates on. Each step's output becomes part of the context (the program's state) for the next step.
- **Attention as data-dependent computation (4.2.1):** Intermediate reasoning tokens change the attention pattern for subsequent tokens. When the model has "17 x 20 = 340" in its context, the attention for the next forward pass has access to the intermediate result "340"---which could not exist in the context without being explicitly generated.
- **ICL's ordering sensitivity and format fragility (5.2.1):** Extended to CoT: the order and quality of intermediate reasoning steps matter. An error in step 2 propagates to step 3. This is the same "fragility" the student encountered in ICL, but now applied to self-generated context rather than provided examples.
- **"Between retrieval and comprehension" (5.2.1):** CoT does not make the model "comprehend" or "reason." It gives the model more computational steps, each of which is the same mechanistic forward pass. The model is no more "thinking" during CoT than during ICL. It is running more forward passes on a richer context.

**Potentially misleading prior analogies:**
- "Attention is the interpreter" could lead the student to think CoT works because the model "interprets" the reasoning steps. The lesson must emphasize that each intermediate token triggers a new forward pass---it is additional computation, not deeper interpretation of existing tokens.

### Scope Boundaries

**This lesson IS about:**
- Why "let's think step by step" works mechanically (intermediate tokens as additional forward passes)
- The fixed computation budget per forward pass as the constraint CoT addresses
- When CoT helps (multi-step problems exceeding single-forward-pass capacity) and when it does not (simple tasks)
- Process supervision vs outcome supervision (how to evaluate reasoning chains)
- The limits of CoT (error propagation, quality vs quantity of reasoning tokens)
- Zero-shot CoT ("let's think step by step") vs few-shot CoT (examples with reasoning chains)

**This lesson is NOT about:**
- Reasoning models trained with RL to use CoT effectively (Lesson 4: reasoning-models)
- Test-time compute scaling (Lesson 4)
- Search during inference, tree-of-thought, or beam search over reasoning paths (Lesson 4)
- Implementing CoT in production systems
- Self-consistency or majority voting over multiple CoT paths (MENTIONED at most)
- Automated chain-of-thought generation or optimization
- Mathematical proof of computational limitations of transformers

**Target depth:**
- Intermediate tokens as computation (the core CoT mechanism): DEVELOPED
- When CoT helps vs does not (problem-difficulty criterion): DEVELOPED
- Process supervision vs outcome supervision: INTRODUCED
- Zero-shot CoT vs few-shot CoT distinction: DEVELOPED
- Self-consistency / majority voting: MENTIONED
- Limits of CoT (error propagation, quality over quantity): INTRODUCED

### Lesson Outline

#### 1. Context + Constraints
What this lesson is about: why generating intermediate reasoning tokens gives the model more computation and when this helps. What we are NOT covering: reasoning models trained to use CoT (next lesson), test-time compute scaling, search algorithms during inference.

#### 2. Recap (brief)
One paragraph connecting two ideas the student already has: (1) autoregressive generation is a feedback loop where each generated token becomes input for the next forward pass, and (2) every token prediction runs through the same fixed architecture (N transformer blocks). "You built the generate() method: sample a token, append it, run another forward pass. And you know that each forward pass runs through the same 12 transformer blocks, no matter what the question is. This lesson is about what happens when you connect these two facts."

No full re-teach needed. Both concepts are at DEVELOPED depth and were used recently (autoregressive loop in ICL discussion, architecture in prompt-engineering's causal masking connection).

#### 3. Hook (demo / before-after)
**Type:** Before/after contrast + puzzle.

Present multi-step arithmetic: "17 x 24 = ?"

**Direct prompt:** "What is 17 x 24?" Model outputs "384" (wrong) or similar incorrect answer.

**CoT prompt:** "What is 17 x 24? Let's work through this step by step." Model outputs:
- 17 x 20 = 340
- 17 x 4 = 68
- 340 + 68 = 408

Same model. Same weights. Same question. Different answer. The only difference is that the second prompt caused the model to generate intermediate tokens.

GradientCard (puzzle): "The model did not become smarter. Its weights did not change. Its architecture did not change. It ran the same forward pass it always runs. Yet it went from wrong to right. What changed?"

#### 4. Explain Part 1: The Fixed Computation Budget

**Problem before solution.** "Every time the model predicts a token, it runs one forward pass: the input goes through N transformer blocks, and out comes a probability distribution over the vocabulary. This is true whether the question is '2 + 2' or '17 x 24' or 'prove the Riemann hypothesis.' The model gets the same computational budget per token, no matter what."

Walk through the constraint:
1. Token embedding + positional encoding (one vector per token)
2. N transformer blocks, each running MHA + FFN on the full context
3. Output projection to vocabulary logits
4. Sample one token

This runs identically for every token prediction. The model cannot "think harder" on a difficult problem. There is no "difficulty knob." N = 12 for GPT-2, N = 96 for GPT-3---but whatever N is, it is the same for every token.

**Callback to generate():** "Look at the code you wrote in building-nanogpt. Each iteration of the loop calls forward() once. One forward pass, one token. That is the model's entire computational budget for that token."

**The question:** "What if a problem requires more computation than one forward pass provides?"

#### 5. Explain Part 2: Tokens as Computation (Core Mechanism)

**The insight:** "The autoregressive loop is not just a generation mechanism. It is a computational amplifier."

Walk through step by step:
1. The model sees "17 x 24 = ? Let's work through this step by step."
2. Forward pass #1: The model generates "17" (starting the reasoning).
3. Forward pass #2: Context now includes "17". The model generates "x".
4. Forward pass #3: Context now includes "17 x". The model generates "20".
5. ...and so on through "17 x 20 = 340, 17 x 4 = 68, 340 + 68 = 408."
6. Each forward pass takes the ENTIRE context (original question + all previously generated reasoning tokens) as input.
7. The intermediate results ("340", "68") are now IN THE CONTEXT. Subsequent forward passes can attend to them.

**The "of course" beat:** "You already knew this. Autoregressive generation is a feedback loop: outputs become inputs. Each token gets a full forward pass through N blocks. If you generate more tokens, you get more forward passes. The intermediate reasoning tokens are not decoration---they are additional computation. Of course they help: more forward passes means more computation, and the intermediate results are available in the context for subsequent forward passes to attend to."

**Address misconception 1 immediately:** "It is tempting to say the model 'thinks through' the problem. It does not. Each token is generated by the same mechanical process: forward pass, sample from distribution, append. The model is not deliberating. It is generating tokens that happen to contain useful intermediate results, and those results feed back as context for subsequent forward passes. The 'thinking' is the token-by-token feedback loop, not an internal deliberation."

**Address misconception 2:** "CoT is not the model 'showing its work'---exposing reasoning it would have done internally. If the model could compute 17 x 24 internally, it would get the right answer without CoT. It cannot, because one forward pass through N blocks is not enough computation for multi-digit multiplication. The intermediate tokens are ENABLING computation that could not happen in a single pass, not displaying computation that already happened."

**Visual (inline SVG):** "Computation per Problem" diagram. Left: direct answer---one forward pass arrow from "17 x 24 = ?" to "384" (wrong). Right: CoT---multiple forward pass arrows, each taking the growing context as input. Context grows visibly with each step: "17 x 24... step by step" -> "+17 x 20 = 340" -> "+17 x 4 = 68" -> "+340 + 68 = 408". Each arrow is a full forward pass. Count: 1 pass vs ~15 passes. Label: "Same model, same architecture, 15x more computation."

**Symbolic callback:** Connect to the generate() code. Show the loop annotated: "iteration 1 = forward pass 1 = one token. Direct answer: ~1 iteration. CoT: ~15 iterations. Each iteration is a full forward pass through N blocks."

#### 6. Check 1 (Predict-and-verify)
Present two prompts for the same problem:

**Problem:** "A farmer has 3 fields. Each field has 7 rows of corn. Each row has 12 plants. How many corn plants does the farmer have?"

**Prompt A:** "Answer directly: A farmer has 3 fields..."
**Prompt B:** "Think step by step: A farmer has 3 fields..."

**Predict:** (1) Which prompt is more likely to get the right answer? (2) How many forward passes does each prompt use? (3) What intermediate results would appear in Prompt B's output?

**Reveal:** Prompt B produces: "3 fields x 7 rows = 21 rows total. 21 rows x 12 plants = 252 plants." Each intermediate result (21, 252) is generated by its own forward pass and becomes available in context for the next step. Prompt A must compute 3 x 7 x 12 in a single forward pass---mapping the entire problem to the answer in N transformer blocks.

#### 7. Elaborate: When CoT Helps and When It Does Not

**Positive example 2 (word problem):** "If a store has 3 shelves with 8 books each and removes 5 books, how many are left?" Direct answer may produce 24 (ignoring the removal) or 19 (correct). CoT: "3 shelves x 8 books = 24 books. 24 - 5 = 19 books." The intermediate result (24) is crucial---without it in the context, the model must hold the multiplication result "in mind" across the subtraction, which may exceed single-pass capacity.

**Negative example 1 (factual recall):** "What is the capital of France? Let's think step by step." The model may produce: "France is a country in Western Europe. Its capital and largest city is Paris." The answer is still "Paris"---same as without CoT. The additional tokens added no useful intermediate computation. Factual recall is a single lookup in the model's parametric knowledge, not a multi-step reasoning problem.

GradientCard: "CoT helps when the problem requires more computation than one forward pass provides. Factual recall, sentiment classification, simple text completion---these fit within a single pass. Multi-step arithmetic, word problems, logical reasoning, constraint satisfaction---these exceed single-pass capacity."

**Negative example 2 (error propagation):** Show a CoT chain where step 2 contains an error: "17 x 20 = 350" (wrong). The model continues from this error: "17 x 4 = 68. 350 + 68 = 418." The final answer is wrong because the intermediate step was wrong, and the model does not "catch" the error.

GradientCard: "The model is not reasoning---it is generating tokens that feed back as context. If the context contains an error, subsequent forward passes build on the error. CoT quality matters, not just quantity."

**Address misconception 4 (longer is always better):** Connect to ICL's finding that more examples do not always help and the RAG finding that irrelevant documents dilute attention. A rambling 15-step chain can be worse than a concise 3-step chain because irrelevant intermediate tokens add noise to the context.

**Address misconception 5 (CoT is a new emergent capability):** "CoT is not a new mechanism. It is the same autoregressive loop you implemented in building-nanogpt, applied to reasoning problems. Small models fail at CoT not because they lack the loop---they have the same generate() method---but because they cannot generate useful intermediate steps. The mechanism is universal; the usefulness depends on the quality of intermediate tokens."

#### 8. Explain Part 3: Zero-Shot CoT vs Few-Shot CoT

**Zero-shot CoT:** Simply adding "Let's think step by step" to the prompt (Kojima et al. 2022). No examples needed. The instruction triggers the model to generate reasoning tokens because that phrasing is well-represented in pretraining data.

**Few-shot CoT:** Providing examples that include reasoning chains (Wei et al. 2022). The model learns the format of step-by-step reasoning from the examples, just like it learns any task via ICL.

**Connection to prior lessons:** Zero-shot CoT is a prompt engineering technique (a format instruction that triggers a specific output pattern). Few-shot CoT is an ICL technique (examples with reasoning chains as the "program"). Both work for the same mechanistic reason: they cause the model to generate intermediate tokens that provide additional forward passes.

Brief note on self-consistency (Wang et al. 2022): generate multiple reasoning chains, take the majority-vote answer. MENTIONED only---the idea that sampling multiple chains and aggregating is more robust than a single chain. "If one chain can have errors, running several and voting reduces the chance of error. We'll see a more principled version of this idea in the next lesson on reasoning models."

#### 9. Explain Part 4: Process Supervision vs Outcome Supervision

**Transition:** "We said CoT quality matters---errors in intermediate steps propagate. How do you evaluate whether a reasoning chain is good?"

**Two approaches:**
- **Outcome supervision:** Evaluate only the final answer. If the answer is correct, the chain is good. If wrong, the chain is bad. Simple but loses information---a chain with correct reasoning and a calculation error at the last step is marked identically to a chain with completely wrong logic.
- **Process supervision:** Evaluate each step individually. Is step 1 correct? Is step 2 a valid logical follow-up to step 1? This is more informative but much harder---you need per-step labels, not just a final answer.

**Why it matters:** If you are training a model to produce better reasoning chains (which is what Lesson 4 will cover), the supervision signal determines what the model learns. Outcome supervision rewards any chain that reaches the right answer, even if the reasoning is flawed (the model might get lucky). Process supervision rewards correct reasoning steps, which is more aligned with producing reliable chains.

GradientCard: "Outcome supervision asks 'did you get the right answer?' Process supervision asks 'did you reason correctly at every step?' The distinction matters because the model can reach the right answer through wrong reasoning, and wrong reasoning will eventually produce wrong answers on harder problems."

**Concrete example:** A math problem where two different reasoning chains reach the correct answer. Chain A has correct reasoning at every step. Chain B has an error in step 2 that happens to cancel out in step 3, producing the correct final answer. Outcome supervision rates both equally. Process supervision rates Chain A higher. Chain B's reasoning pattern will fail on a different problem where the errors don't happen to cancel.

**Depth target:** INTRODUCED. The student understands the distinction and why it matters, but has not practiced applying it. Full development happens in Lesson 4 (reasoning-models) where process supervision connects to RL training.

#### 10. Check 2 (Transfer question)
A developer is building a system that uses an LLM to grade math homework. The LLM reads a student's solution (with intermediate steps) and decides if it is correct.

Questions:
1. Should the LLM use CoT to reason about whether each step is correct? Why or why not?
2. Is this more like process supervision or outcome supervision?
3. What failure mode should the developer worry about?

Expected answer: (1) Yes---evaluating multi-step mathematical reasoning requires more computation than a single forward pass. The LLM needs to check each step, which benefits from generating intermediate verification tokens. (2) Process supervision---evaluating each step individually rather than just checking the final answer. (3) Error propagation: if the LLM makes an error in evaluating step 2, it may approve subsequent incorrect steps. Also, the LLM might "continue" the student's reasoning rather than evaluating it---generating tokens that extend the student's work rather than critically examining it.

#### 11. Practice (Notebook exercises)
**Notebook:** `notebooks/5-2-3-chain-of-thought.ipynb`

**Exercise structure:** 4 exercises, cumulative theme (exploring when and why CoT works), but individually completable.

- **Exercise 1 (Guided): Direct vs CoT comparison on arithmetic.** Use an LLM API to solve 10 multi-step arithmetic problems with and without CoT. Compare accuracy. Predict-before-run: predict which problems will benefit from CoT and which will not. Include both simple (single-step) and complex (multi-step) problems. Insight: CoT helps on multi-step problems but not single-step ones. Scaffolding: API call pattern and problem list provided, student constructs prompts and tallies results.

- **Exercise 2 (Supported): Token counting as computation measurement.** For 5 problems solved with CoT, count the intermediate tokens generated. Compare token count to problem complexity (number of reasoning steps needed). Plot tokens vs steps. Insight: more complex problems generate more intermediate tokens, and each token is an additional forward pass. The model "allocates" computation proportional to problem difficulty---not because it decides to, but because more complex problems require more intermediate results. Scaffolding: token counting utility provided, student runs problems and analyzes the relationship.

- **Exercise 3 (Supported): Error propagation experiment.** Take a multi-step problem, manually corrupt one intermediate step (e.g., change "17 x 20 = 340" to "17 x 20 = 350" in the context), and ask the model to continue. Observe that the error propagates to the final answer. Then try corrupting at different positions (early step vs late step) and compare impact. Insight: errors in earlier steps have larger impact because more subsequent forward passes build on the error. The model does not "catch" errors---it continues from whatever context exists. Scaffolding: corruption function and first example provided, student designs additional corruptions and interprets results.

- **Exercise 4 (Independent): Identify the CoT boundary.** Find the boundary between problems that benefit from CoT and problems that do not. Design a set of problems of increasing complexity in a domain of the student's choice (arithmetic, logic puzzles, word problems). Test each with and without CoT. Identify the approximate complexity threshold where CoT starts helping. Insight: the boundary corresponds to the point where the problem exceeds single-forward-pass computational capacity. This is task-dependent and model-dependent. No skeleton provided---the student designs the experiment.

#### 12. Summarize
Key takeaways:
- Chain-of-thought works because each intermediate token triggers an additional forward pass, giving the model more computation per problem
- The model's computational budget per token is fixed by architecture (N transformer blocks). CoT is a way to exceed that budget by generating more tokens.
- CoT helps on multi-step problems that exceed single-forward-pass capacity. It does not help on simple tasks (factual recall, classification).
- The model does not "think" or "reason" during CoT---it generates tokens that feed back as context for subsequent forward passes. The mechanism is the same autoregressive loop the student already understands.
- CoT quality matters more than quantity. Errors in intermediate steps propagate. Irrelevant reasoning adds noise.
- Process supervision evaluates each reasoning step; outcome supervision evaluates only the final answer. Process supervision is harder but more informative.

Mental model echo: "A forward pass is the model's thinking budget per token. CoT is spending more budget by generating more tokens. The budget is fixed; the spending is not."

#### 13. Next Step
"You now understand why chain-of-thought works: intermediate tokens give the model more computation via additional forward passes. But CoT as we've seen it is ad hoc---you add 'let's think step by step' and hope the model generates useful reasoning. What if you trained the model to reason effectively? What if you could make the model spend more computation on harder problems and less on easier ones? The next lesson covers reasoning models---RL-trained CoT, test-time compute scaling, and the paradigm shift from 'bigger model' to 'more thinking time.'"

---

## Checklists

### Prerequisite Audit
- [x] Every assumed concept listed with required depth
- [x] Each traced via the records (not the curriculum plan)
- [x] Depth match verified for each
- [x] No untaught concepts remain
- [x] No multi-concept jumps in widgets/exercises
- [x] All gaps have explicit resolution plans (no gaps found)

### Pedagogical Design
- [x] Narrative motivation stated as a coherent paragraph (problem before solution)
- [x] At least 3 modalities planned for the core concept, each with rationale (5 modalities: concrete example, visual, verbal/analogy, symbolic/code, intuitive)
- [x] At least 2 positive examples + 1 negative example, each with stated purpose (2 positive + 2 negative)
- [x] At least 3 misconceptions identified with negative examples (5 misconceptions)
- [x] Cognitive load = 2 new concepts (within limit)
- [x] Every new concept connected to at least one existing concept (autoregressive loop from 4.1.1/4.3.1, fixed architecture from 4.2.5/4.3.1, ICL from 5.2.1, prompt engineering from 5.2.2)
- [x] Scope boundaries explicitly stated

---

## Review — 2026-02-19 (Iteration 1/3)

### Summary
- Critical: 0
- Improvement: 4
- Polish: 3

### Verdict: NEEDS REVISION

No critical findings. The lesson is solid pedagogically—the student would not be lost or form wrong mental models. However, four improvement findings identify areas where the lesson is significantly weaker than it could be. Another pass is warranted.

### Findings

#### [IMPROVEMENT] — Notebook Exercise 2 is pre-filled despite being labeled "Supported"

**Location:** Notebook cell `cell-10` (Exercise 2, Step 2)
**Issue:** Exercise 2 is labeled "Supported," meaning the student should write code with scaffolding but NOT have the solution pre-filled. The cell has a `# TODO` comment and `# YOUR CODE HERE (10-20 lines)` marker, but the entire plotting solution (40+ lines) is filled in below that marker. The student sees "YOUR CODE HERE" and then immediately sees the complete implementation. This defeats the purpose of a Supported exercise—the student has nothing to actually write.
**Student impact:** The student reads "YOUR CODE HERE" but the code is already there. They have two choices: (1) ignore the marker and just run the cell, learning nothing from the plotting exercise, or (2) try to write their own code but see the solution right below, contaminating their thinking. Either way, the scaffolding contract is broken.
**Suggested fix:** Move the complete plotting code into a `<details>` solution block in the markdown cell after the exercise. Leave only the scaffold in the code cell: the variable references (`token_data`, field names), axis labels, and plot structure hints. The student should write the actual `plt.scatter()`, `ax.barh()`, and aggregation code themselves. Keep the `# YOUR CODE HERE` marker with the hints, remove the implementation below it.

#### [IMPROVEMENT] — Aside on "Two Pieces, One Insight" mentions depth levels directly

**Location:** Section 3 (Recap), aside block "Two Pieces, One Insight"
**Issue:** The aside says "Both pieces are at DEVELOPED depth. This lesson is the connection." The student has no concept of "DEVELOPED depth"—depth levels are internal tracking used by the curriculum system, not a concept the student has been taught. This breaks the fourth wall and exposes scaffolding the student should not see.
**Student impact:** The student reads "DEVELOPED depth" and has no idea what this means. It feels like reading someone else's internal notes. Minor confusion, but it undermines trust in the lesson's voice.
**Suggested fix:** Replace with language that communicates the same idea without internal terminology. For example: "You understand both pieces well from previous lessons. This lesson is the connection."

#### [IMPROVEMENT] — The scratchpad/working-memory analogy from the plan is absent

**Location:** Sections 5-6 (Explain Part 1 and Part 2)
**Issue:** The planning document explicitly designed a verbal/analogy modality: "Asking a model to answer 17 x 24 in one token is like asking a human to multiply two-digit numbers without writing anything down. Writing intermediate results is not 'trying harder'—it is using external memory to decompose a problem that does not fit in working memory." This analogy is entirely absent from the built lesson. The "thinking budget" analogy appears in asides and the summary, but the scratchpad/working-memory analogy—which maps directly to the student's lived experience of doing mental math—was not implemented.
**Student impact:** The student misses a powerful modality. The "thinking budget" metaphor is abstract. The scratchpad analogy is visceral—every person has experienced the difficulty of doing multi-digit multiplication without writing anything down. This would make the "tokens as computation" insight feel immediately obvious rather than requiring the student to reason through the forward-pass argument.
**Suggested fix:** Add the scratchpad analogy in Section 5 (The Fixed Computation Budget), after establishing that the model gets the same compute per token. Something like: "Asking a model to answer '17 × 24' in one forward pass is like asking you to multiply two-digit numbers without writing anything down. Some people can manage it; most cannot. Writing intermediate results is not 'trying harder'—it is using external memory to decompose a problem that exceeds your working memory. The model's context window is its scratchpad."

#### [IMPROVEMENT] — Misconception 3 ("CoT always helps") is not explicitly named as a misconception

**Location:** Section 10 (When CoT Helps and When It Does Not)
**Issue:** The planning document identifies five misconceptions and specifies that each should be explicitly named. Misconceptions 1 and 2 are named clearly in Section 7 with GradientCards titled "Misconception: ..." Misconception 3 ("CoT always helps—you should always ask for step-by-step reasoning") is addressed implicitly through the negative example of factual recall and the criterion GradientCard, but it is never explicitly called out as a misconception the student might hold. The pattern from Section 7 (explicit naming with "Misconception:" prefix) is not continued.
**Student impact:** The student sees the factual recall example and the criterion, but may not connect this to their own likely belief that "CoT is always good." Without explicit naming, the misconception is addressed indirectly. Students who already hold this belief may nod along at the criterion without realizing it contradicts their assumption.
**Suggested fix:** Add explicit misconception framing before or around the factual recall negative example. Either a brief paragraph ("You might assume that CoT always helps—after all, more computation is always better, right? It is not.") or rename the "Negative Example" GradientCard to include the misconception framing: "Misconception: 'Always use CoT'" or similar.

#### [POLISH] — The "STRETCH Lesson" aside in the scope section uses internal terminology

**Location:** Section 2 (Constraints), aside block "STRETCH Lesson"
**Issue:** The aside title says "STRETCH Lesson." Like the depth-level issue, this is internal curriculum terminology the student has not been introduced to. The body text is fine—it explains the reframing concept in student-facing language. But the title itself is jargon.
**Student impact:** Mild confusion. The student might wonder what a "STRETCH" lesson is as opposed to other lessons. Not disorienting, but slightly odd.
**Suggested fix:** Change the title to something student-facing like "A New Perspective" or "Reframing What You Know." The body text works as-is.

#### [POLISH] — Summary item count is high (6 items)

**Location:** Section 17 (Summary)
**Issue:** The SummaryBlock contains 6 items, each with a headline and description. For a lesson with 2 new concepts, 6 summary items is a lot—the student may feel overwhelmed reviewing them all. Items 4 and 5 (model does not "think" and quality over quantity) overlap significantly with the misconception sections earlier.
**Student impact:** The summary feels like a wall of text rather than a crisp "what I learned." The student might skim rather than internalize.
**Suggested fix:** Consider consolidating items 4 and 5 into a single item about CoT limitations (model does not reason, errors propagate, quality matters). This would reduce to 5 items, which is more digestible while still covering all the ground.

#### [POLISH] — Computation diagram's "~15N blocks" label does not match the walked example

**Location:** Section 8 (Visual: Computation Diagram)
**Issue:** The diagram's bottom text says "CoT: ~15 forward passes = ~15N blocks of computation." The walked example in the diagram itself shows 4 forward passes (pass 1, pass 2, pass 3, pass 4). The ~15 figure comes from the text explanation in Section 6, where the full token-by-token walkthrough describes passes #1 through #15. But the diagram only shows 4 grouped steps, so "~15" in the diagram's label feels disconnected from what is visually shown.
**Student impact:** Minor cognitive friction. The student counts 4 arrows in the diagram but reads "~15 forward passes" in the label. They need to understand that each step in the diagram involves multiple tokens/passes, but the diagram does not make that explicit.
**Suggested fix:** Either (a) add a small annotation to the diagram noting that each step box involves multiple tokens and therefore multiple forward passes, or (b) change the label to say "4+ forward passes" to match the visual, with a note that the full chain involves ~15 total.

### Review Notes

**What works well:**
- The narrative arc is strong. The recap connects precisely to the prompt-engineering cliffhanger, the hook creates genuine curiosity with the 17 x 24 puzzle, and the "of course" beat in Section 6 lands well because both prerequisite concepts are at DEVELOPED depth.
- The misconception handling in Section 7 is excellent—both misconceptions are explicitly named, concretely grounded, and placed immediately after the core mechanism is explained (before the student can form them).
- The connection to prior concepts is thorough and specific. References to `generate()`, the ICL "formula has not changed" callback, and the prompt-engineering RAG/context-stuffing parallel are all natural and reinforce learning.
- The notebook is well-designed overall: the exercise progression (establish fact -> quantify -> test limits -> find boundary) follows a genuine experimental arc. Exercise 1 is properly Guided, Exercise 4 is properly Independent with a `<details>` solution.
- The scope boundaries are clearly stated and respected. The lesson does not drift into reasoning models or test-time compute scaling.

**Patterns to watch:**
- Internal terminology leaking into student-facing content (DEVELOPED, STRETCH) appeared twice. This suggests a systematic habit worth checking in other lessons.
- The pre-filled code in a Supported exercise (Exercise 2) is the same issue found in the ICL lesson review (iteration 1). This pattern may recur.

---

## Review — 2026-02-19 (Iteration 2/3)

### Summary
- Critical: 0
- Improvement: 0
- Polish: 2

### Verdict: PASS

All four improvement findings from iteration 1 have been properly addressed. No new critical or improvement issues were introduced by the fixes. The lesson is pedagogically sound: it teaches the core concept (intermediate tokens as computation) through 5 modalities, addresses all 5 planned misconceptions, uses 2 positive and 2 negative examples effectively, respects scope boundaries, and maintains a compelling narrative arc. Two minor polish findings remain, neither of which affects the student's learning experience.

### Findings

#### [POLISH] — Notebook uses spaced em dashes throughout

**Location:** Notebook `5-2-3-chain-of-thought.ipynb`, multiple cells (setup comment, exercise explanations, key takeaways)
**Issue:** The notebook consistently uses spaced em dashes (`word — word`) rather than unspaced (`word—word`). Examples: "Setup — self-contained for Google Colab", "it's that CoT generates intermediate tokens — it's that...", "The API returns `completion_tokens` — the number of tokens..." The writing style rule requires no spaces around em dashes. This same issue has appeared in other notebook reviews.
**Student impact:** Negligible. The student reads the notebook content the same way regardless of em dash spacing. This is a consistency issue with the project's style conventions, not a pedagogical concern.
**Suggested fix:** Find-and-replace ` — ` with `—` in all markdown cells and print statements in the notebook. Also remove the unused `import json` from the setup cell while editing.

#### [POLISH] — Misconception 4 ("longer is always better") not explicitly labeled as a misconception

**Location:** Section 11 (Quality Over Quantity)
**Issue:** Misconceptions 1, 2, and 3 are explicitly named with "Misconception:" prefixed GradientCard titles. Misconception 4 ("longer chains are always better") is addressed substantively through the error propagation example and the "Quality Matters, Not Just Quantity" GradientCard, but it is not framed with the same "Misconception:" prefix pattern. This is an inconsistency in how misconceptions are presented, not a gap in coverage.
**Student impact:** Minimal. The content clearly communicates that longer chains are not always better and explains why. The student would not form the misconception. The inconsistency in framing is aesthetic, not pedagogical.
**Suggested fix:** No action needed. The misconception is thoroughly addressed through the error propagation example and the quality-over-quantity argument. Adding another "Misconception:" card would be redundant since the content already makes the point. If desired for consistency, the "Quality Matters, Not Just Quantity" GradientCard title could be changed to `Misconception: "Longer chains are always better"` but this is optional.

### Iteration 1 Fix Verification

All 7 findings from iteration 1 have been verified as resolved:

| Finding | Status | Verification |
|---------|--------|-------------|
| Notebook Exercise 2 pre-filled code | FIXED | Cell `cell-10` now contains only scaffold (`# TODO`, variable hints, `# YOUR CODE HERE`). Complete solution moved to `<details>` block in cell `cell-11`. |
| "DEVELOPED depth" internal terminology in aside | FIXED | Aside now reads "You understand both pieces well from previous lessons. This lesson is the connection." |
| Scratchpad/working-memory analogy missing | FIXED | Added to Section 5 (The Fixed Computation Budget): "Asking a model to answer '17 x 24' in one forward pass is like asking you to multiply two-digit numbers without writing anything down..." Full analogy present with scratchpad framing. |
| Misconception 3 not explicitly named | FIXED | GradientCard titled `Misconception: "Always use CoT"` added in Section 10, with explicit framing: "You might assume that CoT always helps—after all, more computation is always better, right? It is not." |
| "STRETCH Lesson" aside title | FIXED | Changed to "A New Perspective." Body text unchanged. |
| Summary 6 items | FIXED | Consolidated to 5 items. Items 4 and 5 (model does not "think" + quality over quantity) merged into a single item covering both CoT limitations. |
| Computation diagram "~15N blocks" label | FIXED | Bottom text now reads "CoT: 4+ steps, each involving multiple tokens and forward passes" matching the 4 visual steps, with "The only difference: how many tokens are generated" as the summary. |

### Review Notes

**What works well (carries forward from iteration 1):**
- The narrative arc remains strong after the fixes. The scratchpad analogy in Section 5 makes the "tokens as computation" insight more visceral and accessible.
- The explicit "Misconception: 'Always use CoT'" framing in Section 10 completes the pattern established in Section 7 for misconceptions 1 and 2.
- The consolidated 5-item summary is crisper and more digestible than the original 6 items.
- The computation diagram's updated label is now consistent with what the student visually sees.
- The notebook's Exercise 2 scaffolding contract is properly maintained with the solution in a `<details>` block.

**No new issues introduced by the fixes.** All changes were targeted and did not affect surrounding content.
