# Course: Deep Learning Foundations

## Goal
Take a software engineer from "uses AI tools" to "understands how models actually work" — building intuition through deliberate practice, interactive exploration, and implementation.

## Series

| Series | Title | Status | Description |
|--------|-------|--------|-------------|
| 1 | Foundations | complete | Neural networks, backprop, optimization, regularization |
| 2 | PyTorch | complete | Practical implementation skills |
| 3 | CNNs | complete | Convolutional networks, spatial reasoning, architectures |
| 4 | LLMs & Transformers | complete | Transformers, deep KQV intuition, build GPT from scratch, finetuning, alignment |
| 5 | Recent LLM Advances | complete | RLHF, constitutional AI, reasoning models, multimodal |
| 6 | Stable Diffusion | complete | Classical diffusion, DDPM, latent diffusion, LoRA fine-tuning |
| 7 | Post-SD Advances | complete | ControlNet, SDXL, consistency models, flow matching |
| 8 | Special Topics | active | Standalone deep dives — catch-all for one-off topics across any domain |

## Module Status

| Module | Title | Lessons | Status |
|--------|-------|---------|--------|
| 1.1 | The Learning Problem | 6 | complete |
| 1.2 | From Linear to Neural | 4 | complete |
| 1.3 | Training Neural Networks | 7 | complete |
| 2.1 | PyTorch Core | 4 of 4 | complete |
| 2.2 | Real Data | 3 of 3 | complete |
| 2.3 | Practical Patterns | 3 of 3 | complete |
| 3.1 | Convolutions | 3 of 3 | complete |
| 3.2 | Modern Architectures | 3 of 3 | complete |
| 3.3 | Seeing What CNNs See | 2 of 2 | complete |
| 4.1 | Language Modeling Fundamentals | 3 of 3 | complete |
| 4.2 | Attention & the Transformer | 6 of 6 | complete |
| 4.3 | Building & Training GPT | 4 of 4 | complete |
| 4.4 | Beyond Pretraining | 5 of 5 | complete |
| 6.1 | Generative Foundations | 4 of 4 | complete |
| 6.2 | Diffusion | 5 of 5 | complete |
| 6.3 | Architecture & Conditioning | 5 of 5 | complete |
| 5.1 | Advanced Alignment | 4 of 4 | complete |
| 6.4 | Stable Diffusion | 3 of 3 | complete |
| 5.2 | Reasoning & In-Context Learning | 4 of 4 | complete |
| 6.5 | Customization | 3 of 3 | complete |
| 7.1 | Controllable Generation | 3 of 3 | complete |
| 7.2 | The Score-Based Perspective | 2 of 2 | complete |
| 5.3 | Scaling Architecture | 3 of 3 | complete |
| 7.3 | Fast Generation | 3 of 3 | complete |
| 7.4 | Next-Generation Architectures | 4 of 4 | complete |
| 8.1 | Vision & Vision-Language Models | 2 of 2 | complete |

## Progression
Series 1 (Foundations) establishes the complete path from "what is learning?" through linear models, neural networks, and training. Series 2 (PyTorch) bridges theory to practice — hands-on implementation right after the concepts. Series 3 (CNNs) covers convolutional architectures with PyTorch in hand, needed for understanding U-Net in Stable Diffusion. Series 4-7 cover the target domains: LLMs and generative models. Series 8 (Special Topics) is an open-ended catch-all for standalone deep dives into any topic worth exploring.
