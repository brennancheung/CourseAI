# Module 1.3: Training Neural Networks — Record

**Goal:** Teach how neural networks compute gradients and learn via backpropagation, then practical training skills.
**Status:** Complete (7 of 7 lessons built)

## Concept Index

| Concept | Depth | Lesson | Notes |
|---------|-------|--------|-------|
| Chain rule for composed functions | DEVELOPED | backpropagation | dy/dx = dy/dg · dg/dx; "effects multiply through the chain" |
| Backpropagation algorithm | DEVELOPED | backpropagation | Forward pass + backward pass = all gradients |
| Forward pass | DEVELOPED | backprop-worked-example | Concrete: x=2 through 2-layer network, every intermediate value computed and saved |
| Backward pass | DEVELOPED | backprop-worked-example | 7-step chain rule with real numbers, all 4 gradients computed |
| Local derivatives | DEVELOPED | backprop-worked-example | Concretely shown: each step is "incoming gradient × local derivative" |
| Multi-layer gradient computation | DEVELOPED | backprop-worked-example | 4 gradients (dL/dw1, dL/db1, dL/dw2, dL/db2) computed through 2 layers |
| Weight update with real numbers | DEVELOPED | backprop-worked-example | theta_new = theta_old - 0.1 * gradient, loss verified to decrease |
| Numerical gradient verification | DEVELOPED | backprop-worked-example | (L(w+eps) - L(w))/eps compared to analytical gradient — they match |
| Dying ReLU (concrete) | DEVELOPED | backprop-worked-example | w1=-1 → z1=-1.9 → ReLU=0 → gradient=0 → layer 1 can't learn |
| Computational efficiency of backprop | INTRODUCED | backpropagation | O(1) passes vs O(N) naive — million-fold speedup |
| Vanishing gradients (why) | INTRODUCED | backpropagation | Small derivatives multiply → near-zero; why ReLU > sigmoid |
| Autograd / automatic differentiation | MENTIONED | backpropagation | Frameworks compute gradients automatically |
| Computational graph notation | DEVELOPED | computational-graphs | Nodes = operations, edges = data flow; draw any computation as a graph |
| Fan-out gradient summation | DEVELOPED | computational-graphs | When a value feeds multiple paths, sum the gradients from each path |
| Automatic differentiation (mechanism) | INTRODUCED | computational-graphs | PyTorch builds a computational graph during forward pass, walks it backward for loss.backward() |
| Forward pass as graph traversal (left-to-right) | DEVELOPED | computational-graphs | Push values through nodes left-to-right, saving intermediate results |
| Backward pass as graph traversal (right-to-left) | DEVELOPED | computational-graphs | At every node: incoming gradient x local derivative = outgoing gradient |
| No path = no gradient | INTRODUCED | computational-graphs | A parameter not connected to the loss gets zero gradient; links to dying ReLU |
| Mini-batch gradient computation | DEVELOPED | batching-and-sgd | Same formula as full gradient but over B random samples instead of N; polling analogy grounds the statistical argument |
| Stochastic gradient descent (SGD) | DEVELOPED | batching-and-sgd | Mini-batch SGD is the default; spectrum from batch=1 (too noisy) to batch=ALL (too slow); "stochastic" = random |
| Epochs (cycling through data) | DEVELOPED | batching-and-sgd | One epoch = one pass through all data; iterations per epoch = N/B; total iterations = (N/B) x num_epochs |
| Gradient noise as beneficial | INTRODUCED | batching-and-sgd | Noisy gradients help escape sharp minima into wide ones that generalize better; "the hill is shaking" |
| Batch size as hyperparameter | DEVELOPED | batching-and-sgd | Common values 32-256 (powers of 2); tradeoff between gradient accuracy and update frequency |
| Shuffling data each epoch | DEVELOPED | batching-and-sgd | Without shuffling, sorted data causes batches to be non-representative; gradients oscillate instead of converging |
| Sharp vs wide minima | INTRODUCED | batching-and-sgd | Sharp minimum = steep walls, fragile; wide minimum = flat, robust to perturbations, better generalization |
| Batch size / learning rate interaction | MENTIONED | batching-and-sgd | Larger batches can tolerate larger learning rates; planted for optimizers lesson |
| Exponential moving average (EMA) | DEVELOPED | optimizers | v_t = beta * v_{t-1} + (1-beta) * x_t; taught through weather forecasting analogy + 5-step concrete walkthrough; beta controls memory (0.9 = "90% old, 10% new") |
| Momentum (SGD with momentum) | DEVELOPED | optimizers | EMA of gradients → smooths gradient direction; classical momentum formula v_t = beta * v_{t-1} + g_t (accumulates rather than averages); bowling ball analogy (inertia plows through noise); solves ravine problem by canceling oscillations and accumulating consistent signal |
| RMSProp | INTRODUCED | optimizers | Per-parameter adaptive learning rates; s_t = EMA of squared gradients, divide gradient by sqrt(s_t); "volume knob per instrument" analogy; self-normalizing property demonstrated with concrete numbers (50,000x gradient difference → same step size) |
| Adam optimizer | DEVELOPED | optimizers | Combines momentum + RMSProp + bias correction; m_t (momentum) + v_t (RMSProp) + hat corrections; defaults: lr=0.001, beta1=0.9, beta2=0.999; strong default but not universal best |
| Per-parameter learning rates | INTRODUCED | optimizers | Motivation: different parameters have vastly different gradient magnitudes; one global LR cannot serve both; RMSProp/Adam implement via adaptive scaling |
| Optimizer comparison (Adam vs SGD+momentum) | INTRODUCED | optimizers | Adam converges faster but can settle in sharper minima; SGD+momentum slower but often generalizes better on well-understood problems; no free lunch principle |
| Learning rate scaling across optimizers | INTRODUCED | optimizers | lr=0.01 for SGD ≠ lr=0.01 for Adam; Adam internally rescales gradients so effective step size is different; typical: 0.01 for SGD, 0.001 for Adam |
| Ravine problem (loss landscape) | INTRODUCED | optimizers | Elongated loss landscape with high curvature in one direction, low in another; vanilla SGD zigzags across ravine, momentum smooths through |
| Gradient noise smoothing via momentum | INTRODUCED | optimizers | Momentum smooths mini-batch noise from SGD; "heavy bowling ball less affected by hill shaking" — reinforces batching-and-sgd mental model |
| Vanishing gradients (quantitative) | DEVELOPED | training-dynamics | From INTRODUCED (backpropagation, 1.3); concrete layer-by-layer table: 0.25^N through 10 layers; sigmoid derivative curve (max=0.25, bell-shaped); telephone game analogy |
| Exploding gradients | DEVELOPED | training-dynamics | Mirror of vanishing: local derivatives > 1.0 multiply to huge values (2.0^10 = 1024); NaN symptom; caused by large weight magnitudes; unified framing with vanishing |
| Gradient pathology as product instability | DEVELOPED | training-dynamics | Unifying insight: vanishing and exploding are the same root cause — product of local derivatives is unstable unless each factor ≈ 1.0 |
| Xavier initialization | DEVELOPED | training-dynamics | Var(w) = 1/n_in; preserves signal variance across layers for sigmoid/tanh; derived from "n_in * Var(w) = 1" |
| He initialization | DEVELOPED | training-dynamics | Var(w) = 2/n_in; accounts for ReLU zeroing ~50% of neurons; paired with Xavier as ReLU-specific variant |
| Weight initialization (general principle) | DEVELOPED | training-dynamics | Each layer should neither amplify nor dampen the signal; variance of output should match variance of input |
| Batch normalization | INTRODUCED | training-dynamics | Normalize activations between layers during training; learned gamma/beta parameters; stabilizes gradient flow; allows deeper networks and higher LR |
| Sigmoid derivative properties | DEVELOPED | training-dynamics | Max derivative = 0.25 at z=0; bell-shaped curve approaching 0 in saturated regions; explains why sigmoid causes vanishing gradients |
| Dying ReLU (contextualized) | DEVELOPED (reinforced) | training-dynamics | Contextualized within broader vanishing gradient framework; ReLU trades gradual vanishing for binary alive/dead; better tradeoff in practice |
| Gradient clipping | MENTIONED | training-dynamics | Practical band-aid for exploding gradients: cap gradient magnitude at a threshold; named but not developed |
| Skip connections / ResNets | MENTIONED | training-dynamics | Teased as technique that pushed depth further (152 layers); deferred to future module |
| Naive weight initialization failure | DEVELOPED | training-dynamics | Uniform [0,1] fails: all-positive weights cause activations to grow exponentially; naive normal better but variance still wrong without n_in scaling |
| Variance (statistical concept in NN context) | INTRODUCED | training-dynamics | Measures spread of values; grounded in gradient magnitudes the student just computed; needed for initialization formulas |
| Overfitting / generalization vs memorization | DEVELOPED | overfitting-and-regularization | From INTRODUCED (1.1) to DEVELOPED; now with training curve diagnostic (train/val divergence), model capacity framing, neural network context |
| Bias-variance tradeoff (operationalized) | DEVELOPED | overfitting-and-regularization | From INTRODUCED (1.1) to DEVELOPED; now operationalized through training curves and regularization techniques |
| Training curves as diagnostic tool | DEVELOPED | overfitting-and-regularization | Reading train vs val loss divergence; the "scissors" pattern; three phases (learning, sweet spot, overfitting); gap = overfitting |
| Dropout | DEVELOPED | overfitting-and-regularization | Randomly set fraction of neuron outputs to zero during training; training-vs-inference distinction; creates implicit ensemble of sub-networks; p=0.5 default |
| Weight decay / L2 regularization | DEVELOPED | overfitting-and-regularization | L_total = L_data + lambda * sum(w_i^2); modified update rule w_new = w_old * (1 - alpha*2*lambda) - alpha*dL_data/dw; weights literally decay toward zero each step |
| Early stopping | DEVELOPED | overfitting-and-regularization | Monitor validation loss per epoch; patience hyperparameter (5-20 epochs); save best model weights; simplest and most universal technique |
| Regularization (general concept) | INTRODUCED | overfitting-and-regularization | Category of techniques that constrain models to prevent memorization; deliberately increases training loss to improve validation loss |
| Model capacity / expressiveness | INTRODUCED | overfitting-and-regularization | More parameters = more capacity = ability to fit arbitrarily complex functions including noise; parameter-to-data ratio matters |
| Validation loss as separate metric | DEVELOPED | overfitting-and-regularization | From implicit to DEVELOPED; plotted alongside training loss as the primary evaluation metric; the metric that actually matters |
| Train/val/test splits (operationalized) | DEVELOPED | overfitting-and-regularization | From INTRODUCED (1.1) to DEVELOPED; now operationalized through validation curve monitoring and early stopping |
| AdamW | MENTIONED | overfitting-and-regularization | Named as "Adam + weight decay," the practical default optimizer; deferred to PyTorch series for implementation |

## Per-Lesson Summaries

### backpropagation
Chain rule review as core mathematical tool. Scale problem: 175B parameters need gradients. Forward pass = compute + save, Backward pass = chain rule from loss back through layers. Single-neuron concrete example: z = wx+b → a = ReLU(z) → L = (y-a)². "Local × Local × Local" — each layer independent. Efficiency: backprop O(1) vs naive O(N). Interactive: BackpropFlowExplorer. NOT covered: multi-layer worked example with numbers, computational graphs, automatic differentiation details, batching, optimizers.

### backprop-worked-example
Concrete worked example: 2-layer network (1 neuron each), 4 parameters (w1=0.5, b1=0.1, w2=-0.3, b2=0.2), x=2, y=1. Full forward pass with every intermediate value (z1=1.1, a1=1.1, y_hat=-0.13, L=1.2769). Full backward pass in 7 numbered steps, each showing "incoming gradient × local derivative." Check-your-understanding: trace the 4 local derivatives that multiply for dL/dw1. Dead ReLU negative example: w1=-1 → z1=-1.9 → gradient dies. Numerical gradient verification: analytical vs (L(w+eps)-L(w))/eps. Complete training step: apply updates with lr=0.1, loss decreases. Interactive: BackpropCalculator widget (adjust parameters, run backward pass, apply updates, see dead ReLU). NOT covered: multi-neuron layers (matrices), computational graphs, batching, optimizers.

### computational-graphs
Visual notation for organizing gradient computations: operations become nodes, data flow becomes edges. Teaches graph anatomy (nodes, edges, forward values in blue, gradients in red, upstream/downstream terminology). Three examples of increasing complexity: (1) f(x) = (x+1)^2 — simplest graph, 2 operations, establishes forward left-to-right and backward right-to-left; (2) the same 2-layer network from backprop-worked-example mapped to a graph — same numbers, proves graph is notation not new algorithm; (3) f(x) = x*(x+1) — fan-out pattern where x feeds two paths, gradients sum. Symbolic modality: full chain rule expansion for dL/dw1 with each factor mapped to a graph hop ("4 meaningful multiplications backward through the graph"). Negative example: no path from variable c to output = no gradient, connects to dying ReLU as a "broken link." Connection to autograd: PyTorch builds this graph during forward pass, loss.backward() walks it backward. Interactive: ComputationalGraphExplorer (3 graph modes, graph view vs step-by-step toggle, hover/tap node detail showing "incoming x local = outgoing"). Two predict-and-verify checks with collapsible reveals. NOT covered: implementing computational graphs in code, dynamic vs static graphs, vectorized operations, batching, optimizers.

### batching-and-sgd
Scale problem as hook: ImageNet 1.2M images, full-batch = 6+ hours per update. Polling analogy: random sample of 50 people estimates city average height, random sample of 32 data points estimates full gradient. Mini-batch formula shown side-by-side with full-batch (same structure, different N). Concrete walkthrough: 1000 houses, batch_size=50 = 20 batches per epoch, walked through shuffle-split-process steps. Gradient arrow visualization: one precise full-batch arrow vs several noisy mini-batch arrows clustering around true direction. Epoch/iteration/step vocabulary with arithmetic (N/B formula). Full SGD pseudocode shown with connection to existing training loop ("inner 4 lines are the same"). Negative example: sorted dataset (all cats then all dogs) causes oscillation; ComparisonRow visual. SGD spectrum: batch=1 (Pure SGD, basketball-player polling analogy), batch=32-256 (mini-batch, the default), batch=ALL (full-batch, too slow). Gradient noise as feature: 1D cross-section SVG showing smooth path into sharp minimum vs noisy path escaping into wide minimum. SGDExplorer widget: 2D contour loss landscape with two minima (sharp + wide), batch size selector (1/8/32/128/ALL), play/pause/step/reset, speed control, training loss curve, numbered experiments guiding discovery order. NOT covered: optimizers beyond vanilla SGD (no momentum, no Adam), learning rate schedules, data loading/augmentation, GPU parallelism, non-convex optimization theory, convergence guarantees, matrix/vectorized batch operations.

### optimizers
Ravine problem as hook: elongated loss landscape where vanilla SGD zigzags across steep walls instead of descending along gentle floor. "What if the ball could remember where it's been going?" EMA taught as prerequisite tool: weather forecasting analogy (yesterday matters more than last week), formula v_t = beta * v_{t-1} + (1-beta) * x_t, concrete 5-step numerical walkthrough with gradients [2.0, -1.5, 3.0, -0.5, 2.5] at beta=0.9 showing smoothing effect. Momentum: bowling ball vs tennis ball analogy (inertia plows through bumps), applies EMA to gradients, classical momentum formula v_t = beta * v_{t-1} + g_t (accumulates rather than averages, scaling absorbed into LR), ComparisonRow showing ravine gradients: across-ravine oscillations cancel (+5,-5,+5,-5 → ~0), along-ravine consistent signals accumulate (+0.1,+0.1,+0.1 → builds up). Check-your-understanding: momentum ball overshoots curved ravines due to inertia. RMSProp: per-parameter gradient magnitude problem (w1 grad=0.001, w2 grad=50 → one LR can't serve both), "volume knob per instrument" analogy, formula s_t = EMA of g_t^2 then divide by sqrt(s_t), concrete numerical table showing 50,000x gradient difference → both parameters take ~0.32 step size (self-normalizing). Adam: combines momentum (smooth direction) + RMSProp (normalize magnitude) + bias correction (fixes cold-start where m_0=0,v_0=0 cause early steps too small), formula shown with all 4 components, defaults lr=0.001/beta1=0.9/beta2=0.999, explicit callout that Adam's LR ≠ SGD's LR (0.001 vs 0.01). OptimizerExplorer widget: 2D ravine contour, 4 optimizers (vanilla SGD/momentum/RMSProp/Adam), color-coded trajectories showing zigzag vs smooth vs adaptive paths, internal state display (vx/vy for momentum, sx/sy for RMSProp, both for Adam) with per-optimizer explanations, learning rate slider, play/pause/step/reset, training loss curve, 5 guided experiments. "Adam Always Wins" misconception addressed: ComparisonRow showing Adam=fast convergence but can settle in sharp minima, SGD+momentum=slower but often better final generalization on well-understood problems; validation loss matters more than training loss; no free lunch. Transfer question: colleague says "Adam converged in 50 epochs, SGD hasn't after 100, Adam is better" — missing that convergence speed ≠ final quality, SGD might need 3-5x more epochs but end up better. Five misconceptions addressed with concrete counter-examples: (1) Adam always beats SGD, (2) momentum just makes training faster (it changes DIRECTION not just speed), (3) Adam has no hyperparameters (defaults are starting points not universal), (4) more sophisticated = always better, (5) LR means same thing across optimizers. NOT covered: optimizer implementation in code (deferred to PyTorch), learning rate schedules, second-order methods, AdamW/weight decay, convergence proofs, other optimizers (Adagrad/NAG/LAMB), hyperparameter tuning strategies.

### training-dynamics
Depth paradox as hook: 2-layer network trains normally, 10-layer network flatlines — same optimizer, same data, only difference is depth. Chain rule product recap: gradient at layer k is a product of N-k activation derivatives, each at most 0.25 for sigmoid. Vanishing gradients developed from INTRODUCED: layer-by-layer table showing 0.25^N decay through 10 layers (layer 1 learns a million times slower than layer 10); sigmoid derivative curve SVG (bell-shaped, peaking at 0.25, shaded tails < 0.1); telephone game analogy (each person loses fidelity, message unintelligible by person 10). Misconception addressed: vanishing ≠ zero (0.00000095 is not 0; dying ReLU is exactly 0 — different, sharper problem). Misconception addressed: "just increase the LR" fails because global knob can't fix per-layer problem (LR=100 helps layer 1 but destroys layer 10). Exploding gradients as mirror image: local derivatives > 1.0 from large weight magnitudes (ReLU derivative = 1.0, so weight term dominates); 2.0^10 = 1024; telephone game extended (each person amplifies, whisper becomes scream); NaN symptom means exploding, not vanishing. Unifying frame: vanishing and exploding are the same problem — product of local derivatives is unstable, only stable when each factor ≈ 1.0. Check 1: why ReLU reduces vanishing (1^10 = 1 vs 0.25^10, but dying ReLU trades gradual vanishing for binary alive/dead). Weight initialization: principle is each layer should preserve signal magnitude; variance explanation grounded in gradient magnitudes from preceding sections; Xavier Var(w) = 1/n_in derived from n_in * Var(w) = 1; He Var(w) = 2/n_in accounts for ReLU zeroing ~50% neurons; ComparisonRow Xavier vs He; before/after variance comparison (naive: 1.0 → 1.2M, Xavier: 1.0 → 0.85). Naive normal also addressed (zero-centered but variance still wrong). Batch normalization at INTRODUCED depth: connects to familiar input normalization, extends to every layer during training; formula with learned gamma/beta; misconception addressed (BN ≠ preprocessing); practical impact list; inline SVG showing 20-layer with/without BN training curves. TrainingDynamicsExplorer widget: parameterized gradient model (not real backprop), controls for layers (2-20), activation (sigmoid/ReLU/tanh), initialization (naive uniform/naive normal/Xavier/He), batch norm toggle; two visualizations (gradient magnitude bar chart with log scale + healthy band, simulated training loss curve); 5 guided experiments in TryThisBlock. Historical arc: pre-2012 (sigmoid, random init, 2-3 layers), 2012 AlexNet (ReLU, 8 layers), 2015 ResNet (He + BN + skip connections, 152 layers). Check 2: 15-layer ReLU+He, loss goes NaN → exploding not vanishing; suggested fixes: lower LR, gradient clipping, add BN, check data. Modern baseline: ReLU + He init + batch norm. NOT covered: skip connections (mentioned), gradient clipping (mentioned), implementing BN in code (deferred to PyTorch), layer/group normalization variants, internal covariate shift, full Xavier/He mathematical derivation.

### overfitting-and-regularization
Series 1 capstone (lesson 7 of 7). Full-circle callback to What Is Learning? (lesson 1): reconnects to the OverfittingWidget, the study/exam analogy, and the "perfect fit trap." Model capacity framing: more parameters = ability to memorize any dataset including noise; parameter-to-data ratio matters, not absolute dataset size. Training curves as diagnostic tool: plot train loss (blue) AND validation loss (red) over epochs; three annotated phases (learning, sweet spot, overfitting); the "scissors" pattern = overfitting. Gap between curves IS overfitting, measured directly. Connected to optimizers lesson: "validation loss matters more than training loss" — now the student sees WHY. Regularization introduced as a category: any technique that constrains the model to prevent memorization; deliberately increases training loss to improve validation loss. Three techniques taught independently: (1) Dropout: randomly silence a fraction (p=0.5 default) of neurons during training, all active at inference; "studying by covering parts of your notes" analogy; creates implicit ensemble of sub-networks that agree on signal, disagree on noise; connected to "noise as a feature" and polling analogy from batching-and-sgd. (2) Weight decay / L2 regularization: penalty on large weights L_total = L_data + lambda*sum(w_i^2); modified update rule adds decay factor (1 - alpha*2*lambda); "budget constraint on parameters" analogy; large weights = sharp sensitive functions = overfitting; connected to loss landscape (L2 adds bowl centered at w=0, biasing toward wide smooth minima); AdamW named as practical default. (3) Early stopping: monitor validation loss, stop when patience exhausted, save best weights; "checking practice test score while studying" analogy (echoes exam analogy from lesson 1); explicitly reframes "ball rolling downhill" — training and validation are different landscapes, ball can reach bottom of training while climbing UP validation. Priority order: (1) always early stopping (free), (2) AdamW instead of Adam (built-in weight decay), (3) add dropout if needed. Over-regularizing addressed: regularization is medicine for overfitting, not a supplement for all models (low-capacity + heavy dropout = worse underfitting). The complete training recipe: Xavier/He init + batch norm + AdamW + dropout (if needed) + early stopping. RegularizationExplorer widget: model capacity presets (low/medium/high), dropout toggle + rate slider, weight decay toggle + lambda slider, early stopping toggle + patience slider, epochs slider; two visualizations (training curves with scissors pattern + function fit echoing OverfittingWidget from lesson 1); status badges showing overfitting severity and loss values; best-val-epoch marker always visible as diagnostic reference. Two comprehension checks: (1) predict-and-verify with specific loss numbers at epochs 50 vs 150, (2) transfer question diagnosing a colleague's unregularized model. ModuleCompleteBlock marks completion of Module 1.3 and Series 1. NOT covered: L1 regularization (mentioned as alternative), data augmentation, dropout variants, PyTorch implementation, cross-validation, hyperparameter tuning strategies, theoretical bias-variance decomposition.

## Key Mental Models and Analogies

| Model/Analogy | Established In | Used Again In |
|---------------|---------------|---------------|
| "Effects multiply through the chain" | backpropagation | backprop-worked-example (now with real numbers) |
| "Local × Local × Local" | backpropagation | backprop-worked-example (proven: 4 local derivatives multiply to give dL/dw1) |
| "Error signal propagates backward" | backpropagation | backprop-worked-example (concrete: -2.26 → -2.486/0.678 → 1.356) |
| "One forward + one backward = ALL gradients" | backpropagation | backprop-worked-example (demonstrated: 4 gradients from 1 pass) |
| "Incoming gradient × local derivative" | backprop-worked-example | (New: the recipe for every backward step) |
| "Gradient check: nudge and measure" | backprop-worked-example | (New: debugging tool for verifying gradients) |
| "The graph IS the chain rule, drawn instead of written" | computational-graphs | (New: computational graphs are notation, not a new algorithm) |
| "Each chain rule factor = one hop backward through the graph" | computational-graphs | (New: symbolic-to-visual bridge for chain rule) |
| "Fan-out = sum the gradients" | computational-graphs | (New: when a value feeds multiple paths, total gradient is the sum) |
| "No path = no gradient = doesn't learn" | computational-graphs | (New: connects graph structure to learning; explains dying ReLU visually) |
| "Polling analogy: random sample estimates the whole" | batching-and-sgd | (New: mini-batch gradient ~ full gradient, like polling 50 people estimates city average) |
| "The ball is still rolling downhill, but now the hill is shaking" | batching-and-sgd | (New: extends ball-on-hill from 1.1; noise from mini-batches = shaking = helpful) |
| "Same heartbeat, new rhythm" | batching-and-sgd | (New: training loop unchanged, but wrapped in epoch + batch loops) |
| "Noise as a feature, not a bug" | batching-and-sgd | (New: gradient noise escapes sharp minima into wide ones) |
| "Weather forecasting: recent days matter more" | optimizers | (New: EMA analogy — yesterday's temperature matters more than last week's) |
| "Bowling ball vs tennis ball" | optimizers | (New: momentum gives the ball inertia — heavy ball plows through bumps, light ball bounces on every bump) |
| "Heavy ball less affected by shaking hill" | optimizers | (New: momentum smooths mini-batch noise; connects to batching-and-sgd "hill is shaking" model) |
| "Volume knob per instrument" | optimizers | (New: RMSProp gives each parameter its own learning rate adjustment, like mixing a song) |
| "Oscillations cancel, consistent signals accumulate" | optimizers | (New: why momentum works on ravines — cross-ravine gradients (+5,-5,+5,-5) cancel via EMA, along-ravine gradients (+0.1,+0.1,+0.1) accumulate) |
| "Self-normalizing property" | optimizers | (New: RMSProp divides by gradient magnitude history, so parameters with large gradients automatically get smaller steps) |
| "Adam = momentum + RMSProp" | optimizers | (New: combines both ideas — smooth direction AND normalize magnitude) |
| "No free lunch: fast convergence ≠ better generalization" | optimizers | (New: Adam reaches A minimum faster, SGD+momentum often finds a BETTER minimum) |
| "Telephone game for gradient flow" | training-dynamics | (New: each layer is a person passing a message; each person loses/amplifies fidelity — vanishing = fidelity lost, exploding = amplified to screaming) |
| "Products of local derivatives: the stability question" | training-dynamics | (New: unifying frame — vanishing and exploding are the same problem; stable only when each factor ≈ 1.0) |
| "Each layer should preserve the signal" | training-dynamics | (New: the design principle behind Xavier/He initialization; variance in = variance out = gradient products near 1.0) |
| "Flatline = vanishing, NaN = exploding" | training-dynamics | (New: diagnostic symptom guide — loss barely decreasing means early layers frozen; loss suddenly NaN means gradients hit infinity) |
| "ReLU + He init + batch norm = modern baseline" | training-dynamics | (New: the practical starting point for any new deep network; each piece solves a specific problem in the gradient flow) |
| "Global knob can't fix per-layer problem" | training-dynamics | (New: extends per-parameter LR insight from optimizers — increasing LR to fix vanishing in layer 1 destroys layer 10) |
| "The scissors pattern" | overfitting-and-regularization | (New: when training and validation loss diverge, they look like opening scissors; if scissors open, you are overfitting; every regularization technique aims to keep scissors closed) |
| "Studying by covering parts of your notes" | overfitting-and-regularization | (New: dropout = randomly covering note sections each study session; you learn connections between sections; on exam day (inference), all notes available) |
| "Budget constraint on parameters" | overfitting-and-regularization | (New: weight decay = lambda is a budget; big weights cost more; tight budget forces simpler, smoother solutions focused on dominant pattern) |
| "Checking practice test score while studying" | overfitting-and-regularization | (New: early stopping echoes exam analogy from lesson 1; stop studying when practice test score stops improving; not giving up — stopping at peak understanding) |
| "Training and validation are different landscapes" | overfitting-and-regularization | (New: reframes ball-on-hill; ball reaches bottom of training hill but may climb UP validation hill; early stopping watches the right hill) |
| "Regularization increases training loss — that is the point" | overfitting-and-regularization | (New: the reframe — training loss alone was never the goal; higher training loss with lower validation loss = better model) |
| "Regularization is medicine for overfitting, not a supplement" | overfitting-and-regularization | (New: over-regularizing a small model makes underfitting worse; apply only when diagnosing overfitting) |
| "The complete training recipe" | overfitting-and-regularization | (New: Xavier/He init + batch norm + AdamW + dropout (if needed) + early stopping; the modern baseline for any neural network training run) |
