'use client'

import { LessonLayout } from '@/components/lessons/LessonLayout'
import { Row } from '@/components/layout/Row'
import {
  LessonHeader,
  ObjectiveBlock,
  ConstraintBlock,
  SectionHeader,
  InsightBlock,
  TipBlock,
  SummaryBlock,
  ModuleCompleteBlock,
  NextStepBlock,
} from '@/components/lessons'
import { ExternalLink } from 'lucide-react'

const NOTEBOOK_URL =
  'https://colab.research.google.com/github/brennanku/CourseAI/blob/main/notebooks/6-2-5-build-a-diffusion-model.ipynb'

/**
 * Build a Diffusion Model (DDPM Capstone)
 *
 * Lesson 5 in Module 6.2 (Diffusion). Lesson 9 overall in Series 6.
 * Cognitive load: CONSOLIDATE.
 *
 * This is a notebook-driven capstone lesson. The lesson page provides
 * brief framing, context/constraints, and a Colab launch link.
 * The notebook IS the lesson: implement the complete DDPM pipeline
 * from scratch (forward process, training loop, sampling) on MNIST.
 *
 * Core concepts: all APPLIED (forward process, training algorithm,
 * sampling algorithm implemented from scratch).
 *
 * Previous: Sampling and Generation (module 6.2, lesson 4)
 * Next: Module 6.3 (Latent Diffusion)
 */

export function BuildADiffusionModelLesson() {
  return (
    <LessonLayout>
      {/* Header */}
      <Row>
        <Row.Content>
          <LessonHeader
            title="Build a Diffusion Model"
            description="Implement the complete DDPM pipeline from scratch&mdash;forward process, training loop, and sampling&mdash;on real image data."
            category="Diffusion"
          />
        </Row.Content>
      </Row>

      {/* Objective */}
      <Row>
        <Row.Content>
          <ObjectiveBlock>
            Prove you understand every piece of the diffusion pipeline by
            building one from scratch. Implement the forward process, a simple
            denoising network, the training loop, and the sampling
            algorithm&mdash;then generate images from pure noise.
          </ObjectiveBlock>
        </Row.Content>
        <Row.Aside>
          <TipBlock title="Everything You Need">
            Every line of code in this notebook comes from the last four
            lessons. The forward process formula, the training algorithm, the
            sampling loop&mdash;you already know all of it. This lesson is
            translation, not discovery.
          </TipBlock>
        </Row.Aside>
      </Row>

      {/* Context + Constraints */}
      <Row>
        <Row.Content>
          <ConstraintBlock
            title="Scope for This Lesson"
            items={[
              'Implement the forward process (noise schedule + closed-form formula)',
              'Build a minimal denoising network (simple encoder-decoder with skip connections)',
              'Write the full DDPM training loop on MNIST',
              'Implement the sampling algorithm and generate images from noise',
              'Experience the computational cost of 1,000-step pixel-space sampling',
              'NOT: full U-Net with attention or sinusoidal embeddings—that is Module 6.3',
              'NOT: conditional generation or classifier-free guidance',
              'NOT: DDIM or accelerated samplers',
              'NOT: high-resolution images—MNIST (28×28) only',
            ]}
          />
        </Row.Content>
      </Row>

      {/* Context */}
      <Row>
        <Row.Content>
          <SectionHeader
            title="Four Lessons, One Pipeline"
            subtitle="You have learned every piece. Now assemble them."
          />
          <div className="space-y-4">
            <p className="text-muted-foreground">
              In <strong>The Diffusion Idea</strong>, you built the intuition:
              destruction is easy, creation from scratch is impossibly hard, but
              undoing one small step of destruction is learnable. In{' '}
              <strong>The Forward Process</strong>, you derived the math: the
              closed-form formula that turns any clean image into any noise
              level in one step. In <strong>Learning to Denoise</strong>, you
              traced the training algorithm: predict the noise, compare with
              MSE, update weights. In{' '}
              <strong>Sampling and Generation</strong>, you walked through the
              reverse process: 1,000 small steps from pure noise to a generated
              image.
            </p>
            <p className="text-muted-foreground">
              You have never written a single line of diffusion code. This
              notebook closes that gap. Every formula becomes a function. Every
              algorithm becomes a loop. And at the end, digits emerge from pure
              noise&mdash;generated by a model you built from scratch.
            </p>
          </div>
        </Row.Content>
        <Row.Aside>
          <InsightBlock title="No New Theory">
            This is a <strong>CONSOLIDATE</strong> lesson. Zero new concepts.
            The only challenge is translation&mdash;turning math and algorithms
            you already understand into working PyTorch code.
          </InsightBlock>
        </Row.Aside>
      </Row>

      {/* Architecture expectations */}
      <Row>
        <Row.Content>
          <div className="space-y-4">
            <p className="text-muted-foreground">
              The denoising network is deliberately simple&mdash;a minimal
              encoder-decoder with skip connections and basic timestep
              conditioning. It is <strong>not</strong> the full U-Net used in
              real diffusion systems. Think of it as the autoencoder from{' '}
              <strong>Autoencoders</strong> with two additions: skip
              connections so fine details survive the bottleneck, and timestep
              embedding so the network knows which noise level it is working at.
            </p>
            <p className="text-muted-foreground">
              Your generated digits will be recognizable but imperfect. That is
              the point. The architecture improvements in Module 6.3 are what
              take quality from &ldquo;recognizable&rdquo; to
              &ldquo;impressive.&rdquo; Today, the simplest architecture that
              works is enough to prove diffusion works.
            </p>
          </div>
        </Row.Content>
      </Row>

      {/* Hook: Challenge Preview */}
      <Row>
        <Row.Content>
          <SectionHeader
            title="What You Will Build"
            subtitle="The payoff for four lessons of theory"
          />
          <div className="space-y-4">
            <p className="text-muted-foreground">
              By the end of this notebook, you will generate an 8&times;8 grid
              of MNIST digits&mdash;64 images that have never existed in the
              training set, conjured entirely from Gaussian noise by a model
              you built and trained yourself. The digits will be imperfect but
              unmistakably real: varied shapes, different classes, each one a
              unique sample from the learned distribution.
            </p>
            <p className="text-muted-foreground">
              You will also discover <strong>why pixel-space diffusion was
              never the final answer</strong>. Generating those 64 images will
              take an uncomfortably long time. That wait is not a bug in your
              code&mdash;it is a fundamental property of 1,000-step
              pixel-space DDPM. And it is exactly what motivates latent
              diffusion in Module 6.3.
            </p>
          </div>
        </Row.Content>
        <Row.Aside>
          <InsightBlock title="Both Payoff and Pain">
            This notebook delivers the emotional climax of the
            module&mdash;generation from scratch&mdash;and the deliberate
            sting that motivates everything that comes next.
          </InsightBlock>
        </Row.Aside>
      </Row>

      {/* Colab Link */}
      <Row>
        <Row.Content>
          <SectionHeader
            title="Open the Notebook"
            subtitle="The notebook is the lesson"
          />
          <div className="rounded-lg border-2 border-primary/50 bg-primary/5 p-6">
            <div className="space-y-4">
              <p className="text-muted-foreground">
                This lesson lives entirely in a Colab notebook. You will
                implement the forward process, build the network, write the
                training loop, train on MNIST, implement sampling, and generate
                images&mdash;all with guided scaffolding and callbacks to the
                lessons where each concept was taught.
              </p>
              <a
                href={NOTEBOOK_URL}
                target="_blank"
                rel="noopener noreferrer"
                className="inline-flex items-center gap-2 px-4 py-2 rounded-md bg-primary text-primary-foreground font-medium text-sm hover:bg-primary/90 transition-colors"
              >
                <ExternalLink className="w-4 h-4" />
                Open in Google Colab
              </a>
              <p className="text-xs text-muted-foreground">
                The notebook has 6 parts: setup and noise schedule, forward
                process, simple U-Net, training loop, sampling, and reflection.
                Each part builds on the previous&mdash;work through them in
                order.
              </p>
            </div>
          </div>
        </Row.Content>
        <Row.Aside>
          <TipBlock title="Estimated Time">
            Allow 60&ndash;90 minutes for the full notebook. Training takes
            20&ndash;30 minutes on a Colab GPU. Sampling will take several
            minutes&mdash;that wait is part of the lesson.
          </TipBlock>
        </Row.Aside>
      </Row>

      {/* Summary */}
      <Row>
        <Row.Content>
          <SummaryBlock
            title="What You Will Take Away"
            items={[
              {
                headline:
                  'Every formula from the module becomes a line of PyTorch code.',
                description:
                  'The closed-form formula becomes q_sample(). The 7-step training algorithm becomes the training loop. The reverse step formula becomes the sampling function.',
              },
              {
                headline:
                  'A working diffusion model that generates recognizable digits from pure noise.',
                description:
                  'Imperfect but real—images that never existed in the training set, generated by a model you built from scratch.',
              },
              {
                headline:
                  'Firsthand experience of the 1,000-step sampling cost.',
                description:
                  'You will measure the time yourself. One training step: milliseconds. One sampling run: seconds. A grid of 64 images: minutes. This pain motivates latent diffusion.',
              },
            ]}
          />
        </Row.Content>
      </Row>

      {/* Module Complete */}
      <Row>
        <Row.Content>
          <ModuleCompleteBlock
            module="6.2"
            title="Diffusion Models"
            achievements={[
              'The diffusion insight: destruction is easy, small-step reversal is learnable',
              'Forward process math: noise schedules, alpha-bar, the closed-form formula',
              'DDPM training: predict noise with MSE loss',
              'DDPM sampling: iterative reverse process from pure noise to generated image',
              'Built a complete diffusion model from scratch and generated images',
            ]}
            nextModule="6.3"
            nextTitle="Latent Diffusion"
          />
        </Row.Content>
      </Row>

      {/* Bridge to next module */}
      <Row>
        <Row.Content>
          <div className="space-y-4">
            <p className="text-muted-foreground">
              You built a working diffusion model. You experienced generation
              from pure noise. You also experienced the 1,000-step wait. Module
              6.3 answers the question that wait raises: what if you ran
              diffusion in a compressed latent space instead of pixel space?
              That is latent diffusion&mdash;the core idea behind Stable
              Diffusion.
            </p>
          </div>
        </Row.Content>
      </Row>

      {/* Next Step */}
      <Row>
        <Row.Content>
          <NextStepBlock
            href="/app"
            title="Up Next: Latent Diffusion (Module 6.3)"
            description="Run diffusion in a compressed latent space&mdash;faster sampling, higher resolution, and the architecture behind Stable Diffusion."
          />
        </Row.Content>
      </Row>
    </LessonLayout>
  )
}
