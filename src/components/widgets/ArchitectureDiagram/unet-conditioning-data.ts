import type { ArchitectureDiagramData } from './types'

export const UNET_CONDITIONING_DATA: ArchitectureDiagramData = {
  canvasWidth: 1050,
  canvasHeight: 680,
  groups: [
    {
      id: 'timestep-embedding',
      title: 'Timestep Embedding',
      x: 170,
      y: 15,
      width: 660,
      height: 85,
      fillColor: 'rgba(245, 158, 11, 0.05)',
      strokeColor: '#92400e',
      strokeStyle: 'dashed',
    },
    {
      id: 'encoder',
      title: 'Encoder (Downsampling)',
      x: 70,
      y: 195,
      width: 215,
      height: 345,
      fillColor: 'rgba(59, 130, 246, 0.04)',
      strokeColor: '#1e3a5f',
      strokeStyle: 'dashed',
    },
    {
      id: 'bottleneck',
      title: 'Bottleneck',
      x: 390,
      y: 540,
      width: 270,
      height: 95,
      fillColor: 'rgba(168, 85, 247, 0.05)',
      strokeColor: '#581c87',
      strokeStyle: 'dashed',
    },
    {
      id: 'decoder',
      title: 'Decoder (Upsampling)',
      x: 765,
      y: 195,
      width: 215,
      height: 345,
      fillColor: 'rgba(16, 185, 129, 0.04)',
      strokeColor: '#064e3b',
      strokeStyle: 'dashed',
    },
  ],
  nodes: [
    // Timestep Embedding pipeline (y=55, horizontal)
    {
      id: 't',
      label: 't (integer)',
      description: 'The diffusion timestep — an integer from 0 to T indicating how much noise has been added. This is the only signal telling the network "how noisy is this input?"',
      groupId: 'timestep-embedding',
      x: 260,
      y: 55,
      width: 110,
      height: 40,
      fillColor: '#1e293b',
      strokeColor: '#f59e0b',
    },
    {
      id: 'sin',
      label: 'Sinusoidal\nEncoding',
      description: 'Converts the scalar timestep into a high-dimensional vector using sin/cos at different frequencies — the same trick from positional encoding in Transformers. This gives the network a rich representation of time.',
      groupId: 'timestep-embedding',
      x: 410,
      y: 55,
      width: 120,
      height: 40,
      fillColor: '#1e293b',
      strokeColor: '#f59e0b',
    },
    {
      id: 'mlp',
      label: 'MLP\n(2-layer)',
      description: 'A small 2-layer MLP (linear → SiLU → linear) that transforms the sinusoidal encoding into a learned embedding. This lets the network learn its own notion of "time" beyond the fixed sin/cos basis.',
      groupId: 'timestep-embedding',
      x: 555,
      y: 55,
      width: 110,
      height: 40,
      fillColor: '#1e293b',
      strokeColor: '#f59e0b',
    },
    {
      id: 'temb',
      label: 't_emb\n(512-dim)',
      description: 'The final timestep embedding vector. This gets broadcast to every block in the U-Net via per-block linear projections that produce γ(t) and β(t) for adaptive group normalization.',
      groupId: 'timestep-embedding',
      x: 700,
      y: 55,
      width: 120,
      height: 40,
      fillColor: '#1e293b',
      strokeColor: '#f59e0b',
    },

    // Input (above encoder, not in group)
    {
      id: 'in',
      label: 'Input:\nnoisy image',
      description: 'The noisy input image x_t at timestep t. The U-Net\'s job is to predict the noise that was added, so this noise can be subtracted to recover a cleaner image.',
      x: 175,
      y: 150,
      width: 130,
      height: 40,
      fillColor: '#1e293b',
      strokeColor: '#6366f1',
    },

    // Encoder blocks (left column, x=175)
    {
      id: 'e0',
      label: '64×64×64\nConv + AdaGN(t)',
      description: 'First encoder block. Convolves at full resolution, then applies Adaptive Group Normalization using the timestep embedding. The AdaGN replaces standard GroupNorm parameters with learned functions of t: γ(t)·GroupNorm(h) + β(t).',
      groupId: 'encoder',
      x: 175,
      y: 265,
      width: 160,
      height: 50,
      fillColor: '#1e293b',
      strokeColor: '#3b82f6',
    },
    {
      id: 'e1',
      label: '32×32×128\nConv + AdaGN(t)',
      description: 'Second encoder block. After downsampling to 32×32, doubles the channel count to 128. Each block has its own learned projection from t_emb to block-specific γ(t) and β(t).',
      groupId: 'encoder',
      x: 175,
      y: 370,
      width: 160,
      height: 50,
      fillColor: '#1e293b',
      strokeColor: '#3b82f6',
    },
    {
      id: 'e2',
      label: '16×16×256\nConv + AdaGN(t)',
      description: 'Third encoder block. Downsampled to 16×16 with 256 channels. At this resolution, the receptive field covers large spatial regions, capturing high-level structure.',
      groupId: 'encoder',
      x: 175,
      y: 475,
      width: 160,
      height: 50,
      fillColor: '#1e293b',
      strokeColor: '#3b82f6',
    },

    // Bottleneck (center bottom)
    {
      id: 'b',
      label: '8×8×512\nConv + AdaGN(t)',
      description: 'The bottleneck — lowest resolution (8×8) with highest channel count (512). This is where the network processes the most compressed representation. Some architectures also add self-attention here.',
      groupId: 'bottleneck',
      x: 525,
      y: 585,
      width: 180,
      height: 50,
      fillColor: '#1e293b',
      strokeColor: '#a855f7',
    },

    // Decoder blocks (right column, x=875)
    {
      id: 'd2',
      label: '16×16×256\nConv + AdaGN(t)',
      description: 'First decoder block. Upsamples from 8×8 back to 16×16. Receives a skip connection from E2, concatenating encoder features to recover spatial detail lost during downsampling.',
      groupId: 'decoder',
      x: 875,
      y: 475,
      width: 160,
      height: 50,
      fillColor: '#1e293b',
      strokeColor: '#10b981',
    },
    {
      id: 'd1',
      label: '32×32×128\nConv + AdaGN(t)',
      description: 'Second decoder block. Upsamples to 32×32, receives skip connection from E1. The combination of coarse-to-fine upsampling + skip connections is what gives U-Nets their power for dense prediction tasks.',
      groupId: 'decoder',
      x: 875,
      y: 370,
      width: 160,
      height: 50,
      fillColor: '#1e293b',
      strokeColor: '#10b981',
    },
    {
      id: 'd0',
      label: '64×64×64\nConv + AdaGN(t)',
      description: 'Final decoder block. Back at full resolution. Receives skip connection from E0 to preserve fine spatial details. The final conv layer maps back to the image channel count.',
      groupId: 'decoder',
      x: 875,
      y: 265,
      width: 160,
      height: 50,
      fillColor: '#1e293b',
      strokeColor: '#10b981',
    },

    // Output (above decoder, not in group)
    {
      id: 'out',
      label: 'Output:\npredicted noise',
      description: 'The predicted noise ε_θ(x_t, t). During sampling, this is subtracted from x_t (scaled appropriately) to get a less noisy x_{t-1}. The U-Net is trained to minimize ||ε - ε_θ(x_t, t)||².',
      x: 875,
      y: 150,
      width: 130,
      height: 40,
      fillColor: '#1e293b',
      strokeColor: '#6366f1',
    },
  ],
  edges: [
    // Timestep embedding pipeline
    { id: 'e-t-sin', from: 't', to: 'sin', style: 'solid', color: '#f59e0b' },
    { id: 'e-sin-mlp', from: 'sin', to: 'mlp', style: 'solid', color: '#f59e0b' },
    { id: 'e-mlp-temb', from: 'mlp', to: 'temb', style: 'solid', color: '#f59e0b' },

    // Data path: Input → Encoder → Bottleneck → Decoder → Output
    { id: 'e-in-e0', from: 'in', to: 'e0', style: 'solid' },
    { id: 'e-e0-e1', from: 'e0', to: 'e1', style: 'solid', label: 'downsample' },
    { id: 'e-e1-e2', from: 'e1', to: 'e2', style: 'solid', label: 'downsample' },
    { id: 'e-e2-b', from: 'e2', to: 'b', style: 'solid', label: 'downsample' },
    { id: 'e-b-d2', from: 'b', to: 'd2', style: 'solid', label: 'upsample' },
    { id: 'e-d2-d1', from: 'd2', to: 'd1', style: 'solid', label: 'upsample' },
    { id: 'e-d1-d0', from: 'd1', to: 'd0', style: 'solid', label: 'upsample' },
    { id: 'e-d0-out', from: 'd0', to: 'out', style: 'solid' },

    // Timestep conditioning — label only the first, rest are unlabeled
    { id: 'e-temb-e0', from: 'temb', to: 'e0', style: 'dashed', color: '#f59e0b', label: 'γ(t), β(t)' },
    { id: 'e-temb-e1', from: 'temb', to: 'e1', style: 'dashed', color: '#f59e0b' },
    { id: 'e-temb-e2', from: 'temb', to: 'e2', style: 'dashed', color: '#f59e0b' },
    { id: 'e-temb-b', from: 'temb', to: 'b', style: 'dashed', color: '#f59e0b' },
    { id: 'e-temb-d2', from: 'temb', to: 'd2', style: 'dashed', color: '#f59e0b' },
    { id: 'e-temb-d1', from: 'temb', to: 'd1', style: 'dashed', color: '#f59e0b' },
    { id: 'e-temb-d0', from: 'temb', to: 'd0', style: 'dashed', color: '#f59e0b' },

    // Skip connections — horizontal through center
    {
      id: 'e-skip-e0-d0',
      from: 'e0',
      to: 'd0',
      style: 'dashed',
      color: '#64748b',
      label: 'skip',
      waypoints: [{ x: 525, y: 265 }],
    },
    {
      id: 'e-skip-e1-d1',
      from: 'e1',
      to: 'd1',
      style: 'dashed',
      color: '#64748b',
      label: 'skip',
      waypoints: [{ x: 525, y: 370 }],
    },
    {
      id: 'e-skip-e2-d2',
      from: 'e2',
      to: 'd2',
      style: 'dashed',
      color: '#64748b',
      label: 'skip',
      waypoints: [{ x: 525, y: 475 }],
    },
  ],
}
